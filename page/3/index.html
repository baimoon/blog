<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/blog/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/blog/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/blog/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico?v=5.0.1" />






<meta name="description" content="Baimoon&apos;s blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Baimoon's Note">
<meta property="og:url" content="http://baimoon.github.io/page/3/index.html">
<meta property="og:site_name" content="Baimoon's Note">
<meta property="og:description" content="Baimoon&apos;s blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Baimoon's Note">
<meta name="twitter:description" content="Baimoon&apos;s blog">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://baimoon.github.io/page/3/"/>

  <title> Baimoon's Note </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/blog/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Baimoon's Note</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/blog/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/blog/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/09/18/ganglia-installAndConfig/" itemprop="url">
                  Ganglia Install And Config
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-18T17:30:07+08:00" content="2016-09-18">
              2016-09-18
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/ganglia/" itemprop="url" rel="index">
                    <span itemprop="name">ganglia</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是ganglia的安装和配置的笔记</p>
<h1 id="Ganglia的安装"><a href="#Ganglia的安装" class="headerlink" title="Ganglia的安装"></a>Ganglia的安装</h1><p>首先，ganglia由gmond、gmetad和gweb三部分组成。</p>
<h2 id="gmond"><a href="#gmond" class="headerlink" title="gmond"></a>gmond</h2><p>gmond（Ganglia Monitoring Daemon）是一种轻量级服务，安装在每台需要手机指标数据的节点主机上。它通过侦听/通告协议与集群内其他节点共享数据。<br>gmond的安装很简单，其所依赖的库，libconfuse、pkgconfig、PCRE和APR等在大多数现行的linux上都有安装。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum install ganglia-gmond</div></pre></td></tr></table></figure></p>
<h2 id="gmetad"><a href="#gmetad" class="headerlink" title="gmetad"></a>gmetad</h2><p>gmetad （Ganglia Meta Daemon）是一种从其他gmetad或gmond源收集指标数据，并将数据以RRD格式存储到磁盘的服务。gmetad为从主机组收集的特定指标信息提供了简单的查询机制，并支持分级授权，使得创建联合检测域成为可能。<br>gmetad除了需要安装gmond所需的依赖之外，还需要RDDtool库。它用来存储和显示从其他gmetad和gmond源收集的时间序列数据。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum install ganglia-gmetad</div></pre></td></tr></table></figure></p>
<h2 id="gweb"><a href="#gweb" class="headerlink" title="gweb"></a>gweb</h2><p>完整的Ganglia不能缺少网络接口：gweb（Ganglia Web）。gweb是一种利用浏览器显示gmetad所存储数据的PHP前端。<br>Ganglia 3.4的Web接口是一个独立的发布包，其源代码也是独立的。gweb 3.4支持gmond/gmetad 3.4.x及以上版本；gweb未来版本可能需要与gmond/gmetad未来版本相匹配。建议安装或更新gweb的时候查看安装文档，以获取更多信息。<br>安装gweb需要如下需求：</p>
<ul>
<li>Apache Web Server</li>
<li>PHP 5.2级更新版本</li>
<li>PHP JSON扩展的安装和启用</li>
</ul>
<p>首先安装Apache和PHP<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum install httpd php</div></pre></td></tr></table></figure></p>
<p>用户还需要启用PHP的JSON扩展，通过检查/etc/php.d/json.ini文件来检查JSON的扩展状态，如果已经启用扩展，文件中应该包含下面的语句：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">extension=json.ini</div></pre></td></tr></table></figure></p>
<p>下载最新的gweb(<a href="https://sourceforge.net/projects/ganglia/files/gweb/)，然后编译Makefile来安装gweb2">https://sourceforge.net/projects/ganglia/files/gweb/)，然后编译Makefile来安装gweb2</a>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tar -xvzf ganglia-web-major.minor.release.tar.gz</div><div class="line"><span class="built_in">cd</span> ganglia-web-major.minor.release</div></pre></td></tr></table></figure></p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/09/18/ganglia-installAndConfig/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/09/10/spark-tuningSpark/" itemprop="url">
                  Tuning Spark
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-10T23:28:56+08:00" content="2016-09-10">
              2016-09-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是Tuning Spark文档的翻译，原文档<a href="http://spark.apache.org/docs/latest/tuning.html" title="Tuning Spark">请参考</a>，本文主要用于个人学习。</p>
<h1 id="Tuning-Spark"><a href="#Tuning-Spark" class="headerlink" title="Tuning Spark"></a>Tuning Spark</h1><p>因为大多数Spark计算是内存中的计算，因此集群中的任何资源都能够成为Spark程序的瓶颈：CPU、网络带宽或内存。通常，如果数据装载到内存中，瓶颈可能是网络带宽，但是有些时候，你还需要做一些调整，例如以序列化格式存储RDDs，以降低内存的使用。本指南覆盖了两个主题：数据序列化和内存调整，其中数据序列化对好的网络性能是至关重要的，并且还可以降低内存的使用。我们还会概述其他一些小的主题。</p>
<h2 id="Data-Serialization"><a href="#Data-Serialization" class="headerlink" title="Data Serialization"></a>Data Serialization</h2><p>序列化在任何分布式application的执行中扮演了很重要的角色。那些序列化缓慢的对象或消费很大数量byte的格式将极大的减慢计算。通常，这是优化一个Spark application首先要调整的东西。Spark的目标是在方便（允许你在你的操作中使用任何Java类型）和性能之间达到一个平衡。它提供了两种序列化库：</p>
<ul>
<li>Java serialization: 默认，Spark使用Java的ObjectOutputStream框架进行序列化对象操作，能够和任何你实现了java.io.Serializable接口的类型一起工作。通过继承java.io.Externalizable，你能够更加近的控制你的序列化的执行。Java序列化是灵活的，但是通常是慢的，这导致对于很多类会有很大的序列化格式。</li>
<li>Kryo serialization: Spark能够使用Kryo库（版本2）来更快的序列化对象。相对于Java序列化Kryo显然是更快且更加简单的（通常是10倍还多），但是不支持所有的可序列化类型，并且需要你在程序中将你要使用的类进行注册以便获取更好的执行。</li>
</ul>
<p>通过使用SparkConf初始化你的job，并调用conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”)来转换为使用Kryo。这个设置不止为了wroker节点混洗数据配置序列化生成器，当序列化RDDs到磁盘时也有用。Kryo不作为默认序列化生成器的唯一原因是需要自定义注册，但是我们推荐在任何网络集中型application中使用它。<br>Spark自动的很多常用的核心Scala类包含Kryo序列化生成器，涵盖在Twitter chill库的AllScalaRegistrar中。<br>要使用Kryo注册你自己的自定义类，使用registerKryoClasses方法。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</div><div class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</div><div class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div></pre></td></tr></table></figure></p>
<p><a href="https://github.com/EsotericSoftware/kryo" title="Kryo">Kryo documntation</a>描述了更加高级的注册选项，如添加自定义序列化编码器。<br>如果你的对象很大，你可能需要增加spark.kryoserializer.buffer配置的值。这个值需要足够大以便保存你要序列化的最大对象。<br>最后，如果你没有注册你的自定义，Kryo将仍然能够工作，但是它将存储每个对象的全类名，这样损耗很大。</p>
<h2 id="Memory-Tuning"><a href="#Memory-Tuning" class="headerlink" title="Memory Tuning"></a>Memory Tuning</h2><p>在调整内存的用法中，有三种值得考虑：被你的对象使用的内存的总量（你可能想要将整个数据集装配到内存中）、访问这些对象的开销和垃圾回收的开销（如果你在这些对象）。<br>默认，Java对象是快速访问的，但是在它们字段的内部很容消费掉比原始数据多2-5倍的空间 。这是因为有如下原因：</p>
<ul>
<li>每个不同的Java对象有一个”object header”，它大概是是16个字节，并包含信息，诸如一个指向它的class的指针。对于一个带有非常少数据的对象（假设一个Int字段），这可能要比数据大很多。</li>
<li>Java Strings在原生的string数据上有一个大概40字节的开销（因为它们被存储到一个Chars数组中，并且保存了额外的数据，如长度），并且每个字符以两个字节存储，因为String内部使用的UTF-16进行编码。因此一个包含10个字符的字符串很容易消耗掉60个字节。</li>
<li>常见的集合类，如HashMap和LinkedList，使用linked数据结构，每个数据项是一个包装对象（如Map.Entry）。这种对象不只有header，而且还有指针（通常是8个字节）指向列表中的下一个对象。</li>
<li>原始类型常常以包装对象来存储，诸如java.lang.Integer。</li>
</ul>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/09/10/spark-tuningSpark/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/09/08/spark-streaming-kafka/" itemprop="url">
                  Spark Streaming + Kafka Integration Guide
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-08T16:45:09+08:00" content="2016-09-08">
              2016-09-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文是Spark Streaming + Kafka Integration Guide文档的翻译，原文请<a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html" title="Spark Streaming + Kafka Integration Guide" target="_blank" rel="external">参考</a>。另外，本文主要用于个人学习使用。</p>
<h1 id="Spark-Streaming-Kafka-Integration-Guide"><a href="#Spark-Streaming-Kafka-Integration-Guide" class="headerlink" title="Spark Streaming + Kafka Integration Guide"></a>Spark Streaming + Kafka Integration Guide</h1><p><a href="http://kafka.apache.org/" title="Apache Kafka" target="_blank" rel="external">Apache Kafka</a>是一个发布-订阅的消息队列，作为一个分布式的、分片的、副本提交的日志服务。这里解释如何配置Spark Streaming来从Kafka接收数据。有两种方法来达到这个目的 - 老的方法是使用Receiver和Kafka的高级API，还有一个新的实验性解决方法（在Spark 1.3版本中引入），不需要使用Receiver。它们有不同的编程模式、执行特征和语义保证，因此请仔细阅读。</p>
<h2 id="Approach-1-Receiver-based-Approach"><a href="#Approach-1-Receiver-based-Approach" class="headerlink" title="Approach 1: Receiver-based Approach"></a>Approach 1: Receiver-based Approach</h2><p>这个方法使用一个receiver来接收数据。这个Receiver使用Kafka高级别consumer API来实现。和所有receivers一样，通过一个Receiver从Kafka接收到的数据被存储到Spark executors中，然后Spark Streaming启动jobs来处理数据。<br>然而，根据默认配置，这个解决方法会因为故障而丢失数据（查看<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#receiver-reliability" title="Receiver Reliability" target="_blank" rel="external">receiver reliabillity</a>。要确保零数据丢失，你需要额外的在Spark Streaming中启用Write Ahead Logs(从Spark 1.2版本中引入)。这将同步的将从Kafka接收到的数据到以写ahead日志的方式波存到分布式文件系统中（如HDFS），因此所有的数据能够从故障中恢复。查看Streaming programming guide中<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#deploying-applications" title="Deploying Applications" target="_blank" rel="external">Deploying section</a>获取关于Write Ahead Logs的详细信息）。<br>接下来，我们讨论如何在你的streaming application中使用这个方法。</p>
<p>1、<strong>Linking:</strong> 对于使用SBT或Maven项目描述的Scala或Java application，使用如下的坐标链接你的streaming application（查看programming guide中的[Linking section]获取更多信息）。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">groupId = org.apache.spark</div><div class="line">artifactId = spark-streaming-kafka<span class="number">-0</span><span class="number">-8</span>_2<span class="number">.11</span></div><div class="line">version = <span class="number">2.0</span><span class="number">.0</span></div></pre></td></tr></table></figure></p>
<p>对于Python application，你需要在部署你的application时添加上面的库以及它的依赖。查看Deploying分项的内容。<br>2、<strong>Programming:</strong> 在streaming application代码中，导入KafkaUtils并创建一个输入DStream，如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka._</div><div class="line"></div><div class="line"><span class="keyword">val</span> kafkaStream = <span class="type">KafkaUtils</span>.createStream(streamingContext, </div><div class="line">    [<span class="type">ZK</span> quorum], [consumer group id], [per-topic number of <span class="type">Kafka</span> partitions to consume])</div></pre></td></tr></table></figure></p>
<p>你还可以指定key和value的类以及它们使用createStream的变体的相关解码类。查看<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.kafka.KafkaUtils$" title="KafkaUtils" target="_blank" rel="external">API docs</a>和<a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/KafkaWordCount.scala" title="KafkaWordCount" target="_blank" rel="external">example</a>。</p>
<h4 id="需要注意的几点："><a href="#需要注意的几点：" class="headerlink" title="需要注意的几点："></a>需要注意的几点：</h4><ul>
<li>Kafka中topic的partitions与Spark Streaming中RDDs生成的partitions没有关联。因此在KafkaUtils.createStream()中增加特定于topic的partition的数量只是增加使用的线程的数量，使用这些线程在单个receiver中对topic进行消费。它不会增加Spark数据的并发处理。参考主要文档获取它的更多信息。</li>
<li>多个Kafka输入DStream能够使用不同的group和topics来创建，以便使用多个receiver来并行接收数据。</li>
<li>如果你使用一个可靠的文件系统（像HDFS）启用了Write Ahead logs，接收到的数据已经被复制到日志中。因此，对于输入流存储的存储级别为StorageLevel.MEMORY_AND_DISK_SER（那就是KafkaUtils.createStream(…, StorageLevel.MEMORY_AND_DISK_SER)）。</li>
</ul>
<p>3、<strong>Deploying:</strong> 对于任何Spark application，spark-submit被使用来启动你的application。然而，对于Scala/Java application和Python application之间有轻微的不同。<br>对于Scala和Java application，如果你使用SBT或Maven作为项目管理，那么打包spark-streaming-kafka-0-8_2.11和它的依赖到application的JAR中。确保spark-core_2.11和spark-streaming_2.11作为依赖进行标记，因为他们已经安装到Spark中了。然后，使用spark-submit来启动你的application（查看主要编程指南中的<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#deploying-applications" title="Deploying Applications" target="_blank" rel="external">Deploying section</a>）。<br>对于Python application，它缺少SBT/Maven项目管理，spark-streaming-kafka-0-8_2.11和它的依赖可以使用–packages直接添加到spark-submit（查看<a href="http://spark.apache.org/docs/latest/submitting-applications.html" title="Submitting Applications" target="_blank" rel="external">Application Submission Guide</a>）。这样：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0 ...</div></pre></td></tr></table></figure></p>
<p>或者，你能够从<a href="http://search.maven.org/#search|ga|1|a%3A%22spark-streaming-kafka-0-8-assembly_2.11%22%20AND%20v%3A%222.0.0%22" target="_blank" rel="external">Maven repository</a>下载Maven坐标的JAR并使用–jars将其添加到spark-submit。</p>
<h2 id="Approach-2-Direct-Approach-No-Receivers"><a href="#Approach-2-Direct-Approach-No-Receivers" class="headerlink" title="Approach 2: Direct Approach (No Receivers)"></a>Approach 2: Direct Approach (No Receivers)</h2><p>这个新的五receiver的直接解决方法已经在Spark 1.3版本中引入，来确保健壮的端对端的保证。代替receivers来接收数据，这个方法周期性的查询Kafka来获取每个topic + partition中最后的offset，然后相应的定义每个batch中处理offset的范围。当处理数据的job启动时，使用Kafka的简单API从Kafka中读取定义的offset范围（类似从文件系统中读取文件）。注意这是一个在Spark 1.3中引入的实验性特征，用于Scala和Java API，在Spark 1.4中才有了Python的API。<br>这个解决方案在基于receiver的解决方案上有如下优势：</p>
<ul>
<li><em>Simplified Parallelism:</em> 不需要创建多个输入Kafka streams以及联合他们。使用directStream，Spark Streaming将会创建和要消费的Kafka partitions数量相同的RDD partitions，这样将病习惯你的从Kafka读取数据。因此在Kafka和RDD partitions之间有一个一对一的映射，这样更加容易理解和调整。</li>
<li><em>Efficiency:</em> 在第一个解决方案中要实现零数据丢失需要将数据存储到Write Ahead Log中，这种方式是进一步的复制数据。这实际上是效率低的，因为数据实际上复制了两次-一次被Kafka，另一次被Write Ahead Log。第二种解决方案消除了这个问题，因为没有了receiver，因此不需要Write Ahead Log。只要你有足够的Kafka保留时间，message能够从Kafka中恢复。</li>
<li><em>Exactly-once semantics:</em> 第一种解决方案使用Kafka的高级API将消费掉的offset存储到Zookeeper中。这是从Kafka消费数据的传统方式。而这种解决方案（和write ahead log混合）能够保证零数据丢失（例如至少一次的语义），在一些故障下，有很小的可能性是一些数据会消费两次。这种存在是因为由Spark Streaming可靠的接收的数据和由Zookeeper跟踪的offsets之间的矛盾造成的。因此，在第二解决方案中，我们使用了简单的Kafka API，简单的API没有使用Zookeeper。通过Spark Streaming中它的checkpoints来跟踪offset。这消除了Spark Streaming和Zookeeper/Kafka之间的差异，因此尽管有故障，但是被Spark Streaming接收到的记录实际上只有一次。为了达到输出你的结果只有一次的语义，你的输出操作将数据保存到外部存储中必须是幂等或原子事务的，这样来保存结果和offset（查看主编程指南中<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#semantics-of-output-operations" title="Semantics of output operations" target="_blank" rel="external">Semantics of output operations</a>获取更多信息）。</li>
</ul>
<p>注意，这种解决方案中一个不利条件是不会在Zookeeper中更新offset，因此那些基于Zookeeper的监控工具将不会更新进度。然而，你能够在这种解决方案中访问每个batch中处理过的offsets并自己更新到Zookeeper（参考下面）。<br>接下来，我们讨论如何在你的application中使用这种解决方案。</p>
<ul>
<li><p><strong>Linking:</strong> 这种解决方案在Scala和Java application中支持。使用如下的坐标来连接你的SBT/Maven项目（查看主要编程指南中<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#linking" title="Linking" target="_blank" rel="external">Linking section</a>获取更多信息）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">groupId = org.apache.spark</div><div class="line">artifactId = spark-streaming-kafka<span class="number">-0</span><span class="number">-8</span>_2<span class="number">.11</span></div><div class="line">version = <span class="number">2.0</span><span class="number">.0</span></div></pre></td></tr></table></figure>
</li>
<li><p><strong>Programming:</strong> 在Streaming application代码中，引入KafkaUtils并如下创建一个输入DStream。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka._</div><div class="line"></div><div class="line"><span class="keyword">val</span> directKafkaStream = <span class="type">KafkaUtils</span>.createDirectStream[</div><div class="line">    [key <span class="class"><span class="keyword">class</span>], [value class], [key decoder class], [value decoder class] ](<span class="params"></span></span></div><div class="line">    streamingContext, [map of <span class="type">Kafka</span> parameters], [set of topics to consume])</div></pre></td></tr></table></figure>
</li>
</ul>
<p>你还可以传递一个<em>messageHandler</em>到<em>createDirectStream</em>来访问<em>MessageAndMetadata</em>，MessageAndMetadata包含了关于当前message的元数据和要将它转换成的目标类型。查看<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.kafka.KafkaUtils$" title="KafkaUtils" target="_blank" rel="external">API docs</a>和<a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala" title="DirectKafkaWordCount" target="_blank" rel="external">examples</a>。<br>在Kafka的参数中，你必须指定metadata.broker.list或bootstrap.servers。默认，它将从每个Kafka partition的最后的offset处开始消费。如果你在Kafka参数中设置auto.offset.reset为smallest，那么它将从最小的offset处开始消费。<br>使用KafkaUtils.createDirectStream的其他变量，你还能够从任意offset处开始消费。此外，如果你想要访问每个batch中已经消费的Kafka offset，你可以如下这么做。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Hold a reference to the current offset ranges, so it can be used downstream</span></div><div class="line"><span class="keyword">var</span> offsetRanges = <span class="type">Array</span>[<span class="type">OffsetRange</span>]()</div><div class="line"></div><div class="line">directKafkaStream.transform &#123; rdd =&gt;</div><div class="line">  offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</div><div class="line">  rdd</div><div class="line">&#125;.map &#123;</div><div class="line">          ...</div><div class="line">&#125;.foreachRDD &#123; rdd =&gt;</div><div class="line">  <span class="keyword">for</span> (o &lt;- offsetRanges) &#123;</div><div class="line">    println(<span class="string">s"<span class="subst">$&#123;o.topic&#125;</span> <span class="subst">$&#123;o.partition&#125;</span> <span class="subst">$&#123;o.fromOffset&#125;</span> <span class="subst">$&#123;o.untilOffset&#125;</span>"</span>)</div><div class="line">  &#125;</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>如果你想要基于Zookeeper的Kafka监控工具来展示streaming application的进度，你还可以使用这个来更新Zookeeper。<br>注意到HasOffsetRanges的类型转换，将只有它在第一个方法在directKafkaStream上调用完成后才会成功，不会晚于一个方法链（不知道这句是否正确）。你能够使用transform()方法来代替foreachRDD()方法来作为你调用的第一个方法以便访问offsets，然后调用进一步的Spark方法。然而，需要注意的是在任何shuffle或repartition方法之后，RDD partition和Kafka partition之间的一对一映射将不再维持，例如reduceByKey()方法或window()方法。<br>另一个需要注意的是，因为这个解决方案没有使用receiver，标准的receiver-related（spark.streaming.receiver.<em>格式的配置）将不会应用到由这种解决方案生成的输入DStreams上（但是会应用到其他输入DStream上）。相反，会是用spark.streaming.kafka.</em>的配置。一个重要的东西是spark.streaming.kafka.maxRatePerPartition，它将限制该API的从每个Kafka partition读取数据的速度（每秒message的条数）。</p>
<ul>
<li><strong>Deploying:</strong> 这跟第一种解决方案中的相同。</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/08/17/spark-streaming/" itemprop="url">
                  Spark Streaming Programming Guide
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-17T16:40:48+08:00" content="2016-08-17">
              2016-08-17
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是Spark Streaming手册的翻译文档，会随着自己的实现进行更新，官方文档请<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" title="Spark Streming Programming Guide">参考</a>。</p>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>Spark Streaming是核心Spark API的一个延伸，它对实时数据流进行可扩展的、高吞吐量的、容灾的进行处理。数据可以从很多源（如Kafka、Flume、Kinesis或TCP socket）进行提取，然后被复杂的算法组合处理，这些复杂的算法可以使用高级别的函数，如map、reduce、join和window。最后，被处理过的数据可以推出到外部文件系统、数据库和实时图表中。实际上你可以在数据流上应用Spark的<a href="http://spark.apache.org/docs/latest/ml-guide.html" title="Machine Learning Library (MLlib) Guide">机器学习</a>和<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html" title="GraphX Programming Guide">图处理</a>。<br><img src="http://oaavtz33a.bkt.clouddn.com/streaming-arch.png" alt="spark streaming architecture" title="spark streaming architecture"><br>在内部，它如下工作。Spark Streaming接收实时的输入数据流，并将数据划分到批次中，然后在批次中数据被Spark引擎处理并生成最终的结果流。<br><img src="http://oaavtz33a.bkt.clouddn.com/streaming-flow.png" alt="Spark Streaming data flow" title="Spark Streaming data flow"><br>Spark Streaming提供了一个高级别的抽象，叫做discretized stream或DStream，它代表了一个连续的数据流。DStream可以从来自数据源（如Kafka、Flume和Kinesis）的输入数据流创建，也可以通过在其他DStream上应用高级别的操作来创建，一个DStream以一个<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD" title="RDD">RDDs</a>序列来表示。<br>本指南展示了如何开始使用DStreams来编写Spark Streaming程序。你可以使用Scala、Java或Python（从Spark1.2中引入）来编写Spark Streaming程序，这些语言的代码都会在本指南中提供。你会发现tabs在本指南中随处可见，是你可以在不同语言的代码片段之间任意选择。<br><strong>注意：</strong>在Python中有少量的APIs是不同或不可用的。贯穿整个指南，你会发现<em>Python API</em>标签高亮了这些不同。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/08/17/spark-streaming/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/08/09/spark-configuration/" itemprop="url">
                  Configuration
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-09T10:28:26+08:00" content="2016-08-09">
              2016-08-09
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是对Spark配置的翻译，主要用于本人学习使用，原文<a href="http://spark.apache.org/docs/latest/configuration.html" title="Spark Configuration">请参考</a></p>
<p>Spark提供了三个用于对系统配置的位置：</p>
<ul>
<li>Spark properties控制大多数application参数，可以通过使用SparkConf对象设置或通过Java系统属性设置。</li>
<li>Environment variables可以设置每台机器的设置，如IP地址，通过每个节点上的conf/spark-env.sh脚本。</li>
<li>Logging可以通过log4j.properties来配置。</li>
</ul>
<h1 id="Spark-Properties"><a href="#Spark-Properties" class="headerlink" title="Spark Properties"></a>Spark Properties</h1><p>Spark属性控制大多数application设置，并且对每个application进行独立配置。这些属性可以直接在SparkConf上设置，SparkConf会传递给你的SparkContext.SparkConf，来允许你控制一些常用属性（如master的URI和application的名称等），通过set()方法达到和key-value对一样。例如，我们可以使用两个线程来初始化一个application，如下：<br>注意我们使用local[2]运行，意味着两个线程-表示最低的并行，这样能够帮助我们发现那些只有在分布式context上运行才会出现的bug。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</div><div class="line">             .setMaster(<span class="string">"local[2]"</span>)</div><div class="line">             .setAppName(<span class="string">"CountingSheep"</span>)</div><div class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div></pre></td></tr></table></figure></p>
<p>注意在本地模式中我们可以使用多个线程，但是在像Spark Streaming中，我们实际上要求使用多个线程，来避免任何饥饿情况的发生。<br>指定时间属性时需要配置时间单位，下面的格式是可以接受的：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">25ms (milliseconds)</div><div class="line">5s (seconds)</div><div class="line">10m or 10min (minutes)</div><div class="line">3h (hours)</div><div class="line">5d (days)</div><div class="line">1y (years)</div></pre></td></tr></table></figure></p>
<p>指定字节大小的属性应该配置一个大小单位，下面的格式是可以接受的：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">1b (bytes)</div><div class="line">1k or 1kb (kibibytes = 1024 bytes)</div><div class="line">1m or 1mb (mebibytes = 1024 kibibytes)</div><div class="line">1g or 1gb (gibibytes = 1024 mebibytes)</div><div class="line">1t or 1tb (tebibytes = 1024 gibibytes)</div><div class="line">1p or 1pb (pebibytes = 1024 tebibytes)</div></pre></td></tr></table></figure></p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/08/09/spark-configuration/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/07/27/scala-sbt/" itemprop="url">
                  scala-sbt
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-27T18:36:41+08:00" content="2016-07-27">
              2016-07-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/scala/" itemprop="url" rel="index">
                    <span itemprop="name">scala</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文介绍一些关于sbt的使用，关于sbt的安装，请参考<a href="https://baimoon.github.io/blog/2016/07/27/unix-install" title="sbt install">sbt install</a></p>
<h1 id="使用sbt进行编译"><a href="#使用sbt进行编译" class="headerlink" title="使用sbt进行编译"></a>使用sbt进行编译</h1><p>对于sbt来说，最简单的工程就是某个目录下只有一个scala文件。对于这种文件，在当前目录下运行sbt，进入sbt控制台后执行run即可。</p>
<blockquote>
<p>vim HelloWorld.scala</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloWorld</span></span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</div><div class="line">    println(<span class="string">"Hello, SBT"</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$sbt</span>   </div><div class="line">&gt;run</div><div class="line">[info] Updating &#123;file:/Users/renweiming/tmp/&#125;tmp...</div><div class="line">[info] Resolving org.fusesource.jansi<span class="comment">#jansi;1.4 ...</span></div><div class="line">[info] Done updating.</div><div class="line">[info] Compiling 1 Scala <span class="built_in">source</span> to /Users/renweiming/tmp/target/scala-2.10/classes...</div><div class="line">[info] Running HelloWorld </div><div class="line">Hello, SBT</div><div class="line">[success] Total time: 4 s, completed 2016-7-27 23:19:35</div></pre></td></tr></table></figure></p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/07/27/scala-sbt/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/07/27/unix-install/" itemprop="url">
                  Unix Install
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-27T14:21:22+08:00" content="2016-07-27">
              2016-07-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/unix/" itemprop="url" rel="index">
                    <span itemprop="name">unix</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文记录了常用软件的安装，主要针对的是CentOS，其他系统会进行特殊标注</p>
<h1 id="Python-2-7-8的安装"><a href="#Python-2-7-8的安装" class="headerlink" title="Python 2.7.8的安装"></a>Python 2.7.8的安装</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">yum --enablerepo=ius install python27 -y</div><div class="line">yum --enablerepo=ius install python27-devel -y</div><div class="line">yum --enablerepo=ius install python27-pip -y</div></pre></td></tr></table></figure>
<h1 id="Sbt的安装"><a href="#Sbt的安装" class="headerlink" title="Sbt的安装"></a>Sbt的安装</h1><blockquote>
<p>sbt的<a href="https://github.com/CSUG/real_world_scala/blob/master/02_sbt.markdown" title="simple build tool">GitHub</a></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">wget https://dl.bintray.com/sbt/native-packages/sbt/0.13.12/sbt-0.13.12.zip</div><div class="line">unzip sbt-0.13.12.zip</div><div class="line"><span class="built_in">export</span> PATH=<span class="variable">$SBT_HOME</span>/bin:<span class="variable">$PATH</span></div></pre></td></tr></table></figure>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/07/27/unix-install/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/07/27/unix-commonCommand/" itemprop="url">
                  Unix Common Command
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-27T14:07:29+08:00" content="2016-07-27">
              2016-07-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/unix/" itemprop="url" rel="index">
                    <span itemprop="name">unix</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>该文记录了自己常用的unix的命令</p>
<h1 id="awk的使用"><a href="#awk的使用" class="headerlink" title="awk的使用"></a>awk的使用</h1><h2 id="用例"><a href="#用例" class="headerlink" title="用例"></a>用例</h2><h3 id="计算文件中某列的总和"><a href="#计算文件中某列的总和" class="headerlink" title="计算文件中某列的总和"></a>计算文件中某列的总和</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ vim test.log</div><div class="line">a 1</div><div class="line">b 3</div><div class="line">c 5</div><div class="line">d 6</div><div class="line"></div><div class="line">$ cat test.log | awk &apos;&#123;a = a + $2&#125; END &#123;print a&#125;&apos;</div><div class="line">15</div></pre></td></tr></table></figure>
<h3 id="使用awk对字符串进行拆分"><a href="#使用awk对字符串进行拆分" class="headerlink" title="使用awk对字符串进行拆分"></a>使用awk对字符串进行拆分</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat 1103kick.log | awk <span class="string">'&#123;split($0, a, "\""); print a[2]&#125;'</span> <span class="comment"># 按照"拆分，然后输出第2个数据</span></div></pre></td></tr></table></figure>
<h3 id="在awk中打印单引号"><a href="#在awk中打印单引号" class="headerlink" title="在awk中打印单引号"></a>在awk中打印单引号</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">test.log</div><div class="line">287901341</div><div class="line">285315045</div><div class="line">288588058</div><div class="line">283098937</div><div class="line">287076722</div><div class="line">284529273</div><div class="line"></div><div class="line">head test.log | awk &apos;&#123;print &quot;mv &quot; &quot;&apos;\&apos;&apos;&quot; $1 &quot;&apos;\&apos;&apos;&quot;&#125;&apos;</div><div class="line">mv &apos;287901341&apos;</div><div class="line">mv &apos;285315045&apos;</div><div class="line">mv &apos;288588058&apos;</div><div class="line">mv &apos;283098937&apos;</div><div class="line">mv &apos;287076722&apos;</div><div class="line">mv &apos;284529273&apos;</div></pre></td></tr></table></figure>
<h3 id="AWK中使用判断"><a href="#AWK中使用判断" class="headerlink" title="AWK中使用判断"></a>AWK中使用判断</h3><p>在AWK中是可以使用if判断语句的，if(判断条件) 执行语句; else if (判断条件) 执行语句; else 执行语句。<br>下面的例子，判断文件夹中的所有文件，如果文件的大小为0，则打印该文件的文件名。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ls -l | awk -F &apos; &apos; &apos;if($4 == 0) print $10&apos;</div></pre></td></tr></table></figure></p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/07/27/unix-commonCommand/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/07/26/spark-clusterModeOverview/" itemprop="url">
                  Cluster Mode Overview
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-26T15:06:27+08:00" content="2016-07-26">
              2016-07-26
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是对<a href="http://spark.apache.org/docs/latest/cluster-overview.html," title="Cluster Mode Overview">Cluster Mode Overview</a>的翻译，请多提宝贵意见。</p>
<p>这个文档给出了关于Spark如何在集群上运行的简短的概述，通过它是对相关组件的理解更加容易。通过读取<a href="https://baimoon.github.io/blog/2016/07/25/spark-submittingApplications," title="Submitting Applications">application submission guide</a>来学习如何在一个集群上发布application。</p>
<h2 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h2><p>Spark application在集群上作为一组独立的进程运行，并通过你的主程序（被称为驱动程序）中的SparkContext进行协调。<br>特别的，要在一个cluster运行，SparkContext能够连接到几个类型的cluster managers（Spark自己的standalone cluster manager、Mesos或YARN），通过application来分配资源。一旦连接到，Spark要求集群中node上的executors进程来为你的application计算并存储数据。接下来，它发送你的应用程序代码（通过JAR或Python文件传递给SparkContext）给executors。最终，SparkContext发送tasks给executors来执行。</p>
<p><img src="http://oaavtz33a.bkt.clouddn.com/217b79993ba1e6a7fa5adfeab2f8a430.png" alt="架构" title="架构"></p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/07/26/spark-clusterModeOverview/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/07/25/spark-submittingApplications/" itemprop="url">
                  Submitting Applications
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-25T14:22:23+08:00" content="2016-07-25">
              2016-07-25
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是对spark文档之Submitting Applications章节的翻译，原文档<a href="http://spark.apache.org/docs/latest/submitting-applications.html," title="Submitting Application">连接</a><br>在Spark的bin目录下的spark-submit脚本被用来发布应用程序到集群中。它能够使用所有被Spark支持的cluster managers的统一接口，因此你不需要为每个application进行配置。</p>
<h2 id="Bundling-Your-Application’s-Dependencies-绑定你的应用程序的依赖"><a href="#Bundling-Your-Application’s-Dependencies-绑定你的应用程序的依赖" class="headerlink" title="Bundling Your Application’s Dependencies(绑定你的应用程序的依赖)"></a>Bundling Your Application’s Dependencies(绑定你的应用程序的依赖)</h2><p>如果你的代码依赖其他项目，你需要将它们打包到你的应用程序中，以便将它分发到集群。为了这样做，需要创建一个assembly jar（或）来包含你的代码和代码的依赖。sbt和Maven都有assembly插件。当创建assembly jar的时候，作为被提供的依赖列出Spark和Hadoop；这些不需要被捆绑，因为他们在运行时，cluster管理器会提供。一旦你有了一个assembly的jar包，你能够调用bin/spark-submit脚本在传递你的jar时进行解析。<br>对于Python，你可以使用spark-submit的–py-files参数来添加.py，.zip或.egg的文件，来发布你的application。如果你依赖多个Python文件，我们推荐将它们打包到.zip或.egg中。</p>
<h2 id="Launching-Applications-with-spark-submit"><a href="#Launching-Applications-with-spark-submit" class="headerlink" title="Launching Applications with spark-submit"></a>Launching Applications with spark-submit</h2><p>一旦一个用户application被捆绑，那么这个application可以使用bin/spark-submit脚本来发布。这个脚本需要使用Spark和它的依赖来设置classpath，并支持不同的cluster manager和Spark支持的部署模式：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit \</div><div class="line">  --class &lt;main-class&gt;  --master &lt;master-url&gt; \</div><div class="line">  --deploy-mode &lt;deploy-mode&gt; \</div><div class="line">  --conf &lt;key&gt;=&lt;value&gt; \</div><div class="line">  ... <span class="comment">#other options  </span></div><div class="line">  &lt;application-jar&gt; \</div><div class="line">  [application-arguments]</div></pre></td></tr></table></figure></p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/07/25/spark-submittingApplications/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/07/25/spark-programmingGuide/" itemprop="url">
                  Programming Guide
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-25T13:59:13+08:00" content="2016-07-25">
              2016-07-25
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是对spark编程指南的翻译，主要用于自己对spark的理解，原文档<a href="http://spark.apache.org/docs/latest/programming-guide.html" title="Spark Programming Guide">链接</a></p>
<h1 id="Linking-with-Spark"><a href="#Linking-with-Spark" class="headerlink" title="Linking with Spark"></a>Linking with Spark</h1><p>Spark 2.0.0默认使用Scala 2.11进行构建和部署。（Spark也可以使用其他版本的Scala进行构建）要使用Scala来编写application，你需要使用一个合适的Scala版本（如2.11.X）。<br>要编写一个Spark application，你需要在添加Spark的Maven依赖。Spark在Maven中可用的坐标为：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">groupId = org.apache.spark</div><div class="line">artifactId = spark-core_2.11</div><div class="line">version = <span class="number">2.0</span>.0</div></pre></td></tr></table></figure></p>
<p>另外，如果你想要访问一个HDFS集群，你需要添加与你的HDFS对应版本的hadoop-client依赖：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">groupId = org.apache.hadoop</div><div class="line">artifactId = hadoop-client</div><div class="line">version = &lt;your-hdfs-version&gt;</div></pre></td></tr></table></figure></p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/07/25/spark-programmingGuide/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/07/25/spark-quickStart/" itemprop="url">
                  Quick Start
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-25T10:52:04+08:00" content="2016-07-25">
              2016-07-25
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是Spark 快速开始的翻译文档，会随着自己的实现进行更新</p>
<h2 id="Spark-shell的启动方式"><a href="#Spark-shell的启动方式" class="headerlink" title="Spark shell的启动方式"></a>Spark shell的启动方式</h2><p>spark提供了一种简单的方法来学习API，那就是Spark的shell。可以以Scala或python的方式来启动shell。<br>Scala的启动方式：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-shell</div></pre></td></tr></table></figure></p>
<p>python的启动方式：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/pyspark</div></pre></td></tr></table></figure></p>
<p>spark的主要抽象是项目的分布式集合，被称为Resilient Distributed Database(RDD)。RDD能够根据Hadoop的输入格式（诸如HDFS文件）来创建，或者由其他RDD转换成为RDD。我们根据Spark源码中的README文件来创建一个新的RDD：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala &gt; <span class="keyword">val</span> textFile = sc.textFile(<span class="string">"README.md"</span>)</div><div class="line">textFile:spark.<span class="type">RDD</span>[<span class="type">String</span>]= spark.<span class="type">MappedRDD</span>@<span class="number">2</span>ee9b6e3</div></pre></td></tr></table></figure></p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/07/25/spark-quickStart/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/07/17/jzmq/" itemprop="url">
                  CentOS install Jzmq
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-17T14:52:35+08:00" content="2016-07-17">
              2016-07-17
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/jzmq/" itemprop="url" rel="index">
                    <span itemprop="name">jzmq</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文介绍了如何在CentOS系统上安装Jzmq</p>
<h1 id="安装CZMQ"><a href="#安装CZMQ" class="headerlink" title="安装CZMQ"></a>安装CZMQ</h1><h2 id="首先要求系统安装了zmq，在CentOS系统中，可以直接使用yum进行安装："><a href="#首先要求系统安装了zmq，在CentOS系统中，可以直接使用yum进行安装：" class="headerlink" title="首先要求系统安装了zmq，在CentOS系统中，可以直接使用yum进行安装："></a>首先要求系统安装了zmq，在CentOS系统中，可以直接使用yum进行安装：</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum install czmq</div></pre></td></tr></table></figure>
<h1 id="安装JZMQ"><a href="#安装JZMQ" class="headerlink" title="安装JZMQ"></a>安装JZMQ</h1><h2 id="在安装了zmq之后，需要下载Jzmq，进行编译和安装："><a href="#在安装了zmq之后，需要下载Jzmq，进行编译和安装：" class="headerlink" title="在安装了zmq之后，需要下载Jzmq，进行编译和安装："></a>在安装了zmq之后，需要下载Jzmq，进行编译和安装：</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/zeromq/jzmq.git</div><div class="line"><span class="built_in">cd</span> jzmq-jni/</div><div class="line">./autogen.sh</div><div class="line">./configure</div><div class="line">make</div><div class="line">make install</div></pre></td></tr></table></figure>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/07/17/jzmq/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/blog/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/blog/">1</a><a class="page-number" href="/blog/page/2/">2</a><span class="page-number current">3</span>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/blog/uploads/avatar.png"
               alt="baimoon" />
          <p class="site-author-name" itemprop="name">baimoon</p>
          <p class="site-description motion-element" itemprop="description">Baimoon's blog</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/blog/archives">
              <span class="site-state-item-count">53</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/blog/categories">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/blog/tags">
                <span class="site-state-item-count">24</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/baimoon" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://gallery.xrange.org" title="xrange" target="_blank">xrange</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016-07 - 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">baimoon</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/blog/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/blog/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/blog/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/blog/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
