<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/blog/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/blog/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/blog/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico?v=5.0.1" />






<meta name="description" content="Baimoon&apos;s blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Baimoon's Note">
<meta property="og:url" content="http://baimoon.github.io/page/2/index.html">
<meta property="og:site_name" content="Baimoon's Note">
<meta property="og:description" content="Baimoon&apos;s blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Baimoon's Note">
<meta name="twitter:description" content="Baimoon&apos;s blog">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://baimoon.github.io/page/2/"/>

  <title> Baimoon's Note </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/blog/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Baimoon's Note</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/blog/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/blog/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/10/spark-2-3-1-SparkSQL-DataFramesAndDatasetsGuide/" itemprop="url">
                  Spark 2.3.1 Spark SQL DataFrames and DatasetsGuide
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-10T15:21:30+08:00" content="2018-08-10">
              2018-08-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Spark-SQL-DataFrames-and-Dataset-Guide"><a href="#Spark-SQL-DataFrames-and-Dataset-Guide" class="headerlink" title="Spark SQL, DataFrames and Dataset Guide"></a>Spark SQL, DataFrames and Dataset Guide</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Spark SQL是一个用于结构化数据处理的Spark模块。与Spark RDD API不同，由Spark SQL提供的这些接口在结构化数据和结构化计算执行方面提供了更多信息。在内部，Spark SQL使用了这个额外信息来执行额外的优化。有几种与Spark SQL交互的方法，包括SQL和Dataset API。当计算一个结果时，相同的计算引擎会被使用，与你执行计算使用的API／语言无关。这种统一意味着开发者能够轻松在那些提供更加原始的方式处理给定转换的不同API之间进行来回切换。<br>本篇中所有例子使用的样例数据包含在Spark中，并能够使用spark-shell、pyspark shell或sparkR shell来运行。</p>
<h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p>Spark SQL的一种用法时执行SQL查询。Spark SQL还能够被用来从Hive实例中读取数据。关于如何配置这个特性，请参考<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables" title="Hive Tables " target="_blank" rel="external">Hive Tables</a>。当在另一种编程语言中执行SQL时，结果会作为一个Dataset/DataFrame来返回。你还能够使用<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-spark-sql-cli" title="command-line" target="_blank" rel="external">command-line</a>或<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server" title="JDBC/ODBC" target="_blank" rel="external">JDBC/ODBC</a>的方式与SQL接口进行交互。</p>
<h3 id="Datasets-and-DataFrames"><a href="#Datasets-and-DataFrames" class="headerlink" title="Datasets and DataFrames"></a>Datasets and DataFrames</h3><p>一个Dataset就是一个分布式数据集。Dataset作为一个新接口在Spark 1.6中被添加，它提供了RDD的优点（强类型、能够使用强大的lambda函数）和Spark SQL的优化执行引擎的有点。一个Dataset能够根据JVM对象来构造，然后使用函数转换（map、flatMap、filter）进行操作。Dataset的API在Scala和Java中时可用的。Python还不支持Dataset API。但是因为Python的动态特性，Dataset API的很多优点已经可用了（例如你可以很自然的通过名称来访问某一行的一个字段 row.columnName）。对于R语言也是如此。<br>一个DataFrame是一个带有列名的数据集。它在概念上等同于关系数据库中的一个表或者一个是R语言或Python语言中data frame，但是底层具更加优化。DataFrame可以根据各种资源进行构建，例如：结构化的数据文件、Hive中的表、外部数据库以及已经存在的RDD。DataFrame API在Scala、Java、Python和R语言中都可用。在Scala和Java中，一个DataFrame相当于一个有很多行的Dataset。在<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" title="Scala API" target="_blank" rel="external">Scala API</a>中，DataFrame相当于一个Dataset[Row]类型。而在<a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html" title="Java API" target="_blank" rel="external">Java API</a>中，用户需要使用Dataset<row>来表述一个DataFrame。<br>在本文中，我们将经常引用Scala/Java由有Row组成的Dataset来表述DataFrame。</row></p>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><h3 id="Starting-Point-SparkSession"><a href="#Starting-Point-SparkSession" class="headerlink" title="Starting Point: SparkSession"></a>Starting Point: SparkSession</h3><p>Spark中，所有功能的切入点是<a href="http://spark.apache.org/docs/latest/api/java/index.html#org.apache.spark.sql.SparkSession" title="SparkSession" target="_blank" rel="external">SparkSession</a>类。要创建一个基本的SparkSession，只需要使用SparkSession.builder()<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"></div><div class="line">SparkSession spark = SparkSession</div><div class="line">  .builder()</div><div class="line">  .appName(<span class="string">"Java Spark SQL basic example"</span>)</div><div class="line">  .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</div><div class="line">  .getOrCreate();</div></pre></td></tr></table></figure></p>
<p>在Spark库的“examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java”目录下，查看完整的示例代码。<br>SparkSession是Spark 2.0的内置功能，用于提供Hive特性，包括用来写HiveQL查询、<br>访问Hive UDFs已经从Hive表中读取数据。要使用这些特性，你不需要配置Hive。</p>
<h3 id="Creating-DataFrames"><a href="#Creating-DataFrames" class="headerlink" title="Creating DataFrames"></a>Creating DataFrames</h3><p>使用SparkSession，application能够从一个已经存在的RDD、一个Hive表或<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources" title="Spark data sources" target="_blank" rel="external">Spark data sources</a>来创建DataFrame。<br>作为一个例子，下面的代码根据一个JSON文件中的内容来创建一个DataFrame：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></div><div class="line">df.show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看Spark库中的“examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java”。</p>
<h3 id="Untyped-Dataset-Operations-aka-DataFrame-Operations"><a href="#Untyped-Dataset-Operations-aka-DataFrame-Operations" class="headerlink" title="Untyped Dataset Operations(aka DataFrame Operations)"></a>Untyped Dataset Operations(aka DataFrame Operations)</h3><p>在Scala、Java、Python和R语言中，DataFrames针对不同的语言提供不同的结构化数据操作。正如上面提到的，在Spark2.0中，在Scala和Java的API中，DataFrames是以Dataset<row>来表述的。这些操作也被称为“无类型转换”，与强类型转换的Scala/Java Dataset的类型形成对比。<br>这里，我们展示了使用Dataset进行结构化数据处理的基本示例：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// col("...") is preferable to df.col("...")</span></div><div class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.spark.sql.functions.col;</div><div class="line"></div><div class="line"><span class="comment">// Print the schema in a tree format</span></div><div class="line">df.printSchema();</div><div class="line"><span class="comment">// root</span></div><div class="line"><span class="comment">// |-- age: long (nullable = true)</span></div><div class="line"><span class="comment">// |-- name: string (nullable = true)</span></div><div class="line"></div><div class="line"><span class="comment">// Select only the "name" column</span></div><div class="line">df.select(<span class="string">"name"</span>).show();</div><div class="line"><span class="comment">// +-------+</span></div><div class="line"><span class="comment">// |   name|</span></div><div class="line"><span class="comment">// +-------+</span></div><div class="line"><span class="comment">// |Michael|</span></div><div class="line"><span class="comment">// |   Andy|</span></div><div class="line"><span class="comment">// | Justin|</span></div><div class="line"><span class="comment">// +-------+</span></div><div class="line"></div><div class="line"><span class="comment">// Select everybody, but increment the age by 1</span></div><div class="line">df.select(col(<span class="string">"name"</span>), col(<span class="string">"age"</span>).plus(<span class="number">1</span>)).show();</div><div class="line"><span class="comment">// +-------+---------+</span></div><div class="line"><span class="comment">// |   name|(age + 1)|</span></div><div class="line"><span class="comment">// +-------+---------+</span></div><div class="line"><span class="comment">// |Michael|     null|</span></div><div class="line"><span class="comment">// |   Andy|       31|</span></div><div class="line"><span class="comment">// | Justin|       20|</span></div><div class="line"><span class="comment">// +-------+---------+</span></div><div class="line"></div><div class="line"><span class="comment">// Select people older than 21</span></div><div class="line">df.filter(col(<span class="string">"age"</span>).gt(<span class="number">21</span>)).show();</div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// |age|name|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// | 30|Andy|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"></div><div class="line"><span class="comment">// Count people by age</span></div><div class="line">df.groupBy(<span class="string">"age"</span>).count().show();</div><div class="line"><span class="comment">// +----+-----+</span></div><div class="line"><span class="comment">// | age|count|</span></div><div class="line"><span class="comment">// +----+-----+</span></div><div class="line"><span class="comment">// |  19|    1|</span></div><div class="line"><span class="comment">// |null|    1|</span></div><div class="line"><span class="comment">// |  30|    1|</span></div><div class="line"><span class="comment">// +----+-----+</span></div></pre></td></tr></table></figure></row></p>
<p>完整的样例代码，查看Spark库的examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java。<br>在Dataset上能够执行的操作类型列表，可以查看<a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html" title="API Document" target="_blank" rel="external">API Document</a>。<br>除了简单的列引用和计算外，Dataset还有一个丰富的函数库，包括字符串的操作、日期的计算以及常用的数学操作等。完整的列表可以在<a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html" title="DataFrame Function Reference" target="_blank" rel="external">DataFrame Function Reference</a>找到。</p>
<h3 id="Running-SQL-Queries-Programmatically"><a href="#Running-SQL-Queries-Programmatically" class="headerlink" title="Running SQL Queries Programmatically"></a>Running SQL Queries Programmatically</h3><p>SparkSession上的sql函数使application能够执行SQL查询，并返回一个Dataset<row>作为结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">import org.apache.spark.sql.Dataset;</div><div class="line">import org.apache.spark.sql.Row;</div><div class="line"></div><div class="line">// Register the DataFrame as a SQL temporary view</div><div class="line">df.createOrReplaceTempView(&quot;people&quot;);</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(&quot;SELECT * FROM people&quot;);</div><div class="line">sqlDF.show();</div><div class="line">// +----+-------+</div><div class="line">// | age|   name|</div><div class="line">// +----+-------+</div><div class="line">// |null|Michael|</div><div class="line">// |  30|   Andy|</div><div class="line">// |  19| Justin|</div><div class="line">// +----+-------+</div></pre></td></tr></table></figure></row></p>
<p>完整的代码，请查看Spark库中的 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h3 id="Global-Temporary-View"><a href="#Global-Temporary-View" class="headerlink" title="Global Temporary View"></a>Global Temporary View</h3><p>在Spark SQL中，临时视图是session范围的，将会伴随着创建它的那个session的终止而消失。如果你想要跨session共享一个临时视图，并让它存活到application终止，你可以创建一个全局临时视图。全局视图与一个名为‘global_temp’的由系统保护的数据库进行绑定，我们必须使用这个特殊的名字来引用它，如：SELECT * FROM global_temp.view1。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Register the DataFrame as a global temporary view</span></div><div class="line">df.createGlobalTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Global temporary view is tied to a system preserved database `global_temp`</span></div><div class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"></div><div class="line"><span class="comment">// Global temporary view is cross-session</span></div><div class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看“examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java”。</p>
<h3 id="Creating-Datasets"><a href="#Creating-Datasets" class="headerlink" title="Creating Datasets"></a>Creating Datasets</h3><p>Dataset与RDD类似，不同的是它没有使用Java序列化或Kryo，它们使用了一个特殊的<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder" title="Encoder" target="_blank" rel="external">Encoder</a>来序列化对象，以便这些对象的处理或跨网络传输。虽然encoder和标准序列化器都能够将一个对象转换为字节，encoder是动态编码产生的，并且使用一种格式来允许Spark执行很多操作(filtering， sorting 和 hashing)，而不需要讲字节反编译为对象。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.Arrays;</div><div class="line"><span class="keyword">import</span> java.util.Collections;</div><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> String name;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> age;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> name;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.name = name;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> age;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.age = age;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Create an instance of a Bean class</span></div><div class="line">Person person = <span class="keyword">new</span> Person();</div><div class="line">person.setName(<span class="string">"Andy"</span>);</div><div class="line">person.setAge(<span class="number">32</span>);</div><div class="line"></div><div class="line"><span class="comment">// Encoders are created for Java beans</span></div><div class="line">Encoder&lt;Person&gt; personEncoder = Encoders.bean(Person.class);</div><div class="line">Dataset&lt;Person&gt; javaBeanDS = spark.createDataset(</div><div class="line">  Collections.singletonList(person),</div><div class="line">  personEncoder</div><div class="line">);</div><div class="line">javaBeanDS.show();</div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// |age|name|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// | 32|Andy|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"></div><div class="line"><span class="comment">// Encoders for most common types are provided in class Encoders</span></div><div class="line">Encoder&lt;Integer&gt; integerEncoder = Encoders.INT();</div><div class="line">Dataset&lt;Integer&gt; primitiveDS = spark.createDataset(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), integerEncoder);</div><div class="line">Dataset&lt;Integer&gt; transformedDS = primitiveDS.map(</div><div class="line">    (MapFunction&lt;Integer, Integer&gt;) value -&gt; value + <span class="number">1</span>,</div><div class="line">    integerEncoder);</div><div class="line">transformedDS.collect(); <span class="comment">// Returns [2, 3, 4]</span></div><div class="line"></div><div class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping based on name</span></div><div class="line">String path = <span class="string">"examples/src/main/resources/people.json"</span>;</div><div class="line">Dataset&lt;Person&gt; peopleDS = spark.read().json(path).as(personEncoder);</div><div class="line">peopleDS.show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div></pre></td></tr></table></figure></p>
<p>完整的示例，请查看 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h3 id="Interoperating-with-RDDs"><a href="#Interoperating-with-RDDs" class="headerlink" title="Interoperating with RDDs"></a>Interoperating with RDDs</h3><p>Spark SQL支持两种不同方法来将存在的RDD转换为Dataset。第一种方法是使用反射来推导包含特殊类型对象的RDD的模式。这种反射的方法代码更加简单，而且如果在你写Spark application时已经知道了模式时，工作的会很好。<br>第二种方法是通过一个程序接口来创建Dataset，这个程序接口允许你构建一个模式，并且将它应用到一个已经存在的RDD上。但是这个方法比较冗长，它允许你只有在运行时才知道列和列类型时来构造Dataset。</p>
<h4 id="Inferring-the-Schema-Using-Reflection"><a href="#Inferring-the-Schema-Using-Reflection" class="headerlink" title="Inferring the Schema Using Reflection"></a>Inferring the Schema Using Reflection</h4><p>Spark SQL支持自动将一个JavaBean的RDD转换为一个DataFrame。BeanInfo使用反射机制获得，定义了表的模式。当前，Spark SQL不支持那些包含了Map类型字段的JavaBean，但是对于嵌套的JavaBean以及嵌套了List或Array类型的字段给予了充分的支持。你可以通过创建一个实现了Serializable接口以及为所有字段生成getter和setter方法的类来创建一个JavaBean。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"></div><div class="line"><span class="comment">// Create an RDD of Person objects from a text file</span></div><div class="line">JavaRDD&lt;Person&gt; peopleRDD = spark.read()</div><div class="line">  .textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</div><div class="line">  .javaRDD()</div><div class="line">  .map(line -&gt; &#123;</div><div class="line">    String[] parts = line.split(<span class="string">","</span>);</div><div class="line">    Person person = <span class="keyword">new</span> Person();</div><div class="line">    person.setName(parts[<span class="number">0</span>]);</div><div class="line">    person.setAge(Integer.parseInt(parts[<span class="number">1</span>].trim()));</div><div class="line">    <span class="keyword">return</span> person;</div><div class="line">  &#125;);</div><div class="line"></div><div class="line"><span class="comment">// Apply a schema to an RDD of JavaBeans to get a DataFrame</span></div><div class="line">Dataset&lt;Row&gt; peopleDF = spark.createDataFrame(peopleRDD, Person.class);</div><div class="line"><span class="comment">// Register the DataFrame as a temporary view</span></div><div class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></div><div class="line">Dataset&lt;Row&gt; teenagersDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The columns of a row in the result can be accessed by field index</span></div><div class="line">Encoder&lt;String&gt; stringEncoder = Encoders.STRING();</div><div class="line">Dataset&lt;String&gt; teenagerNamesByIndexDF = teenagersDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</div><div class="line">    stringEncoder);</div><div class="line">teenagerNamesByIndexDF.show();</div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |       value|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |Name: Justin|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"></div><div class="line"><span class="comment">// or by field name</span></div><div class="line">Dataset&lt;String&gt; teenagerNamesByFieldDF = teenagersDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.&lt;String&gt;getAs(<span class="string">"name"</span>),</div><div class="line">    stringEncoder);</div><div class="line">teenagerNamesByFieldDF.show();</div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |       value|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |Name: Justin|</span></div><div class="line"><span class="comment">// +------------+</span></div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h4 id="Programmatically-Specifying-the-Schema"><a href="#Programmatically-Specifying-the-Schema" class="headerlink" title="Programmatically Specifying the Schema"></a>Programmatically Specifying the Schema</h4><p>当JavaBean无法提前定义时（例如，记录的结构被编码为一个字符串，或者一个文本数据集将被解析，但是其中的字段可能根据不同的用户而不一样），Dataset<row>能够通过三个步骤来创建。</row></p>
<blockquote>
<p>1、根据原生的RDD创建一个RDD<row>。<br>2、创建一个与第一步骤RDD中Row结构匹配的StructType来描述的模式。<br>3、通过由SparkSession提供的createDataFrame方法，将这个模式应用到RDD<row>。</row></row></p>
</blockquote>
<p>例如：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</div><div class="line"></div><div class="line"><span class="comment">// Create an RDD</span></div><div class="line">JavaRDD&lt;String&gt; peopleRDD = spark.sparkContext()</div><div class="line">  .textFile(<span class="string">"examples/src/main/resources/people.txt"</span>, <span class="number">1</span>)</div><div class="line">  .toJavaRDD();</div><div class="line"></div><div class="line"><span class="comment">// The schema is encoded in a string</span></div><div class="line">String schemaString = <span class="string">"name age"</span>;</div><div class="line"></div><div class="line"><span class="comment">// Generate the schema based on the string of schema</span></div><div class="line">List&lt;StructField&gt; fields = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (String fieldName : schemaString.split(<span class="string">" "</span>)) &#123;</div><div class="line">  StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, <span class="keyword">true</span>);</div><div class="line">  fields.add(field);</div><div class="line">&#125;</div><div class="line">StructType schema = DataTypes.createStructType(fields);</div><div class="line"></div><div class="line"><span class="comment">// Convert records of the RDD (people) to Rows</span></div><div class="line">JavaRDD&lt;Row&gt; rowRDD = peopleRDD.map((Function&lt;String, Row&gt;) record -&gt; &#123;</div><div class="line">  String[] attributes = record.split(<span class="string">","</span>);</div><div class="line">  <span class="keyword">return</span> RowFactory.create(attributes[<span class="number">0</span>], attributes[<span class="number">1</span>].trim());</div><div class="line">&#125;);</div><div class="line"></div><div class="line"><span class="comment">// Apply the schema to the RDD</span></div><div class="line">Dataset&lt;Row&gt; peopleDataFrame = spark.createDataFrame(rowRDD, schema);</div><div class="line"></div><div class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></div><div class="line">peopleDataFrame.createOrReplaceTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// SQL can be run over a temporary view created using DataFrames</span></div><div class="line">Dataset&lt;Row&gt; results = spark.sql(<span class="string">"SELECT name FROM people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations</span></div><div class="line"><span class="comment">// The columns of a row in the result can be accessed by field index or by field name</span></div><div class="line">Dataset&lt;String&gt; namesDS = results.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</div><div class="line">    Encoders.STRING());</div><div class="line">namesDS.show();</div><div class="line"><span class="comment">// +-------------+</span></div><div class="line"><span class="comment">// |        value|</span></div><div class="line"><span class="comment">// +-------------+</span></div><div class="line"><span class="comment">// |Name: Michael|</span></div><div class="line"><span class="comment">// |   Name: Andy|</span></div><div class="line"><span class="comment">// | Name: Justin|</span></div><div class="line"><span class="comment">// +-------------+</span></div></pre></td></tr></table></figure></p>
<p>完整的示例，请查看 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h3 id="Aggregations"><a href="#Aggregations" class="headerlink" title="Aggregations"></a>Aggregations</h3><p>内置的DataFrame函数提供了常用的聚合操作，如count()、countDistinct()、avg()、max()、min()等。然而这些函数是为了DataFrame设计的，Spark SQL同样由类型安全的版本，以便其中一些被用到Scala和Java的强类型Dataset。此外，Spark没有限制用户预定义聚合函数，可以自己来创建聚合函数。</p>
<h4 id="Untyped-User-Defined-Aggregate-Functions"><a href="#Untyped-User-Defined-Aggregate-Functions" class="headerlink" title="Untyped User-Defined Aggregate Functions"></a>Untyped User-Defined Aggregate Functions</h4><p>用户要实现无类型聚合函数，则需要继承<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.UserDefinedAggregateFunction" title="UserDefinedAggregateFunction" target="_blank" rel="external">UserDefinedAggregateFunction</a>抽象类。例如，你一个用户自定义的平均数函数，看起来像这样：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.MutableAggregationBuffer;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.UserDefinedAggregateFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataType;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">private</span> StructType inputSchema;</div><div class="line">  <span class="keyword">private</span> StructType bufferSchema;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="title">MyAverage</span><span class="params">()</span> </span>&#123;</div><div class="line">    List&lt;StructField&gt; inputFields = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">    inputFields.add(DataTypes.createStructField(<span class="string">"inputColumn"</span>, DataTypes.LongType, <span class="keyword">true</span>));</div><div class="line">    inputSchema = DataTypes.createStructType(inputFields);</div><div class="line"></div><div class="line">    List&lt;StructField&gt; bufferFields = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">    bufferFields.add(DataTypes.createStructField(<span class="string">"sum"</span>, DataTypes.LongType, <span class="keyword">true</span>));</div><div class="line">    bufferFields.add(DataTypes.createStructField(<span class="string">"count"</span>, DataTypes.LongType, <span class="keyword">true</span>));</div><div class="line">    bufferSchema = DataTypes.createStructType(bufferFields);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Data types of input arguments of this aggregate function</span></div><div class="line">  <span class="function"><span class="keyword">public</span> StructType <span class="title">inputSchema</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> inputSchema;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Data types of values in the aggregation buffer</span></div><div class="line">  <span class="function"><span class="keyword">public</span> StructType <span class="title">bufferSchema</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> bufferSchema;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// The data type of the returned value</span></div><div class="line">  <span class="function"><span class="keyword">public</span> DataType <span class="title">dataType</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> DataTypes.DoubleType;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Whether this function always returns the same output on the identical input</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">deterministic</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to</span></div><div class="line">  <span class="comment">// standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides</span></div><div class="line">  <span class="comment">// the opportunity to update its values. Note that arrays and maps inside the buffer are still</span></div><div class="line">  <span class="comment">// immutable.</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(MutableAggregationBuffer buffer)</span> </span>&#123;</div><div class="line">    buffer.update(<span class="number">0</span>, <span class="number">0L</span>);</div><div class="line">    buffer.update(<span class="number">1</span>, <span class="number">0L</span>);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Updates the given aggregation buffer `buffer` with new input data from `input`</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">(MutableAggregationBuffer buffer, Row input)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</div><div class="line">      <span class="keyword">long</span> updatedSum = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>);</div><div class="line">      <span class="keyword">long</span> updatedCount = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span>;</div><div class="line">      buffer.update(<span class="number">0</span>, updatedSum);</div><div class="line">      buffer.update(<span class="number">1</span>, updatedCount);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Merges two aggregation buffers and stores the updated buffer values back to `buffer1`</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(MutableAggregationBuffer buffer1, Row buffer2)</span> </span>&#123;</div><div class="line">    <span class="keyword">long</span> mergedSum = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>);</div><div class="line">    <span class="keyword">long</span> mergedCount = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>);</div><div class="line">    buffer1.update(<span class="number">0</span>, mergedSum);</div><div class="line">    buffer1.update(<span class="number">1</span>, mergedCount);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Calculates the final result</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">evaluate</span><span class="params">(Row buffer)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) buffer.getLong(<span class="number">0</span>)) / buffer.getLong(<span class="number">1</span>);</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Register the function to access it</span></div><div class="line">spark.udf().register(<span class="string">"myAverage"</span>, <span class="keyword">new</span> MyAverage());</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">"examples/src/main/resources/employees.json"</span>);</div><div class="line">df.createOrReplaceTempView(<span class="string">"employees"</span>);</div><div class="line">df.show();</div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |   name|salary|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |Michael|  3000|</span></div><div class="line"><span class="comment">// |   Andy|  4500|</span></div><div class="line"><span class="comment">// | Justin|  3500|</span></div><div class="line"><span class="comment">// |  Berta|  4000|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"></div><div class="line">Dataset&lt;Row&gt; result = spark.sql(<span class="string">"SELECT myAverage(salary) as average_salary FROM employees"</span>);</div><div class="line">result.show();</div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |average_salary|</span></div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |        3750.0|</span></div><div class="line"><span class="comment">// +--------------+</span></div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考 examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java 。</p>
<h4 id="Type-Safe-User-Defined-Aggregate-Functions"><a href="#Type-Safe-User-Defined-Aggregate-Functions" class="headerlink" title="Type-Safe User-Defined Aggregate Functions"></a>Type-Safe User-Defined Aggregate Functions</h4><p>强类型Dataset的用户自定义聚合围绕着<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator ‘Aggregator’" target="_blank" rel="external">Aggregator</a>抽象类来解决。例如，一个类型安全的用户自定义平均数看起来是这样：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.TypedColumn;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.Aggregator;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> String name;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">long</span> salary;</div><div class="line"></div><div class="line">  <span class="comment">// Constructors, getters, setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Average</span> <span class="keyword">implements</span> <span class="title">Serializable</span>  </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">long</span> sum;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">long</span> count;</div><div class="line"></div><div class="line">  <span class="comment">// Constructors, getters, setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>&lt;<span class="title">Employee</span>, <span class="title">Average</span>, <span class="title">Double</span>&gt; </span>&#123;</div><div class="line">  <span class="comment">// A zero value for this aggregation. Should satisfy the property that any b + zero = b</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">zero</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Average(<span class="number">0L</span>, <span class="number">0L</span>);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span></div><div class="line">  <span class="comment">// and return it instead of constructing a new object</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">reduce</span><span class="params">(Average buffer, Employee employee)</span> </span>&#123;</div><div class="line">    <span class="keyword">long</span> newSum = buffer.getSum() + employee.getSalary();</div><div class="line">    <span class="keyword">long</span> newCount = buffer.getCount() + <span class="number">1</span>;</div><div class="line">    buffer.setSum(newSum);</div><div class="line">    buffer.setCount(newCount);</div><div class="line">    <span class="keyword">return</span> buffer;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Merge two intermediate values</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">merge</span><span class="params">(Average b1, Average b2)</span> </span>&#123;</div><div class="line">    <span class="keyword">long</span> mergedSum = b1.getSum() + b2.getSum();</div><div class="line">    <span class="keyword">long</span> mergedCount = b1.getCount() + b2.getCount();</div><div class="line">    b1.setSum(mergedSum);</div><div class="line">    b1.setCount(mergedCount);</div><div class="line">    <span class="keyword">return</span> b1;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Transform the output of the reduction</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">finish</span><span class="params">(Average reduction)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) reduction.getSum()) / reduction.getCount();</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Specifies the Encoder for the intermediate value type</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Encoder&lt;Average&gt; <span class="title">bufferEncoder</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> Encoders.bean(Average.class);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Specifies the Encoder for the final output value type</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Encoder&lt;Double&gt; <span class="title">outputEncoder</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> Encoders.DOUBLE();</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">Encoder&lt;Employee&gt; employeeEncoder = Encoders.bean(Employee.class);</div><div class="line">String path = <span class="string">"examples/src/main/resources/employees.json"</span>;</div><div class="line">Dataset&lt;Employee&gt; ds = spark.read().json(path).as(employeeEncoder);</div><div class="line">ds.show();</div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |   name|salary|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |Michael|  3000|</span></div><div class="line"><span class="comment">// |   Andy|  4500|</span></div><div class="line"><span class="comment">// | Justin|  3500|</span></div><div class="line"><span class="comment">// |  Berta|  4000|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"></div><div class="line">MyAverage myAverage = <span class="keyword">new</span> MyAverage();</div><div class="line"><span class="comment">// Convert the function to a `TypedColumn` and give it a name</span></div><div class="line">TypedColumn&lt;Employee, Double&gt; averageSalary = myAverage.toColumn().name(<span class="string">"average_salary"</span>);</div><div class="line">Dataset&lt;Double&gt; result = ds.select(averageSalary);</div><div class="line">result.show();</div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |average_salary|</span></div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |        3750.0|</span></div><div class="line"><span class="comment">// +--------------+</span></div></pre></td></tr></table></figure></p>
<p>完整的示例，请看 examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java 。</p>
<h2 id="Data-Sources"><a href="#Data-Sources" class="headerlink" title="Data Sources"></a>Data Sources</h2><p>Spark SQL通过DataFrame接口支持多种数据源的操作。DataFrame能够使用关系转换进行操作，也可以被用来创建一个临时视图。将DataFrame注册为一个临时视图，将允许你在视图的数据上运行SQL查询。这一章节描述了使用Spark Data Sources加载和保存数据的一般方法，然后介绍内置数据源可用的详细参数。</p>
<h3 id="Generic-Load-Save-Functions"><a href="#Generic-Load-Save-Functions" class="headerlink" title="Generic Load/Save Functions"></a>Generic Load/Save Functions</h3><p>最简单的格式，默认数据源（默认是parquet， 除非通过spark.sql.soiurces.default配置修改过）将被用于所有操作。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; usersDF = spark.read().load(<span class="string">"examples/src/main/resources/users.parquet"</span>);</div><div class="line">usersDF.select(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>).write().save(<span class="string">"namesAndFavColors.parquet"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考 examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java 。</p>
<h4 id="Manually-Specifying-Options"><a href="#Manually-Specifying-Options" class="headerlink" title="Manually Specifying Options"></a>Manually Specifying Options</h4><p>你还可以手动指定想要使用的数据源，以及传递给数据源任何额外的参数。数据源可以通过它的完整限定名（如：org.apache.spark.sql.parquet）来指定，但是对于内置的数据源，你也能够使用它的短名字（json、parquet、jdbc、orc、libsvm、csv、text）。从任何类型数据源加载的DataFrames，通过使用这个语句都可以转为其他类型。<br>要加载一个JSON文件，你可以使用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; peopleDF = spark.read().format(<span class="string">"json"</span>).load(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line">peopleDF.select(<span class="string">"name"</span>, <span class="string">"age"</span>).write().format(<span class="string">"parquet"</span>).save(<span class="string">"namesAndAges.parquet"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考：xamples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>要加载一个CSV文件，你可以使用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; peopleDFCsv = spark.read().format(<span class="string">"csv"</span>)</div><div class="line">  .option(<span class="string">"sep"</span>, <span class="string">";"</span>)</div><div class="line">  .option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)</div><div class="line">  .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</div><div class="line">  .load(<span class="string">"examples/src/main/resources/people.csv"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考：xamples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Run-SQL-on-files-directly"><a href="#Run-SQL-on-files-directly" class="headerlink" title="Run SQL on files directly"></a>Run SQL on files directly</h4><p>除了使用read API加载文件到DataFrame然后查询它之外，你还可以使用SQL直接查询那个文件。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; sqlDF =</div><div class="line">  spark.sql(<span class="string">"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考：xamples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Save-Modes"><a href="#Save-Modes" class="headerlink" title="Save Modes"></a>Save Modes</h4><p>保存操作可以选择一种SaveMode，它指定了如何处理存在的数据。一件非常重要的事情是这些保存模式没有利用任何锁，并且它们不是原子操作。另外，当执行Overwrite模式时，已有的数据将会在写出新数据之前被删掉。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Scala/Java</th>
<th style="text-align:left">Any Language</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">SaveMode.ErrorIfExists(default)</td>
<td style="text-align:left">“error” or “errorifexists” (default)</td>
<td style="text-align:left">当保存一个DataFrame到一个数据源时，如果数据已经存在，预计将抛出一个异常</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Append</td>
<td style="text-align:left">“append”</td>
<td style="text-align:left">当保存一个DataFrame到一个数据源时，如果数据或表格已经存在，DataFrame的内容将被追加到已存在数据</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Overwrite</td>
<td style="text-align:left">“overwrite”</td>
<td style="text-align:left">Overwrite模式意味着，当保存一个DataFrame到一个数据源时，如果数据或表格已经存在，已存在的数据将会被DataFrame的内容所覆盖</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Ignore</td>
<td style="text-align:left">“ignore”</td>
<td style="text-align:left">Ignore模式意味着当保存一个DataFrame到一个数据源时，如果数据已经存在，保存操作将不会保存DataFrame的内容，并且不会修改已经存在的数据。这个操作类似 CREATE TABLE IF NOT EXISTS</td>
</tr>
</tbody>
</table>
<h4 id="Saving-to-Persistent-Tables"><a href="#Saving-to-Persistent-Tables" class="headerlink" title="Saving to Persistent Tables"></a>Saving to Persistent Tables</h4><p>使用saveAsTable命令，DataFrames也可以作为持久化表被保存到Hive metastore中。注意，使用这个功能不需要现有Hive的部署。Spark将会为你创建一个默认的本地Hive metastore(使用Derby)。与createOrReplaceTempView命令不同，saveAsTable将显示DataFrame的内容并创建一个指向Hive metastore中数据的指针。持久化表将在你的Spark程序重启之后持续存在，只要你维持你的连接在相同的metastore。通过在SparkSession上调用table方法（并传递表的名字），就能根据持久化表创建对应的DataFrame。<br>对于基于文件的数据源，如：text、parquet、json等。通过path选项，你可以指定一个自定义表路径，如:df.write.option(“path”, “/some/path”).saveAsTable(“t”)。当这个表被删除，自定义表路径将不会被移除，并且表数据依然存在。如果没有指定自定义表路径，Spark将会把数据写到仓库目录下的默认表路径。当这个表被删除时，默认表路径也会一并被删除。<br>从Spark2.1开始，持久化数据源表格在Hive metastore中有独立的元数据。这样做又一些优点：</p>
<blockquote>
<p>因为metastore只返回查询所需的partition，因此表上的首次查询就不需要查找所有的aprtition。<br>Hive DDL（如ALTER TABLE PARTITION … SET LOCATION），对于使用Datasource APi来创建表都是可用的。</p>
</blockquote>
<p>注意，当创建外部数据源表时（那些带有path选项的），分区信息默认是不会被收集的。要同步分区信息到metastore中，你可以执行MSCK REPAIR TABLE。</p>
<h4 id="Bucketing-Sorting-and-Partitioning"><a href="#Bucketing-Sorting-and-Partitioning" class="headerlink" title="Bucketing, Sorting and Partitioning"></a>Bucketing, Sorting and Partitioning</h4><p>对于基于文件的数据源，还可以对输出进行分组并排序或分组并分区。分组并排序只对持久化表适用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">peopleDF.write().bucketBy(<span class="number">42</span>, <span class="string">"name"</span>).sortBy(<span class="string">"age"</span>).saveAsTable(<span class="string">"people_bucketed"</span>);</div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>当使用Dataset API时，partitioning能够和save以及saveAsTable一起使用。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">usersDF</div><div class="line">  .write()</div><div class="line">  .partitionBy(<span class="string">"favorite_color"</span>)</div><div class="line">  .format(<span class="string">"parquet"</span>)</div><div class="line">  .save(<span class="string">"namesPartByColor.parquet"</span>);</div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>可以对单个表使用partitioning和bucketing：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">peopleDF</div><div class="line">  .write()</div><div class="line">  .partitionBy(<span class="string">"favorite_color"</span>)</div><div class="line">  .bucketBy(<span class="number">42</span>, <span class="string">"name"</span>)</div><div class="line">  .saveAsTable(<span class="string">"people_partitioned_bucketed"</span>);</div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>partitionBy创建了一个在<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#partition-discovery" title="Partition Discovery" target="_blank" rel="external">Partition Discovery</a>章节中描述的目录结构。因此，它对具有高基数的列的适用性有限。相比之下，BucketBy会跨固定数量的bucket来分布部署数据，and can be used when a number of unique values is unbounded.（！！！无法理解）</p>
<h3 id="Parquet-Files"><a href="#Parquet-Files" class="headerlink" title="Parquet Files"></a>Parquet Files</h3><p>Parquet时一种列式文件格式，它被很多其他数据处理系统所支持。Spark SQL对Parquet文件提供了读写支持，并能够自动保护原始数据的模式。当写Parquet文件时，为了兼容的原因，所有列被自动转换为nullable。</p>
<h4 id="Loading-Data-Programmatically"><a href="#Loading-Data-Programmatically" class="headerlink" title="Loading Data Programmatically"></a>Loading Data Programmatically</h4><p>使用上面例子中的数据：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; peopleDF = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line"></div><div class="line"><span class="comment">// DataFrames can be saved as Parquet files, maintaining the schema information</span></div><div class="line">peopleDF.write().parquet(<span class="string">"people.parquet"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Read in the Parquet file created above.</span></div><div class="line"><span class="comment">// Parquet files are self-describing so the schema is preserved</span></div><div class="line"><span class="comment">// The result of loading a parquet file is also a DataFrame</span></div><div class="line">Dataset&lt;Row&gt; parquetFileDF = spark.read().parquet(<span class="string">"people.parquet"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Parquet files can also be used to create a temporary view and then used in SQL statements</span></div><div class="line">parquetFileDF.createOrReplaceTempView(<span class="string">"parquetFile"</span>);</div><div class="line">Dataset&lt;Row&gt; namesDF = spark.sql(<span class="string">"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19"</span>);</div><div class="line">Dataset&lt;String&gt; namesDS = namesDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</div><div class="line">    Encoders.STRING());</div><div class="line">namesDS.show();</div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |       value|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |Name: Justin|</span></div><div class="line"><span class="comment">// +------------+</span></div></pre></td></tr></table></figure></p>
<p>完整示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Partition-Discovery"><a href="#Partition-Discovery" class="headerlink" title="Partition Discovery"></a>Partition Discovery</h4><p>在像Hive这样的系统中，常用的优化方法时进行表分区。在分区表中，数据通常存储在不同的目录中，根据分区列的值，编码到每个分区目录的路径中。所有内置文件源（包括Text/CSV/JSON/ORC/Parquet）都能够自动发现并推断分区信息。例如，我们能够将我们之前使用的数据存储到如下目录结构的分区表中，这个分区表使用两个额外的字段gender和country来作为分区字段：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">path</div><div class="line">└── to</div><div class="line">    └── table</div><div class="line">        ├── gender=male</div><div class="line">        │   ├── ...</div><div class="line">        │   │</div><div class="line">        │   ├── country=US</div><div class="line">        │   │   └── data.parquet</div><div class="line">        │   ├── country=CN</div><div class="line">        │   │   └── data.parquet</div><div class="line">        │   └── ...</div><div class="line">        └── gender=female</div><div class="line">            ├── ...</div><div class="line">            │</div><div class="line">            ├── country=US</div><div class="line">            │   └── data.parquet</div><div class="line">            ├── country=CN</div><div class="line">            │   └── data.parquet</div><div class="line">            └── ...</div></pre></td></tr></table></figure></p>
<p>通过将path/to/table传递给SparkSession.read.parquet或SparkSession.read.load，Spark SQL将自动从路径中获取分区信息。现在返回的DataFrame的模式变成：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">root</div><div class="line">|-- name: string (nullable = true)</div><div class="line">|-- age: long (nullable = true)</div><div class="line">|-- gender: string (nullable = true)</div><div class="line">|-- country: string (nullable = true)</div></pre></td></tr></table></figure></p>
<p>注意，分区列的数据类型是自动推断的。当前支持数字数据类型、日期、时间戳和字符串类型。有些时候，用户可能不想自动推导分区列的数据类型。对于这种情况，自动类型推导能够通过配置项spark.sql.sources.partitionColumnTypeInference.enabled来配置，该配置默认值为True。当类型推导被禁用后，分区列将使用字符串类型。<br>从Spark1.6开始，分区发现默认只能查找给定路径下的。因此，对于上面的那个例子，如果用户传递path/to/table/gender=male给SparkSession.read.parquet或SparkSession.read.load，那么gender将不会被当成一个分区列。如果用户想要具体说明分区开始查找的基本目录，可以在数据源选项中设置basePath。例如，当数据目录为path/to/table/gender=male时，并且设置了basePath为path/to/table/，那么gender将会是一个分区列。</p>
<h4 id="Schema-Merging"><a href="#Schema-Merging" class="headerlink" title="Schema Merging"></a>Schema Merging</h4><p>和ProtocolBuffer、Avro以及Thrift一样，Parquet也支持模式演化。用户可以先从一个简单的schema开始，然后根据需要逐渐增加更多的列。通过这种方式，用户可能最终会得到不同但相互兼容的多个Parquet文件。Parquet数据源能够自动发现这种情况，并合并这些文件的schemas。<br>因为合并schema是一个成本相当高的操作，而且在很多情况是不必要的，因此从1.5.0开始，该功能默认是关闭的。你可以通过以下来启用它：</p>
<blockquote>
<p>当你读区Parquet文件时，设置数据源选项 mergeSchema为true（下面的列子将展示）或者<br>设置全局SQL选项 spark.sql.parquet.mergeSchema为true。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.Arrays;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Square</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> value;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> square;</div><div class="line"></div><div class="line">  <span class="comment">// Getters and setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Cube</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> value;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> cube;</div><div class="line"></div><div class="line">  <span class="comment">// Getters and setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line">List&lt;Square&gt; squares = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> value = <span class="number">1</span>; value &lt;= <span class="number">5</span>; value++) &#123;</div><div class="line">  Square square = <span class="keyword">new</span> Square();</div><div class="line">  square.setValue(value);</div><div class="line">  square.setSquare(value * value);</div><div class="line">  squares.add(square);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Create a simple DataFrame, store into a partition directory</span></div><div class="line">Dataset&lt;Row&gt; squaresDF = spark.createDataFrame(squares, Square.class);</div><div class="line">squaresDF.write().parquet(<span class="string">"data/test_table/key=1"</span>);</div><div class="line"></div><div class="line">List&lt;Cube&gt; cubes = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> value = <span class="number">6</span>; value &lt;= <span class="number">10</span>; value++) &#123;</div><div class="line">  Cube cube = <span class="keyword">new</span> Cube();</div><div class="line">  cube.setValue(value);</div><div class="line">  cube.setCube(value * value * value);</div><div class="line">  cubes.add(cube);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Create another DataFrame in a new partition directory,</span></div><div class="line"><span class="comment">// adding a new column and dropping an existing column</span></div><div class="line">Dataset&lt;Row&gt; cubesDF = spark.createDataFrame(cubes, Cube.class);</div><div class="line">cubesDF.write().parquet(<span class="string">"data/test_table/key=2"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Read the partitioned table</span></div><div class="line">Dataset&lt;Row&gt; mergedDF = spark.read().option(<span class="string">"mergeSchema"</span>, <span class="keyword">true</span>).parquet(<span class="string">"data/test_table"</span>);</div><div class="line">mergedDF.printSchema();</div><div class="line"></div><div class="line"><span class="comment">// The final schema consists of all 3 columns in the Parquet files together</span></div><div class="line"><span class="comment">// with the partitioning column appeared in the partition directory paths</span></div><div class="line"><span class="comment">// root</span></div><div class="line"><span class="comment">//  |-- value: int (nullable = true)</span></div><div class="line"><span class="comment">//  |-- square: int (nullable = true)</span></div><div class="line"><span class="comment">//  |-- cube: int (nullable = true)</span></div><div class="line"><span class="comment">//  |-- key: int (nullable = true)</span></div></pre></td></tr></table></figure>
<p>完整示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Hive-metastore-Parquet-table-conversion"><a href="#Hive-metastore-Parquet-table-conversion" class="headerlink" title="Hive metastore Parquet table conversion"></a>Hive metastore Parquet table conversion</h4><p>当我们向Hive metastore Parquet table写数据或从中读数据时，Spark SQL奖尝试使用自己的Parquet支持来代替Hive SerDe以获取更好的性能。这个行为通过spark.sql.hive.converMetastoreParquet来配置，并且默认为打开的。</p>
<h5 id="Hive-Parquet-Schema-Reconciliation"><a href="#Hive-Parquet-Schema-Reconciliation" class="headerlink" title="Hive/Parquet Schema Reconciliation"></a>Hive/Parquet Schema Reconciliation</h5><p>从表schema处理的角度来看，Hive和Parquet有两个主要区别：</p>
<blockquote>
<p>1、Hive是不区分大小写的，而Parquet是区分大小写的。<br>2、Hive认为所有列nullable，而nullable在Parquet中很重要。</p>
</blockquote>
<p>因为上面的原因，当我们将一个Hive metastore Parquet table转换为一个Spark SQL Parquet table时，我们必须将Hive metastore schema与Parquet schema调整一致。调整的规则为：</p>
<blockquote>
<p>1、两个schema中相同名称的字段不管是否为空必须具有相同的数据类型。调整好的字段应当具有Parquet端的数据类型，因此nullable是具有意义的。<br>2、调整后的schema必须包含Hive metastore schema中定义的字段。<br>    1）只出现在Parquet schema中的字段将从调整后的schema中删掉。<br>    2）只出现在Hive metastore schema中的字段将被作为nullable字段添加到调整后的schema中。</p>
</blockquote>
<h5 id="Metadata-Refreshing"><a href="#Metadata-Refreshing" class="headerlink" title="Metadata Refreshing"></a>Metadata Refreshing</h5><p>Spark SQL为了更好的性能而缓存了Parquet metadata。当Hive metastore Parquet表转换启用时，那些被转换的表的metadata也会被缓存。如果这些表被Hive或其他外部工具更新了，你需要手动刷新它们以保证metadata的一致。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// spark is an existing SparkSession</span></div><div class="line">spark.catalog().refreshTable(<span class="string">"my_table"</span>);</div></pre></td></tr></table></figure></p>
<h4 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h4><p>Parquet的配置可以通过两种方式完成，在SparkSession上使用setConf方法或使用SQL运行SET key=value。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.parquet.binaryAsString</td>
<td style="text-align:left">false</td>
<td style="text-align:left">一些其他产生Parquet的系统，主要是Impala、Hive以及老版本的Spark SQL，这些系统在写Parquet schema时不区分二进制数据和字符串。这个标记告诉Spark SQL为这些系统将二进制数据按照字符串来进行兼容。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.int96AsTimestamp</td>
<td style="text-align:left">true</td>
<td style="text-align:left">一些其他产生Parquet的系统，特别是Impala和Hive，它们使用INT96来存储时间戳。这个标记告诉Spark SQL将INT96按照时间戳来解析，以便为那些系统提供兼容。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.compressio.codec</td>
<td style="text-align:left">snappy</td>
<td style="text-align:left">设置写Parquet文件的压缩编码器。如果没有在表详情的选项/属性中指定”compression”或”parquet.compression”。根据优先级排序：compression &gt; parquet.compression &gt; spark.sql.parquet.compression.codec。该选项可以使用的值有：none、uncompressed、snappy、gzip或lzo。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.filterPushdown.</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为True时，启用Parquet过滤器的push-down优化。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.converMetastoreParquet</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为false时，Spark SQL将对parquet table使用Hive SerDe，而不是使用内置支持。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.mergeSchema</td>
<td style="text-align:left">false</td>
<td style="text-align:left">当设置为true时，Parquet数据源合并从所有数据文件收集的schema，如果是false，将从摘要文件中挑选schema，如果没有摘要文件可用，则随机选择一个文件。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.optimizer.metadataOnly.</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为true时，启用metadata-only查询优化，这个优化使用表的metadata来产生分区列，而不是通过对表扫描。当所有扫描过的列示分区列，且查询操作有一个满足distinct语意的聚合操作时，适用。</td>
</tr>
</tbody>
</table>
<h3 id="ORC-Files"><a href="#ORC-Files" class="headerlink" title="ORC Files"></a>ORC Files</h3><p>从Spark 2.3开始，Spark支持向量ORC reader，这个reader使用新的ORC文件格式来读取ORC文件。因此新增了如下配置。当spark.sql.orc.impl被设置为native且spark.sql.orc.enableVectorizedReader被设置为true时，向量读取器将用于读区原生的ORC表（这些表使用USING ORC语句创建）。对于Hive ORC serde表（使用USING HIVE OPTIONS），当spark.sql.hive.convertMetastoreOrc也被设置为true时，向量reader被使用。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.orc.impl</td>
<td style="text-align:left">hive</td>
<td style="text-align:left">ORC实现类的名字。可以是native和hive中的一个。native意味着对构建于Apache ORC 1.4.1上的原生ORC支持。hive意味着对Hive 1.2.1中的ORC库进行支持。</td>
</tr>
<tr>
<td style="text-align:left">spark.slql.orc.enableVectorizedReader</td>
<td style="text-align:left">true</td>
<td style="text-align:left">在native实现中启用向量化orc编码。如果为false，一个新的非向量化ORC reader被用于native实现。对于hive实现，本项可以忽略。</td>
</tr>
</tbody>
</table>
<h3 id="JSON-Datasets"><a href="#JSON-Datasets" class="headerlink" title="JSON Datasets"></a>JSON Datasets</h3><p>Spark SQL能够自动推导一个JSON dataset的schema并将它加载为一个Dataset<row>。这个转换能够在一个Dataset<string>上或一个JSON文件上使用SparkSession.read().json()来完成。<br>注意，提供的json文件不是一个典型的JSON文件。每一行必须是一个独立有效的JSON对象（其实这句话的意思就是，一个json数据必须独立一行，不能跨多行）。关于更多的信息，请查看<a href="http://jsonlines.org/" title="JSON Lines text format, also called newline-delimited JSON" target="_blank" rel="external">JSON Lines text format, also called newline-delimited JSON</a>.<br>要想解析多行JSON文件，需要设置multiLine选项为true。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line"><span class="comment">// A JSON dataset is pointed to by path.</span></div><div class="line"><span class="comment">// The path can be either a single text file or a directory storing text files</span></div><div class="line">Dataset&lt;Row&gt; people = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The inferred schema can be visualized using the printSchema() method</span></div><div class="line">people.printSchema();</div><div class="line"><span class="comment">// root</span></div><div class="line"><span class="comment">//  |-- age: long (nullable = true)</span></div><div class="line"><span class="comment">//  |-- name: string (nullable = true)</span></div><div class="line"></div><div class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></div><div class="line">people.createOrReplaceTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></div><div class="line">Dataset&lt;Row&gt; namesDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>);</div><div class="line">namesDF.show();</div><div class="line"><span class="comment">// +------+</span></div><div class="line"><span class="comment">// |  name|</span></div><div class="line"><span class="comment">// +------+</span></div><div class="line"><span class="comment">// |Justin|</span></div><div class="line"><span class="comment">// +------+</span></div><div class="line"></div><div class="line"><span class="comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span></div><div class="line"><span class="comment">// a Dataset&lt;String&gt; storing one JSON object per string.</span></div><div class="line">List&lt;String&gt; jsonData = Arrays.asList(</div><div class="line">        <span class="string">"&#123;\"name\":\"Yin\",\"address\":&#123;\"city\":\"Columbus\",\"state\":\"Ohio\"&#125;&#125;"</span>);</div><div class="line">Dataset&lt;String&gt; anotherPeopleDataset = spark.createDataset(jsonData, Encoders.STRING());</div><div class="line">Dataset&lt;Row&gt; anotherPeople = spark.read().json(anotherPeopleDataset);</div><div class="line">anotherPeople.show();</div><div class="line"><span class="comment">// +---------------+----+</span></div><div class="line"><span class="comment">// |        address|name|</span></div><div class="line"><span class="comment">// +---------------+----+</span></div><div class="line"><span class="comment">// |[Columbus,Ohio]| Yin|</span></div><div class="line"><span class="comment">// +---------------+----+</span></div></pre></td></tr></table></figure></string></row></p>
<p>完整的示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h3 id="Hive-Tables"><a href="#Hive-Tables" class="headerlink" title="Hive Tables"></a>Hive Tables</h3><p>Spark SQL还支持对Apache Hive读写数据。然而，因为Hive有很多的依赖，而这些依赖默认没有包含在Spark的发布中。如果Hive的依赖能够在classpath中找到，Spark将自动加载它们。注意，这些Hive依赖也必须在所有worker节点上存在，因为它们需要访问Hive的序列化和反序列化库（SerDes）以便访问Hive上存储的数据。<br>Hive的配置是通过替换conf/目录下的hive-site.xml、core-site.xml（安全配置）和hdfs-sit.xml（HDFS配置）来完成的。<br>当使用Hive工作时，必须实例化支持Hive的SparkSession，包括连接到已有的Hive metastore、支持Hive serdes以及Hive自定义函数。即使没有Hive环境也能够启用Hive支持。当没有通过hive-site.xml进行配置时，context自动在当前目录创建metastore_db，并创建一个由spark.sql.warehouse.dir配置指定的目录，默认目录在Spark application启动的当前目录中的spark-warehouse。注意hive-site.xml中的hive.metastore.warehouse.dir属性在Spark2.0.0中废弃了，取而代之是使用spark.sql.warehouse.dir来指定数据库在仓库中的位置。你可以需要为启动Spark appliction的用户开放写权限。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.File;</div><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> key;</div><div class="line">  <span class="keyword">private</span> String value;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getKey</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> key;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setKey</span><span class="params">(<span class="keyword">int</span> key)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.key = key;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getValue</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> value;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setValue</span><span class="params">(String value)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.value = value;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// warehouseLocation points to the default location for managed databases and tables</span></div><div class="line">String warehouseLocation = <span class="keyword">new</span> File(<span class="string">"spark-warehouse"</span>).getAbsolutePath();</div><div class="line">SparkSession spark = SparkSession</div><div class="line">  .builder()</div><div class="line">  .appName(<span class="string">"Java Spark Hive Example"</span>)</div><div class="line">  .config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation)</div><div class="line">  .enableHiveSupport()</div><div class="line">  .getOrCreate();</div><div class="line"></div><div class="line">spark.sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive"</span>);</div><div class="line">spark.sql(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Queries are expressed in HiveQL</span></div><div class="line">spark.sql(<span class="string">"SELECT * FROM src"</span>).show();</div><div class="line"><span class="comment">// +---+-------+</span></div><div class="line"><span class="comment">// |key|  value|</span></div><div class="line"><span class="comment">// +---+-------+</span></div><div class="line"><span class="comment">// |238|val_238|</span></div><div class="line"><span class="comment">// | 86| val_86|</span></div><div class="line"><span class="comment">// |311|val_311|</span></div><div class="line"><span class="comment">// ...</span></div><div class="line"></div><div class="line"><span class="comment">// Aggregation queries are also supported.</span></div><div class="line">spark.sql(<span class="string">"SELECT COUNT(*) FROM src"</span>).show();</div><div class="line"><span class="comment">// +--------+</span></div><div class="line"><span class="comment">// |count(1)|</span></div><div class="line"><span class="comment">// +--------+</span></div><div class="line"><span class="comment">// |    500 |</span></div><div class="line"><span class="comment">// +--------+</span></div><div class="line"></div><div class="line"><span class="comment">// The results of SQL queries are themselves DataFrames and support all normal functions.</span></div><div class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(<span class="string">"SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The items in DataFrames are of type Row, which lets you to access each column by ordinal.</span></div><div class="line">Dataset&lt;String&gt; stringsDS = sqlDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Key: "</span> + row.get(<span class="number">0</span>) + <span class="string">", Value: "</span> + row.get(<span class="number">1</span>),</div><div class="line">    Encoders.STRING());</div><div class="line">stringsDS.show();</div><div class="line"><span class="comment">// +--------------------+</span></div><div class="line"><span class="comment">// |               value|</span></div><div class="line"><span class="comment">// +--------------------+</span></div><div class="line"><span class="comment">// |Key: 0, Value: val_0|</span></div><div class="line"><span class="comment">// |Key: 0, Value: val_0|</span></div><div class="line"><span class="comment">// |Key: 0, Value: val_0|</span></div><div class="line"><span class="comment">// ...</span></div><div class="line"></div><div class="line"><span class="comment">// You can also use DataFrames to create temporary views within a SparkSession.</span></div><div class="line">List&lt;Record&gt; records = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> key = <span class="number">1</span>; key &lt; <span class="number">100</span>; key++) &#123;</div><div class="line">  Record record = <span class="keyword">new</span> Record();</div><div class="line">  record.setKey(key);</div><div class="line">  record.setValue(<span class="string">"val_"</span> + key);</div><div class="line">  records.add(record);</div><div class="line">&#125;</div><div class="line">Dataset&lt;Row&gt; recordsDF = spark.createDataFrame(records, Record.class);</div><div class="line">recordsDF.createOrReplaceTempView(<span class="string">"records"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Queries can then join DataFrames data with data stored in Hive.</span></div><div class="line">spark.sql(<span class="string">"SELECT * FROM records r JOIN src s ON r.key = s.key"</span>).show();</div><div class="line"><span class="comment">// +---+------+---+------+</span></div><div class="line"><span class="comment">// |key| value|key| value|</span></div><div class="line"><span class="comment">// +---+------+---+------+</span></div><div class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></div><div class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></div><div class="line"><span class="comment">// |  4| val_4|  4| val_4|</span></div><div class="line"><span class="comment">// ...</span></div></pre></td></tr></table></figure></p>
<p>完整示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java。</p>
<h4 id="Specifying-storage-format-for-Hive-table"><a href="#Specifying-storage-format-for-Hive-table" class="headerlink" title="Specifying storage format for Hive table"></a>Specifying storage format for Hive table</h4><p>当你创建一个Hive表时，你需要指定这个表应该如何从文件系统读写数据，例如”input format”和”output format”。你还需要定义这个表应该如何将data反序列化为row，或者如何将row序列化为data，如”serde”。下面的选项可以被用来指定存储格式（”serde”、”input format”、”output format”），如：CREATE TABLE src(id int) USING hive OPTIONS(fileFormat ‘parquet’)。默认情况下，我们将以简单文本的格式读取table。值得注意的是，在创建table的时候，存储handler还不被支持，你可以在Hive端使用存储handler来创建一个table，然后使用Spark SQL来读区它。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">fileFormat</td>
<td style="text-align:left">用来说明文件格式的存储格式包，包括”serde”、”input format”和”output format”。当前我们支持6中文件格式：sequencefile、rcfile、orc、parquet、textfile和avro。</td>
</tr>
<tr>
<td style="text-align:left">inputFormat\outputFormat</td>
<td style="text-align:left">这两个选项用来指定”InputFormat“和”OutputFormat“类的名字，例如：org.apache.hadoop.hive.qllio.orc.OrcInputFormat。这两个选项应该成对出现，如果你设置了”fileFormat”选项，那么你不能分别指定它们。</td>
</tr>
<tr>
<td style="text-align:left">serde</td>
<td style="text-align:left">这个选项指定了一个serde类。当设置了‘fileFormat’选项时，如果给定的‘fileFormat’已经包含了serde信息，那么不要设置这个选项。目前，“sequencefile”、“textfile”和“rcfile”不包含serde信息，因此你可以为这3种文件格式设置此选项。</td>
</tr>
<tr>
<td style="text-align:left">fieldDelim, escapeDelim, collectionDelim, mapkeyDelim, lineDelim</td>
<td style="text-align:left">这个选项只能被用于”textfile”的文件格式。它们定义了文件的换行符。</td>
</tr>
</tbody>
</table>
<p>其他属性使用OPTIONS进行定义，将作为Hive serde属性来考虑。</p>
<h4 id="Interacting-with-Different-Versions-of-Hive-Metastore"><a href="#Interacting-with-Different-Versions-of-Hive-Metastore" class="headerlink" title="Interacting with Different Versions of Hive Metastore"></a>Interacting with Different Versions of Hive Metastore</h4><p>Spark SQL的Hive支持的最重要部分是与Hive metastore的交互，它使Spark SQL能够访问Hive表中的metadata。从Spark 1.4.0开始，使用下面描述的配置，Spark有一个独立的包用来访问不同版本的Hive metadata。注意，无论要去访问的metastore的Hive是什么版本，在Spark SQL内部将针对Hive 1.2.1进行编译，并使用这些类作为内部执行（serdes、UDFs、UDAFs等）。<br>下面的选项能够被用来配置获取metadata的Hive的版本：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.hive.metastore.version</td>
<td style="text-align:left">1.2.1</td>
<td style="text-align:left">Hive metadata的版本。可用的选项从0.12.0到1.2.1</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.metastore.jars</td>
<td style="text-align:left">builtin</td>
<td style="text-align:left">被用于实例化HiveMetastoreClient的jar的位置。这个属性有三个选项：<br>1）builtin： 使用Hive 1.2.1，当-Phive被启用时，它与Spark assembly绑定。当这个选择了这个选项，spark.sql.hive.metastore.version必须是1.2.1或为定义。<br> 2) maven：从Maven库中下载指定版本的Hive jars。这个配置对于生产环境通常不推荐。<br> 3）JVM的标准classpath格式。这个classpath必须包含了Hive和它的依赖，以及对应的版本的Hadoop。这些jar只需要存在于driver上，但是如果你实在yarn资源管理器的集群上，那么你必须确保它们和你的application一起被打包。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.metastore. sharedPrefixes</td>
<td style="text-align:left">com.mysql.jdbc,org.postgresql, com.microsoft.sqlserver,oracla.jdbc</td>
<td style="text-align:left">那些需要使用类加载器加载的用于在Spark SQL和指定版本的Hive之间共享的类前缀，类前缀是一个逗号分隔的列表。一个需要被共享的类就是JDBC driver，它需要访问metastore。其他需要共享的类是那些需要与已经共享类交互的类。例如，由log4j使用的自定义appender。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.metastore. barrierPrefixes</td>
<td style="text-align:left">(empty)</td>
<td style="text-align:left">Spark SQL所连接的每个版本的Hive都应明确加载的类的前缀，列表以逗号分隔。例如，通常需要被共享的Hive UDFs在一个前缀中被声明（如，org.apache.spark.*）</td>
</tr>
</tbody>
</table>
<h3 id="JDBC-To-Other-Databases"><a href="#JDBC-To-Other-Databases" class="headerlink" title="JDBC To Other Databases"></a>JDBC To Other Databases</h3><p>Spark SQL还有一个数据源，可以使用JDBC从其他数据库读取数据。这个功能比使用<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.JdbcRDD" title="jdbcRDD" target="_blank" rel="external">jdbcRDD</a>更加受欢迎。这是因为结果是作为一个DataFrame被返回，这样很容易的使用Spark SQL进行处理或与其他数据源相连接。JDBC数据源在Java或Python中使用起来也很容易，因为它不需要用户提供一个ClassTag。（注意，这不同于Spark SQL JDBC Server，Spark SQL JDBC Server允许其他application使用Spark SQL运行查询）<br>你需要在spark classpath中添加对应数据库的JDBC driver。例如，要从Spark shell连接到postgres，你应该运行如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar</div></pre></td></tr></table></figure></p>
<p>使用Data Source API，远程数据库中的表可以被加载为一个DataFrame或Spark SQL临时视图。用户可以在数据源选项中指定JDBC的连接属性。连接通畅需要提供user和password属性，来登陆数据源。除了连接属性外，Spark还支持如下的选项，这些选项忽略大小写：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">url</td>
<td style="text-align:left">进行连接的JDBC URL。特定数据源的连接属性可能会在URL中设置。如：jdbc:postgresql://localhost/test?user=fred&amp;password=secret。</td>
</tr>
<tr>
<td style="text-align:left">dbtable</td>
<td style="text-align:left">要读取的JDBC表。注意，在SQL查询中的From子句中有效的任何东西，都能使用。例如，你可以在括号中使用子查询来代替全表。</td>
</tr>
<tr>
<td style="text-align:left">driver</td>
<td style="text-align:left">连接URL的JDBC driver的类名。</td>
</tr>
<tr>
<td style="text-align:left">partitionColumn, lowerBound, upperBound</td>
<td style="text-align:left">这些选项中的一个被指定，那么所有的都必须被指定。此外，numPartitions必须被指定。它们描述了多个worker并行读取表数据时，应该如何分区。partitionColumn必须是表中的数值列。注意，lowerBound和upperBound仅仅用来决定分区的幅度，而不是过滤表中的行。因此表中的所有行都将被分区并返回。这个选项只能被用于读取。</td>
</tr>
<tr>
<td style="text-align:left">numPartitions</td>
<td style="text-align:left">并行读写表的最大分区数。这也确定了JDBC连接的最大并发。如果写的分区数量超过了这个限制，我们可以在写数据之前调用coalesce(numPartitions)来减少它。</td>
</tr>
<tr>
<td style="text-align:left">fetchsize</td>
<td style="text-align:left">JDBC的提取大小，它确定了每次通信能够取得多少行。它能够帮助提升那些默认fetch size低的JDBC dirver的性能（比如，Orache的fetch size为10）。这个选项只能用于读操作。</td>
</tr>
<tr>
<td style="text-align:left">batch</td>
<td style="text-align:left">JDBC的batch大小，它确定了每次通信能够插入多少行。这能够帮助提升JDBC dirver的性能。这个选项只能用于写操作。默认值为1000。</td>
</tr>
<tr>
<td style="text-align:left">isolationLevel</td>
<td style="text-align:left">事务的隔离级别，应用于当前连接。它可以是：NONE\READ_COMMITTED\ READ_UNCOMMITTED\REPEATABLE_READ\SERIALIZABLE中的一个，通过JDBC连接对象来定义标准事务的隔离级别，默认为READ_UNCOMMITTED。这个选项只能用于写操作。请参考java.sql.Connection文档。</td>
</tr>
<tr>
<td style="text-align:left">sessionInitStatement</td>
<td style="text-align:left">session初始化声明。在到远程数据库的session被打开之后，开始读取数据之前，这个选项执行一个自定义语句（PL/SQL块）。使用这个来实现session的初始化代码。例如：option(“色上司哦那I逆天S塔特闷它”， “”“BEGIN execute immediate ‘alter session set “_serial_direct_read”=true’; END; “””)</td>
</tr>
<tr>
<td style="text-align:left">truncate</td>
<td style="text-align:left">这是一个与JDBC writer相关操作。当启用了SaveMode.Overwrite，这个选项控制删除已存在的表，而不是先drop表然后再创建表。这个更加有效率，并且避免了表的metadata被删除。然而在某些情况下，它无法工作，如新数据有不同的schema。该选项默认值为false。这个选项只用于写操作。</td>
</tr>
<tr>
<td style="text-align:left">createTableOptions</td>
<td style="text-align:left">这是一个与JDBC writer相关的操作。如果设置，该选项允许在创建表的时候设置特定数据库表和分区的选项（如，CREATE TABLE T(name string) ENGINE=InnoDB）。这个选项只能被用于写操作。</td>
</tr>
<tr>
<td style="text-align:left">createTableColumnTypes</td>
<td style="text-align:left">当创建表时，用来代替默认的数据库列类型。数据类型信息使用与CREATE TABLE columns语句（如：”name CHAR(64), comments VARCHAR(1024)”）相同的格式被指定。被指定的数据类型应该是有效的spark sql数据类型。本选项只能用于写操作。</td>
</tr>
<tr>
<td style="text-align:left">customSchema</td>
<td style="text-align:left">自定义schema用于从JDBC连接中读取数据。例如，”id DECIMAL(38, 0), name STRING”。你也可以指定部分字段，其他的时候默认类型映射。例如：”id DECIMAL(38, 0)”。列名称应该与JDBC表的相关列名称一致。用户可以指定Spark SQL的相关数据类型，而不是使用默认的。这个选项只能被用于读操作。</td>
</tr>
</tbody>
</table>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span></div><div class="line"><span class="comment">// Loading data from a JDBC source</span></div><div class="line">Dataset&lt;Row&gt; jdbcDF = spark.read()</div><div class="line">  .format(<span class="string">"jdbc"</span>)</div><div class="line">  .option(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>)</div><div class="line">  .option(<span class="string">"dbtable"</span>, <span class="string">"schema.tablename"</span>)</div><div class="line">  .option(<span class="string">"user"</span>, <span class="string">"username"</span>)</div><div class="line">  .option(<span class="string">"password"</span>, <span class="string">"password"</span>)</div><div class="line">  .load();</div><div class="line"></div><div class="line">Properties connectionProperties = <span class="keyword">new</span> Properties();</div><div class="line">connectionProperties.put(<span class="string">"user"</span>, <span class="string">"username"</span>);</div><div class="line">connectionProperties.put(<span class="string">"password"</span>, <span class="string">"password"</span>);</div><div class="line">Dataset&lt;Row&gt; jdbcDF2 = spark.read()</div><div class="line">  .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</div><div class="line"></div><div class="line"><span class="comment">// Saving data to a JDBC source</span></div><div class="line">jdbcDF.write()</div><div class="line">  .format(<span class="string">"jdbc"</span>)</div><div class="line">  .option(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>)</div><div class="line">  .option(<span class="string">"dbtable"</span>, <span class="string">"schema.tablename"</span>)</div><div class="line">  .option(<span class="string">"user"</span>, <span class="string">"username"</span>)</div><div class="line">  .option(<span class="string">"password"</span>, <span class="string">"password"</span>)</div><div class="line">  .save();</div><div class="line"></div><div class="line">jdbcDF2.write()</div><div class="line">  .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</div><div class="line"></div><div class="line"><span class="comment">// Specifying create table column data types on write</span></div><div class="line">jdbcDF.write()</div><div class="line">  .option(<span class="string">"createTableColumnTypes"</span>, <span class="string">"name CHAR(64), comments VARCHAR(1024)"</span>)</div><div class="line">  .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</div></pre></td></tr></table></figure>
<p>完整示例代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h3 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h3><blockquote>
<p>1、JDBC dirver类对于client session和所有executor的主类加载器是可访问的。这是因为Java的DriverManager会做一个安全检查，当DriverManager要打开一个连接时，检查结果会忽略所有主类加载器无法访问的driver。一个简便的方法是修改所有worker节点的compute_classpath.sh来包含你的driver JAR。<br>2、一些数据库，如H2，需要将所有名字转换为大写。你需要在Spark SQL中使用大写来引用那些名字。</p>
</blockquote>
<h2 id="Performance-Tuning"><a href="#Performance-Tuning" class="headerlink" title="Performance Tuning"></a>Performance Tuning</h2><p>通过将数据缓存到内存或开启一些创新选项，一些工作量是可以优化提升性能的。</p>
<h3 id="Caching-Data-In-Memory"><a href="#Caching-Data-In-Memory" class="headerlink" title="Caching Data In Memory"></a>Caching Data In Memory</h3><p>通过调用spark.catalog.cacheTable(“tableName”)或dataFrame.cache()，Spark能够使用内存中列式格式来缓存表。Spark SQL将扫描需要的列，并自动调整压缩，以达到最小的内存使用和GC压力。你可以使用spark.catalog.uncacheTable(“tableName”)，将table从内存中移除。<br>配置内存缓存可以通过两种方式来实现：在SparkSession上调用setConf方法，或者使用SQL来执行SET key=value命令。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql. inMemoryColumnarStorage.compressed</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为true的时候，Spark SQL将基于数据的统计自动为每一列选择一种压缩编码器。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql. imMemoryColumnarStorage.batchSize</td>
<td style="text-align:left">10000</td>
<td style="text-align:left">控制列式缓存的批量大小。较大的批量size会影响内存会提高内存的利用率和压缩，但是会产生内存溢出的风险。</td>
</tr>
</tbody>
</table>
<h3 id="Other-Configuration-Options"><a href="#Other-Configuration-Options" class="headerlink" title="Other Configuration Options"></a>Other Configuration Options</h3><p>下面的选项也能够被用来提高查询的效率。随着Spark的优化，这些选项在未来可能会被废弃。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.files.maxPartitionbytes</td>
<td style="text-align:left">134217728 (128 MB)</td>
<td style="text-align:left">读取文件时，单个分区的最大字节数。</td>
</tr>
<tr>
<td style="text-align:left">saprk.sql.files.openCostInBytes</td>
<td style="text-align:left">4194304 (4 MB)</td>
<td style="text-align:left">打开一个文件的成本，通过在同一时间能够扫描的字节数来测量。当推送多个文件到一个partition时非常有用。提高这个值会更好，这样写小文件的partition要比写大文件的partition更加快（写小文件的partitin优先调度）。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.broadcastTimeout</td>
<td style="text-align:left">300</td>
<td style="text-align:left">broadcast连接的等待时间，以秒为单位。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.broadcastJoinThreshold</td>
<td style="text-align:left">10485760 (10 MB)</td>
<td style="text-align:left">当执行join操作时，为那些需要广播到所有worker节点的表设置最大字节数。通过设置这个值为-1，广播操作可以被禁用。注意，当前的统计只支持那些运行了ANALYZE TABLE <tablename> COMPUTE STATISTICS命令的Hive Metastore表。</tablename></td>
</tr>
<tr>
<td style="text-align:left">spark.sql.shuffle.partitions</td>
<td style="text-align:left">200</td>
<td style="text-align:left">当为join或aggregation操作而混洗数据时，用来配置使用partitions的数量。</td>
</tr>
</tbody>
</table>
<h3 id="Broadcast-Hint-for-SQL-Queries"><a href="#Broadcast-Hint-for-SQL-Queries" class="headerlink" title="Broadcast Hint for SQL Queries"></a>Broadcast Hint for SQL Queries</h3><p>BROADCAST hint指导Spark在使用其他表或视图join指定表时，如何广播指定表。在Spark决定join方法时，broadcast hash join被优先考虑，即使统计高于spark.sql.autoBroadcastJoinThreshold的配置。当join两边都被指定了，Spark广播具有较低统计的那边。注意Spark不保证BHJ（broadcast hash join）总是被选择，因为不是所有的情况都支持BHJ。当broadcast nested loop join被选择时，我们仍然最重提示。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.spark.sql.functions.broadcast;</div><div class="line">broadcast(spark.table(<span class="string">"src"</span>)).join(spark.table(<span class="string">"records"</span>), <span class="string">"key"</span>).show();</div></pre></td></tr></table></figure></p>
<h2 id="Distributed-SQL-Engine"><a href="#Distributed-SQL-Engine" class="headerlink" title="Distributed SQL Engine"></a>Distributed SQL Engine</h2><p>使用Spark SQL的JDBC/ODBC或command-line interface，Spark SQL也能够具有分布式查询引擎的行为。在这种模式中，终端用户或application能够直接与Spark SQL交互来运行SQL查询，而不需要写任何的代码。</p>
<h3 id="Running-the-Thrift-JDBC-ODBC-server"><a href="#Running-the-Thrift-JDBC-ODBC-server" class="headerlink" title="Running the Thrift JDBC/ODBC server"></a>Running the Thrift JDBC/ODBC server</h3><p>Thrift JDBC/ODBC server实现了相当于Hive 1.2.1中的HiveServer2。你可以使用Spark或Hive1.2.1的beeline脚本来测试JDBC server。<br>要启动JDBC/ODBC server，在Spark目录中运行如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./sbin/start-thriftserver.sh</div></pre></td></tr></table></figure></p>
<p>这个脚本接受所有bin/spark-submit命令的行的参数，并增加了一个–hiveconf选项用来指定Hive属性。你可以执行 ./sbin/start-thriftserver.sh –help来获取完整的可用属性列表。默认，这个server监听的是本地的10000端口。要想重写这个丢昂扣，你可以修改环境变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">export HIVE_SERVER2_THRIFT_PORT=&lt;listening-port&gt;</div><div class="line">export HIVE_SERVER2_THRIFT_BIND_HOST=&lt;listening-host&gt;</div><div class="line">./sbin/start-thriftserver.sh \</div><div class="line">  --master &lt;master-uri&gt; \</div><div class="line">  ...</div></pre></td></tr></table></figure></p>
<p>或者修改系统属性：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">./sbin/start-thriftserver.sh \</div><div class="line">  --hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \</div><div class="line">  --hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \</div><div class="line">  --master &lt;master-uri&gt;</div><div class="line">  ...</div></pre></td></tr></table></figure></p>
<p>现在，你可以使用beeline来测试Thrift JDBC/ODBC server：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/beeline</div></pre></td></tr></table></figure></p>
<p>在beeline中连接JDBC/ODBC server可以使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">beeline&gt; !connect jdbc:hive2://localhost:10000</div></pre></td></tr></table></figure></p>
<p>beeline将会询问你用户名和密码。在非安全模式中，输入你机器的用户名和空白的密码。对于安全模式，请遵循<a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients" title="beeline documentation" target="_blank" rel="external">beeline documentation</a>的指导。</p>
<p>通过替换conf/中hive-site.xml、core-site.mxl和hdfs-site.xml来完成Hive的配置。</p>
<p>你可能还需要使用Hive提供的beeline脚本。</p>
<p>Thrift JDBC server还支持通过HTTP协议发送thrift RPC messages。要启用HTTP模式，可以如下修改系统属性，或者修改conf中的hive-site.xml：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive.server2.transport.mode - Set this to value: http</div><div class="line">hive.server2.thrift.http.port - HTTP port number to listen on; default is 10001</div><div class="line">hive.server2.http.endpoint - HTTP endpoint; default is cliservice</div></pre></td></tr></table></figure></p>
<p>要进行测试，使用beeline以http模式连接到JDBC/ODBC server：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">beeline&gt; !connect jdbc:hive2://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?hive.server2.transport.mode=http;hive.server2.thrift.http.path=&lt;http_endpoint&gt;</div></pre></td></tr></table></figure></p>
<h4 id="Running-the-Spark-SQL-CLI"><a href="#Running-the-Spark-SQL-CLI" class="headerlink" title="Running the Spark SQL CLI"></a>Running the Spark SQL CLI</h4><p>Spark SQL CLI是一个方便的工具用来在本地模式中运行Hive metastore服务并执行来自命令的查询输入。注意，Spark SQL CLI不能与Thrift JDBC server通信，<br>要启动Spark SQL  CLI，在Spark目录中运行如下脚本：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-sql</div></pre></td></tr></table></figure></p>
<p>通过替换conf/中hive-site.xml、core-site.mxl和hdfs-site.xml来完成Hive的配置。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><h3 id="Data-Types"><a href="#Data-Types" class="headerlink" title="Data Types"></a>Data Types</h3><p>Spark SQL和DataFrame支持如下数据类型：</p>
<blockquote>
<p>1、Numeric types<br>        ByteType：声明一个一个字节的有符号的整型。数值范围从-128到127。<br>        ShortType：声明一个两字节的有符号的整型。数值范围从-32768到32767。<br>        IntegerType：声明一个四字节的有符号的整型。数值范围从-2147483648到2147483647。<br>        LongType：声明一个八个字节的有符号的整型。数值范围从-9223372036854775808到9223372036854775807。<br>        FloatType：声明一个四字节的单精度浮点数值。<br>        DoubleType：声明一个八字节的双精度浮点数。<br>        DecimlType：声明一个任意精度的有符号的十进制数值。内部由java.math.BigDecimal支持。一个DecimlType由一个任意精度的不能整型值和一个32位的整型组成。<br>2、Strubg type<br>        声明一个字符串值。<br>3、Binary type<br>        BinaryType：声明一个字节序列值。<br>4、Boolean type<br>        BooleanType：声明一个boolean值。<br>5、Datetime type<br>        TimestampType：声明一个由year、month、day、hour、minute和second字段的值组成。<br>        DateType：声明一个由year、month和day字段的值组成。<br>6、Complex types<br>        ArrayType(elementType, containsNull)：声明一个elementType类型序列。containsNull用来检测ArrayType中是否包含null的值。<br>        MapType(keyType, valueType, valueContainsNull)：由一组key-value对组成。key的数据类型由KeyType来描述，value的数据类型由valueType来描述。对于MapType的一个值，keys不允许为null。valueContainsNull<br>        被用来检测MapTypte的values中是否包含null值。<br>        StructType(fields)：StructFields(fields)序列。<br>                StructField(name, datatype, nullable): StructType类型的字段。字段的名称通过name指定。字段的数据类型通过datatype来指定。nullable用来决定这个fields的values是否可以有null。</p>
</blockquote>
<p>Spark SQL的所有数据类型都位于org.apache.spark.sql.types包中。要访问或创建一种数据类型，请使用org.apache.spark.sql.types.DataTypes中提供的接口方法。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Data type</th>
<th style="text-align:left">Value type in Java</th>
<th style="text-align:left">API to access or create a data type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ByteType</td>
<td style="text-align:left">byte or Byte</td>
<td style="text-align:left">DataTypes.ByteType</td>
</tr>
<tr>
<td style="text-align:left">ShortType</td>
<td style="text-align:left">short or Short</td>
<td style="text-align:left">DataTypes.ShortType</td>
</tr>
<tr>
<td style="text-align:left">IntegerType</td>
<td style="text-align:left">int or Integer</td>
<td style="text-align:left">DataTypes.IntegerType</td>
</tr>
<tr>
<td style="text-align:left">LongType</td>
<td style="text-align:left">long or Long</td>
<td style="text-align:left">DataTypes.LongType</td>
</tr>
<tr>
<td style="text-align:left">FloatType</td>
<td style="text-align:left">float or Float</td>
<td style="text-align:left">DataTypes.FloatType</td>
</tr>
<tr>
<td style="text-align:left">DoubleType</td>
<td style="text-align:left">double or Double</td>
<td style="text-align:left">DataTypes.DoubleType</td>
</tr>
<tr>
<td style="text-align:left">DecimalType</td>
<td style="text-align:left">java.math.BigDecimal</td>
<td style="text-align:left">DataTypes.createDecimalType() DataTypes.createDecimalType(precision, scale)</td>
</tr>
<tr>
<td style="text-align:left">StringType</td>
<td style="text-align:left">String</td>
<td style="text-align:left">DataTypes.StringType</td>
</tr>
<tr>
<td style="text-align:left">BinaryType</td>
<td style="text-align:left">byte[]</td>
<td style="text-align:left">DataTypes.BinaryType</td>
</tr>
<tr>
<td style="text-align:left">BooleanType</td>
<td style="text-align:left">boolean or Boolean</td>
<td style="text-align:left">DataTypes.BooleanType</td>
</tr>
<tr>
<td style="text-align:left">TimestampType</td>
<td style="text-align:left">java.sql.Timestamp</td>
<td style="text-align:left">DataTypes.TimestampType</td>
</tr>
<tr>
<td style="text-align:left">DateType</td>
<td style="text-align:left">java.sql.Date</td>
<td style="text-align:left">DateTypes.DateType</td>
</tr>
<tr>
<td style="text-align:left">ArrayType</td>
<td style="text-align:left">java.util.List</td>
<td style="text-align:left">DataTypes.createArrayType(elementType) 注意：containsNull的值为true。</td>
</tr>
<tr>
<td style="text-align:left">MapType</td>
<td style="text-align:left">java.util.Map</td>
<td style="text-align:left">DataTypes.createMapType(keyType, valueType) 注意，valueContainsNull的值将为true</td>
</tr>
<tr>
<td style="text-align:left">StructType</td>
<td style="text-align:left">org.apache.spark.sql.Row</td>
<td style="text-align:left">DataTypes.createStructType(fields)</td>
</tr>
<tr>
<td style="text-align:left">StructField</td>
<td style="text-align:left">The value type in Java of the data type of this field</td>
<td style="text-align:left">DataTypes.createStructField(name, dataType, nullable)</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/10/spark-2-3-1-QuickStart/" itemprop="url">
                  spark_2.3.1_QuickStart
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-10T15:20:56+08:00" content="2018-08-10">
              2018-08-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h1><p>本指南快速的介绍如何使用Spark。我们将通过Spark的交互式shell（用Python或Scala）首先引入API，然后展示如何用Java、Scala和Python写application。<br>要遵循这个指南，首先需要从Spark的网站上下载Spark包。因为我们不使用HDFS，因此你可以现在任何版本的Hadoop。<br>注意，在Spark 2.0之前，Spark的主要程序接口是Resillent Distributed Dataset(RDD)。在Spark 2.0之后，RDD被Dataset所代替，Dataset类似于RDD的强类型，但是底层有更佳丰富的优化。RDD接口仍然被支持，你可以在<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html" title="RDD programming guide" target="_blank" rel="external">RDD programming guide</a>。然而，我们高度推荐你使用Dataset，它比RDD有更好的性能。查看<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" title="SQL programming guide" target="_blank" rel="external">SQL programming guide</a> 以获取更多关于Dataset的详细信息。</p>
<h2 id="Interactive-Analysis-with-the-Spak-Shell"><a href="#Interactive-Analysis-with-the-Spak-Shell" class="headerlink" title="Interactive Analysis with the Spak Shell"></a>Interactive Analysis with the Spak Shell</h2><h3 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h3><p>Spark的shell提供了简单的方式来学习API，以及一种强大的工具来交互式的分析数据。可以通过Scala（它运行在Java虚拟机上，因此它是学习已有Java库的很好方式）或Python来使用。通过在Spark目录下运行如下脚本来启动：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-shell</div></pre></td></tr></table></figure></p>
<p>Spark的主要抽象是一个名为Dataset的分布式项目（数据条目–一条条的数据）集合。Dataset可以通过Hadoop InputFormates（如HDFS文件）来创建，或者由其他Dataset来转换。我们根据Spark源目录下README文件中的文本来创建一个新的Dataset：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val textFile = spark.read.textFile(&quot;README.md&quot;)</div><div class="line">textFile: org.apache.spark.sql.Dataset[String] = [value: string]</div></pre></td></tr></table></figure></p>
<p>通过调用一些action，你可以直接冲Dataset获取值，或者将这个Dataset转换为另一个新的Dataset。对于更多的细节，请查看<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset &#39;API doc" target="_blank" rel="external">API doc</a>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; textFile.count() // Number of items in this Dataset</div><div class="line">res0: Long = 126 // May be different from yours as README.md will change over time, similar to other outputs</div><div class="line"></div><div class="line">scala&gt; textFile.first() // First item in this Dataset</div><div class="line">res1: String = # Apache Spark</div></pre></td></tr></table></figure></p>
<p>现在，我们将这个Dataset转换为一个新的。我们调用filter，将会返回一个包含文件子集合的新的Dataset。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val linesWithSpark = textFile.filter(line =&gt; line.contains(&quot;Spark&quot;))</div><div class="line">linesWithSpark: org.apache.spark.sql.Dataset[String] = [value: string]</div></pre></td></tr></table></figure></p>
<p>我们可以将转换和action串联在一起：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; textFile.filter(line =&gt; line.contains(&quot;Spark&quot;)).count() // How many lines contain &quot;Spark&quot;?</div><div class="line">res3: Long = 15</div></pre></td></tr></table></figure></p>
<h2 id="More-on-Dataset-Operations"><a href="#More-on-Dataset-Operations" class="headerlink" title="More on Dataset Operations"></a>More on Dataset Operations</h2><p>Dataset的转换和action可以被用于更加复杂的计算。假设我们要找出含有打你最多的一行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; textFile.map(line =&gt; line.split(&quot; &quot;).size).reduce((a, b) =&gt; if (a &gt; b) a else b)</div><div class="line">res4: Long = 15</div></pre></td></tr></table></figure></p>
<p>它首先将一个行映射为一个数值，这创建了一个新的Dataset。reduce在Dataset上被调用，用来找到最大的数。map和reduce的参数是Scala的函数（闭包），也可以使用任何语言的特性或Scala/Java库。例如，我们在任意地方调用函数的声明（引入）。我们将使用Math.max()函数来使代码更加容易理解：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; import java.lang.Math</div><div class="line">import java.lang.Math</div><div class="line"></div><div class="line">scala&gt; textFile.map(line =&gt; line.split(&quot; &quot;).size).reduce((a, b) =&gt; Math.max(a, b))</div><div class="line">res5: Int = 15</div></pre></td></tr></table></figure></p>
<p>一个常用的数据流是MapReduce。Spark能够很轻松的实现MapReduce流：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val wordCounts = textFile.flatMap(line =&gt; line.split(&quot; &quot;)).groupByKey(identity).count()</div><div class="line">wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]</div></pre></td></tr></table></figure></p>
<p>这里，我们调用flatMap将行的Dataset转换为一个单词的Dataset，接着利用groupbyKey和count的组合来计算每个单词在文件中出现的次数(String, Long对)从而生成一个新的Dataset。要在shell中收集单词的数量，我们可以调用collect：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; wordCounts.collect()</div><div class="line">res6: Array[(String, Int)] = Array((means,1), (under,2), (this,3), (Because,1), (Pyhon,2), (agree,1), (cluster.,1), ...)</div></pre></td></tr></table></figure></p>
<h2 id="Caching"><a href="#Caching" class="headerlink" title="Caching"></a>Caching</h2><p>Spark还支持将数据集合缓存到集群端内存缓存中。这在数据被反复访问时非常有用，例如当查询一个非常热门的数据集时，又或是在运行一个类似PageRank这样的迭代算法时。作为一个简单的例子，我们将linesWithSpark数据进行缓存：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">scala&gt; linesWithSpark.cache()</div><div class="line">res7: linesWithSpark.type = [value: string]</div><div class="line"></div><div class="line">scala&gt; linesWithSpark.count()</div><div class="line">res8: Long = 15</div><div class="line"></div><div class="line">scala&gt; linesWithSpark.count()</div><div class="line">res9: Long = 15</div></pre></td></tr></table></figure></p>
<p>使用Spark来分析并缓存一个100行的文本开起来很愚蠢。有意思的是，这些相同的函数可以被用在非常大的数据集上，即使它们跨越数十个甚至数百个节点。你可以通过连接bin/spark-shell到一个集群来进行交互式操作，就像<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#using-the-shell" title="RDD programming guide" target="_blank" rel="external">RDD programming guide</a>中描述的。</p>
<h2 id="Self-Contained-Applications"><a href="#Self-Contained-Applications" class="headerlink" title="Self-Contained Applications"></a>Self-Contained Applications</h2><p>假设我们想要使用Spark API写一个自包含的application。我们将使用Scala(利用sbt)、Java(利用Maven)和Pyton(利用pip)来实现一个简单的application。<br>这里我们将使用Maven来构建一个application JAR，其他类似的构建系统也可以。<br>我们将创建一个非常简单的Spark application，SimpleApp.java<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">/* SimpleApp.java */</div><div class="line">import org.apache.spark.sql.SparkSession;</div><div class="line">import org.apache.spark.sql.Dataset;</div><div class="line"></div><div class="line">public class SimpleApp &#123;</div><div class="line">  public static void main(String[] args) &#123;</div><div class="line">    String logFile = &quot;YOUR_SPARK_HOME/README.md&quot;; // Should be some file on your system</div><div class="line">    SparkSession spark = SparkSession.builder().appName(&quot;Simple Application&quot;).getOrCreate();</div><div class="line">    Dataset&lt;String&gt; logData = spark.read().textFile(logFile).cache();</div><div class="line"></div><div class="line">    long numAs = logData.filter(s -&gt; s.contains(&quot;a&quot;)).count();</div><div class="line">    long numBs = logData.filter(s -&gt; s.contains(&quot;b&quot;)).count();</div><div class="line"></div><div class="line">    System.out.println(&quot;Lines with a: &quot; + numAs + &quot;, lines with b: &quot; + numBs);</div><div class="line"></div><div class="line">    spark.stop();</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这段代码用来计算Spark README文件中包含’a’的行数，和包含’b’的行数。注意你需要将YOUR_SPARK_HOME替换为Spark的安装位置。和之前使用Spark shell不同，Spark shell会初始化它自己的SparkSession，而在代码中初始化SparkSession是程序的一部分。<br>要构建这个程序，我们还需要写一个Maven的pom.xml文件，在这个文件中列出Spark的依赖。注意Spark的依赖和Scala的版本要对应。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&lt;project&gt;</div><div class="line">  &lt;groupId&gt;edu.berkeley&lt;/groupId&gt;</div><div class="line">  &lt;artifactId&gt;simple-project&lt;/artifactId&gt;</div><div class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</div><div class="line">  &lt;name&gt;Simple Project&lt;/name&gt;</div><div class="line">  &lt;packaging&gt;jar&lt;/packaging&gt;</div><div class="line">  &lt;version&gt;1.0&lt;/version&gt;</div><div class="line">  &lt;dependencies&gt;</div><div class="line">    &lt;dependency&gt; &lt;!-- Spark dependency --&gt;</div><div class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">      &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</div><div class="line">      &lt;version&gt;2.3.1&lt;/version&gt;</div><div class="line">    &lt;/dependency&gt;</div><div class="line">  &lt;/dependencies&gt;</div><div class="line">&lt;/project&gt;</div></pre></td></tr></table></figure></p>
<p>我们根据规范列出了Maven的目录结构：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ find .</div><div class="line">./pom.xml</div><div class="line">./src</div><div class="line">./src/main</div><div class="line">./src/main/java</div><div class="line">./src/main/java/SimpleApp.java</div></pre></td></tr></table></figure></p>
<p>现在我们可以使用Maven进行打包，并使用./bin/spark-submit来执行它。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># Package a JAR containing your application</div><div class="line">$ mvn package</div><div class="line">...</div><div class="line">[INFO] Building jar: &#123;..&#125;/&#123;..&#125;/target/simple-project-1.0.jar</div><div class="line"></div><div class="line"># Use spark-submit to run your application</div><div class="line">$ YOUR_SPARK_HOME/bin/spark-submit \</div><div class="line">  --class &quot;SimpleApp&quot; \</div><div class="line">  --master local[4] \</div><div class="line">  target/simple-project-1.0.jar</div><div class="line">...</div><div class="line">Lines with a: 46, Lines with b: 23</div></pre></td></tr></table></figure></p>
<h2 id="Where-to-Go-from-Here"><a href="#Where-to-Go-from-Here" class="headerlink" title="Where to Go from Here"></a>Where to Go from Here</h2><p>恭喜你运行了自己的第一个Spark application！</p>
<blockquote>
<p>对于API的更深了解，可以从<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html" title="RDD programming guide" target="_blank" rel="external">RDD programming guide</a>和<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" title="SQL programming guide" target="_blank" rel="external">SQL programming guide</a>或者查看 ‘Programming Guides’菜单来了解其他组件。<br>想要在集群上运行application，去<a href="http://spark.apache.org/docs/latest/cluster-overview.html" title="deployment overview" target="_blank" rel="external">deployment overview</a>。<br>最后，Spark在examples目录中包含了一些例子（Scala, Java, Python, R）。你可以如下运行它们：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># For Scala and Java, use run-example:</div><div class="line">./bin/run-example SparkPi</div><div class="line"></div><div class="line"># For Python examples, use spark-submit directly:</div><div class="line">./bin/spark-submit examples/src/main/python/pi.py</div><div class="line"></div><div class="line"># For R examples, use spark-submit directly:</div><div class="line">./bin/spark-submit examples/src/main/r/dataframe.R</div></pre></td></tr></table></figure></p>
</blockquote>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/10/spark-2-3-1-MonitoringAndInstrumentation/" itemprop="url">
                  spark_2.3.1_MonitoringAndInstrumentation
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-10T15:20:35+08:00" content="2018-08-10">
              2018-08-10
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/10/spark-2-3-1-ClusterModeOverview/" itemprop="url">
                  Spark 2.3.1 Cluster Mode Overview
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-10T11:17:30+08:00" content="2018-08-10">
              2018-08-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Cluster-Mode-Overview"><a href="#Cluster-Mode-Overview" class="headerlink" title="Cluster Mode Overview"></a>Cluster Mode Overview</h1><p>本文档对Spark如何在集群上运行给出了一个简短的浏览，以便更加容易理解相关组件。通过看application submission guide来学习关于在集群上启动一个applicaiton的信息。</p>
<h2 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h2><p>Spark的application作为一组独立的进程在集群上运行，通过你主程序（被称为driver）中的SparkContext对象来协调合作。<br>具体来说，要在集群上运行application，SparkContext能够连接到某种类型的集群管理器（Spark自己的standalone集群管理器、Mesos或YARN），集群管理器能够跨application分配资源。一旦连接成功，Spark得到集群节点上的executor，这些executor为你的application执行计算以及存储数据。接下来SparkContext发送你的application代码（由传递给你SparkContext的JAR或Python文件定义）到executor。最终，SparkContext发送任务到executor来运行。</p>
<p>此处是图片<img src=""><br>关于这个结构，有一些有用的东西需要注意：</p>
<blockquote>
<p>1、每个application会得到自己的executor进程，这些executor在这个application持续期间保持不变并以多线程运行任务。这样的好处是application彼此隔离，无论是在调度方面（每个driver调度它自己的任务）还是executor方面（来自不同application的任务运行在不同的JVM中）。因此，这也意味着数据在不同的Spark application之间是不能共享的，除非借助其他外部存储。<br>2、Spark与底层集群管理器无关。只要它能够得到executor进程，并且它们能够彼此通信，这样即使是在支持其他application的集群管理器上也能相对简单的运行。<br>3、在driver程序整个生命周期内，它必须监听并接受来自它的executor的连接（查看网路配置章节spark.driver.port）。因此，driver程序对于它的worker节点来说必须是可以迅指的。<br>4、因为driver在集群上调度任务，因此它应该靠近worker节点，最好是在相同的局域网内。如果你想要远程向集群发送请求，最好为你的driver打开一个RPC，让它就近提交，而不是在远离worker节点的地方运行driver。</p>
</blockquote>
<h2 id="Cluster-Manager-Types"><a href="#Cluster-Manager-Types" class="headerlink" title="Cluster Manager Types"></a>Cluster Manager Types</h2><p>系统当前支持3种集群管理器：</p>
<blockquote>
<p>Stangalone - Spark自带的一种集群管理器，使用它能够很容易的构建集群。<br>Apache Mesos - 一个很普遍的集群管理器，它还能够运行Hadoop的MapReduce和服务应用。<br>Hadoop YARN -    Hadoop2种的资源管理器。<br>Kubernetes - 一个开源的系统，用于自动部署、扩展以及管理application。</p>
</blockquote>
<h2 id="Submitting-Applications"><a href="#Submitting-Applications" class="headerlink" title="Submitting Applications"></a>Submitting Applications</h2><p>使用spark-submit脚本可以将application提交到任何类型集群。<a href="/blog/2018/08/09/spark-2-3-1-submit-applications">application submission guide</a>描述了应该如何做。</p>
<h2 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h2><p>每个driver程序都有一个Web UI，端口一般是4040，这个web UI展示了运行的任务、executor已经存储的使用情况。通过在浏览器中输入http://<driver-node>:4040就可以访问这个UI。<a href="http://spark.apache.org/docs/latest/monitoring.html" title="monitoring guide" target="_blank" rel="external">monitoring guide</a>描述了其他的监控项。</driver-node></p>
<h2 id="Job-Scheduling"><a href="#Job-Scheduling" class="headerlink" title="Job Scheduling"></a>Job Scheduling</h2><p>spark给出了两种资源分配，一种是跨applications（在集群管理器级别上），一种是application中（在相同SparkContext上出现多次计算的情况）。<a href="http://spark.apache.org/docs/latest/job-scheduling.html" title="job scheduling overview" target="_blank" rel="external">job scheduling overview</a>描述了详细信息。</p>
<h2 id="Glossary"><a href="#Glossary" class="headerlink" title="Glossary"></a>Glossary</h2><p>下面的表格列出了常用的一些集群概念：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Application</td>
<td style="text-align:left">在Spark上构建的用户程序。由一个driver程序和集群上的executors组成。</td>
</tr>
<tr>
<td style="text-align:left">Application jar</td>
<td style="text-align:left">一个包含了用户的Spark application的jar。在某些情况下用户可能想要创建一个”uber jar”来包含他们的application以及依赖。用户的jar应当不要包含Hadoop或Spark的库，因为这些库会在运行时自动被添加。</td>
</tr>
<tr>
<td style="text-align:left">Driver program</td>
<td style="text-align:left">运行application的main函数并创建SparkContext的进程。</td>
</tr>
<tr>
<td style="text-align:left">Cluster</td>
<td style="text-align:left">一个额外的服务，用来获取集群上的资源（如standalong manager、Mesos或YARN）。</td>
</tr>
<tr>
<td style="text-align:left">Deploy mode</td>
<td style="text-align:left">用来区分在哪里运行driver进程。在“cluster”模式中，系统在集群内部启动driver。在“client”模式中，在集群之外启动driver。</td>
</tr>
<tr>
<td style="text-align:left">Worker node</td>
<td style="text-align:left">集群中任何可以运行application的节点。</td>
</tr>
<tr>
<td style="text-align:left">Executor</td>
<td style="text-align:left">在worker节点上启动的用来处理application的进程，它执行任务并在内存或磁盘上保存数据。每个application都有自己的exectors。</td>
</tr>
<tr>
<td style="text-align:left">Task</td>
<td style="text-align:left">发送给executor的一个工作单元。</td>
</tr>
<tr>
<td style="text-align:left">Job</td>
<td style="text-align:left">由多个tasks组成的一个并行计算，并为一个spark action产生结果（如 save、collect）。 你将会在driver的日志中看到它们。</td>
</tr>
<tr>
<td style="text-align:left">Stage</td>
<td style="text-align:left">每个job被划分为一更小的task，称为stage，这些stage相互依赖（类似MapReduce中map阶段和reduce阶段）。你将会在driver的日志中看到他们。</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/09/spark-2-3-1-submit-applications/" itemprop="url">
                  Spark 2.3.1 Submit Applications
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-09T15:33:36+08:00" content="2018-08-09">
              2018-08-09
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Submitting-Applications"><a href="#Submitting-Applications" class="headerlink" title="Submitting Applications"></a>Submitting Applications</h1><p>Spark的bin目录下的spark-submit脚本用于在一个集群上启动一个应用。通过一个统一的接口，它可以使用所有Spark支持的集群管理器，因此你不需要针对每种集群管理器来单独配置你的应用。</p>
<h2 id="Bunding-Your-Application’s-Dependencies"><a href="#Bunding-Your-Application’s-Dependencies" class="headerlink" title="Bunding Your Application’s Dependencies"></a>Bunding Your Application’s Dependencies</h2><p>如果你的代码依赖其他项目，那么你需要将它们和你的应用一并打包，以便分发代码到一个spark集群。要完成这些，需要创建一个assembly jar(uber jar)来包含你的代码和代码的依赖。sbt和Maven都有assembly插件。当创建assembly jar时，排除Spark和Hadoop提供的依赖，因为这些不需要绑定，因为这些将由集群管理器在运行时提供。一旦你弄好了assembly jar，你就可以如下所示在调用 bin/spark-submit脚本是传递你的jar。<br>对于Python，你可以使用spark-submit的–py-files参数来添加.py、.zip或.egg文件，让他们和你的应用一起分发。如果你依赖多个python文件，我们推荐将他们打到一个.zip或.egg包中。</p>
<h2 id="Launching-Applications-with-spark-submit"><a href="#Launching-Applications-with-spark-submit" class="headerlink" title="Launching Applications with spark-submit"></a>Launching Applications with spark-submit</h2><p>一旦一个用户应用被绑定，就可以使用bin/spark-submit脚本来启动这个应用。这个脚本负责设置Spark的classpath和它依赖，而且脚本支持由Spark支持的不同的集群管理器和部署模式。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit \</div><div class="line">  --class &lt;main-class&gt; \</div><div class="line">  --master &lt;master-url&gt; \</div><div class="line">  --deploy-mode &lt;deploy-mode&gt; \</div><div class="line">  --conf &lt;key&gt;=&lt;value&gt; \</div><div class="line">  ... # other options</div><div class="line">  &lt;application-jar&gt; \</div><div class="line">  [application-arguments]</div></pre></td></tr></table></figure></p>
<p>一些常用的选项：</p>
<blockquote>
<p>–class：你应用的执行程序(如：org.apache.spark.example.SparkPi)<br>–master：集群的master URL（如：spark://23.195.26.187:7077）<br>–deploy-mode：在worker节点(cluster)上部署你的driver还是在本地作为一个额外的客户端(client)来部署。默认是client。<br>–conf：以key=velue格式配置的任意的Spark配置属性。对于包含空格的值，使用双引号包含起来，如”key=value”。<br>application-jar：指向你的应用程序和它依赖的jar的路径。这个URL必须是你集群内部全局可见，例如，一个hdfs://路径或一个在所有节点上都存在的file://路径。<br>application-arguments：任何需要传递给你的主类的主方法的参数。</p>
</blockquote>
<p>常见的部署策略是，在一个与你的worker机位置相同的gateway机器上提交你的应用。在这种设置中，client模式是合适的，driver在spark-submit进程中被直接启动，这种方式像是集群的一个client。这个应用的输入和输出被打印到控制台。因此这种模式特别适合那些涉及REPL的应用。</p>
<p>此外，如果你的应用是用一个远离worker机器的机器上提交的，通常使用cluster模式来降低drivers和executors之间的网络传输。目前，standalone模式还不能够为Python应用提供cluster模式。</p>
<p>对于Python应用，在<application-jar>处传递一个.py来代替一个jar，在–py-files中添加.zip、.egg或.py，作为搜索目录。</application-jar></p>
<p>这里有一些选项可用，用来指定使用的集群管理器。例如，对于cluster部署模式的standalone管理器管理的Saprk集群，你可以指定 –supervise 来保证在非0退出代码时，driver被自动重启。要枚举spark-submit所有可用的选项，使用–help运行spark-submit。这里有些常用的选项：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"># Run application locally on 8 cores</div><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master local[8] \</div><div class="line">  /path/to/examples.jar \</div><div class="line">  100</div><div class="line"></div><div class="line"># Run on a Spark standalone cluster in client deploy mode</div><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master spark://207.184.161.138:7077 \</div><div class="line">  --executor-memory 20G \</div><div class="line">  --total-executor-cores 100 \</div><div class="line">  /path/to/examples.jar \</div><div class="line">  1000</div><div class="line"></div><div class="line"># Run on a Spark standalone cluster in cluster deploy mode with supervise</div><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master spark://207.184.161.138:7077 \</div><div class="line">  --deploy-mode cluster \</div><div class="line">  --supervise \</div><div class="line">  --executor-memory 20G \</div><div class="line">  --total-executor-cores 100 \</div><div class="line">  /path/to/examples.jar \</div><div class="line">  1000</div><div class="line"></div><div class="line"># Run on a YARN cluster</div><div class="line">export HADOOP_CONF_DIR=XXX</div><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master yarn \</div><div class="line">  --deploy-mode cluster \  # can be client for client mode</div><div class="line">  --executor-memory 20G \</div><div class="line">  --num-executors 50 \</div><div class="line">  /path/to/examples.jar \</div><div class="line">  1000</div><div class="line"></div><div class="line"># Run a Python application on a Spark standalone cluster</div><div class="line">./bin/spark-submit \</div><div class="line">  --master spark://207.184.161.138:7077 \</div><div class="line">  examples/src/main/python/pi.py \</div><div class="line">  1000</div><div class="line"></div><div class="line"># Run on a Mesos cluster in cluster deploy mode with supervise</div><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master mesos://207.184.161.138:7077 \</div><div class="line">  --deploy-mode cluster \</div><div class="line">  --supervise \</div><div class="line">  --executor-memory 20G \</div><div class="line">  --total-executor-cores 100 \</div><div class="line">  http://path/to/examples.jar \</div><div class="line">  1000</div><div class="line"></div><div class="line"># Run on a Kubernetes cluster in cluster deploy mode</div><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master k8s://xx.yy.zz.ww:443 \</div><div class="line">  --deploy-mode cluster \</div><div class="line">  --executor-memory 20G \</div><div class="line">  --num-executors 50 \</div><div class="line">  http://path/to/examples.jar \</div><div class="line">  1000</div></pre></td></tr></table></figure></p>
<h2 id="Master-URLs"><a href="#Master-URLs" class="headerlink" title="Master URLs"></a>Master URLs</h2><p>传递给Spark的master URL可以是下面格式中的一个：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Master URL</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">local</td>
<td style="text-align:left">使用单个线程以本地模式运行Spark</td>
</tr>
<tr>
<td style="text-align:left">local[K]</td>
<td style="text-align:left">使用K个线程以本地模式运行Spark</td>
</tr>
<tr>
<td style="text-align:left">local[K, F]</td>
<td style="text-align:left">使用K个线程以本地模式运行Spark，允许最多F个失败</td>
</tr>
<tr>
<td style="text-align:left">local[*]</td>
<td style="text-align:left">使用与你机器逻辑核数相同的线程，以本地模式运行Spark</td>
</tr>
<tr>
<td style="text-align:left">local[*, F]</td>
<td style="text-align:left">使用于你机器逻辑核数相同的香橙，以本地模式运行Spark，允许最多F个失败</td>
</tr>
<tr>
<td style="text-align:left">spark://HOST:PORT</td>
<td style="text-align:left">连接到给定的以standalone模式运行的集群的Master。端口必须是你的master所配置使用的，默认为7077</td>
</tr>
<tr>
<td style="text-align:left">spark://HOST1:PORT1,HOST2:PORT2</td>
<td style="text-align:left">连接到使用了Zookeeper以standalone模式运行的带有standby master的集群。这个列表必须包含了使用Zookeeper配置的高可用集群的所有master的host。端口必须是你的master所配置使用的，默认为7077。</td>
</tr>
<tr>
<td style="text-align:left">mesos://HOST:PORT</td>
<td style="text-align:left">连接到给定的以MESOS模式运行的集群。端口必须是你的配置中使用的，默认为5050。或者，对于使用了Zookeeper的Mesos集群，使用mesos://zk://…配合–deploy-mode cluster来提交，HOST:PORT应该被配置为连接到MesosClusterDispatcher。</td>
</tr>
<tr>
<td style="text-align:left">yarn</td>
<td style="text-align:left">以cluster或client模式连接到yarn集群，连接模式通过 –deploy-mode来指定。这个集群的位置将基于HADOOP_CONF_DIR或YARN_CONF_DIR变量来找到。</td>
</tr>
<tr>
<td style="text-align:left">k8s://HOST:PORT</td>
<td style="text-align:left">以cluster模式连接到Kubernetes集群。Client模式当前还不支持，将会在未来被支持。HOST和PORT指向[Kubernetes API Server]。默认使用TLS连接。想要强制使用不安全的连接，你可以使用k8s://<a href="http://HOST:PORT。" target="_blank" rel="external">http://HOST:PORT。</a></td>
</tr>
</tbody>
</table>
<h2 id="Loading-Configuration-from-a-File"><a href="#Loading-Configuration-from-a-File" class="headerlink" title="Loading Configuration from a File"></a>Loading Configuration from a File</h2><p>spark-submit脚本能够从一个属性文件中加载默认的Spark配置属性值，并传递它们到你的应用。默认它将从<br>Spark目录的conf/spark-defaults.conf中读取选项。<br>加载默认Spark配置，这种方式可以避免给spark-submit设置有确切值的选项（有些选项的值是固定的）。例如，如果设置了spark.master属性，你就可以在spark-submit中忽略–master项了。通常，在SparkConf中设置的值具有最高优先级，其次是传递给spark-submit的值，最后是默认文件里的值。</p>
<p>如果你无法确认配置项的值来自哪里，你可以在运行spark-submit是使用-verbose选项，将细粒度的调试信息打印出来。</p>
<h2 id="Advanced-Dependency-Management"><a href="#Advanced-Dependency-Management" class="headerlink" title="Advanced Dependency Management"></a>Advanced Dependency Management</h2><p>在使用spark-submit的时候，应用程序jar以及使用–jars选项包含的人和jar将会自动传输到集群。–jars后面提供的URLs必须以逗号分隔。那个列表被包含在driver和executor的classpath中。目录范围在–jars中不起作用。<br>Spark使用如下的URL模式来允许不同的策略传递jar：</p>
<blockquote>
<p>file: 绝对路径，并且file:/ URLs由driver的HTTP文件服务提供服务，每个executor从driver的HTTP服务拉取文件。<br>hdfs:、http:、https:、ftp: 这些按照期望的那样从URI拉取文件和Jars。<br>local: 一个以local:/开头的URI，希望作为每个worker节点上的本地文件而存在。这意味着将不会发生网络IO。这种适用于将较大文件或jar推送到每个worker或通过NFS、GlusterFS等共享较大文件或Jar的方式。</p>
</blockquote>
<p>注意，JARs和文件会为每个运行在executor节点上的SparkContext拷贝一份到工作目录。随着时间的推移，这将耗费大量的空间，因此需要清理。对于使用YARN的方式，清理将会自动方式；对于使用standalone方式的，自动清理工作可以通过spark.worker.cleanup.appDataTtl属性配置。</p>
<p>用户还可以通过使用-packages提供以逗号分隔的Maven坐标列表来包含任何其他依赖。使用此命令时，所有传递的依赖都将被处理。另外，使用–repositories选项，还可以用来添加maven库。多个库之间使用逗号分隔。这些命令可以被pyspark、spark-shell以及spark-submit来使用来包含Saprk包。<br>对于Python，–py-files选项可以被用来分发.egg、.zip以及.py文件到executors。</p>
<h2 id="More-Information"><a href="#More-Information" class="headerlink" title="More Information"></a>More Information</h2><p>一旦你部署了你的应用，cluster mode overview 描述了分布式执行中的各个组件，以及如何监控和调试应用。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/09/spark-2-3-1-overview/" itemprop="url">
                  Spark 2.3.1 Overview
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-09T12:45:35+08:00" content="2018-08-09">
              2018-08-09
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Spark-Overview"><a href="#Spark-Overview" class="headerlink" title="Spark Overview"></a>Spark Overview</h1><p>Apache Spark是一个很快的用于一般目的的集群计算系统。它在Java、Python和R语言上提供了高级别的API，并且提供了一个支持一般图计算的优化引擎。它还提供了一组丰富的高级别的工具，包括SQL和结构化数据处理所需要的Spark SQL、机器学习所需要的MLlib、图处理所需要的GraphX以及Spark Streaming。</p>
<h2 id="Downloading"><a href="#Downloading" class="headerlink" title="Downloading"></a>Downloading</h2><p>从项目网站的下载页获取Spark。这个文档为是针对的Spark2.3.1版本。Spark为了使用HDFS和YARN使用了Hadoop客户端库。这个下载中预置了一些常用的Hadoop版本。用户还可以下载一个”Hadoop free”库通过Spark的classpath指定Hadoop版本来运行Spark。Scala和Java用户可以在自己的项目的中使用Spark的Mave依赖来包含Spark，而Python用户在未来也可以从PyPI中安装Spark。</p>
<p>如果你喜欢从源码构建Spark，可以通过这个链接来操作。</p>
<p>Spark能够运行在Windowns和类UNIX的系统上。在一台机器上以本地模式运行很容易–你需要做的事情就是在你的系统路径中安装java或者在环境变量JAVA_HOME中指向Java的安装。</p>
<p>Spark运行在Java 8+， Python 2.7+/3.4+或R 3.1+上。对于Scala API，Spark2.3.1使用的是Scala2.11。你需要使用一个合适Scala版本（Scala2.11+）。</p>
<p>注意，对于Java 7、Python 2.6以及2.6.5以前的Hadoop版本的支持，已经在Spark 2.2.0中移除。对于Scala2.10版本的支持在Spark 2.3.0中移除了。</p>
<h2 id="Running-the-Examples-and-Shell"><a href="#Running-the-Examples-and-Shell" class="headerlink" title="Running the Examples and Shell"></a>Running the Examples and Shell</h2><p>Spark带有一些简单的样例程序。Scala、Java和R的样例都在examples/src/main目录下。想要运行一个Java或Scala样例程序，需要使用顶级Spark目录下bin/run-example <class> [params]。如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/run-example SparkPi 10</div></pre></td></tr></table></figure></class></p>
<p>你还可以通过Scala shell的一个修改版，以交互的方式运行Saprk。这对于学习这个框架是很好的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-shell --master local[2]</div></pre></td></tr></table></figure></p>
<p>其中的–master选项指定了一个分布式集群的master的URL，或者使用一个线程以本地模式运行，或者local[N]表示使用N个线程以本地模式运行。你可以从使用本地模式做测试来开始。对于选项的全部列表，使用使用–help选项来运行Spark shell。<br>Spark还提供了一个Python的API。想要在Python解析器中以交互方式运行Spark，可以使用 bin/pyspark：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/pyspark --master local[2]</div></pre></td></tr></table></figure></p>
<p>样例application也提供了Python版本。如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit examples/src/main/python/pi.py 10</div></pre></td></tr></table></figure></p>
<p>从Spark1.4开始Spark也提供了R API的样例。要以R解析器中以交互方式运行Spark，可以使用 bin/sparkR:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/sparkR --master local[2]</div></pre></td></tr></table></figure></p>
<p>样例程序同样也提供了R语言版本的，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit examples/src/main/r/dataframe.R</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/07/13/HiveStudy/" itemprop="url">
                  Hive Study
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-07-13T15:53:05+08:00" content="2018-07-13">
              2018-07-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">bigdata</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文用来记录自己在使用Hive Sql方面的一些经验。</p>
<h1 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h1><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 创建了一个带有两个分区的表，这个表按照partition_date和hour进行分区</div><div class="line">CREATE EXTERNAL TABLE `user.user_action`(</div><div class="line">     `action` string COMMENT '&#123;"chs_name":"", "description":"","etl":"","value":"","remark":""&#125;',</div><div class="line">     `num` double comment '&#123;"chs_name":"", "description":"","etl":"","value":"","remark":""&#125;'</div><div class="line">  )</div><div class="line">PARTITIONED BY ( `partition_date` string COMMENT '分区日期',  `hour` string COMMENT '小时')</div><div class="line">ROW FORMAT DELIMITED</div><div class="line">    --TODO: 导入MYSQL的表建议'\t'分隔</div><div class="line">    FIELDS TERMINATED BY '\t'</div><div class="line">    COLLECTION ITEMS TERMINATED BY '\002'</div><div class="line">    MAP KEYS TERMINATED BY '\003'</div><div class="line">    LINES TERMINATED BY '\n'</div><div class="line">STORED as textfile;</div></pre></td></tr></table></figure>
<h1 id="查询数据并将数据写入到表中"><a href="#查询数据并将数据写入到表中" class="headerlink" title="查询数据并将数据写入到表中"></a>查询数据并将数据写入到表中</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> user.user_action</div><div class="line"><span class="keyword">partition</span>(partition_date = <span class="string">'20180602'</span>, <span class="keyword">hour</span>=<span class="string">'0'</span>)</div><div class="line"><span class="keyword">select</span> <span class="keyword">action</span>,</div><div class="line">       <span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> n</div><div class="line">  <span class="keyword">from</span> (</div><div class="line">        <span class="keyword">select</span> <span class="keyword">action</span>,</div><div class="line">               <span class="keyword">num</span></div><div class="line">          <span class="keyword">from</span> (</div><div class="line">               <span class="keyword">select</span> momo_id, </div><div class="line">                      event_num_map </div><div class="line">                 <span class="keyword">from</span> db.event_summary </div><div class="line">                <span class="keyword">where</span> partition_date = <span class="string">'20180602'</span></div><div class="line">                  <span class="keyword">and</span> <span class="keyword">size</span>(event_num_map)&gt;<span class="number">0</span></div><div class="line">          )a</div><div class="line">          LATERAL <span class="keyword">VIEW</span> EXPLODE(event_num_map)t <span class="keyword">AS</span> <span class="keyword">action</span>, <span class="keyword">num</span></div><div class="line">    </div><div class="line">  )b</div><div class="line">  <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">action</span>;</div></pre></td></tr></table></figure>
<p>该表从db.event_summary中查询数据然后吸入到user.user_action表中。需要注意db.event_summay中的event_num_map字段是一个map，map的key是action，value是action的数量。这里使用了一个函数LATERAL VIEW EXPLODE，用来map展开。</p>
<h1 id="一些常用函数"><a href="#一些常用函数" class="headerlink" title="一些常用函数"></a>一些常用函数</h1><h2 id="ROW-NUMBER-OVER-函数"><a href="#ROW-NUMBER-OVER-函数" class="headerlink" title="ROW_NUMBER() OVER()函数"></a>ROW_NUMBER() OVER()函数</h2><p>ROW_NUMBER() OVER()函数用来为每条记录返回一个行号，可以用来对记录进行排序并返回该序号，需要从1开始排序。<br>OVER()是一个聚合函数，可以对记录进行分组和排序。ROW_NUMBER()不能单独使用，必须搭配OVER()才能使用，否则会报错。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select *, row_number() over() as r from mytable;</div></pre></td></tr></table></figure></p>
<p>配合partition by/order by<br>按照某个字段排序后返回行号<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select *, row_number() over(partition by aaaaab order by num desc) r from mytable;</div></pre></td></tr></table></figure></p>
<p>按照aaaaab分组后，并根据aaaaaab进行倒序排列。</p>
<h1 id="SQL中的类型转换"><a href="#SQL中的类型转换" class="headerlink" title="SQL中的类型转换"></a>SQL中的类型转换</h1><p>需要使用cast()函数进行类型转换。</p>
<blockquote>
<p>cast(str_column as int) </p>
</blockquote>
<h1 id="一些经验的总结"><a href="#一些经验的总结" class="headerlink" title="一些经验的总结"></a>一些经验的总结</h1><h2 id="一个表中分时段记录内容的统一查询"><a href="#一个表中分时段记录内容的统一查询" class="headerlink" title="一个表中分时段记录内容的统一查询"></a>一个表中分时段记录内容的统一查询</h2><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>遇到的情况是这样的，有一个表A，表A中有24个字段（event_0_map … event_24_map）用来记录对应小时内每个用户各自发生的一些事情的数量。表结构如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">id string = 1000010</div><div class="line">event_0_map = &#123;&apos;event0&apos;:200, &apos;event2&apos;:100&#125;</div><div class="line">...</div><div class="line">event_24_map = &#123;&apos;event0&apos;:500, &apos;event2&apos;:800&#125;</div><div class="line">partition = &apos;20180101&apos;</div></pre></td></tr></table></figure></p>
<p>现在有一个需求：需要统计每个小时发生事件最多的前100个事件</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>因为是每个小时执行的任务，而且每个小时的数据是存放在不同的字段里面，而字段名在SQL中是不可以拼接的，如：event_24_map，无法来拼接，因此有两种方案。</p>
<h4 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h4><p>生成24个任务，每个任务的SQL都一样，只是查询的字段不一样<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">select action,</div><div class="line">       num</div><div class="line">  from (</div><div class="line">       select id, </div><div class="line">              event_0_map </div><div class="line">         from online.tableA </div><div class="line">        where partition = &apos;$&#123;partition_date&#125;&apos;</div><div class="line">          and size(event_0_map)&gt;0</div><div class="line">  )a0</div><div class="line">  LATERAL VIEW EXPLODE(event_0_map)t AS action, num</div></pre></td></tr></table></figure></p>
<h4 id="方案二-推荐"><a href="#方案二-推荐" class="headerlink" title="方案二(推荐)"></a>方案二(推荐)</h4><p>将所有的字段同时解析，生成一个大表，再对大表进行过滤查询<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">select action,</div><div class="line">       num</div><div class="line">  from (</div><div class="line">		select action,</div><div class="line">		       num,</div><div class="line">		       &apos;00&apos; as hour</div><div class="line">		  from (</div><div class="line">		       select id, </div><div class="line">		              event_0_map </div><div class="line">		         from online.tableA </div><div class="line">		        where partition = &apos;$&#123;partition_date&#125;&apos;</div><div class="line">		          and size(event_0_map)&gt;0</div><div class="line">		  )a0</div><div class="line">		  LATERAL VIEW EXPLODE(event_0_map)t AS action, num</div><div class="line">		union all</div><div class="line">		select action,</div><div class="line">		       num,</div><div class="line">		       &apos;01&apos; as hour</div><div class="line">		  from (</div><div class="line">		       select id, </div><div class="line">		              event_2_map </div><div class="line">		         from online.tableA </div><div class="line">		        where partition = &apos;$&#123;partition_date&#125;&apos;</div><div class="line">		          and size(event_2_map)&gt;0</div><div class="line">		  )a1</div><div class="line">		  LATERAL VIEW EXPLODE(event_2_map)t AS action, num</div><div class="line">	) data</div><div class="line"> where hour = &apos;$&#123;partition_hour&#125;&apos;</div></pre></td></tr></table></figure></p>
<h2 id="表的删除和恢复"><a href="#表的删除和恢复" class="headerlink" title="表的删除和恢复"></a>表的删除和恢复</h2><p>在使用Hive的表的过程中，难免会有对表进行删除的情况，其实把表删除后，数据文件还是存在的，那么如何将数据按照新表的结构恢复一下呢？可以如下操作，但是需要注意的是，对于新增的字段，值是NULL。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">drop table db.table_test;</div><div class="line">...</div><div class="line">create table xxx...</div><div class="line">...</div><div class="line">MSCK REPAIR TABLE db.table_test;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/07/10/easyUseMapreduce/" itemprop="url">
                  Easy Use Mapreduce
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-07-10T15:19:55+08:00" content="2018-07-10">
              2018-07-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">bigdata</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文用来记录MR的使用，已经遇到的一些问题和解决方法</p>
<p>#使用Python执行MR</p>
<h2 id="Mapper的写法"><a href="#Mapper的写法" class="headerlink" title="Mapper的写法"></a>Mapper的写法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#! /usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="comment">#加载编码</span></div><div class="line">reload(sys)</div><div class="line">sys.setdefaultencoding(<span class="string">'utf-8'</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">    j = json.loads(line.strip())</div><div class="line">    <span class="keyword">print</span> <span class="string">"%s\t%s"</span> % (j.get(<span class="string">"name"</span>), j.get(<span class="string">"age"</span>))</div></pre></td></tr></table></figure>
<h2 id="Reducer的写法"><a href="#Reducer的写法" class="headerlink" title="Reducer的写法"></a>Reducer的写法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#! /usr/bin/env python</span></div><div class="line">  <span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line">  </div><div class="line">  <span class="keyword">import</span> sys</div><div class="line">  <span class="keyword">import</span> json</div><div class="line">  <span class="keyword">import</span> time</div><div class="line">  </div><div class="line">  <span class="comment">#加载编码</span></div><div class="line">  reload(sys)</div><div class="line">  sys.setdefaultencoding(<span class="string">'utf-8'</span>)</div><div class="line">  </div><div class="line">  uri_count = &#123;&#125;</div><div class="line">  <span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">      data = line.strip().split(<span class="string">"\t"</span>)</div><div class="line">      key = <span class="string">"%s-%s"</span> % (data[<span class="number">0</span>], data[<span class="number">1</span>])</div><div class="line">      c = uri_count.get(key, <span class="number">0</span>)</div><div class="line">      uri_count[key] = c + <span class="number">1</span></div><div class="line">  </div><div class="line">  <span class="keyword">for</span> key <span class="keyword">in</span> uri_count:</div><div class="line">      <span class="keyword">print</span> <span class="string">"%s\t%s"</span> % (key, uri_count.get(key, <span class="number">0</span>))</div></pre></td></tr></table></figure>
<p>从上面的代码可以看出来，python的脚本需要从标准输入(sys.stdin)中接入数据。</p>
<h2 id="执行Mapreduce"><a href="#执行Mapreduce" class="headerlink" title="执行Mapreduce"></a>执行Mapreduce</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">/home/hadoop/yarn-2.8.1/bin/hadoop jar /home/hadoop/yarn-2.8.1/share/hadoop/tools/lib/hadoop-streaming-2.8.1.jar \</div><div class="line">-D mapreduce.job.queuename=bigdata.queue \</div><div class="line">-input hdfs://nameservice1/data/mylogs/api_request/2018/07/03/*/* \</div><div class="line">-output /tmp/20180703SpecialUri \</div><div class="line">-mapper &quot;specialUriMapper.py&quot; \</div><div class="line">-reducer &quot;specialUriReducer.py&quot; \</div><div class="line">-file /home/hadoop/script/user_action/specialUriMapper.py \</div><div class="line">-file /home/hadoop/script/user_action/specialUriReducer.py \</div><div class="line">-file /home/hadoop/script/user_action/kickA.log</div></pre></td></tr></table></figure>
<p>参数说明：</p>
<blockquote>
<p>-D mapreduce.job.queuename用指定需要运行MR的队列<br>-input MR的输入<br>-output MR的输出<br>-mapper 指定执行MR中Mapper的程序<br>-reducer 指定执行MR中Reducer的程序<br>-file 需要一起上传的文件，如果python程序中使用了其他的数据文件，可以通过这个参数一起上传。</p>
</blockquote>
<p>其他一些参数：</p>
<blockquote>
<p>-D mapreduce.job.name Job的名称<br>-D mapreduce.job.user.name<br>-D mapreduce.job.node-label-expression<br>-D mapreduce.job.queuename<br>-D mapreduce.map.memory.mb<br>-D mapreduce.reduce.memory.mb</p>
</blockquote>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/06/29/netty-study/" itemprop="url">
                  netty-study
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-06-29T11:15:01+08:00" content="2018-06-29">
              2018-06-29
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/netty/" itemprop="url" rel="index">
                    <span itemprop="name">netty</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>NETTY学习笔记</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/05/24/kafka-script/" itemprop="url">
                  kafka-script
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-05-24T17:53:42+08:00" content="2017-05-24">
              2017-05-24
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文主要讨论kafka服务的相关启动和关闭脚本。</p>
<h1 id="kafka-server-start-sh"><a href="#kafka-server-start-sh" class="headerlink" title="kafka-server-start.sh"></a>kafka-server-start.sh</h1><p>Kafka服务的启动脚本，正确的用法为 kafka-server-start.sh [-daemon] server.properties<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"># 如果执行脚本时传入的参数小于1个，则退出执行并提示用户需要指定服务属性配置文件， 此处也说明了执行kafka-server-start.sh的正确用法</div><div class="line">if [ $# -lt 1 ];</div><div class="line">then</div><div class="line">	echo &quot;USAGE: $0 [-daemon] server.properties&quot;</div><div class="line">	exit 1</div><div class="line">fi</div><div class="line"></div><div class="line"># $0 表示的是当前shell的文件名，dirname用来获取当前shell文件的所在目录</div><div class="line">base_dir=$(dirname $0)</div><div class="line"></div><div class="line"># 读取环境变量中的KAFKA_LOG4J_OPTS的信息，如果没有配置该环境变量，则将kafka目录下conf中的log4j.properties作为配置添加到环境变量中，配置给KAFKA_LOG4J_OPTS</div><div class="line">if [ &quot;x$KAFKA_LOG4J_OPTS&quot; = &quot;x&quot; ]; then</div><div class="line">    export KAFKA_LOG4J_OPTS=&quot;-Dlog4j.configuration=file:$base_dir/../config/log4j.properties&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 读取环境变量中KAFKA_HEAP_OPTS的信息，如果没有配置该环境变量，则使用默认配置&quot;-Xmx1G -Xms1G&quot;来配置，并添加到环境变量&quot;KAFKA_HEAP_OPTS&quot;中</div><div class="line">if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then</div><div class="line">    export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 定义一个额外的参数 name，为kafka服务指定了进程名</div><div class="line">EXTRA_ARGS=&quot;-name kafkaServer -loggc&quot;</div><div class="line"></div><div class="line"># 如果服务要作为后台进程运行，则需要添加-daemon参数，而且这个参数必须是第一个参数，如果第一个参数是-daemon，则为进程添加自定义的名称</div><div class="line">COMMAND=$1</div><div class="line">case $COMMAND in</div><div class="line">  -daemon)</div><div class="line">    EXTRA_ARGS=&quot;-daemon &quot;$EXTRA_ARGS</div><div class="line">    shift</div><div class="line">    ;;</div><div class="line">  *)</div><div class="line">    ;;</div><div class="line">esac</div><div class="line"></div><div class="line"># 启动kafka服务，由此处也可以看出来，可以使用kafka-run-class.sh来执行相关的类，其中$@表示的是命令行传入的所有参数，这里要启动的类名为kafka.Kafka</div><div class="line">exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka $@</div></pre></td></tr></table></figure></p>
<h1 id="kafka-server-stop-sh"><a href="#kafka-server-stop-sh" class="headerlink" title="kafka-server-stop.sh"></a>kafka-server-stop.sh</h1><p>Kafka服务的停止脚本，其实就是查找KafkaServer对应的进程号，并kill。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 在进程中过滤包含&quot;kafka.Kafka&quot;且不包含&quot;grep&quot;的java进程，截取进程号kill掉</div><div class="line">ps ax | grep -i &apos;kafka\.Kafka&apos; | grep java | grep -v grep | awk &apos;&#123;print $1&#125;&apos; | xargs kill -SIGTERM</div></pre></td></tr></table></figure></p>
<h1 id="kafka-run-class-sh"><a href="#kafka-run-class-sh" class="headerlink" title="kafka-run-class.sh"></a>kafka-run-class.sh</h1><p>kafka-run-class.sh是用来运行class的脚本。正确的用法为 kafka-run-class.sh [-daemon] [-name servicename] [-loggc] classname [opts]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div></pre></td><td class="code"><pre><div class="line"># 验证kafka-run-class脚本的参数</div><div class="line">if [ $# -lt 1 ];</div><div class="line">then</div><div class="line">  echo &quot;USAGE: $0 [-daemon] [-name servicename] [-loggc] classname [opts]&quot;</div><div class="line">  exit 1</div><div class="line">fi</div><div class="line"></div><div class="line"># 获取Kafka的基目录，就是当前目录（bin）的上一层目录</div><div class="line">base_dir=$(dirname $0)/..</div><div class="line"></div><div class="line"># 创建Kafka的日志目录，首先从环境变量“LOG_DIR”中读取，如果没有配置LOG_DIR，则使用Kafka基目录下的logs目录作为日志目录</div><div class="line"># create logs directory</div><div class="line">if [ &quot;x$LOG_DIR&quot; = &quot;x&quot; ]; then</div><div class="line">    LOG_DIR=&quot;$base_dir/logs&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 如果日志目录目存在则创建日志目录</div><div class="line">if [ ! -d &quot;$LOG_DIR&quot; ]; then</div><div class="line">    mkdir -p &quot;$LOG_DIR&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 获取Scala的版本号，首先从环境变量 SCALA_VERSION 中读取，如果没有配置，则使用默认值 2.10.4</div><div class="line">if [ -z &quot;$SCALA_VERSION&quot; ]; then</div><div class="line">	SCALA_VERSION=2.10.4</div><div class="line">fi</div><div class="line"></div><div class="line"># 获取Scala库的版本号，首先从环境变量 SCALA_BINARY_VERSION 中读取，如果没有配置，则使用默认值 2.10</div><div class="line">if [ -z &quot;$SCALA_BINARY_VERSION&quot; ]; then</div><div class="line">	SCALA_BINARY_VERSION=2.10</div><div class="line">fi</div><div class="line"></div><div class="line"># 这里开始加载各种依赖的jar包，并将这些jar包添加到CLASSPATH环境变量中，由此也可以看出运行完整的Kafka服务（支持各种consumer／producer）需要依赖的jar包</div><div class="line"># run ./gradlew copyDependantLibs to get all dependant jars in a local dir</div><div class="line"></div><div class="line"># 将Kafka依赖Scala的jar包添加到CLASSPATH中</div><div class="line">for file in $base_dir/core/build/dependant-libs-$&#123;SCALA_VERSION&#125;*/*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka的示例jar添加到CLASSPATH中</div><div class="line">for file in $base_dir/examples/build/libs//kafka-examples*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将kafka的hadoop consumer相关jar包添加到CLASSPATH中</div><div class="line">for file in $base_dir/contrib/hadoop-consumer/build/libs//kafka-hadoop-consumer*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka的hadoop producer相关jar包添加到CLASSPATH中</div><div class="line">for file in $base_dir/contrib/hadoop-producer/build/libs//kafka-hadoop-producer*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka客户端相关的jar包添加到CLASSPATH中</div><div class="line">for file in $base_dir/clients/build/libs/kafka-clients*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka的libs下的jar包添加到CLASSPATH中</div><div class="line"># classpath addition for release</div><div class="line">for file in $base_dir/libs/*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka依赖的Scala对应版本的库添加到CLASSPATH中</div><div class="line">for file in $base_dir/core/build/libs/kafka_$&#123;SCALA_BINARY_VERSION&#125;*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 以下是Java管理扩展的设置</div><div class="line"># 如果没有在环境变量中设置KAFKA_JMX_OPTS，则将Kafka的JMX配置关闭</div><div class="line"># JMX settings</div><div class="line">if [ -z &quot;$KAFKA_JMX_OPTS&quot; ]; then</div><div class="line">  KAFKA_JMX_OPTS=&quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false  -Dcom.sun.management.jmxremote.ssl=false &quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 如果设置了KAFKA_JMX_OPTS环境变量，则利用这个值来设置变量KAFKA_JMX_OPTS的值，该值用于指定虚拟机的信息</div><div class="line"># JMX port to use</div><div class="line">if [  $JMX_PORT ]; then</div><div class="line">  KAFKA_JMX_OPTS=&quot;$KAFKA_JMX_OPTS -Dcom.sun.management.jmxremote.port=$JMX_PORT &quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># Log4j的配置</div><div class="line"># Log4j settings 如果环境变量中没有设置KAFKA_LOG4J_OPTS，则使用Kafka基目录下conf/tools-log4j.properties来设置KAFKA_LOG4J_OPTS变量</div><div class="line">if [ -z &quot;$KAFKA_LOG4J_OPTS&quot; ]; then</div><div class="line">  KAFKA_LOG4J_OPTS=&quot;-Dlog4j.configuration=file:$base_dir/config/tools-log4j.properties&quot;</div><div class="line">fi</div><div class="line"># 根据环境变量LOG_DIR和KAFKA_LOG4J_OPTS来生成变量KAFKA_LOG4J_OPTS的新的值</div><div class="line">KAFKA_LOG4J_OPTS=&quot;-Dkafka.logs.dir=$LOG_DIR $KAFKA_LOG4J_OPTS&quot;</div><div class="line"></div><div class="line"># 判断环境变量KAFKA_OPTS是否有相关设置</div><div class="line"># Generic jvm settings you want to add</div><div class="line">if [ -z &quot;$KAFKA_OPTS&quot; ]; then</div><div class="line">  KAFKA_OPTS=&quot;&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 判断环境变量JAVA_HOME中是否有值，如果不存在则使用默认的java，如果有，则使用该目录下指定的java</div><div class="line"># Which java to use</div><div class="line">if [ -z &quot;$JAVA_HOME&quot; ]; then</div><div class="line">  JAVA=&quot;java&quot;</div><div class="line">else</div><div class="line">  JAVA=&quot;$JAVA_HOME/bin/java&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># Kafka的内存配置，如果环境变量KAFKA_HEAP_OPTS的值为空，则设置值为默认值-Xmx256M</div><div class="line"># Memory options</div><div class="line">if [ -z &quot;$KAFKA_HEAP_OPTS&quot; ]; then</div><div class="line">  KAFKA_HEAP_OPTS=&quot;-Xmx256M&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 如果没有设置环境变量KAFKA_JVM-PERFORMANCE_OPTS，则使用默认值进行配置</div><div class="line"># JVM performance options</div><div class="line">if [ -z &quot;$KAFKA_JVM_PERFORMANCE_OPTS&quot; ]; then</div><div class="line">  KAFKA_JVM_PERFORMANCE_OPTS=&quot;-server -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -Djava.awt.headless=true&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"></div><div class="line"># 这里对脚本传入的参数进行解析，提取守护进程名／是否后台运行／GC日志这个三个信息</div><div class="line"># 第一个case，如果循环到了-name参数，则读取-name的下一参数，下一个参数必定是后台进程的名字，而且控制台的输出日志文件也是该名字</div><div class="line"># 第二个case，如果循环到了-loggc，则表示要记录GC日志，记录GC日志的另一个要求是配置KAFKA_GC_LOG_OPTS环境变量</div><div class="line"># 第三个case，如果循环到了-daemon，则表示服务以后台进程的方式运行</div><div class="line">while [ $# -gt 0 ]; do</div><div class="line">  COMMAND=$1</div><div class="line">  case $COMMAND in</div><div class="line">    -name)</div><div class="line">      DAEMON_NAME=$2</div><div class="line">      CONSOLE_OUTPUT_FILE=$LOG_DIR/$DAEMON_NAME.out</div><div class="line">      shift 2</div><div class="line">      ;;</div><div class="line">    -loggc)</div><div class="line">      if [ -z &quot;$KAFKA_GC_LOG_OPTS&quot;] ; then</div><div class="line">        GC_LOG_ENABLED=&quot;true&quot;</div><div class="line">      fi</div><div class="line">      shift</div><div class="line">      ;;</div><div class="line">    -daemon)</div><div class="line">      DAEMON_MODE=&quot;true&quot;</div><div class="line">      shift</div><div class="line">      ;;</div><div class="line">    *)</div><div class="line">      break</div><div class="line">      ;;</div><div class="line">  esac</div><div class="line">done</div><div class="line"></div><div class="line"></div><div class="line"># 如果启用了GC日志，GC日志的名字为后台进程的名字[-name指定]-gc.log。</div><div class="line"># GC options</div><div class="line">GC_FILE_SUFFIX=&apos;-gc.log&apos;</div><div class="line">GC_LOG_FILE_NAME=&apos;&apos;</div><div class="line">if [ &quot;x$GC_LOG_ENABLED&quot; = &quot;xtrue&quot; ]; then</div><div class="line">  GC_LOG_FILE_NAME=$DAEMON_NAME$GC_FILE_SUFFIX</div><div class="line">  KAFKA_GC_LOG_OPTS=&quot;-Xloggc:$LOG_DIR/$GC_LOG_FILE_NAME -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps &quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 启动Java进程，将上面的所有信息整合在一起，使用指定的Java，还有各种参数，这里区分了运行模式，其实就是将进程作为后台进程运行还是前台进程运行而已</div><div class="line"># Launch mode</div><div class="line">if [ &quot;x$DAEMON_MODE&quot; = &quot;xtrue&quot; ]; then</div><div class="line">  nohup $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS &quot;$@&quot; &gt; &quot;$CONSOLE_OUTPUT_FILE&quot; 2&gt;&amp;1 &lt; /dev/null &amp;</div><div class="line">else</div><div class="line">  exec $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS &quot;$@&quot;</div><div class="line">fi</div></pre></td></tr></table></figure></p>
<h1 id="kafka-topics-sh"><a href="#kafka-topics-sh" class="headerlink" title="kafka-topics.sh"></a>kafka-topics.sh</h1><p>kafka-topics.sh是用来操作Kafka的Topic的脚本，其内部通过kafka-run-class.sh脚本来调用kafka.admin.TopicCommand来实现Topic的操作。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">exec $(dirname $0)/kafka-run-class.sh kafka.admin.TopicCommand $@</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/03/08/redis-lua/" itemprop="url">
                  redis_lua
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-08T10:32:34+08:00" content="2017-03-08">
              2017-03-08
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/02/23/shell-study/" itemprop="url">
                  Shell Study
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-23T22:31:10+08:00" content="2017-02-23">
              2017-02-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/unix/" itemprop="url" rel="index">
                    <span itemprop="name">unix</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <hr>
<p>本文记录一些自己在使用shell进行批量操作、任务调度等工作时用到的一些shell的基础知识，在此记录以备翻阅和查找。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2017/02/23/shell-study/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/02/08/java-execute-command/" itemprop="url">
                  Java execute command
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-08T11:27:12+08:00" content="2017-02-08">
              2017-02-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/Java/" itemprop="url" rel="index">
                    <span itemprop="name">Java</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>在Java的使用过程中，难免需要去执行linux命令（执行shell也是linux命令），那么应该如何做呢？本文将进行一些演示。</p>
<h1 id="所依赖的相关类"><a href="#所依赖的相关类" class="headerlink" title="所依赖的相关类"></a>所依赖的相关类</h1><p>要在Java中执行linux命令有两种方式，依赖于三个类。我们先介绍这三个类，然后在使用这三个类，组合两种方案来进行说明。</p>
<h2 id="java-lang-Process"><a href="#java-lang-Process" class="headerlink" title="java.lang.Process"></a>java.lang.Process</h2><h3 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h3><p>ProcessBuilder.start()和Runtime.exec方法创建一个本地进程，并返回一个Process子类的实例，该实例用来控制进程并获得相关信息。Process类提供了执行<br>当Process对象没有更多的引用时，不是删除子进程，而是继续异步执行子进程。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2017/02/08/java-execute-command/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/02/04/python-subprocess/" itemprop="url">
                  Python Subprocess
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-04T18:38:47+08:00" content="2017-02-04">
              2017-02-04
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在平时python的使用过程中，难免会遇到调用服务器命令的时候。直接调用普通的命令基本上都没有什么问题，令人比较麻烦的是带有控制台的命令，例如python、beeline、spark-shell。虽然向python、spark都有相关的脚本文件或者jar来避免直接使用控制台命令的调用，然后有些时候还是不免会用到控制台的方式，那么对于带有控制台的命令行应该如何实现呢？本文将使用subprocess，并以beeline为背景来实现使用python执行带有控制台的命令行命令。<br>首先看看参考代码，代码是以执行Hive的beeline命令行为例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">beeline</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"# 1 建立连接"</span></div><div class="line">        self.p = subprocess.Popen([<span class="string">'apache-hive-0.14.0-bin/bin/beeline'</span>], stdin=subprocess.PIPE,</div><div class="line">                             stdout=subprocess.PIPE)</div><div class="line">        <span class="keyword">print</span> &gt;&gt; self.p.stdin, <span class="string">'!connect jdbc:hive2://hdfs001:2181,hdfs002:2181,hdfs003:2181,hdfs004:2181,hdfs005:2181/;serviceDiscoveryMode=zookeeper userName password\n'</span></div><div class="line">        self.p.stdin.flush()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">submit</span><span class="params">(self, hql)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"# 2 输入命令"</span></div><div class="line">        <span class="keyword">print</span> &gt;&gt; self.p.stdin, hql</div><div class="line">        self.p.stdin.flush()</div><div class="line"></div><div class="line">        <span class="keyword">print</span> <span class="string">"# 3 等待关闭"</span></div><div class="line">        <span class="keyword">print</span> &gt;&gt; self.p.stdin, <span class="string">"!q"</span></div><div class="line">        self.p.wait()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hadoop_get</span><span class="params">(self, from_, to_)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"# 4 下载数据"</span></div><div class="line">        (status, output) = commands.getstatusoutput(<span class="string">" "</span>.join((<span class="string">"hadoop-2.6.0/bin/hadoop fs -text"</span>, from_+<span class="string">'*'</span>, <span class="string">'&gt;'</span>, to_)))</div><div class="line">        <span class="keyword">print</span> status, output</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">queryDataByDate</span><span class="params">(start_date, end_date, local_path)</span>:</span></div><div class="line">    sql = <span class="string">"""</span></div><div class="line">    create table database.table_%s_%s</div><div class="line">       ROW FORMAT DELIMITED</div><div class="line">       FIELDS TERMINATED BY '-@-'</div><div class="line">       NULL DEFINED AS '...'</div><div class="line">    STORED AS TEXTFILE</div><div class="line">    AS</div><div class="line">    SELECT * FROM DB.TABLE_NAME;</div><div class="line">"""</div><div class="line">    b = beeline()</div><div class="line">    s = sql % (start_date, end_date, start_date, end_date)</div><div class="line">    b.submit((sql % (start_date, end_date, start_date, end_date)))</div><div class="line"></div><div class="line">    fileName = <span class="string">'feed_%s_%s'</span> % (start_date, end_date)</div><div class="line">    b.hadoop_get((<span class="string">"HDFS_PATH/%s/"</span> % (fileName)), (<span class="string">"LOCAL_PATH/%s"</span> % fileName))</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="keyword">if</span> len(sys.argv) &lt; <span class="number">2</span>:</div><div class="line">        <span class="keyword">print</span> <span class="string">"请输入要获取feed的开始日期和结束日志，如：20160105"</span></div><div class="line">        exit(<span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"正在执行%s文件，来查询%s-%s之间的数据:"</span> % (sys.argv[<span class="number">0</span>], sys.argv[<span class="number">1</span>], sys.argv[<span class="number">2</span>])</div><div class="line">    queryDataByDate(sys.argv[<span class="number">1</span>], sys.argv[<span class="number">2</span>], <span class="string">"/data/"</span>)</div></pre></td></tr></table></figure></p>
<p>该代码块的主要流程是，在初始化beeline对象时调用beeline命令，并进行连接（<strong>init</strong>方法中实现了全部的操作）;然后是提交需要beeline执行的sql（submit方法中实现）;最后是将sql执行的结果从HDFS中取到本地（hadoop_get方法中实现）。queryDataByData方法就是对beeline类中各个方法的一个集成调用。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/02/04/python-datetime/" itemprop="url">
                  Python Datetime
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-04T18:38:47+08:00" content="2017-02-04">
              2017-02-04
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>在使用python写调度任务的时候，离不开的必然有日期和时间的处理；最常见的有根据字符串生成时间、将时间生成指定格式的字符串、日期时间的计算（加减）等等。在python中对日期时间进行操作的包为datetime。下面就对该包的一些常用操作和对应的参数进行介绍。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2017/02/04/python-datetime/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/01/13/doubleArray-trie/" itemprop="url">
                  Double-Array trie
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-01-13T16:01:54+08:00" content="2017-01-13">
              2017-01-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/数据结构/" itemprop="url" rel="index">
                    <span itemprop="name">数据结构</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文主要用来学习Double-Array trie的相关知识。</p>
<p><a href="https://github.com/digitalstain/DoubleArrayTrie" title="Double-Array trie" target="_blank" rel="external">源码的github地址</a></p>
<p>Double-Array trie是Trie结构的压缩形式，仅用两个数组来表示Trie，这个结构有效的结合数字搜索树(Digital Search Tree)检索时间高效的特点和链式表示的Trie空间结构紧凑的特点。双数组Trie的本质是一个确定有限状态自动机(DFA)，每个节点代表自动机的一个状态，根据不同的变量，进行状态转移，当到达结束状态或无法转移时，完成一次查询操作。在双数组所有键中，包含的字符之间的联系都是通过简单的数学加法运算表示的，不仅提高了检索速度，而且省去了链式结构中使用的大量指针，节省了存储空间。</p>
<p>在了解Double-Array trie之前，我们先了解一下“确定有限状态自动机”。在数学理论中，确定有限状态自动机或确定有限自动机（deterministic finite automation, DFA）是一个能实现状态转移的自动机。对于一个给定的属于该自动机的状态和一个属于该自动机字母表的字符，它能够根据实现给定的函数转移到下一个状态。简单的说，就是当前状态根据一个公式和状态的确定值，能够到达另外一个状态，而且要到达的状态是确定的。如图：<br><a href="&quot;确定有限自动机&quot;">确定有限自动状态机</a><br>图中的每个字代表一个状态，并且每个状态都有一个固定的变量。</p>
<p>在了解了确定优先状态自动机之后，我们来了解一下Double-Array trie。Double-Array trie的核心是使用两个整型数组base和check来分别存储状态以及前驱状态。说的简单一些，base用来存储状态，check用来验证。在状态的转移过程中，有如下公式：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">check[t]=s</div><div class="line">base[s]+c=t <span class="comment">//其中t和s是数组下标</span></div></pre></td></tr></table></figure></p>
<p>上面的公式表示 base[s]的值 + 状态的变量 = t下标，check[t]的值 = s下标。</p>
<p>举例来说明：</p>
<p>在学习Douoble-Array trie和看DoubleArrayTrie源码的时候，参考了以下文章，在此表示感谢：<br><a href="http://www.hankcs.com/program/java/%E5%8F%8C%E6%95%B0%E7%BB%84trie%E6%A0%91doublearraytriejava%E5%AE%9E%E7%8E%B0.html" title="双数组Trie树(DoubleArrayTrie)Java实现" target="_blank" rel="external">双数组Trie树(DoubleArrayTrie)Java实现</a><br><a href="http://www.cnblogs.com/zhangchaoyang/articles/4508266.html" title="Double Array Trie" target="_blank" rel="external">Double Array Trie</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/12/30/JPinYin/" itemprop="url">
                  JPinYin
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-12-30T16:28:15+08:00" content="2016-12-30">
              2016-12-30
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文主要介绍JPinYin的使用和配置，<a href="https://github.com/stuxuhai/jpinyin" title="Jpinyin">github的地址</a>。</p>
<h1 id="简单介绍"><a href="#简单介绍" class="headerlink" title="简单介绍"></a>简单介绍</h1><p>Jpinyin是一个开源的用于将汉字转换为拼音的java库。</p>
<h2 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h2><p>1、准确、完善的字库：Unicode编码从4E00-9FA5范围及3007(〇)的20903个汉字中，除了46个异体字（不存在标准拼音）Jpinyin都能转换。<br>2、拼音转换速度快：经测试，转换Unicode编码范围的20902个汉字，Jpinyin耗时约为100毫秒。<br>3、支持多种拼音格式：Jpinyin支持多种拼音输出格式：带声调、不带声调、数字表示声调以及拼音首字母格式输出。<br>4、常见多音字识别：Jpinyin支持常见多音字的识别，其中包括词组、成语、地名等；<br>5、简繁体中文互转。<br>6、支持用户自定义字典。</p>
<h2 id="Maven依赖"><a href="#Maven依赖" class="headerlink" title="Maven依赖"></a>Maven依赖</h2><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">   &lt;groupId&gt;com.github.stuxuhai&lt;/groupId&gt;</div><div class="line">   &lt;artifactId&gt;jpinyin&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;1.1.8&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure>
<h2 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">String str = <span class="string">"你好世界"</span>;</div><div class="line">PinyinHelper.convertToPinyinString(str, <span class="string">","</span>, PinyinFormat.WITH_TONE_MARK); <span class="comment">// nǐ,hǎo,shì,jiè</span></div><div class="line">PinyinHelper.convertToPinyinString(str, <span class="string">","</span>, PinyinFormat.WITH_TONE_NUMBER); <span class="comment">// ni3,hao3,shi4,jie4</span></div><div class="line">PinyinHelper.convertToPinyinString(str, <span class="string">","</span>, PinyinFormat.WITHOUT_TONE); <span class="comment">// ni,hao,shi,jie</span></div><div class="line">PinyinHelper.getShortPinyin(str); <span class="comment">// nhsj</span></div><div class="line">PinyinHelper.addPinyinDict(<span class="string">"user.dict"</span>);  <span class="comment">// 添加用户自定义字典</span></div></pre></td></tr></table></figure>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/12/30/JPinYin/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/12/19/scikitImage-ACrashCourseOnNumPyForImages/" itemprop="url">
                  scikit image - a crash course on NumPy for images
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-12-19T12:43:26+08:00" content="2016-12-19">
              2016-12-19
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/图像处理/" itemprop="url" rel="index">
                    <span itemprop="name">图像处理</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文用来学习scikit-image的官方文档的<a href="http://scikit-image.org/docs/stable/user_guide/numpy_images.html" title="a crash course on NumPy for images">a crash course on NumPy for images</a>，<a href="http://scikit-image.org/docs/stable/user_guide/numpy_images.html" title="a crash course on NumPy for images">原链接</a></p>
<h1 id="A-crash-course-on-NumPy-for-images"><a href="#A-crash-course-on-NumPy-for-images" class="headerlink" title="A crash course on NumPy for images"></a>A crash course on NumPy for images</h1><p>scikit-image是以NumPy数组的方式来操作图像。因此图像很大一部分的操作将是使用NumPy：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> skimage <span class="keyword">import</span> data</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera = data.camera()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>type(camera)</div><div class="line">&lt;type <span class="string">'numpy.ndarray'</span>&gt;</div></pre></td></tr></table></figure></p>
<p>检索图像的几何以及像素数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.shape</div><div class="line">(<span class="number">512</span>, <span class="number">512</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.size</div><div class="line"><span class="number">262144</span></div></pre></td></tr></table></figure></p>
<p>检索关于灰度值的统计信息：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.min(), camera.max()</div><div class="line">(<span class="number">0</span>, <span class="number">255</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.mean()</div><div class="line"><span class="number">118.31400299072266</span></div></pre></td></tr></table></figure></p>
<p>代表图片的NumPy数组可以是浮点数类型的不同整数。查看<a href="http://scikit-image.org/docs/stable/user_guide/data_types.html#data-types" title="Image data type and what the mean">Image data type and what the mean</a>获取关于这些类型的更多信息，以及scikit-image如何处理它们。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/12/19/scikitImage-ACrashCourseOnNumPyForImages/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/12/16/scikitImage-gettingStarted/" itemprop="url">
                  scikit image - getting started
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-12-16T16:42:31+08:00" content="2016-12-16">
              2016-12-16
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/图像处理/" itemprop="url" rel="index">
                    <span itemprop="name">图像处理</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文用来学习scikit-image的官方文档的入门手册，<a href="http://scikit-image.org/docs/stable/user_guide/getting_started.html" title="Getting started" target="_blank" rel="external">原链接</a></p>
<h1 id="Getting-started"><a href="#Getting-started" class="headerlink" title="Getting started"></a>Getting started</h1><p>scikit-image是一个图像处理的Python包，它使用numpy数组来工作。这个包作为skimage被引入：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> skimage</div></pre></td></tr></table></figure></p>
<p>skimage的大多数函数将在子模块中找到：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> skimage <span class="keyword">import</span> data</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera = data.camera()</div></pre></td></tr></table></figure></p>
<p>一个包含子模块和函数的web页面可以在<a href="http://scikit-image.org/docs/stable/api/api.html" title="API Reference" target="_blank" rel="external">API reference</a>中找到。<br>在scikit-image中，图片相当于一个NumPy数组，例如，一个2-D的数组表示了一个灰度的2-D图片<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>type(camera)</div><div class="line">&lt;type <span class="string">'numpy.ndarray'</span>&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># An image with 512 rows and 512 columns</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.shape</div><div class="line">(<span class="number">512</span>, <span class="number">512</span>)</div></pre></td></tr></table></figure></p>
<p>skimage.data模块提供了一组返回示例图片的函数，这些图片可以用来快速学习scikit-image的函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>coins = data.coins()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> skimage <span class="keyword">import</span> filters</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>threshold_value = filters.threshold_otsu(coins)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>threshold_value</div><div class="line"><span class="number">107</span></div></pre></td></tr></table></figure></p>
<p>当然，还可以使用skimage.io.imread()从图片文件来加载自己的图片信息，加载后的图片也是作为一个NumPy数组：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> os</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>filename = os.path.join(skimage.data_dir, <span class="string">'moon.png'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> skimage <span class="keyword">import</span> io</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>moon = io.imread(filename)</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/11/10/log4j-configuration/" itemprop="url">
                  Log4j Configuration
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-11-10T15:25:48+08:00" content="2016-11-10">
              2016-11-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/Logging/" itemprop="url" rel="index">
                    <span itemprop="name">Logging</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文用来学习关于Log4j的配置（通过配置文件的方式），<a href="http://logging.apache.org/log4j/2.x/manual/configuration.html" title="Configuration">原文档连接</a></p>
<h1 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h1><p>将日志请求插入到application代码中需要相当多的计划和努力。观察表明，大约百分之四的代码用于记录日志。因此，即使是一般大小的application也将会有数以千计的日志片段嵌套在代码中。考虑到这个数量，如何不需要手动修改就能管理这些日志片段就显得十分重要。<br>配置Log4j 2版本，能够通过下面四种方法中的任意一种来完成：<br>1、通过一个以XML、JSON、YAML或properties格式的配置文件。<br>2、编程方式，通过创建一个ConfigurationFactory和Configuration实现。<br>3、编程方式，通过调用在Configuration接口中的APIs来添加组件到默认配置。<br>4、编程方式，通过调用内部Logger类的方法。<br>本文主要关注通过一个配置文件来配置Log4j。对于通过编程方式来配置Log4j，可以在<a href="http://logging.apache.org/log4j/2.x/manual/extending.html" title="Extending Log4j 2">Extending Log4j 2</a>和<a href="http://logging.apache.org/log4j/2.x/manual/customconfig.html" title="Programmatic Log4j Configuration">Programmatic Log4j Configuration</a>。<br>注意，不同于Log4j 1.x，Log4j 2的公共API没有公开关于添加、修改和移除appenders和filter的方法，或者以任何方式来操作配置。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/11/10/log4j-configuration/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/blog/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/blog/">1</a><span class="page-number current">2</span><a class="page-number" href="/blog/page/3/">3</a><a class="extend next" rel="next" href="/blog/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/blog/uploads/avatar.png"
               alt="baimoon" />
          <p class="site-author-name" itemprop="name">baimoon</p>
          <p class="site-description motion-element" itemprop="description">Baimoon's blog</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/blog/archives">
              <span class="site-state-item-count">56</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/blog/categories">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/blog/tags">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/baimoon" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://gallery.xrange.org" title="xrange" target="_blank">xrange</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016-07 - 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">baimoon</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/blog/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/blog/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/blog/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/blog/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
