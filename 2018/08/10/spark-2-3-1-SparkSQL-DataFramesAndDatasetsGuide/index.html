<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/blog/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/blog/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/blog/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="spark," />








  <link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico?v=5.0.1" />






<meta name="description" content="Spark SQL, DataFrames and Dataset GuideOverviewSpark SQL是一个用于结构化数据处理的Spark模块。与Spark RDD API不同，由Spark SQL提供的这些接口在结构化数据和结构化计算执行方面提供了更多信息。在内部，Spark SQL使用了这个额外信息来执行额外的优化。有几种与Spark SQL交互的方法，包括SQL和Dataset A">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark 2.3.1 Spark SQL DataFrames and DatasetsGuide">
<meta property="og:url" content="http://baimoon.github.io/2018/08/10/spark-2-3-1-SparkSQL-DataFramesAndDatasetsGuide/index.html">
<meta property="og:site_name" content="Baimoon's Note">
<meta property="og:description" content="Spark SQL, DataFrames and Dataset GuideOverviewSpark SQL是一个用于结构化数据处理的Spark模块。与Spark RDD API不同，由Spark SQL提供的这些接口在结构化数据和结构化计算执行方面提供了更多信息。在内部，Spark SQL使用了这个额外信息来执行额外的优化。有几种与Spark SQL交互的方法，包括SQL和Dataset A">
<meta property="og:updated_time" content="2018-08-17T07:29:21.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark 2.3.1 Spark SQL DataFrames and DatasetsGuide">
<meta name="twitter:description" content="Spark SQL, DataFrames and Dataset GuideOverviewSpark SQL是一个用于结构化数据处理的Spark模块。与Spark RDD API不同，由Spark SQL提供的这些接口在结构化数据和结构化计算执行方面提供了更多信息。在内部，Spark SQL使用了这个额外信息来执行额外的优化。有几种与Spark SQL交互的方法，包括SQL和Dataset A">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://baimoon.github.io/2018/08/10/spark-2-3-1-SparkSQL-DataFramesAndDatasetsGuide/"/>

  <title> Spark 2.3.1 Spark SQL DataFrames and DatasetsGuide | Baimoon's Note </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/blog/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Baimoon's Note</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/blog/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/blog/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Spark 2.3.1 Spark SQL DataFrames and DatasetsGuide
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-10T15:21:30+08:00" content="2018-08-10">
              2018-08-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Spark-SQL-DataFrames-and-Dataset-Guide"><a href="#Spark-SQL-DataFrames-and-Dataset-Guide" class="headerlink" title="Spark SQL, DataFrames and Dataset Guide"></a>Spark SQL, DataFrames and Dataset Guide</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Spark SQL是一个用于结构化数据处理的Spark模块。与Spark RDD API不同，由Spark SQL提供的这些接口在结构化数据和结构化计算执行方面提供了更多信息。在内部，Spark SQL使用了这个额外信息来执行额外的优化。有几种与Spark SQL交互的方法，包括SQL和Dataset API。当计算一个结果时，相同的计算引擎会被使用，与你执行计算使用的API／语言无关。这种统一意味着开发者能够轻松在那些提供更加原始的方式处理给定转换的不同API之间进行来回切换。<br>本篇中所有例子使用的样例数据包含在Spark中，并能够使用spark-shell、pyspark shell或sparkR shell来运行。</p>
<h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p>Spark SQL的一种用法时执行SQL查询。Spark SQL还能够被用来从Hive实例中读取数据。关于如何配置这个特性，请参考<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables" title="Hive Tables " target="_blank" rel="external">Hive Tables</a>。当在另一种编程语言中执行SQL时，结果会作为一个Dataset/DataFrame来返回。你还能够使用<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-spark-sql-cli" title="command-line" target="_blank" rel="external">command-line</a>或<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server" title="JDBC/ODBC" target="_blank" rel="external">JDBC/ODBC</a>的方式与SQL接口进行交互。</p>
<h3 id="Datasets-and-DataFrames"><a href="#Datasets-and-DataFrames" class="headerlink" title="Datasets and DataFrames"></a>Datasets and DataFrames</h3><p>一个Dataset就是一个分布式数据集。Dataset作为一个新接口在Spark 1.6中被添加，它提供了RDD的优点（强类型、能够使用强大的lambda函数）和Spark SQL的优化执行引擎的有点。一个Dataset能够根据JVM对象来构造，然后使用函数转换（map、flatMap、filter）进行操作。Dataset的API在Scala和Java中时可用的。Python还不支持Dataset API。但是因为Python的动态特性，Dataset API的很多优点已经可用了（例如你可以很自然的通过名称来访问某一行的一个字段 row.columnName）。对于R语言也是如此。<br>一个DataFrame是一个带有列名的数据集。它在概念上等同于关系数据库中的一个表或者一个是R语言或Python语言中data frame，但是底层具更加优化。DataFrame可以根据各种资源进行构建，例如：结构化的数据文件、Hive中的表、外部数据库以及已经存在的RDD。DataFrame API在Scala、Java、Python和R语言中都可用。在Scala和Java中，一个DataFrame相当于一个有很多行的Dataset。在<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" title="Scala API" target="_blank" rel="external">Scala API</a>中，DataFrame相当于一个Dataset[Row]类型。而在<a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html" title="Java API" target="_blank" rel="external">Java API</a>中，用户需要使用Dataset<row>来表述一个DataFrame。<br>在本文中，我们将经常引用Scala/Java由有Row组成的Dataset来表述DataFrame。</row></p>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><h3 id="Starting-Point-SparkSession"><a href="#Starting-Point-SparkSession" class="headerlink" title="Starting Point: SparkSession"></a>Starting Point: SparkSession</h3><p>Spark中，所有功能的切入点是<a href="http://spark.apache.org/docs/latest/api/java/index.html#org.apache.spark.sql.SparkSession" title="SparkSession" target="_blank" rel="external">SparkSession</a>类。要创建一个基本的SparkSession，只需要使用SparkSession.builder()<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"></div><div class="line">SparkSession spark = SparkSession</div><div class="line">  .builder()</div><div class="line">  .appName(<span class="string">"Java Spark SQL basic example"</span>)</div><div class="line">  .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</div><div class="line">  .getOrCreate();</div></pre></td></tr></table></figure></p>
<p>在Spark库的“examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java”目录下，查看完整的示例代码。<br>SparkSession是Spark 2.0的内置功能，用于提供Hive特性，包括用来写HiveQL查询、<br>访问Hive UDFs已经从Hive表中读取数据。要使用这些特性，你不需要配置Hive。</p>
<h3 id="Creating-DataFrames"><a href="#Creating-DataFrames" class="headerlink" title="Creating DataFrames"></a>Creating DataFrames</h3><p>使用SparkSession，application能够从一个已经存在的RDD、一个Hive表或<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources" title="Spark data sources" target="_blank" rel="external">Spark data sources</a>来创建DataFrame。<br>作为一个例子，下面的代码根据一个JSON文件中的内容来创建一个DataFrame：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></div><div class="line">df.show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看Spark库中的“examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java”。</p>
<h3 id="Untyped-Dataset-Operations-aka-DataFrame-Operations"><a href="#Untyped-Dataset-Operations-aka-DataFrame-Operations" class="headerlink" title="Untyped Dataset Operations(aka DataFrame Operations)"></a>Untyped Dataset Operations(aka DataFrame Operations)</h3><p>在Scala、Java、Python和R语言中，DataFrames针对不同的语言提供不同的结构化数据操作。正如上面提到的，在Spark2.0中，在Scala和Java的API中，DataFrames是以Dataset<row>来表述的。这些操作也被称为“无类型转换”，与强类型转换的Scala/Java Dataset的类型形成对比。<br>这里，我们展示了使用Dataset进行结构化数据处理的基本示例：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// col("...") is preferable to df.col("...")</span></div><div class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.spark.sql.functions.col;</div><div class="line"></div><div class="line"><span class="comment">// Print the schema in a tree format</span></div><div class="line">df.printSchema();</div><div class="line"><span class="comment">// root</span></div><div class="line"><span class="comment">// |-- age: long (nullable = true)</span></div><div class="line"><span class="comment">// |-- name: string (nullable = true)</span></div><div class="line"></div><div class="line"><span class="comment">// Select only the "name" column</span></div><div class="line">df.select(<span class="string">"name"</span>).show();</div><div class="line"><span class="comment">// +-------+</span></div><div class="line"><span class="comment">// |   name|</span></div><div class="line"><span class="comment">// +-------+</span></div><div class="line"><span class="comment">// |Michael|</span></div><div class="line"><span class="comment">// |   Andy|</span></div><div class="line"><span class="comment">// | Justin|</span></div><div class="line"><span class="comment">// +-------+</span></div><div class="line"></div><div class="line"><span class="comment">// Select everybody, but increment the age by 1</span></div><div class="line">df.select(col(<span class="string">"name"</span>), col(<span class="string">"age"</span>).plus(<span class="number">1</span>)).show();</div><div class="line"><span class="comment">// +-------+---------+</span></div><div class="line"><span class="comment">// |   name|(age + 1)|</span></div><div class="line"><span class="comment">// +-------+---------+</span></div><div class="line"><span class="comment">// |Michael|     null|</span></div><div class="line"><span class="comment">// |   Andy|       31|</span></div><div class="line"><span class="comment">// | Justin|       20|</span></div><div class="line"><span class="comment">// +-------+---------+</span></div><div class="line"></div><div class="line"><span class="comment">// Select people older than 21</span></div><div class="line">df.filter(col(<span class="string">"age"</span>).gt(<span class="number">21</span>)).show();</div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// |age|name|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// | 30|Andy|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"></div><div class="line"><span class="comment">// Count people by age</span></div><div class="line">df.groupBy(<span class="string">"age"</span>).count().show();</div><div class="line"><span class="comment">// +----+-----+</span></div><div class="line"><span class="comment">// | age|count|</span></div><div class="line"><span class="comment">// +----+-----+</span></div><div class="line"><span class="comment">// |  19|    1|</span></div><div class="line"><span class="comment">// |null|    1|</span></div><div class="line"><span class="comment">// |  30|    1|</span></div><div class="line"><span class="comment">// +----+-----+</span></div></pre></td></tr></table></figure></row></p>
<p>完整的样例代码，查看Spark库的examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java。<br>在Dataset上能够执行的操作类型列表，可以查看<a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html" title="API Document" target="_blank" rel="external">API Document</a>。<br>除了简单的列引用和计算外，Dataset还有一个丰富的函数库，包括字符串的操作、日期的计算以及常用的数学操作等。完整的列表可以在<a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html" title="DataFrame Function Reference" target="_blank" rel="external">DataFrame Function Reference</a>找到。</p>
<h3 id="Running-SQL-Queries-Programmatically"><a href="#Running-SQL-Queries-Programmatically" class="headerlink" title="Running SQL Queries Programmatically"></a>Running SQL Queries Programmatically</h3><p>SparkSession上的sql函数使application能够执行SQL查询，并返回一个Dataset<row>作为结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">import org.apache.spark.sql.Dataset;</div><div class="line">import org.apache.spark.sql.Row;</div><div class="line"></div><div class="line">// Register the DataFrame as a SQL temporary view</div><div class="line">df.createOrReplaceTempView(&quot;people&quot;);</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(&quot;SELECT * FROM people&quot;);</div><div class="line">sqlDF.show();</div><div class="line">// +----+-------+</div><div class="line">// | age|   name|</div><div class="line">// +----+-------+</div><div class="line">// |null|Michael|</div><div class="line">// |  30|   Andy|</div><div class="line">// |  19| Justin|</div><div class="line">// +----+-------+</div></pre></td></tr></table></figure></row></p>
<p>完整的代码，请查看Spark库中的 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h3 id="Global-Temporary-View"><a href="#Global-Temporary-View" class="headerlink" title="Global Temporary View"></a>Global Temporary View</h3><p>在Spark SQL中，临时视图是session范围的，将会伴随着创建它的那个session的终止而消失。如果你想要跨session共享一个临时视图，并让它存活到application终止，你可以创建一个全局临时视图。全局视图与一个名为‘global_temp’的由系统保护的数据库进行绑定，我们必须使用这个特殊的名字来引用它，如：SELECT * FROM global_temp.view1。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Register the DataFrame as a global temporary view</span></div><div class="line">df.createGlobalTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Global temporary view is tied to a system preserved database `global_temp`</span></div><div class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"></div><div class="line"><span class="comment">// Global temporary view is cross-session</span></div><div class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看“examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java”。</p>
<h3 id="Creating-Datasets"><a href="#Creating-Datasets" class="headerlink" title="Creating Datasets"></a>Creating Datasets</h3><p>Dataset与RDD类似，不同的是它没有使用Java序列化或Kryo，它们使用了一个特殊的<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder" title="Encoder" target="_blank" rel="external">Encoder</a>来序列化对象，以便这些对象的处理或跨网络传输。虽然encoder和标准序列化器都能够将一个对象转换为字节，encoder是动态编码产生的，并且使用一种格式来允许Spark执行很多操作(filtering， sorting 和 hashing)，而不需要讲字节反编译为对象。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.Arrays;</div><div class="line"><span class="keyword">import</span> java.util.Collections;</div><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> String name;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> age;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> name;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.name = name;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> age;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.age = age;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Create an instance of a Bean class</span></div><div class="line">Person person = <span class="keyword">new</span> Person();</div><div class="line">person.setName(<span class="string">"Andy"</span>);</div><div class="line">person.setAge(<span class="number">32</span>);</div><div class="line"></div><div class="line"><span class="comment">// Encoders are created for Java beans</span></div><div class="line">Encoder&lt;Person&gt; personEncoder = Encoders.bean(Person.class);</div><div class="line">Dataset&lt;Person&gt; javaBeanDS = spark.createDataset(</div><div class="line">  Collections.singletonList(person),</div><div class="line">  personEncoder</div><div class="line">);</div><div class="line">javaBeanDS.show();</div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// |age|name|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// | 32|Andy|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"></div><div class="line"><span class="comment">// Encoders for most common types are provided in class Encoders</span></div><div class="line">Encoder&lt;Integer&gt; integerEncoder = Encoders.INT();</div><div class="line">Dataset&lt;Integer&gt; primitiveDS = spark.createDataset(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), integerEncoder);</div><div class="line">Dataset&lt;Integer&gt; transformedDS = primitiveDS.map(</div><div class="line">    (MapFunction&lt;Integer, Integer&gt;) value -&gt; value + <span class="number">1</span>,</div><div class="line">    integerEncoder);</div><div class="line">transformedDS.collect(); <span class="comment">// Returns [2, 3, 4]</span></div><div class="line"></div><div class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping based on name</span></div><div class="line">String path = <span class="string">"examples/src/main/resources/people.json"</span>;</div><div class="line">Dataset&lt;Person&gt; peopleDS = spark.read().json(path).as(personEncoder);</div><div class="line">peopleDS.show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div></pre></td></tr></table></figure></p>
<p>完整的示例，请查看 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h3 id="Interoperating-with-RDDs"><a href="#Interoperating-with-RDDs" class="headerlink" title="Interoperating with RDDs"></a>Interoperating with RDDs</h3><p>Spark SQL支持两种不同方法来将存在的RDD转换为Dataset。第一种方法是使用反射来推导包含特殊类型对象的RDD的模式。这种反射的方法代码更加简单，而且如果在你写Spark application时已经知道了模式时，工作的会很好。<br>第二种方法是通过一个程序接口来创建Dataset，这个程序接口允许你构建一个模式，并且将它应用到一个已经存在的RDD上。但是这个方法比较冗长，它允许你只有在运行时才知道列和列类型时来构造Dataset。</p>
<h4 id="Inferring-the-Schema-Using-Reflection"><a href="#Inferring-the-Schema-Using-Reflection" class="headerlink" title="Inferring the Schema Using Reflection"></a>Inferring the Schema Using Reflection</h4><p>Spark SQL支持自动将一个JavaBean的RDD转换为一个DataFrame。BeanInfo使用反射机制获得，定义了表的模式。当前，Spark SQL不支持那些包含了Map类型字段的JavaBean，但是对于嵌套的JavaBean以及嵌套了List或Array类型的字段给予了充分的支持。你可以通过创建一个实现了Serializable接口以及为所有字段生成getter和setter方法的类来创建一个JavaBean。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"></div><div class="line"><span class="comment">// Create an RDD of Person objects from a text file</span></div><div class="line">JavaRDD&lt;Person&gt; peopleRDD = spark.read()</div><div class="line">  .textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</div><div class="line">  .javaRDD()</div><div class="line">  .map(line -&gt; &#123;</div><div class="line">    String[] parts = line.split(<span class="string">","</span>);</div><div class="line">    Person person = <span class="keyword">new</span> Person();</div><div class="line">    person.setName(parts[<span class="number">0</span>]);</div><div class="line">    person.setAge(Integer.parseInt(parts[<span class="number">1</span>].trim()));</div><div class="line">    <span class="keyword">return</span> person;</div><div class="line">  &#125;);</div><div class="line"></div><div class="line"><span class="comment">// Apply a schema to an RDD of JavaBeans to get a DataFrame</span></div><div class="line">Dataset&lt;Row&gt; peopleDF = spark.createDataFrame(peopleRDD, Person.class);</div><div class="line"><span class="comment">// Register the DataFrame as a temporary view</span></div><div class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></div><div class="line">Dataset&lt;Row&gt; teenagersDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The columns of a row in the result can be accessed by field index</span></div><div class="line">Encoder&lt;String&gt; stringEncoder = Encoders.STRING();</div><div class="line">Dataset&lt;String&gt; teenagerNamesByIndexDF = teenagersDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</div><div class="line">    stringEncoder);</div><div class="line">teenagerNamesByIndexDF.show();</div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |       value|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |Name: Justin|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"></div><div class="line"><span class="comment">// or by field name</span></div><div class="line">Dataset&lt;String&gt; teenagerNamesByFieldDF = teenagersDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.&lt;String&gt;getAs(<span class="string">"name"</span>),</div><div class="line">    stringEncoder);</div><div class="line">teenagerNamesByFieldDF.show();</div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |       value|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |Name: Justin|</span></div><div class="line"><span class="comment">// +------------+</span></div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h4 id="Programmatically-Specifying-the-Schema"><a href="#Programmatically-Specifying-the-Schema" class="headerlink" title="Programmatically Specifying the Schema"></a>Programmatically Specifying the Schema</h4><p>当JavaBean无法提前定义时（例如，记录的结构被编码为一个字符串，或者一个文本数据集将被解析，但是其中的字段可能根据不同的用户而不一样），Dataset<row>能够通过三个步骤来创建。</row></p>
<blockquote>
<p>1、根据原生的RDD创建一个RDD<row>。<br>2、创建一个与第一步骤RDD中Row结构匹配的StructType来描述的模式。<br>3、通过由SparkSession提供的createDataFrame方法，将这个模式应用到RDD<row>。</row></row></p>
</blockquote>
<p>例如：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</div><div class="line"></div><div class="line"><span class="comment">// Create an RDD</span></div><div class="line">JavaRDD&lt;String&gt; peopleRDD = spark.sparkContext()</div><div class="line">  .textFile(<span class="string">"examples/src/main/resources/people.txt"</span>, <span class="number">1</span>)</div><div class="line">  .toJavaRDD();</div><div class="line"></div><div class="line"><span class="comment">// The schema is encoded in a string</span></div><div class="line">String schemaString = <span class="string">"name age"</span>;</div><div class="line"></div><div class="line"><span class="comment">// Generate the schema based on the string of schema</span></div><div class="line">List&lt;StructField&gt; fields = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (String fieldName : schemaString.split(<span class="string">" "</span>)) &#123;</div><div class="line">  StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, <span class="keyword">true</span>);</div><div class="line">  fields.add(field);</div><div class="line">&#125;</div><div class="line">StructType schema = DataTypes.createStructType(fields);</div><div class="line"></div><div class="line"><span class="comment">// Convert records of the RDD (people) to Rows</span></div><div class="line">JavaRDD&lt;Row&gt; rowRDD = peopleRDD.map((Function&lt;String, Row&gt;) record -&gt; &#123;</div><div class="line">  String[] attributes = record.split(<span class="string">","</span>);</div><div class="line">  <span class="keyword">return</span> RowFactory.create(attributes[<span class="number">0</span>], attributes[<span class="number">1</span>].trim());</div><div class="line">&#125;);</div><div class="line"></div><div class="line"><span class="comment">// Apply the schema to the RDD</span></div><div class="line">Dataset&lt;Row&gt; peopleDataFrame = spark.createDataFrame(rowRDD, schema);</div><div class="line"></div><div class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></div><div class="line">peopleDataFrame.createOrReplaceTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// SQL can be run over a temporary view created using DataFrames</span></div><div class="line">Dataset&lt;Row&gt; results = spark.sql(<span class="string">"SELECT name FROM people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations</span></div><div class="line"><span class="comment">// The columns of a row in the result can be accessed by field index or by field name</span></div><div class="line">Dataset&lt;String&gt; namesDS = results.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</div><div class="line">    Encoders.STRING());</div><div class="line">namesDS.show();</div><div class="line"><span class="comment">// +-------------+</span></div><div class="line"><span class="comment">// |        value|</span></div><div class="line"><span class="comment">// +-------------+</span></div><div class="line"><span class="comment">// |Name: Michael|</span></div><div class="line"><span class="comment">// |   Name: Andy|</span></div><div class="line"><span class="comment">// | Name: Justin|</span></div><div class="line"><span class="comment">// +-------------+</span></div></pre></td></tr></table></figure></p>
<p>完整的示例，请查看 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h3 id="Aggregations"><a href="#Aggregations" class="headerlink" title="Aggregations"></a>Aggregations</h3><p>内置的DataFrame函数提供了常用的聚合操作，如count()、countDistinct()、avg()、max()、min()等。然而这些函数是为了DataFrame设计的，Spark SQL同样由类型安全的版本，以便其中一些被用到Scala和Java的强类型Dataset。此外，Spark没有限制用户预定义聚合函数，可以自己来创建聚合函数。</p>
<h4 id="Untyped-User-Defined-Aggregate-Functions"><a href="#Untyped-User-Defined-Aggregate-Functions" class="headerlink" title="Untyped User-Defined Aggregate Functions"></a>Untyped User-Defined Aggregate Functions</h4><p>用户要实现无类型聚合函数，则需要继承<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.UserDefinedAggregateFunction" title="UserDefinedAggregateFunction" target="_blank" rel="external">UserDefinedAggregateFunction</a>抽象类。例如，你一个用户自定义的平均数函数，看起来像这样：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.MutableAggregationBuffer;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.UserDefinedAggregateFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataType;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">private</span> StructType inputSchema;</div><div class="line">  <span class="keyword">private</span> StructType bufferSchema;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="title">MyAverage</span><span class="params">()</span> </span>&#123;</div><div class="line">    List&lt;StructField&gt; inputFields = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">    inputFields.add(DataTypes.createStructField(<span class="string">"inputColumn"</span>, DataTypes.LongType, <span class="keyword">true</span>));</div><div class="line">    inputSchema = DataTypes.createStructType(inputFields);</div><div class="line"></div><div class="line">    List&lt;StructField&gt; bufferFields = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">    bufferFields.add(DataTypes.createStructField(<span class="string">"sum"</span>, DataTypes.LongType, <span class="keyword">true</span>));</div><div class="line">    bufferFields.add(DataTypes.createStructField(<span class="string">"count"</span>, DataTypes.LongType, <span class="keyword">true</span>));</div><div class="line">    bufferSchema = DataTypes.createStructType(bufferFields);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Data types of input arguments of this aggregate function</span></div><div class="line">  <span class="function"><span class="keyword">public</span> StructType <span class="title">inputSchema</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> inputSchema;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Data types of values in the aggregation buffer</span></div><div class="line">  <span class="function"><span class="keyword">public</span> StructType <span class="title">bufferSchema</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> bufferSchema;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// The data type of the returned value</span></div><div class="line">  <span class="function"><span class="keyword">public</span> DataType <span class="title">dataType</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> DataTypes.DoubleType;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Whether this function always returns the same output on the identical input</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">deterministic</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to</span></div><div class="line">  <span class="comment">// standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides</span></div><div class="line">  <span class="comment">// the opportunity to update its values. Note that arrays and maps inside the buffer are still</span></div><div class="line">  <span class="comment">// immutable.</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(MutableAggregationBuffer buffer)</span> </span>&#123;</div><div class="line">    buffer.update(<span class="number">0</span>, <span class="number">0L</span>);</div><div class="line">    buffer.update(<span class="number">1</span>, <span class="number">0L</span>);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Updates the given aggregation buffer `buffer` with new input data from `input`</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">(MutableAggregationBuffer buffer, Row input)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</div><div class="line">      <span class="keyword">long</span> updatedSum = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>);</div><div class="line">      <span class="keyword">long</span> updatedCount = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span>;</div><div class="line">      buffer.update(<span class="number">0</span>, updatedSum);</div><div class="line">      buffer.update(<span class="number">1</span>, updatedCount);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Merges two aggregation buffers and stores the updated buffer values back to `buffer1`</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(MutableAggregationBuffer buffer1, Row buffer2)</span> </span>&#123;</div><div class="line">    <span class="keyword">long</span> mergedSum = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>);</div><div class="line">    <span class="keyword">long</span> mergedCount = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>);</div><div class="line">    buffer1.update(<span class="number">0</span>, mergedSum);</div><div class="line">    buffer1.update(<span class="number">1</span>, mergedCount);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Calculates the final result</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">evaluate</span><span class="params">(Row buffer)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) buffer.getLong(<span class="number">0</span>)) / buffer.getLong(<span class="number">1</span>);</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Register the function to access it</span></div><div class="line">spark.udf().register(<span class="string">"myAverage"</span>, <span class="keyword">new</span> MyAverage());</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">"examples/src/main/resources/employees.json"</span>);</div><div class="line">df.createOrReplaceTempView(<span class="string">"employees"</span>);</div><div class="line">df.show();</div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |   name|salary|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |Michael|  3000|</span></div><div class="line"><span class="comment">// |   Andy|  4500|</span></div><div class="line"><span class="comment">// | Justin|  3500|</span></div><div class="line"><span class="comment">// |  Berta|  4000|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"></div><div class="line">Dataset&lt;Row&gt; result = spark.sql(<span class="string">"SELECT myAverage(salary) as average_salary FROM employees"</span>);</div><div class="line">result.show();</div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |average_salary|</span></div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |        3750.0|</span></div><div class="line"><span class="comment">// +--------------+</span></div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考 examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java 。</p>
<h4 id="Type-Safe-User-Defined-Aggregate-Functions"><a href="#Type-Safe-User-Defined-Aggregate-Functions" class="headerlink" title="Type-Safe User-Defined Aggregate Functions"></a>Type-Safe User-Defined Aggregate Functions</h4><p>强类型Dataset的用户自定义聚合围绕着<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator ‘Aggregator’" target="_blank" rel="external">Aggregator</a>抽象类来解决。例如，一个类型安全的用户自定义平均数看起来是这样：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.TypedColumn;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.Aggregator;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> String name;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">long</span> salary;</div><div class="line"></div><div class="line">  <span class="comment">// Constructors, getters, setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Average</span> <span class="keyword">implements</span> <span class="title">Serializable</span>  </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">long</span> sum;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">long</span> count;</div><div class="line"></div><div class="line">  <span class="comment">// Constructors, getters, setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>&lt;<span class="title">Employee</span>, <span class="title">Average</span>, <span class="title">Double</span>&gt; </span>&#123;</div><div class="line">  <span class="comment">// A zero value for this aggregation. Should satisfy the property that any b + zero = b</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">zero</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Average(<span class="number">0L</span>, <span class="number">0L</span>);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span></div><div class="line">  <span class="comment">// and return it instead of constructing a new object</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">reduce</span><span class="params">(Average buffer, Employee employee)</span> </span>&#123;</div><div class="line">    <span class="keyword">long</span> newSum = buffer.getSum() + employee.getSalary();</div><div class="line">    <span class="keyword">long</span> newCount = buffer.getCount() + <span class="number">1</span>;</div><div class="line">    buffer.setSum(newSum);</div><div class="line">    buffer.setCount(newCount);</div><div class="line">    <span class="keyword">return</span> buffer;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Merge two intermediate values</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">merge</span><span class="params">(Average b1, Average b2)</span> </span>&#123;</div><div class="line">    <span class="keyword">long</span> mergedSum = b1.getSum() + b2.getSum();</div><div class="line">    <span class="keyword">long</span> mergedCount = b1.getCount() + b2.getCount();</div><div class="line">    b1.setSum(mergedSum);</div><div class="line">    b1.setCount(mergedCount);</div><div class="line">    <span class="keyword">return</span> b1;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Transform the output of the reduction</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">finish</span><span class="params">(Average reduction)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) reduction.getSum()) / reduction.getCount();</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Specifies the Encoder for the intermediate value type</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Encoder&lt;Average&gt; <span class="title">bufferEncoder</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> Encoders.bean(Average.class);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Specifies the Encoder for the final output value type</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Encoder&lt;Double&gt; <span class="title">outputEncoder</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> Encoders.DOUBLE();</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">Encoder&lt;Employee&gt; employeeEncoder = Encoders.bean(Employee.class);</div><div class="line">String path = <span class="string">"examples/src/main/resources/employees.json"</span>;</div><div class="line">Dataset&lt;Employee&gt; ds = spark.read().json(path).as(employeeEncoder);</div><div class="line">ds.show();</div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |   name|salary|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |Michael|  3000|</span></div><div class="line"><span class="comment">// |   Andy|  4500|</span></div><div class="line"><span class="comment">// | Justin|  3500|</span></div><div class="line"><span class="comment">// |  Berta|  4000|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"></div><div class="line">MyAverage myAverage = <span class="keyword">new</span> MyAverage();</div><div class="line"><span class="comment">// Convert the function to a `TypedColumn` and give it a name</span></div><div class="line">TypedColumn&lt;Employee, Double&gt; averageSalary = myAverage.toColumn().name(<span class="string">"average_salary"</span>);</div><div class="line">Dataset&lt;Double&gt; result = ds.select(averageSalary);</div><div class="line">result.show();</div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |average_salary|</span></div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |        3750.0|</span></div><div class="line"><span class="comment">// +--------------+</span></div></pre></td></tr></table></figure></p>
<p>完整的示例，请看 examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java 。</p>
<h2 id="Data-Sources"><a href="#Data-Sources" class="headerlink" title="Data Sources"></a>Data Sources</h2><p>Spark SQL通过DataFrame接口支持多种数据源的操作。DataFrame能够使用关系转换进行操作，也可以被用来创建一个临时视图。将DataFrame注册为一个临时视图，将允许你在视图的数据上运行SQL查询。这一章节描述了使用Spark Data Sources加载和保存数据的一般方法，然后介绍内置数据源可用的详细参数。</p>
<h3 id="Generic-Load-Save-Functions"><a href="#Generic-Load-Save-Functions" class="headerlink" title="Generic Load/Save Functions"></a>Generic Load/Save Functions</h3><p>最简单的格式，默认数据源（默认是parquet， 除非通过spark.sql.soiurces.default配置修改过）将被用于所有操作。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; usersDF = spark.read().load(<span class="string">"examples/src/main/resources/users.parquet"</span>);</div><div class="line">usersDF.select(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>).write().save(<span class="string">"namesAndFavColors.parquet"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考 examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java 。</p>
<h4 id="Manually-Specifying-Options"><a href="#Manually-Specifying-Options" class="headerlink" title="Manually Specifying Options"></a>Manually Specifying Options</h4><p>你还可以手动指定想要使用的数据源，以及传递给数据源任何额外的参数。数据源可以通过它的完整限定名（如：org.apache.spark.sql.parquet）来指定，但是对于内置的数据源，你也能够使用它的短名字（json、parquet、jdbc、orc、libsvm、csv、text）。从任何类型数据源加载的DataFrames，通过使用这个语句都可以转为其他类型。<br>要加载一个JSON文件，你可以使用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; peopleDF = spark.read().format(<span class="string">"json"</span>).load(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line">peopleDF.select(<span class="string">"name"</span>, <span class="string">"age"</span>).write().format(<span class="string">"parquet"</span>).save(<span class="string">"namesAndAges.parquet"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考：xamples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>要加载一个CSV文件，你可以使用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; peopleDFCsv = spark.read().format(<span class="string">"csv"</span>)</div><div class="line">  .option(<span class="string">"sep"</span>, <span class="string">";"</span>)</div><div class="line">  .option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)</div><div class="line">  .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</div><div class="line">  .load(<span class="string">"examples/src/main/resources/people.csv"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考：xamples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Run-SQL-on-files-directly"><a href="#Run-SQL-on-files-directly" class="headerlink" title="Run SQL on files directly"></a>Run SQL on files directly</h4><p>除了使用read API加载文件到DataFrame然后查询它之外，你还可以使用SQL直接查询那个文件。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; sqlDF =</div><div class="line">  spark.sql(<span class="string">"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考：xamples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Save-Modes"><a href="#Save-Modes" class="headerlink" title="Save Modes"></a>Save Modes</h4><p>保存操作可以选择一种SaveMode，它指定了如何处理存在的数据。一件非常重要的事情是这些保存模式没有利用任何锁，并且它们不是原子操作。另外，当执行Overwrite模式时，已有的数据将会在写出新数据之前被删掉。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Scala/Java</th>
<th style="text-align:left">Any Language</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">SaveMode.ErrorIfExists(default)</td>
<td style="text-align:left">“error” or “errorifexists” (default)</td>
<td style="text-align:left">当保存一个DataFrame到一个数据源时，如果数据已经存在，预计将抛出一个异常</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Append</td>
<td style="text-align:left">“append”</td>
<td style="text-align:left">当保存一个DataFrame到一个数据源时，如果数据或表格已经存在，DataFrame的内容将被追加到已存在数据</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Overwrite</td>
<td style="text-align:left">“overwrite”</td>
<td style="text-align:left">Overwrite模式意味着，当保存一个DataFrame到一个数据源时，如果数据或表格已经存在，已存在的数据将会被DataFrame的内容所覆盖</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Ignore</td>
<td style="text-align:left">“ignore”</td>
<td style="text-align:left">Ignore模式意味着当保存一个DataFrame到一个数据源时，如果数据已经存在，保存操作将不会保存DataFrame的内容，并且不会修改已经存在的数据。这个操作类似 CREATE TABLE IF NOT EXISTS</td>
</tr>
</tbody>
</table>
<h4 id="Saving-to-Persistent-Tables"><a href="#Saving-to-Persistent-Tables" class="headerlink" title="Saving to Persistent Tables"></a>Saving to Persistent Tables</h4><p>使用saveAsTable命令，DataFrames也可以作为持久化表被保存到Hive metastore中。注意，使用这个功能不需要现有Hive的部署。Spark将会为你创建一个默认的本地Hive metastore(使用Derby)。与createOrReplaceTempView命令不同，saveAsTable将显示DataFrame的内容并创建一个指向Hive metastore中数据的指针。持久化表将在你的Spark程序重启之后持续存在，只要你维持你的连接在相同的metastore。通过在SparkSession上调用table方法（并传递表的名字），就能根据持久化表创建对应的DataFrame。<br>对于基于文件的数据源，如：text、parquet、json等。通过path选项，你可以指定一个自定义表路径，如:df.write.option(“path”, “/some/path”).saveAsTable(“t”)。当这个表被删除，自定义表路径将不会被移除，并且表数据依然存在。如果没有指定自定义表路径，Spark将会把数据写到仓库目录下的默认表路径。当这个表被删除时，默认表路径也会一并被删除。<br>从Spark2.1开始，持久化数据源表格在Hive metastore中有独立的元数据。这样做又一些优点：</p>
<blockquote>
<p>因为metastore只返回查询所需的partition，因此表上的首次查询就不需要查找所有的aprtition。<br>Hive DDL（如ALTER TABLE PARTITION … SET LOCATION），对于使用Datasource APi来创建表都是可用的。</p>
</blockquote>
<p>注意，当创建外部数据源表时（那些带有path选项的），分区信息默认是不会被收集的。要同步分区信息到metastore中，你可以执行MSCK REPAIR TABLE。</p>
<h4 id="Bucketing-Sorting-and-Partitioning"><a href="#Bucketing-Sorting-and-Partitioning" class="headerlink" title="Bucketing, Sorting and Partitioning"></a>Bucketing, Sorting and Partitioning</h4><p>对于基于文件的数据源，还可以对输出进行分组并排序或分组并分区。分组并排序只对持久化表适用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">peopleDF.write().bucketBy(<span class="number">42</span>, <span class="string">"name"</span>).sortBy(<span class="string">"age"</span>).saveAsTable(<span class="string">"people_bucketed"</span>);</div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>当使用Dataset API时，partitioning能够和save以及saveAsTable一起使用。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">usersDF</div><div class="line">  .write()</div><div class="line">  .partitionBy(<span class="string">"favorite_color"</span>)</div><div class="line">  .format(<span class="string">"parquet"</span>)</div><div class="line">  .save(<span class="string">"namesPartByColor.parquet"</span>);</div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>可以对单个表使用partitioning和bucketing：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">peopleDF</div><div class="line">  .write()</div><div class="line">  .partitionBy(<span class="string">"favorite_color"</span>)</div><div class="line">  .bucketBy(<span class="number">42</span>, <span class="string">"name"</span>)</div><div class="line">  .saveAsTable(<span class="string">"people_partitioned_bucketed"</span>);</div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>partitionBy创建了一个在<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#partition-discovery" title="Partition Discovery" target="_blank" rel="external">Partition Discovery</a>章节中描述的目录结构。因此，它对具有高基数的列的适用性有限。相比之下，BucketBy会跨固定数量的bucket来分布部署数据，and can be used when a number of unique values is unbounded.（！！！无法理解）</p>
<h3 id="Parquet-Files"><a href="#Parquet-Files" class="headerlink" title="Parquet Files"></a>Parquet Files</h3><p>Parquet时一种列式文件格式，它被很多其他数据处理系统所支持。Spark SQL对Parquet文件提供了读写支持，并能够自动保护原始数据的模式。当写Parquet文件时，为了兼容的原因，所有列被自动转换为nullable。</p>
<h4 id="Loading-Data-Programmatically"><a href="#Loading-Data-Programmatically" class="headerlink" title="Loading Data Programmatically"></a>Loading Data Programmatically</h4><p>使用上面例子中的数据：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; peopleDF = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line"></div><div class="line"><span class="comment">// DataFrames can be saved as Parquet files, maintaining the schema information</span></div><div class="line">peopleDF.write().parquet(<span class="string">"people.parquet"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Read in the Parquet file created above.</span></div><div class="line"><span class="comment">// Parquet files are self-describing so the schema is preserved</span></div><div class="line"><span class="comment">// The result of loading a parquet file is also a DataFrame</span></div><div class="line">Dataset&lt;Row&gt; parquetFileDF = spark.read().parquet(<span class="string">"people.parquet"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Parquet files can also be used to create a temporary view and then used in SQL statements</span></div><div class="line">parquetFileDF.createOrReplaceTempView(<span class="string">"parquetFile"</span>);</div><div class="line">Dataset&lt;Row&gt; namesDF = spark.sql(<span class="string">"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19"</span>);</div><div class="line">Dataset&lt;String&gt; namesDS = namesDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</div><div class="line">    Encoders.STRING());</div><div class="line">namesDS.show();</div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |       value|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |Name: Justin|</span></div><div class="line"><span class="comment">// +------------+</span></div></pre></td></tr></table></figure></p>
<p>完整示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Partition-Discovery"><a href="#Partition-Discovery" class="headerlink" title="Partition Discovery"></a>Partition Discovery</h4><p>在像Hive这样的系统中，常用的优化方法时进行表分区。在分区表中，数据通常存储在不同的目录中，根据分区列的值，编码到每个分区目录的路径中。所有内置文件源（包括Text/CSV/JSON/ORC/Parquet）都能够自动发现并推断分区信息。例如，我们能够将我们之前使用的数据存储到如下目录结构的分区表中，这个分区表使用两个额外的字段gender和country来作为分区字段：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">path</div><div class="line">└── to</div><div class="line">    └── table</div><div class="line">        ├── gender=male</div><div class="line">        │   ├── ...</div><div class="line">        │   │</div><div class="line">        │   ├── country=US</div><div class="line">        │   │   └── data.parquet</div><div class="line">        │   ├── country=CN</div><div class="line">        │   │   └── data.parquet</div><div class="line">        │   └── ...</div><div class="line">        └── gender=female</div><div class="line">            ├── ...</div><div class="line">            │</div><div class="line">            ├── country=US</div><div class="line">            │   └── data.parquet</div><div class="line">            ├── country=CN</div><div class="line">            │   └── data.parquet</div><div class="line">            └── ...</div></pre></td></tr></table></figure></p>
<p>通过将path/to/table传递给SparkSession.read.parquet或SparkSession.read.load，Spark SQL将自动从路径中获取分区信息。现在返回的DataFrame的模式变成：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">root</div><div class="line">|-- name: string (nullable = true)</div><div class="line">|-- age: long (nullable = true)</div><div class="line">|-- gender: string (nullable = true)</div><div class="line">|-- country: string (nullable = true)</div></pre></td></tr></table></figure></p>
<p>注意，分区列的数据类型是自动推断的。当前支持数字数据类型、日期、时间戳和字符串类型。有些时候，用户可能不想自动推导分区列的数据类型。对于这种情况，自动类型推导能够通过配置项spark.sql.sources.partitionColumnTypeInference.enabled来配置，该配置默认值为True。当类型推导被禁用后，分区列将使用字符串类型。<br>从Spark1.6开始，分区发现默认只能查找给定路径下的。因此，对于上面的那个例子，如果用户传递path/to/table/gender=male给SparkSession.read.parquet或SparkSession.read.load，那么gender将不会被当成一个分区列。如果用户想要具体说明分区开始查找的基本目录，可以在数据源选项中设置basePath。例如，当数据目录为path/to/table/gender=male时，并且设置了basePath为path/to/table/，那么gender将会是一个分区列。</p>
<h4 id="Schema-Merging"><a href="#Schema-Merging" class="headerlink" title="Schema Merging"></a>Schema Merging</h4><p>和ProtocolBuffer、Avro以及Thrift一样，Parquet也支持模式演化。用户可以先从一个简单的schema开始，然后根据需要逐渐增加更多的列。通过这种方式，用户可能最终会得到不同但相互兼容的多个Parquet文件。Parquet数据源能够自动发现这种情况，并合并这些文件的schemas。<br>因为合并schema是一个成本相当高的操作，而且在很多情况是不必要的，因此从1.5.0开始，该功能默认是关闭的。你可以通过以下来启用它：</p>
<blockquote>
<p>当你读区Parquet文件时，设置数据源选项 mergeSchema为true（下面的列子将展示）或者<br>设置全局SQL选项 spark.sql.parquet.mergeSchema为true。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.Arrays;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Square</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> value;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> square;</div><div class="line"></div><div class="line">  <span class="comment">// Getters and setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Cube</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> value;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> cube;</div><div class="line"></div><div class="line">  <span class="comment">// Getters and setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line">List&lt;Square&gt; squares = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> value = <span class="number">1</span>; value &lt;= <span class="number">5</span>; value++) &#123;</div><div class="line">  Square square = <span class="keyword">new</span> Square();</div><div class="line">  square.setValue(value);</div><div class="line">  square.setSquare(value * value);</div><div class="line">  squares.add(square);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Create a simple DataFrame, store into a partition directory</span></div><div class="line">Dataset&lt;Row&gt; squaresDF = spark.createDataFrame(squares, Square.class);</div><div class="line">squaresDF.write().parquet(<span class="string">"data/test_table/key=1"</span>);</div><div class="line"></div><div class="line">List&lt;Cube&gt; cubes = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> value = <span class="number">6</span>; value &lt;= <span class="number">10</span>; value++) &#123;</div><div class="line">  Cube cube = <span class="keyword">new</span> Cube();</div><div class="line">  cube.setValue(value);</div><div class="line">  cube.setCube(value * value * value);</div><div class="line">  cubes.add(cube);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Create another DataFrame in a new partition directory,</span></div><div class="line"><span class="comment">// adding a new column and dropping an existing column</span></div><div class="line">Dataset&lt;Row&gt; cubesDF = spark.createDataFrame(cubes, Cube.class);</div><div class="line">cubesDF.write().parquet(<span class="string">"data/test_table/key=2"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Read the partitioned table</span></div><div class="line">Dataset&lt;Row&gt; mergedDF = spark.read().option(<span class="string">"mergeSchema"</span>, <span class="keyword">true</span>).parquet(<span class="string">"data/test_table"</span>);</div><div class="line">mergedDF.printSchema();</div><div class="line"></div><div class="line"><span class="comment">// The final schema consists of all 3 columns in the Parquet files together</span></div><div class="line"><span class="comment">// with the partitioning column appeared in the partition directory paths</span></div><div class="line"><span class="comment">// root</span></div><div class="line"><span class="comment">//  |-- value: int (nullable = true)</span></div><div class="line"><span class="comment">//  |-- square: int (nullable = true)</span></div><div class="line"><span class="comment">//  |-- cube: int (nullable = true)</span></div><div class="line"><span class="comment">//  |-- key: int (nullable = true)</span></div></pre></td></tr></table></figure>
<p>完整示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Hive-metastore-Parquet-table-conversion"><a href="#Hive-metastore-Parquet-table-conversion" class="headerlink" title="Hive metastore Parquet table conversion"></a>Hive metastore Parquet table conversion</h4><p>当我们向Hive metastore Parquet table写数据或从中读数据时，Spark SQL奖尝试使用自己的Parquet支持来代替Hive SerDe以获取更好的性能。这个行为通过spark.sql.hive.converMetastoreParquet来配置，并且默认为打开的。</p>
<h5 id="Hive-Parquet-Schema-Reconciliation"><a href="#Hive-Parquet-Schema-Reconciliation" class="headerlink" title="Hive/Parquet Schema Reconciliation"></a>Hive/Parquet Schema Reconciliation</h5><p>从表schema处理的角度来看，Hive和Parquet有两个主要区别：</p>
<blockquote>
<p>1、Hive是不区分大小写的，而Parquet是区分大小写的。<br>2、Hive认为所有列nullable，而nullable在Parquet中很重要。</p>
</blockquote>
<p>因为上面的原因，当我们将一个Hive metastore Parquet table转换为一个Spark SQL Parquet table时，我们必须将Hive metastore schema与Parquet schema调整一致。调整的规则为：</p>
<blockquote>
<p>1、两个schema中相同名称的字段不管是否为空必须具有相同的数据类型。调整好的字段应当具有Parquet端的数据类型，因此nullable是具有意义的。<br>2、调整后的schema必须包含Hive metastore schema中定义的字段。<br>    1）只出现在Parquet schema中的字段将从调整后的schema中删掉。<br>    2）只出现在Hive metastore schema中的字段将被作为nullable字段添加到调整后的schema中。</p>
</blockquote>
<h5 id="Metadata-Refreshing"><a href="#Metadata-Refreshing" class="headerlink" title="Metadata Refreshing"></a>Metadata Refreshing</h5><p>Spark SQL为了更好的性能而缓存了Parquet metadata。当Hive metastore Parquet表转换启用时，那些被转换的表的metadata也会被缓存。如果这些表被Hive或其他外部工具更新了，你需要手动刷新它们以保证metadata的一致。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// spark is an existing SparkSession</span></div><div class="line">spark.catalog().refreshTable(<span class="string">"my_table"</span>);</div></pre></td></tr></table></figure></p>
<h4 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h4><p>Parquet的配置可以通过两种方式完成，在SparkSession上使用setConf方法或使用SQL运行SET key=value。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.parquet.binaryAsString</td>
<td style="text-align:left">false</td>
<td style="text-align:left">一些其他产生Parquet的系统，主要是Impala、Hive以及老版本的Spark SQL，这些系统在写Parquet schema时不区分二进制数据和字符串。这个标记告诉Spark SQL为这些系统将二进制数据按照字符串来进行兼容。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.int96AsTimestamp</td>
<td style="text-align:left">true</td>
<td style="text-align:left">一些其他产生Parquet的系统，特别是Impala和Hive，它们使用INT96来存储时间戳。这个标记告诉Spark SQL将INT96按照时间戳来解析，以便为那些系统提供兼容。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.compressio.codec</td>
<td style="text-align:left">snappy</td>
<td style="text-align:left">设置写Parquet文件的压缩编码器。如果没有在表详情的选项/属性中指定”compression”或”parquet.compression”。根据优先级排序：compression &gt; parquet.compression &gt; spark.sql.parquet.compression.codec。该选项可以使用的值有：none、uncompressed、snappy、gzip或lzo。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.filterPushdown.</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为True时，启用Parquet过滤器的push-down优化。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.converMetastoreParquet</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为false时，Spark SQL将对parquet table使用Hive SerDe，而不是使用内置支持。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.mergeSchema</td>
<td style="text-align:left">false</td>
<td style="text-align:left">当设置为true时，Parquet数据源合并从所有数据文件收集的schema，如果是false，将从摘要文件中挑选schema，如果没有摘要文件可用，则随机选择一个文件。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.optimizer.metadataOnly.</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为true时，启用metadata-only查询优化，这个优化使用表的metadata来产生分区列，而不是通过对表扫描。当所有扫描过的列示分区列，且查询操作有一个满足distinct语意的聚合操作时，适用。</td>
</tr>
</tbody>
</table>
<h3 id="ORC-Files"><a href="#ORC-Files" class="headerlink" title="ORC Files"></a>ORC Files</h3><p>从Spark 2.3开始，Spark支持向量ORC reader，这个reader使用新的ORC文件格式来读取ORC文件。因此新增了如下配置。当spark.sql.orc.impl被设置为native且spark.sql.orc.enableVectorizedReader被设置为true时，向量读取器将用于读区原生的ORC表（这些表使用USING ORC语句创建）。对于Hive ORC serde表（使用USING HIVE OPTIONS），当spark.sql.hive.convertMetastoreOrc也被设置为true时，向量reader被使用。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.orc.impl</td>
<td style="text-align:left">hive</td>
<td style="text-align:left">ORC实现类的名字。可以是native和hive中的一个。native意味着对构建于Apache ORC 1.4.1上的原生ORC支持。hive意味着对Hive 1.2.1中的ORC库进行支持。</td>
</tr>
<tr>
<td style="text-align:left">spark.slql.orc.enableVectorizedReader</td>
<td style="text-align:left">true</td>
<td style="text-align:left">在native实现中启用向量化orc编码。如果为false，一个新的非向量化ORC reader被用于native实现。对于hive实现，本项可以忽略。</td>
</tr>
</tbody>
</table>
<h3 id="JSON-Datasets"><a href="#JSON-Datasets" class="headerlink" title="JSON Datasets"></a>JSON Datasets</h3><p>Spark SQL能够自动推导一个JSON dataset的schema并将它加载为一个Dataset<row>。这个转换能够在一个Dataset<string>上或一个JSON文件上使用SparkSession.read().json()来完成。<br>注意，提供的json文件不是一个典型的JSON文件。每一行必须是一个独立有效的JSON对象（其实这句话的意思就是，一个json数据必须独立一行，不能跨多行）。关于更多的信息，请查看<a href="http://jsonlines.org/" title="JSON Lines text format, also called newline-delimited JSON" target="_blank" rel="external">JSON Lines text format, also called newline-delimited JSON</a>.<br>要想解析多行JSON文件，需要设置multiLine选项为true。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line"><span class="comment">// A JSON dataset is pointed to by path.</span></div><div class="line"><span class="comment">// The path can be either a single text file or a directory storing text files</span></div><div class="line">Dataset&lt;Row&gt; people = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The inferred schema can be visualized using the printSchema() method</span></div><div class="line">people.printSchema();</div><div class="line"><span class="comment">// root</span></div><div class="line"><span class="comment">//  |-- age: long (nullable = true)</span></div><div class="line"><span class="comment">//  |-- name: string (nullable = true)</span></div><div class="line"></div><div class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></div><div class="line">people.createOrReplaceTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></div><div class="line">Dataset&lt;Row&gt; namesDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>);</div><div class="line">namesDF.show();</div><div class="line"><span class="comment">// +------+</span></div><div class="line"><span class="comment">// |  name|</span></div><div class="line"><span class="comment">// +------+</span></div><div class="line"><span class="comment">// |Justin|</span></div><div class="line"><span class="comment">// +------+</span></div><div class="line"></div><div class="line"><span class="comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span></div><div class="line"><span class="comment">// a Dataset&lt;String&gt; storing one JSON object per string.</span></div><div class="line">List&lt;String&gt; jsonData = Arrays.asList(</div><div class="line">        <span class="string">"&#123;\"name\":\"Yin\",\"address\":&#123;\"city\":\"Columbus\",\"state\":\"Ohio\"&#125;&#125;"</span>);</div><div class="line">Dataset&lt;String&gt; anotherPeopleDataset = spark.createDataset(jsonData, Encoders.STRING());</div><div class="line">Dataset&lt;Row&gt; anotherPeople = spark.read().json(anotherPeopleDataset);</div><div class="line">anotherPeople.show();</div><div class="line"><span class="comment">// +---------------+----+</span></div><div class="line"><span class="comment">// |        address|name|</span></div><div class="line"><span class="comment">// +---------------+----+</span></div><div class="line"><span class="comment">// |[Columbus,Ohio]| Yin|</span></div><div class="line"><span class="comment">// +---------------+----+</span></div></pre></td></tr></table></figure></string></row></p>
<p>完整的示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h3 id="Hive-Tables"><a href="#Hive-Tables" class="headerlink" title="Hive Tables"></a>Hive Tables</h3><p>Spark SQL还支持对Apache Hive读写数据。然而，因为Hive有很多的依赖，而这些依赖默认没有包含在Spark的发布中。如果Hive的依赖能够在classpath中找到，Spark将自动加载它们。注意，这些Hive依赖也必须在所有worker节点上存在，因为它们需要访问Hive的序列化和反序列化库（SerDes）以便访问Hive上存储的数据。<br>Hive的配置是通过替换conf/目录下的hive-site.xml、core-site.xml（安全配置）和hdfs-sit.xml（HDFS配置）来完成的。<br>当使用Hive工作时，必须实例化支持Hive的SparkSession，包括连接到已有的Hive metastore、支持Hive serdes以及Hive自定义函数。即使没有Hive环境也能够启用Hive支持。当没有通过hive-site.xml进行配置时，context自动在当前目录创建metastore_db，并创建一个由spark.sql.warehouse.dir配置指定的目录，默认目录在Spark application启动的当前目录中的spark-warehouse。注意hive-site.xml中的hive.metastore.warehouse.dir属性在Spark2.0.0中废弃了，取而代之是使用spark.sql.warehouse.dir来指定数据库在仓库中的位置。你可以需要为启动Spark appliction的用户开放写权限。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.File;</div><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> key;</div><div class="line">  <span class="keyword">private</span> String value;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getKey</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> key;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setKey</span><span class="params">(<span class="keyword">int</span> key)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.key = key;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getValue</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> value;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setValue</span><span class="params">(String value)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.value = value;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// warehouseLocation points to the default location for managed databases and tables</span></div><div class="line">String warehouseLocation = <span class="keyword">new</span> File(<span class="string">"spark-warehouse"</span>).getAbsolutePath();</div><div class="line">SparkSession spark = SparkSession</div><div class="line">  .builder()</div><div class="line">  .appName(<span class="string">"Java Spark Hive Example"</span>)</div><div class="line">  .config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation)</div><div class="line">  .enableHiveSupport()</div><div class="line">  .getOrCreate();</div><div class="line"></div><div class="line">spark.sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive"</span>);</div><div class="line">spark.sql(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Queries are expressed in HiveQL</span></div><div class="line">spark.sql(<span class="string">"SELECT * FROM src"</span>).show();</div><div class="line"><span class="comment">// +---+-------+</span></div><div class="line"><span class="comment">// |key|  value|</span></div><div class="line"><span class="comment">// +---+-------+</span></div><div class="line"><span class="comment">// |238|val_238|</span></div><div class="line"><span class="comment">// | 86| val_86|</span></div><div class="line"><span class="comment">// |311|val_311|</span></div><div class="line"><span class="comment">// ...</span></div><div class="line"></div><div class="line"><span class="comment">// Aggregation queries are also supported.</span></div><div class="line">spark.sql(<span class="string">"SELECT COUNT(*) FROM src"</span>).show();</div><div class="line"><span class="comment">// +--------+</span></div><div class="line"><span class="comment">// |count(1)|</span></div><div class="line"><span class="comment">// +--------+</span></div><div class="line"><span class="comment">// |    500 |</span></div><div class="line"><span class="comment">// +--------+</span></div><div class="line"></div><div class="line"><span class="comment">// The results of SQL queries are themselves DataFrames and support all normal functions.</span></div><div class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(<span class="string">"SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The items in DataFrames are of type Row, which lets you to access each column by ordinal.</span></div><div class="line">Dataset&lt;String&gt; stringsDS = sqlDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Key: "</span> + row.get(<span class="number">0</span>) + <span class="string">", Value: "</span> + row.get(<span class="number">1</span>),</div><div class="line">    Encoders.STRING());</div><div class="line">stringsDS.show();</div><div class="line"><span class="comment">// +--------------------+</span></div><div class="line"><span class="comment">// |               value|</span></div><div class="line"><span class="comment">// +--------------------+</span></div><div class="line"><span class="comment">// |Key: 0, Value: val_0|</span></div><div class="line"><span class="comment">// |Key: 0, Value: val_0|</span></div><div class="line"><span class="comment">// |Key: 0, Value: val_0|</span></div><div class="line"><span class="comment">// ...</span></div><div class="line"></div><div class="line"><span class="comment">// You can also use DataFrames to create temporary views within a SparkSession.</span></div><div class="line">List&lt;Record&gt; records = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> key = <span class="number">1</span>; key &lt; <span class="number">100</span>; key++) &#123;</div><div class="line">  Record record = <span class="keyword">new</span> Record();</div><div class="line">  record.setKey(key);</div><div class="line">  record.setValue(<span class="string">"val_"</span> + key);</div><div class="line">  records.add(record);</div><div class="line">&#125;</div><div class="line">Dataset&lt;Row&gt; recordsDF = spark.createDataFrame(records, Record.class);</div><div class="line">recordsDF.createOrReplaceTempView(<span class="string">"records"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Queries can then join DataFrames data with data stored in Hive.</span></div><div class="line">spark.sql(<span class="string">"SELECT * FROM records r JOIN src s ON r.key = s.key"</span>).show();</div><div class="line"><span class="comment">// +---+------+---+------+</span></div><div class="line"><span class="comment">// |key| value|key| value|</span></div><div class="line"><span class="comment">// +---+------+---+------+</span></div><div class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></div><div class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></div><div class="line"><span class="comment">// |  4| val_4|  4| val_4|</span></div><div class="line"><span class="comment">// ...</span></div></pre></td></tr></table></figure></p>
<p>完整示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java。</p>
<h4 id="Specifying-storage-format-for-Hive-table"><a href="#Specifying-storage-format-for-Hive-table" class="headerlink" title="Specifying storage format for Hive table"></a>Specifying storage format for Hive table</h4><p>当你创建一个Hive表时，你需要指定这个表应该如何从文件系统读写数据，例如”input format”和”output format”。你还需要定义这个表应该如何将data反序列化为row，或者如何将row序列化为data，如”serde”。下面的选项可以被用来指定存储格式（”serde”、”input format”、”output format”），如：CREATE TABLE src(id int) USING hive OPTIONS(fileFormat ‘parquet’)。默认情况下，我们将以简单文本的格式读取table。值得注意的是，在创建table的时候，存储handler还不被支持，你可以在Hive端使用存储handler来创建一个table，然后使用Spark SQL来读区它。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">fileFormat</td>
<td style="text-align:left">用来说明文件格式的存储格式包，包括”serde”、”input format”和”output format”。当前我们支持6中文件格式：sequencefile、rcfile、orc、parquet、textfile和avro。</td>
</tr>
<tr>
<td style="text-align:left">inputFormat\outputFormat</td>
<td style="text-align:left">这两个选项用来指定”InputFormat“和”OutputFormat“类的名字，例如：org.apache.hadoop.hive.qllio.orc.OrcInputFormat。这两个选项应该成对出现，如果你设置了”fileFormat”选项，那么你不能分别指定它们。</td>
</tr>
<tr>
<td style="text-align:left">serde</td>
<td style="text-align:left">这个选项指定了一个serde类。当设置了‘fileFormat’选项时，如果给定的‘fileFormat’已经包含了serde信息，那么不要设置这个选项。目前，“sequencefile”、“textfile”和“rcfile”不包含serde信息，因此你可以为这3种文件格式设置此选项。</td>
</tr>
<tr>
<td style="text-align:left">fieldDelim, escapeDelim, collectionDelim, mapkeyDelim, lineDelim</td>
<td style="text-align:left">这个选项只能被用于”textfile”的文件格式。它们定义了文件的换行符。</td>
</tr>
</tbody>
</table>
<p>其他属性使用OPTIONS进行定义，将作为Hive serde属性来考虑。</p>
<h4 id="Interacting-with-Different-Versions-of-Hive-Metastore"><a href="#Interacting-with-Different-Versions-of-Hive-Metastore" class="headerlink" title="Interacting with Different Versions of Hive Metastore"></a>Interacting with Different Versions of Hive Metastore</h4><p>Spark SQL的Hive支持的最重要部分是与Hive metastore的交互，它使Spark SQL能够访问Hive表中的metadata。从Spark 1.4.0开始，使用下面描述的配置，Spark有一个独立的包用来访问不同版本的Hive metadata。注意，无论要去访问的metastore的Hive是什么版本，在Spark SQL内部将针对Hive 1.2.1进行编译，并使用这些类作为内部执行（serdes、UDFs、UDAFs等）。<br>下面的选项能够被用来配置获取metadata的Hive的版本：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.hive.metastore.version</td>
<td style="text-align:left">1.2.1</td>
<td style="text-align:left">Hive metadata的版本。可用的选项从0.12.0到1.2.1</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.metastore.jars</td>
<td style="text-align:left">builtin</td>
<td style="text-align:left">被用于实例化HiveMetastoreClient的jar的位置。这个属性有三个选项：<br>1）builtin： 使用Hive 1.2.1，当-Phive被启用时，它与Spark assembly绑定。当这个选择了这个选项，spark.sql.hive.metastore.version必须是1.2.1或为定义。<br> 2) maven：从Maven库中下载指定版本的Hive jars。这个配置对于生产环境通常不推荐。<br> 3）JVM的标准classpath格式。这个classpath必须包含了Hive和它的依赖，以及对应的版本的Hadoop。这些jar只需要存在于driver上，但是如果你实在yarn资源管理器的集群上，那么你必须确保它们和你的application一起被打包。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.metastore. sharedPrefixes</td>
<td style="text-align:left">com.mysql.jdbc,org.postgresql, com.microsoft.sqlserver,oracla.jdbc</td>
<td style="text-align:left">那些需要使用类加载器加载的用于在Spark SQL和指定版本的Hive之间共享的类前缀，类前缀是一个逗号分隔的列表。一个需要被共享的类就是JDBC driver，它需要访问metastore。其他需要共享的类是那些需要与已经共享类交互的类。例如，由log4j使用的自定义appender。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.metastore. barrierPrefixes</td>
<td style="text-align:left">(empty)</td>
<td style="text-align:left">Spark SQL所连接的每个版本的Hive都应明确加载的类的前缀，列表以逗号分隔。例如，通常需要被共享的Hive UDFs在一个前缀中被声明（如，org.apache.spark.*）</td>
</tr>
</tbody>
</table>
<h3 id="JDBC-To-Other-Databases"><a href="#JDBC-To-Other-Databases" class="headerlink" title="JDBC To Other Databases"></a>JDBC To Other Databases</h3><p>Spark SQL还有一个数据源，可以使用JDBC从其他数据库读取数据。这个功能比使用<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.JdbcRDD" title="jdbcRDD" target="_blank" rel="external">jdbcRDD</a>更加受欢迎。这是因为结果是作为一个DataFrame被返回，这样很容易的使用Spark SQL进行处理或与其他数据源相连接。JDBC数据源在Java或Python中使用起来也很容易，因为它不需要用户提供一个ClassTag。（注意，这不同于Spark SQL JDBC Server，Spark SQL JDBC Server允许其他application使用Spark SQL运行查询）<br>你需要在spark classpath中添加对应数据库的JDBC driver。例如，要从Spark shell连接到postgres，你应该运行如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar</div></pre></td></tr></table></figure></p>
<p>使用Data Source API，远程数据库中的表可以被加载为一个DataFrame或Spark SQL临时视图。用户可以在数据源选项中指定JDBC的连接属性。连接通畅需要提供user和password属性，来登陆数据源。除了连接属性外，Spark还支持如下的选项，这些选项忽略大小写：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">url</td>
<td style="text-align:left">进行连接的JDBC URL。特定数据源的连接属性可能会在URL中设置。如：jdbc:postgresql://localhost/test?user=fred&amp;password=secret。</td>
</tr>
<tr>
<td style="text-align:left">dbtable</td>
<td style="text-align:left">要读取的JDBC表。注意，在SQL查询中的From子句中有效的任何东西，都能使用。例如，你可以在括号中使用子查询来代替全表。</td>
</tr>
<tr>
<td style="text-align:left">driver</td>
<td style="text-align:left">连接URL的JDBC driver的类名。</td>
</tr>
<tr>
<td style="text-align:left">partitionColumn, lowerBound, upperBound</td>
<td style="text-align:left">这些选项中的一个被指定，那么所有的都必须被指定。此外，numPartitions必须被指定。它们描述了多个worker并行读取表数据时，应该如何分区。partitionColumn必须是表中的数值列。注意，lowerBound和upperBound仅仅用来决定分区的幅度，而不是过滤表中的行。因此表中的所有行都将被分区并返回。这个选项只能被用于读取。</td>
</tr>
<tr>
<td style="text-align:left">numPartitions</td>
<td style="text-align:left">并行读写表的最大分区数。这也确定了JDBC连接的最大并发。如果写的分区数量超过了这个限制，我们可以在写数据之前调用coalesce(numPartitions)来减少它。</td>
</tr>
<tr>
<td style="text-align:left">fetchsize</td>
<td style="text-align:left">JDBC的提取大小，它确定了每次通信能够取得多少行。它能够帮助提升那些默认fetch size低的JDBC dirver的性能（比如，Orache的fetch size为10）。这个选项只能用于读操作。</td>
</tr>
<tr>
<td style="text-align:left">batch</td>
<td style="text-align:left">JDBC的batch大小，它确定了每次通信能够插入多少行。这能够帮助提升JDBC dirver的性能。这个选项只能用于写操作。默认值为1000。</td>
</tr>
<tr>
<td style="text-align:left">isolationLevel</td>
<td style="text-align:left">事务的隔离级别，应用于当前连接。它可以是：NONE\READ_COMMITTED\ READ_UNCOMMITTED\REPEATABLE_READ\SERIALIZABLE中的一个，通过JDBC连接对象来定义标准事务的隔离级别，默认为READ_UNCOMMITTED。这个选项只能用于写操作。请参考java.sql.Connection文档。</td>
</tr>
<tr>
<td style="text-align:left">sessionInitStatement</td>
<td style="text-align:left">session初始化声明。在到远程数据库的session被打开之后，开始读取数据之前，这个选项执行一个自定义语句（PL/SQL块）。使用这个来实现session的初始化代码。例如：option(“色上司哦那I逆天S塔特闷它”， “”“BEGIN execute immediate ‘alter session set “_serial_direct_read”=true’; END; “””)</td>
</tr>
<tr>
<td style="text-align:left">truncate</td>
<td style="text-align:left">这是一个与JDBC writer相关操作。当启用了SaveMode.Overwrite，这个选项控制删除已存在的表，而不是先drop表然后再创建表。这个更加有效率，并且避免了表的metadata被删除。然而在某些情况下，它无法工作，如新数据有不同的schema。该选项默认值为false。这个选项只用于写操作。</td>
</tr>
<tr>
<td style="text-align:left">createTableOptions</td>
<td style="text-align:left">这是一个与JDBC writer相关的操作。如果设置，该选项允许在创建表的时候设置特定数据库表和分区的选项（如，CREATE TABLE T(name string) ENGINE=InnoDB）。这个选项只能被用于写操作。</td>
</tr>
<tr>
<td style="text-align:left">createTableColumnTypes</td>
<td style="text-align:left">当创建表时，用来代替默认的数据库列类型。数据类型信息使用与CREATE TABLE columns语句（如：”name CHAR(64), comments VARCHAR(1024)”）相同的格式被指定。被指定的数据类型应该是有效的spark sql数据类型。本选项只能用于写操作。</td>
</tr>
<tr>
<td style="text-align:left">customSchema</td>
<td style="text-align:left">自定义schema用于从JDBC连接中读取数据。例如，”id DECIMAL(38, 0), name STRING”。你也可以指定部分字段，其他的时候默认类型映射。例如：”id DECIMAL(38, 0)”。列名称应该与JDBC表的相关列名称一致。用户可以指定Spark SQL的相关数据类型，而不是使用默认的。这个选项只能被用于读操作。</td>
</tr>
</tbody>
</table>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span></div><div class="line"><span class="comment">// Loading data from a JDBC source</span></div><div class="line">Dataset&lt;Row&gt; jdbcDF = spark.read()</div><div class="line">  .format(<span class="string">"jdbc"</span>)</div><div class="line">  .option(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>)</div><div class="line">  .option(<span class="string">"dbtable"</span>, <span class="string">"schema.tablename"</span>)</div><div class="line">  .option(<span class="string">"user"</span>, <span class="string">"username"</span>)</div><div class="line">  .option(<span class="string">"password"</span>, <span class="string">"password"</span>)</div><div class="line">  .load();</div><div class="line"></div><div class="line">Properties connectionProperties = <span class="keyword">new</span> Properties();</div><div class="line">connectionProperties.put(<span class="string">"user"</span>, <span class="string">"username"</span>);</div><div class="line">connectionProperties.put(<span class="string">"password"</span>, <span class="string">"password"</span>);</div><div class="line">Dataset&lt;Row&gt; jdbcDF2 = spark.read()</div><div class="line">  .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</div><div class="line"></div><div class="line"><span class="comment">// Saving data to a JDBC source</span></div><div class="line">jdbcDF.write()</div><div class="line">  .format(<span class="string">"jdbc"</span>)</div><div class="line">  .option(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>)</div><div class="line">  .option(<span class="string">"dbtable"</span>, <span class="string">"schema.tablename"</span>)</div><div class="line">  .option(<span class="string">"user"</span>, <span class="string">"username"</span>)</div><div class="line">  .option(<span class="string">"password"</span>, <span class="string">"password"</span>)</div><div class="line">  .save();</div><div class="line"></div><div class="line">jdbcDF2.write()</div><div class="line">  .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</div><div class="line"></div><div class="line"><span class="comment">// Specifying create table column data types on write</span></div><div class="line">jdbcDF.write()</div><div class="line">  .option(<span class="string">"createTableColumnTypes"</span>, <span class="string">"name CHAR(64), comments VARCHAR(1024)"</span>)</div><div class="line">  .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</div></pre></td></tr></table></figure>
<p>完整示例代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h3 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h3><blockquote>
<p>1、JDBC dirver类对于client session和所有executor的主类加载器是可访问的。这是因为Java的DriverManager会做一个安全检查，当DriverManager要打开一个连接时，检查结果会忽略所有主类加载器无法访问的driver。一个简便的方法是修改所有worker节点的compute_classpath.sh来包含你的driver JAR。<br>2、一些数据库，如H2，需要将所有名字转换为大写。你需要在Spark SQL中使用大写来引用那些名字。</p>
</blockquote>
<h2 id="Performance-Tuning"><a href="#Performance-Tuning" class="headerlink" title="Performance Tuning"></a>Performance Tuning</h2><p>通过将数据缓存到内存或开启一些创新选项，一些工作量是可以优化提升性能的。</p>
<h3 id="Caching-Data-In-Memory"><a href="#Caching-Data-In-Memory" class="headerlink" title="Caching Data In Memory"></a>Caching Data In Memory</h3><p>通过调用spark.catalog.cacheTable(“tableName”)或dataFrame.cache()，Spark能够使用内存中列式格式来缓存表。Spark SQL将扫描需要的列，并自动调整压缩，以达到最小的内存使用和GC压力。你可以使用spark.catalog.uncacheTable(“tableName”)，将table从内存中移除。<br>配置内存缓存可以通过两种方式来实现：在SparkSession上调用setConf方法，或者使用SQL来执行SET key=value命令。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql. inMemoryColumnarStorage.compressed</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为true的时候，Spark SQL将基于数据的统计自动为每一列选择一种压缩编码器。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql. imMemoryColumnarStorage.batchSize</td>
<td style="text-align:left">10000</td>
<td style="text-align:left">控制列式缓存的批量大小。较大的批量size会影响内存会提高内存的利用率和压缩，但是会产生内存溢出的风险。</td>
</tr>
</tbody>
</table>
<h3 id="Other-Configuration-Options"><a href="#Other-Configuration-Options" class="headerlink" title="Other Configuration Options"></a>Other Configuration Options</h3><p>下面的选项也能够被用来提高查询的效率。随着Spark的优化，这些选项在未来可能会被废弃。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.files.maxPartitionbytes</td>
<td style="text-align:left">134217728 (128 MB)</td>
<td style="text-align:left">读取文件时，单个分区的最大字节数。</td>
</tr>
<tr>
<td style="text-align:left">saprk.sql.files.openCostInBytes</td>
<td style="text-align:left">4194304 (4 MB)</td>
<td style="text-align:left">打开一个文件的成本，通过在同一时间能够扫描的字节数来测量。当推送多个文件到一个partition时非常有用。提高这个值会更好，这样写小文件的partition要比写大文件的partition更加快（写小文件的partitin优先调度）。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.broadcastTimeout</td>
<td style="text-align:left">300</td>
<td style="text-align:left">broadcast连接的等待时间，以秒为单位。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.broadcastJoinThreshold</td>
<td style="text-align:left">10485760 (10 MB)</td>
<td style="text-align:left">当执行join操作时，为那些需要广播到所有worker节点的表设置最大字节数。通过设置这个值为-1，广播操作可以被禁用。注意，当前的统计只支持那些运行了ANALYZE TABLE <tablename> COMPUTE STATISTICS命令的Hive Metastore表。</tablename></td>
</tr>
<tr>
<td style="text-align:left">spark.sql.shuffle.partitions</td>
<td style="text-align:left">200</td>
<td style="text-align:left">当为join或aggregation操作而混洗数据时，用来配置使用partitions的数量。</td>
</tr>
</tbody>
</table>
<h3 id="Broadcast-Hint-for-SQL-Queries"><a href="#Broadcast-Hint-for-SQL-Queries" class="headerlink" title="Broadcast Hint for SQL Queries"></a>Broadcast Hint for SQL Queries</h3><p>BROADCAST hint指导Spark在使用其他表或视图join指定表时，如何广播指定表。在Spark决定join方法时，broadcast hash join被优先考虑，即使统计高于spark.sql.autoBroadcastJoinThreshold的配置。当join两边都被指定了，Spark广播具有较低统计的那边。注意Spark不保证BHJ（broadcast hash join）总是被选择，因为不是所有的情况都支持BHJ。当broadcast nested loop join被选择时，我们仍然最重提示。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.spark.sql.functions.broadcast;</div><div class="line">broadcast(spark.table(<span class="string">"src"</span>)).join(spark.table(<span class="string">"records"</span>), <span class="string">"key"</span>).show();</div></pre></td></tr></table></figure></p>
<h2 id="Distributed-SQL-Engine"><a href="#Distributed-SQL-Engine" class="headerlink" title="Distributed SQL Engine"></a>Distributed SQL Engine</h2><p>使用Spark SQL的JDBC/ODBC或command-line interface，Spark SQL也能够具有分布式查询引擎的行为。在这种模式中，终端用户或application能够直接与Spark SQL交互来运行SQL查询，而不需要写任何的代码。</p>
<h3 id="Running-the-Thrift-JDBC-ODBC-server"><a href="#Running-the-Thrift-JDBC-ODBC-server" class="headerlink" title="Running the Thrift JDBC/ODBC server"></a>Running the Thrift JDBC/ODBC server</h3><p>Thrift JDBC/ODBC server实现了相当于Hive 1.2.1中的HiveServer2。你可以使用Spark或Hive1.2.1的beeline脚本来测试JDBC server。<br>要启动JDBC/ODBC server，在Spark目录中运行如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./sbin/start-thriftserver.sh</div></pre></td></tr></table></figure></p>
<p>这个脚本接受所有bin/spark-submit命令的行的参数，并增加了一个–hiveconf选项用来指定Hive属性。你可以执行 ./sbin/start-thriftserver.sh –help来获取完整的可用属性列表。默认，这个server监听的是本地的10000端口。要想重写这个丢昂扣，你可以修改环境变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">export HIVE_SERVER2_THRIFT_PORT=&lt;listening-port&gt;</div><div class="line">export HIVE_SERVER2_THRIFT_BIND_HOST=&lt;listening-host&gt;</div><div class="line">./sbin/start-thriftserver.sh \</div><div class="line">  --master &lt;master-uri&gt; \</div><div class="line">  ...</div></pre></td></tr></table></figure></p>
<p>或者修改系统属性：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">./sbin/start-thriftserver.sh \</div><div class="line">  --hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \</div><div class="line">  --hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \</div><div class="line">  --master &lt;master-uri&gt;</div><div class="line">  ...</div></pre></td></tr></table></figure></p>
<p>现在，你可以使用beeline来测试Thrift JDBC/ODBC server：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/beeline</div></pre></td></tr></table></figure></p>
<p>在beeline中连接JDBC/ODBC server可以使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">beeline&gt; !connect jdbc:hive2://localhost:10000</div></pre></td></tr></table></figure></p>
<p>beeline将会询问你用户名和密码。在非安全模式中，输入你机器的用户名和空白的密码。对于安全模式，请遵循<a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients" title="beeline documentation" target="_blank" rel="external">beeline documentation</a>的指导。</p>
<p>通过替换conf/中hive-site.xml、core-site.mxl和hdfs-site.xml来完成Hive的配置。</p>
<p>你可能还需要使用Hive提供的beeline脚本。</p>
<p>Thrift JDBC server还支持通过HTTP协议发送thrift RPC messages。要启用HTTP模式，可以如下修改系统属性，或者修改conf中的hive-site.xml：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive.server2.transport.mode - Set this to value: http</div><div class="line">hive.server2.thrift.http.port - HTTP port number to listen on; default is 10001</div><div class="line">hive.server2.http.endpoint - HTTP endpoint; default is cliservice</div></pre></td></tr></table></figure></p>
<p>要进行测试，使用beeline以http模式连接到JDBC/ODBC server：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">beeline&gt; !connect jdbc:hive2://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?hive.server2.transport.mode=http;hive.server2.thrift.http.path=&lt;http_endpoint&gt;</div></pre></td></tr></table></figure></p>
<h4 id="Running-the-Spark-SQL-CLI"><a href="#Running-the-Spark-SQL-CLI" class="headerlink" title="Running the Spark SQL CLI"></a>Running the Spark SQL CLI</h4><p>Spark SQL CLI是一个方便的工具用来在本地模式中运行Hive metastore服务并执行来自命令的查询输入。注意，Spark SQL CLI不能与Thrift JDBC server通信，<br>要启动Spark SQL  CLI，在Spark目录中运行如下脚本：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-sql</div></pre></td></tr></table></figure></p>
<p>通过替换conf/中hive-site.xml、core-site.mxl和hdfs-site.xml来完成Hive的配置。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><h3 id="Data-Types"><a href="#Data-Types" class="headerlink" title="Data Types"></a>Data Types</h3><p>Spark SQL和DataFrame支持如下数据类型：</p>
<blockquote>
<p>1、Numeric types<br>        ByteType：声明一个一个字节的有符号的整型。数值范围从-128到127。<br>        ShortType：声明一个两字节的有符号的整型。数值范围从-32768到32767。<br>        IntegerType：声明一个四字节的有符号的整型。数值范围从-2147483648到2147483647。<br>        LongType：声明一个八个字节的有符号的整型。数值范围从-9223372036854775808到9223372036854775807。<br>        FloatType：声明一个四字节的单精度浮点数值。<br>        DoubleType：声明一个八字节的双精度浮点数。<br>        DecimlType：声明一个任意精度的有符号的十进制数值。内部由java.math.BigDecimal支持。一个DecimlType由一个任意精度的不能整型值和一个32位的整型组成。<br>2、Strubg type<br>        声明一个字符串值。<br>3、Binary type<br>        BinaryType：声明一个字节序列值。<br>4、Boolean type<br>        BooleanType：声明一个boolean值。<br>5、Datetime type<br>        TimestampType：声明一个由year、month、day、hour、minute和second字段的值组成。<br>        DateType：声明一个由year、month和day字段的值组成。<br>6、Complex types<br>        ArrayType(elementType, containsNull)：声明一个elementType类型序列。containsNull用来检测ArrayType中是否包含null的值。<br>        MapType(keyType, valueType, valueContainsNull)：由一组key-value对组成。key的数据类型由KeyType来描述，value的数据类型由valueType来描述。对于MapType的一个值，keys不允许为null。valueContainsNull<br>        被用来检测MapTypte的values中是否包含null值。<br>        StructType(fields)：StructFields(fields)序列。<br>                StructField(name, datatype, nullable): StructType类型的字段。字段的名称通过name指定。字段的数据类型通过datatype来指定。nullable用来决定这个fields的values是否可以有null。</p>
</blockquote>
<p>Spark SQL的所有数据类型都位于org.apache.spark.sql.types包中。要访问或创建一种数据类型，请使用org.apache.spark.sql.types.DataTypes中提供的接口方法。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Data type</th>
<th style="text-align:left">Value type in Java</th>
<th style="text-align:left">API to access or create a data type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ByteType</td>
<td style="text-align:left">byte or Byte</td>
<td style="text-align:left">DataTypes.ByteType</td>
</tr>
<tr>
<td style="text-align:left">ShortType</td>
<td style="text-align:left">short or Short</td>
<td style="text-align:left">DataTypes.ShortType</td>
</tr>
<tr>
<td style="text-align:left">IntegerType</td>
<td style="text-align:left">int or Integer</td>
<td style="text-align:left">DataTypes.IntegerType</td>
</tr>
<tr>
<td style="text-align:left">LongType</td>
<td style="text-align:left">long or Long</td>
<td style="text-align:left">DataTypes.LongType</td>
</tr>
<tr>
<td style="text-align:left">FloatType</td>
<td style="text-align:left">float or Float</td>
<td style="text-align:left">DataTypes.FloatType</td>
</tr>
<tr>
<td style="text-align:left">DoubleType</td>
<td style="text-align:left">double or Double</td>
<td style="text-align:left">DataTypes.DoubleType</td>
</tr>
<tr>
<td style="text-align:left">DecimalType</td>
<td style="text-align:left">java.math.BigDecimal</td>
<td style="text-align:left">DataTypes.createDecimalType() DataTypes.createDecimalType(precision, scale)</td>
</tr>
<tr>
<td style="text-align:left">StringType</td>
<td style="text-align:left">String</td>
<td style="text-align:left">DataTypes.StringType</td>
</tr>
<tr>
<td style="text-align:left">BinaryType</td>
<td style="text-align:left">byte[]</td>
<td style="text-align:left">DataTypes.BinaryType</td>
</tr>
<tr>
<td style="text-align:left">BooleanType</td>
<td style="text-align:left">boolean or Boolean</td>
<td style="text-align:left">DataTypes.BooleanType</td>
</tr>
<tr>
<td style="text-align:left">TimestampType</td>
<td style="text-align:left">java.sql.Timestamp</td>
<td style="text-align:left">DataTypes.TimestampType</td>
</tr>
<tr>
<td style="text-align:left">DateType</td>
<td style="text-align:left">java.sql.Date</td>
<td style="text-align:left">DateTypes.DateType</td>
</tr>
<tr>
<td style="text-align:left">ArrayType</td>
<td style="text-align:left">java.util.List</td>
<td style="text-align:left">DataTypes.createArrayType(elementType) 注意：containsNull的值为true。</td>
</tr>
<tr>
<td style="text-align:left">MapType</td>
<td style="text-align:left">java.util.Map</td>
<td style="text-align:left">DataTypes.createMapType(keyType, valueType) 注意，valueContainsNull的值将为true</td>
</tr>
<tr>
<td style="text-align:left">StructType</td>
<td style="text-align:left">org.apache.spark.sql.Row</td>
<td style="text-align:left">DataTypes.createStructType(fields)</td>
</tr>
<tr>
<td style="text-align:left">StructField</td>
<td style="text-align:left">The value type in Java of the data type of this field</td>
<td style="text-align:left">DataTypes.createStructField(name, dataType, nullable)</td>
</tr>
</tbody>
</table>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/blog/tags/spark/" rel="tag">#spark</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/blog/2018/08/10/spark-2-3-1-QuickStart/" rel="next" title="spark_2.3.1_QuickStart">
                <i class="fa fa-chevron-left"></i> spark_2.3.1_QuickStart
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/blog/2018/08/13/ambari/" rel="prev" title="Ambari">
                Ambari <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/blog/uploads/avatar.png"
               alt="baimoon" />
          <p class="site-author-name" itemprop="name">baimoon</p>
          <p class="site-description motion-element" itemprop="description">Baimoon's blog</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/blog/archives">
              <span class="site-state-item-count">43</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/blog/categories">
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/blog/tags">
                <span class="site-state-item-count">23</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/baimoon" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://gallery.xrange.org" title="xrange" target="_blank">xrange</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-SQL-DataFrames-and-Dataset-Guide"><span class="nav-number">1.</span> <span class="nav-text">Spark SQL, DataFrames and Dataset Guide</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Overview"><span class="nav-number">1.1.</span> <span class="nav-text">Overview</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SQL"><span class="nav-number">1.1.1.</span> <span class="nav-text">SQL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Datasets-and-DataFrames"><span class="nav-number">1.1.2.</span> <span class="nav-text">Datasets and DataFrames</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Getting-Started"><span class="nav-number">1.2.</span> <span class="nav-text">Getting Started</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Starting-Point-SparkSession"><span class="nav-number">1.2.1.</span> <span class="nav-text">Starting Point: SparkSession</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Creating-DataFrames"><span class="nav-number">1.2.2.</span> <span class="nav-text">Creating DataFrames</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Untyped-Dataset-Operations-aka-DataFrame-Operations"><span class="nav-number">1.2.3.</span> <span class="nav-text">Untyped Dataset Operations(aka DataFrame Operations)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Running-SQL-Queries-Programmatically"><span class="nav-number">1.2.4.</span> <span class="nav-text">Running SQL Queries Programmatically</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Global-Temporary-View"><span class="nav-number">1.2.5.</span> <span class="nav-text">Global Temporary View</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Creating-Datasets"><span class="nav-number">1.2.6.</span> <span class="nav-text">Creating Datasets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Interoperating-with-RDDs"><span class="nav-number">1.2.7.</span> <span class="nav-text">Interoperating with RDDs</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Inferring-the-Schema-Using-Reflection"><span class="nav-number">1.2.7.1.</span> <span class="nav-text">Inferring the Schema Using Reflection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Programmatically-Specifying-the-Schema"><span class="nav-number">1.2.7.2.</span> <span class="nav-text">Programmatically Specifying the Schema</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Aggregations"><span class="nav-number">1.2.8.</span> <span class="nav-text">Aggregations</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Untyped-User-Defined-Aggregate-Functions"><span class="nav-number">1.2.8.1.</span> <span class="nav-text">Untyped User-Defined Aggregate Functions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Type-Safe-User-Defined-Aggregate-Functions"><span class="nav-number">1.2.8.2.</span> <span class="nav-text">Type-Safe User-Defined Aggregate Functions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Sources"><span class="nav-number">1.3.</span> <span class="nav-text">Data Sources</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Generic-Load-Save-Functions"><span class="nav-number">1.3.1.</span> <span class="nav-text">Generic Load/Save Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Manually-Specifying-Options"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">Manually Specifying Options</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Run-SQL-on-files-directly"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">Run SQL on files directly</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Save-Modes"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">Save Modes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Saving-to-Persistent-Tables"><span class="nav-number">1.3.1.4.</span> <span class="nav-text">Saving to Persistent Tables</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bucketing-Sorting-and-Partitioning"><span class="nav-number">1.3.1.5.</span> <span class="nav-text">Bucketing, Sorting and Partitioning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parquet-Files"><span class="nav-number">1.3.2.</span> <span class="nav-text">Parquet Files</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Loading-Data-Programmatically"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">Loading Data Programmatically</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Partition-Discovery"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">Partition Discovery</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Schema-Merging"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">Schema Merging</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hive-metastore-Parquet-table-conversion"><span class="nav-number">1.3.2.4.</span> <span class="nav-text">Hive metastore Parquet table conversion</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Hive-Parquet-Schema-Reconciliation"><span class="nav-number">1.3.2.4.1.</span> <span class="nav-text">Hive/Parquet Schema Reconciliation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Metadata-Refreshing"><span class="nav-number">1.3.2.4.2.</span> <span class="nav-text">Metadata Refreshing</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Configuration"><span class="nav-number">1.3.2.5.</span> <span class="nav-text">Configuration</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ORC-Files"><span class="nav-number">1.3.3.</span> <span class="nav-text">ORC Files</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JSON-Datasets"><span class="nav-number">1.3.4.</span> <span class="nav-text">JSON Datasets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive-Tables"><span class="nav-number">1.3.5.</span> <span class="nav-text">Hive Tables</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Specifying-storage-format-for-Hive-table"><span class="nav-number">1.3.5.1.</span> <span class="nav-text">Specifying storage format for Hive table</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Interacting-with-Different-Versions-of-Hive-Metastore"><span class="nav-number">1.3.5.2.</span> <span class="nav-text">Interacting with Different Versions of Hive Metastore</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JDBC-To-Other-Databases"><span class="nav-number">1.3.6.</span> <span class="nav-text">JDBC To Other Databases</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Troubleshooting"><span class="nav-number">1.3.7.</span> <span class="nav-text">Troubleshooting</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Performance-Tuning"><span class="nav-number">1.4.</span> <span class="nav-text">Performance Tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Caching-Data-In-Memory"><span class="nav-number">1.4.1.</span> <span class="nav-text">Caching Data In Memory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-Configuration-Options"><span class="nav-number">1.4.2.</span> <span class="nav-text">Other Configuration Options</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Broadcast-Hint-for-SQL-Queries"><span class="nav-number">1.4.3.</span> <span class="nav-text">Broadcast Hint for SQL Queries</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Distributed-SQL-Engine"><span class="nav-number">1.5.</span> <span class="nav-text">Distributed SQL Engine</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Running-the-Thrift-JDBC-ODBC-server"><span class="nav-number">1.5.1.</span> <span class="nav-text">Running the Thrift JDBC/ODBC server</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Running-the-Spark-SQL-CLI"><span class="nav-number">1.5.1.1.</span> <span class="nav-text">Running the Spark SQL CLI</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">1.6.</span> <span class="nav-text">Reference</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Types"><span class="nav-number">1.6.1.</span> <span class="nav-text">Data Types</span></a></li></ol></li></ol></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016-07 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">baimoon</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/blog/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/blog/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/blog/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/blog/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/blog/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/blog/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
