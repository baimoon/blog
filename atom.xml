<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Baimoon&#39;s Note</title>
  
  <link href="/blog/atom.xml" rel="self"/>
  
  <link href="http://baimoon.github.io/"/>
  <updated>2017-03-30T16:29:00.000Z</updated>
  <id>http://baimoon.github.io/</id>
  
  <author>
    <name>baimoon</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>unity3D</title>
    <link href="http://baimoon.github.io/2017/03/29/unity3D/"/>
    <id>http://baimoon.github.io/2017/03/29/unity3D/</id>
    <published>2017-03-29T13:44:39.000Z</published>
    <updated>2017-03-30T16:29:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是学习Unity3D过程中的一些学习笔记，主要为个人学习使用，如能帮助别人纯属意外。</p>
<h2 id="游戏对象以及基本操作"><a href="#游戏对象以及基本操作" class="headerlink" title="游戏对象以及基本操作"></a>游戏对象以及基本操作</h2><p>在Unity3D中，在Unity游戏中，一个游戏由多个场景所组成，每个场景又是由多个游戏对象构成，一个游戏对象可以包含多个组件用来实现不同的功能。在场景中，一个角色、一个模型、一个特效，都属于游戏对象。每个游戏对象可以有多个组件，从而使对象具备某种特性、实现相应的功能。</p>
<p>在Unity3D中，对一个游戏对象进行的基本操作有：</p>
<blockquote>
<p>场景拖动<br>游戏对象的拖动 （对象拖动，可以按照轴拖动，也可以按照面拖动）<br>游戏对象的旋转 （旋转可以按照轴旋转）<br>游戏对象的缩放 （只能按照轴来缩放）<br>选中对象，按F或双击，可以快速定位对象到场景中心。</p>
</blockquote>
<p>（对应快捷键为q、w、e、r）。</p>
<p>视角的操作：</p>
<blockquote>
<p>滚轮可以用来拉近或拉远视角。<br>按下鼠标右键旋转，以当前位置旋转视角。<br>按下Alt和鼠标左键旋转，是根据选中的对象，进行视角的旋转。<br>按下鼠标右键的同时，如果按A、W、S、D键，可以进行场景漫游。</p>
</blockquote>
<h2 id="Terrain地形"><a href="#Terrain地形" class="headerlink" title="Terrain地形"></a>Terrain地形</h2><p>需要创建一个名为Terrain的3D模型。然后编辑Terrain组件。<br>组件中包含一个地形编辑工具列表：<br>第一个，提升或沉降地形的工具，点击左键，来提升地形，按下左键并按下shift键，则沉降。<br>第二个，平坦地形工具。刷出来的是一个平面。<br>第三个，平滑地形工具。使得地形的过渡更加自然。<br>第四个，绘制地形表面纹理工具。<br>第五个，植树工具。<br>第六个，添加地表细节工具。</p>
<h2 id="3D开发基础"><a href="#3D开发基础" class="headerlink" title="3D开发基础"></a>3D开发基础</h2><h3 id="摄像机"><a href="#摄像机" class="headerlink" title="摄像机"></a>摄像机</h3><p>摄像机决定了游戏的最终显示效果。而且一个场景中可以存在多个摄像机。<br>将摄像机以当前游戏对象视角来运行，可以选中摄像机，然后按Ctrl + Shift + F来设定。</p>
<h3 id="坐标系"><a href="#坐标系" class="headerlink" title="坐标系"></a>坐标系</h3><p>世界坐标系和本地坐标系，世界坐标系是整个3D场景的坐标系，本地坐标系（会根据游戏对象的旋转而旋转）是某个游戏对象内独立的坐标系。可以通过左上角的按钮来切换。</p>
<h3 id="网格"><a href="#网格" class="headerlink" title="网格"></a>网格</h3><p>网格勾画了一个模型的基本框架。</p>
<h3 id="纹理"><a href="#纹理" class="headerlink" title="纹理"></a>纹理</h3><p>纹理可以使物体的表面富有细节。添加纹理，需要先创建一个新的材质，然后将材质添加到对象上面。然后通过材质来修改材质，材质属性中，有纹理属性，可以用来设置纹理，纹理可以是一张图片。</p>
<h3 id="材质和着色器"><a href="#材质和着色器" class="headerlink" title="材质和着色器"></a>材质和着色器</h3><p>材质能够将纹理应用到模型上，着色器界定纹理呈现出的最终效果。</p>
<h3 id="工程与应用程序"><a href="#工程与应用程序" class="headerlink" title="工程与应用程序"></a>工程与应用程序</h3><p>一个Unity工程一般包含下面几个文件目录：</p>
<blockquote>
<p>Assets 存放的是项目所需的资源。<br>Library 存放的是所需要的库文件。<br>ProjectSettings 里面存放的是工程设置文件。<br>Temp 里面存放的是临时文件。</p>
</blockquote>
<p>游戏对象的组件<br>Transform：控制对象的位置、旋转和缩放。以及游戏对象的父子关系。<br>Mesh Filter：显示网格。<br>xxx Collider：给物体添加碰撞器。<br>Mesh Rendered：渲染器。可以给物体添加材质、纹理以及渲染方式。<br>Script：脚本也是组件。如果一个脚本想要挂载到一个游戏对象上面，那么它必须继承MonoBehaviour。</p>
<h3 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h3><p>首先，一个脚本属于一个游戏对象的一个组件。如果一个脚本想要能够挂载到一个游戏对象上，那么这个脚本必须是要继承MonoBehaviour的。MonoBehaviour为脚本提供了很多已经实现的方法和属性。</p>
<h4 id="获取脚本所挂载到的对象。"><a href="#获取脚本所挂载到的对象。" class="headerlink" title="获取脚本所挂载到的对象。"></a>获取脚本所挂载到的对象。</h4><p>在脚本中，想要获取当前脚本所挂载的对象，只需要调用属性gameObject（注意大小写）即可得到。通过这个gameObject对象，就可以获得游戏对象的各个属性和其他的组件。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gameObject.name  <span class="comment">//获取游戏对象的名字</span></div></pre></td></tr></table></figure></p>
<h4 id="获取脚本挂载对象的Transform组件"><a href="#获取脚本挂载对象的Transform组件" class="headerlink" title="获取脚本挂载对象的Transform组件"></a>获取脚本挂载对象的Transform组件</h4><p>每个游戏对象都会包含一个名为Transform组件，在继承了MonoBehaviour类的脚本中，可以通过transform属性来获取脚本所挂载到对象的Transform组件。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">transform.position.x <span class="comment">//游戏对象位置的x坐标</span></div></pre></td></tr></table></figure></p>
<h4 id="获取脚本挂载对象的其他组件"><a href="#获取脚本挂载对象的其他组件" class="headerlink" title="获取脚本挂载对象的其他组件"></a>获取脚本挂载对象的其他组件</h4><p>要获取那些不常用的组件，需要使用GetComponent方法。如：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Test t = GetComponent &lt;Test&gt; (); <span class="comment">//Test为组件名称，这里是一个名为Test的Script组件。GetComponent是gameObject的一个方法。</span></div><div class="line">t.age = <span class="number">24</span>;</div></pre></td></tr></table></figure></p>
<h4 id="脚本的生命周期"><a href="#脚本的生命周期" class="headerlink" title="脚本的生命周期"></a>脚本的生命周期</h4>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是学习Unity3D过程中的一些学习笔记，主要为个人学习使用，如能帮助别人纯属意外。&lt;/p&gt;
&lt;h2 id=&quot;游戏对象以及基本操作&quot;&gt;&lt;a href=&quot;#游戏对象以及基本操作&quot; class=&quot;headerlink&quot; title=&quot;游戏对象以及基本操作&quot;&gt;&lt;/a&gt;游戏对象
    
    </summary>
    
      <category term="unity" scheme="http://baimoon.github.io/categories/unity/"/>
    
    
      <category term="unity" scheme="http://baimoon.github.io/tags/unity/"/>
    
  </entry>
  
  <entry>
    <title>redis_lua</title>
    <link href="http://baimoon.github.io/2017/03/08/redis-lua/"/>
    <id>http://baimoon.github.io/2017/03/08/redis-lua/</id>
    <published>2017-03-08T02:32:34.000Z</published>
    <updated>2017-03-08T02:32:34.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Shell Study</title>
    <link href="http://baimoon.github.io/2017/02/23/shell-study/"/>
    <id>http://baimoon.github.io/2017/02/23/shell-study/</id>
    <published>2017-02-23T14:31:10.000Z</published>
    <updated>2017-03-09T16:29:56.000Z</updated>
    
    <content type="html"><![CDATA[<hr>
<p>本文记录一些自己在使用shell进行批量操作、任务调度等工作时用到的一些shell的基础知识，在此记录以备翻阅和查找。</p>
<a id="more"></a>
<h1 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h1><h2 id="变量的定义和使用"><a href="#变量的定义和使用" class="headerlink" title="变量的定义和使用"></a>变量的定义和使用</h2><h3 id="变量的定义"><a href="#变量的定义" class="headerlink" title="变量的定义"></a>变量的定义</h3><p>在定义变量时，变量名不加美元符号，例如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">val_name=<span class="string">"this is value"</span></div></pre></td></tr></table></figure></p>
<p>需要注意的是，变量名和等号之间不能有空格。同时，变量名的命名遵循如下规则：</p>
<blockquote>
<p>首个字符必须是字母(a-z A-Z)。<br>中间不能有空格，可以使用下划线。<br>不能使用标点符号。<br>不能使用bash中的关键字。<br>除了显式的直接赋值，还可以使用语句给变量赋值，如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> file <span class="keyword">in</span> `ls /etc`</div></pre></td></tr></table></figure></p>
</blockquote>
<h3 id="使用变量"><a href="#使用变量" class="headerlink" title="使用变量"></a>使用变量</h3><p>使用一个定义过的变量，只要在变量名前面加美元符号，例如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> <span class="variable">$val_name</span></div><div class="line"><span class="built_in">echo</span> <span class="variable">$&#123;val_name&#125;</span></div></pre></td></tr></table></figure></p>
<p>上面的花括号是可选的，为了帮助解释器识别变量的边界，如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> <span class="string">"this is my <span class="variable">$&#123;val_name&#125;</span>value"</span></div></pre></td></tr></table></figure></p>
<p>注意，重复使用的变量赋值时不可以使用$符号。</p>
<h3 id="只读变量"><a href="#只读变量" class="headerlink" title="只读变量"></a>只读变量</h3><p>使用readonly命令定义可以将变量定义为只读变量，只读变量的值不能被改变。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">baidu_uri=<span class="string">"baidu.com"</span></div><div class="line"><span class="built_in">readonly</span> baidu_uri</div><div class="line">baidu_uri=<span class="string">"google.com"</span></div></pre></td></tr></table></figure></p>
<p>这样的执行就会报错“/bin/sh: NAME: This variable is read only.”。</p>
<h3 id="删除变量"><a href="#删除变量" class="headerlink" title="删除变量"></a>删除变量</h3><p>使用unset命令可以删除变量，但是不能删除只读变量：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/sh</span></div><div class="line">myUrl=<span class="string">"http://www.baimoon.com"</span></div><div class="line"><span class="built_in">unset</span> myUrl</div></pre></td></tr></table></figure></p>
<h3 id="变量类型"><a href="#变量类型" class="headerlink" title="变量类型"></a>变量类型</h3><p>运行shell的时候，会同时存在三种变量：</p>
<blockquote>
<p>局部变量：局部变量在脚本或命令中定义，仅在当前shell实例中有效，其他shell启动的程序不能访问局部变量。<br>环境变量：所有程序，包括shell启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。必要的时候，shel也可以定义环境变量。<br>shell变量：shell变量是由shell程序设置的特殊变量。shell变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了shell的正常运行。</p>
</blockquote>
<h2 id="shell字符串"><a href="#shell字符串" class="headerlink" title="shell字符串"></a>shell字符串</h2><p>字符串是shell编程中最常用最有用的数据类型，字符串可以用单引号，也可以使用双引号，也可以不用引号。<br>使用单引号的限制：</p>
<blockquote>
<p>单引号里的任何字符都会原样输出，单引号中的变量是无效的。<br>单引号字符串中不能出现单引号（对单引号使用转义字符也不行）。</p>
</blockquote>
<p>双引号的优点：</p>
<blockquote>
<p>双引号里可以有变量。<br>双引号里可以出现转义字符。</p>
</blockquote>
<h3 id="拼接字符串"><a href="#拼接字符串" class="headerlink" title="拼接字符串"></a>拼接字符串</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">your_name=<span class="string">"qinjx"</span></div><div class="line">greeting=<span class="string">"hello, "</span><span class="variable">$your_name</span><span class="string">" !"</span></div><div class="line">greeting_1=<span class="string">"hello, <span class="variable">$&#123;your_name&#125;</span> !"</span></div><div class="line"><span class="built_in">echo</span> <span class="variable">$greeting</span> <span class="variable">$greeting_1</span></div></pre></td></tr></table></figure>
<h3 id="获取字符串长度"><a href="#获取字符串长度" class="headerlink" title="获取字符串长度"></a>获取字符串长度</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">string=<span class="string">"abcd"</span></div><div class="line"><span class="built_in">echo</span> <span class="variable">$&#123;#string&#125;</span> <span class="comment">#输出 4</span></div></pre></td></tr></table></figure>
<h3 id="提取子字符串"><a href="#提取子字符串" class="headerlink" title="提取子字符串"></a>提取子字符串</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">string=<span class="string">"runoob is a great site"</span></div><div class="line"><span class="built_in">echo</span> <span class="variable">$&#123;string:1:4&#125;</span> <span class="comment"># 输出 unoo</span></div></pre></td></tr></table></figure>
<h3 id="查找子字符串"><a href="#查找子字符串" class="headerlink" title="查找子字符串"></a>查找子字符串</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">string=<span class="string">"runoob is a great company"</span></div><div class="line"><span class="built_in">echo</span> `expr index <span class="string">"<span class="variable">$string</span>"</span> is`  <span class="comment"># 输出 8</span></div></pre></td></tr></table></figure>
<h2 id="Shell数组"><a href="#Shell数组" class="headerlink" title="Shell数组"></a>Shell数组</h2><p>bash支持一维数组（不支持多维数组），并且没有限定数组的大小。数组的下标由0开始，获取数组的元素需要使用下标，下标可以是整数或算数表达式，算数表达式的值应该大于等于0。</p>
<h3 id="数组定义"><a href="#数组定义" class="headerlink" title="数组定义"></a>数组定义</h3><p>在shell中，用括号来表示数组，数组元素使用空格符号来分割。定义数组的一般形式为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">数组名=(值1 值2 ... 值n)</div></pre></td></tr></table></figure></p>
<p>例如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">array_name=(value0 value1 value2 value3)</div></pre></td></tr></table></figure></p>
<p>或<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">array_name=(</div><div class="line">value0</div><div class="line">value1</div><div class="line">value2</div><div class="line">value3</div><div class="line">)</div></pre></td></tr></table></figure></p>
<p>还可以单独定义数组的各个分量：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">array_name[0]=value0</div><div class="line">array_name[1]=value1</div><div class="line">array_name[n]=valuen</div></pre></td></tr></table></figure></p>
<p>可以不使用连续的下标，而且下标的范围没有限制。</p>
<h3 id="读取数组"><a href="#读取数组" class="headerlink" title="读取数组"></a>读取数组</h3><p>读取数组需要使用下标，一般格式为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$&#123;数组名[下标]&#125;</span></div></pre></td></tr></table></figure></p>
<p>使用@符号可以获取数组中的所有元素：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> <span class="variable">$&#123;array_name[@]&#125;</span></div></pre></td></tr></table></figure></p>
<h3 id="获取数组的长度"><a href="#获取数组的长度" class="headerlink" title="获取数组的长度"></a>获取数组的长度</h3><p>获取数组长度的方法与获取字符串长度的方法相同：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 取得数组元素的个数</span></div><div class="line">length=<span class="variable">$&#123;#array_name[@]&#125;</span></div><div class="line"><span class="comment"># 或者</span></div><div class="line">length=<span class="variable">$&#123;#array_name[*]&#125;</span></div><div class="line"><span class="comment"># 取得数组单个元素的长度</span></div><div class="line">lengthn=<span class="variable">$&#123;#array_name[n]&#125;</span></div></pre></td></tr></table></figure></p>
<h2 id="Shell的注释"><a href="#Shell的注释" class="headerlink" title="Shell的注释"></a>Shell的注释</h2><p>以”#”开头的行就是注释，会被解释器忽略。<br>shell中没有多行注释，只能每行添加一个”#”。</p>
<h2 id="Shell传递参数"><a href="#Shell传递参数" class="headerlink" title="Shell传递参数"></a>Shell传递参数</h2><p>我们可以在执行shell脚本时，向脚本传递参数。脚本获取参数的格式为$n，其中n代表一个数字。参数的下标是从1开始的，0表示执行的文件名。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"><span class="comment"># author:菜鸟教程</span></div><div class="line"><span class="comment"># url:www.runoob.com</span></div><div class="line"></div><div class="line"><span class="built_in">echo</span> <span class="string">"Shell 传递参数实例！"</span>;</div><div class="line"><span class="built_in">echo</span> <span class="string">"执行的文件名：<span class="variable">$0</span>"</span>;</div><div class="line"><span class="built_in">echo</span> <span class="string">"第一个参数为：<span class="variable">$1</span>"</span>;</div><div class="line"><span class="built_in">echo</span> <span class="string">"第二个参数为：<span class="variable">$2</span>"</span>;</div><div class="line"><span class="built_in">echo</span> <span class="string">"第三个参数为：<span class="variable">$3</span>"</span>;</div></pre></td></tr></table></figure></p>
<p>其他参数：</p>
<table>
<thead>
<tr>
<th style="text-align:left">参数表示</th>
<th style="text-align:left">参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$#</td>
<td style="text-align:left">传递到脚本的参数的个数</td>
</tr>
<tr>
<td style="text-align:left">$*</td>
<td style="text-align:left">以一个字符串显示所有向脚本传递的所有参数</td>
</tr>
<tr>
<td style="text-align:left">$$</td>
<td style="text-align:left">运行脚本的当前进程的ID</td>
</tr>
<tr>
<td style="text-align:left">$!</td>
<td style="text-align:left">后台运行的最后一个进程的ID号</td>
</tr>
<tr>
<td style="text-align:left">$@</td>
<td style="text-align:left">与$*相同，但是使用时加引号，并在引号中返回每个参数</td>
</tr>
<tr>
<td style="text-align:left">$-</td>
<td style="text-align:left">显示Shell使用的当前选项，与set命令功能相同</td>
</tr>
<tr>
<td style="text-align:left">$?</td>
<td style="text-align:left">显示命令的退出状态。0表示没有错误，其他值表示有错误</td>
</tr>
</tbody>
</table>
<h3 id="与-的区别"><a href="#与-的区别" class="headerlink" title="$*与$@的区别"></a>$*与$@的区别</h3><blockquote>
<p>相同点：都是引用所有参数。<br>不同点：只有在双引号中体现出来。假设在脚本运行时写了三个参数1 2 3，则$*等价于”1 2 3”（一个参数），而$@等价于”1” “2” “3”（三个参数）。</p>
</blockquote>
<h2 id="Shell基本运算符"><a href="#Shell基本运算符" class="headerlink" title="Shell基本运算符"></a>Shell基本运算符</h2><p>shell支持的基本运算符有：</p>
<blockquote>
<p>算数运算符<br>关系运算符<br>布尔运算符<br>字符串运算符<br>文件测试运算符</p>
</blockquote>
<p>原生的bash不支持简单的数学运算，但是可以通过其他命令来实现，例如awk和expr。其中expr比较常用。expr是一款表达式计算工具，使用它能够完成表达式的求值操作。需要注意的两点：</p>
<blockquote>
<p>表达式和运算符之间必须有空格。<br>完整的表达式要被``包含起来。</p>
</blockquote>
<h3 id="算数运算符"><a href="#算数运算符" class="headerlink" title="算数运算符"></a>算数运算符</h3><table>
<thead>
<tr>
<th style="text-align:left">运算符</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">举例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">+</td>
<td style="text-align:left">加法</td>
<td style="text-align:left"><code>expr $a + $b</code>就是在计算a与b的和</td>
</tr>
<tr>
<td style="text-align:left">-</td>
<td style="text-align:left">减法</td>
<td style="text-align:left"><code>expr $a - $b</code>就是在计算a与b的查</td>
</tr>
<tr>
<td style="text-align:left">*</td>
<td style="text-align:left">乘法</td>
<td style="text-align:left"><code>expr $a \* $b</code>就是在计算a与b的乘积</td>
</tr>
<tr>
<td style="text-align:left">/</td>
<td style="text-align:left">除法</td>
<td style="text-align:left"><code>expr $a / $b</code>就是在计算a与b的商</td>
</tr>
<tr>
<td style="text-align:left">%</td>
<td style="text-align:left">取余</td>
<td style="text-align:left"><code>expr $a % $b</code>就是在计算a与b的余</td>
</tr>
<tr>
<td style="text-align:left">=</td>
<td style="text-align:left">赋值</td>
<td style="text-align:left">a=$b，将b的值赋值给a</td>
</tr>
<tr>
<td style="text-align:left">==</td>
<td style="text-align:left">相等，比较左右两边是否相当</td>
<td style="text-align:left">[ $a == $b ]比较a和b是否相当</td>
</tr>
<tr>
<td style="text-align:left">!=</td>
<td style="text-align:left">不想当，比较左右两边是否不相等</td>
<td style="text-align:left">[ $a != $b ]</td>
</tr>
</tbody>
</table>
<p>注意，</p>
<blockquote>
<p>条件表达式要放在放括号之间，并且要有空格。<br>乘号（<em>）前面必须加反斜杠才能实现乘法运算。<br>在MAC中，shell的expr语法是$(())，此处表达式中的</em>不需要反斜杠来转意。</p>
</blockquote>
<h3 id="关系运算符"><a href="#关系运算符" class="headerlink" title="关系运算符"></a>关系运算符</h3><p>关系运算符只支持数字，不支持字符串，除非字符串的值是数字。</p>
<table>
<thead>
<tr>
<th style="text-align:left">运算符</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">举例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">-eq</td>
<td style="text-align:left">检测两个数是否相等，相等返回true</td>
<td style="text-align:left">[ $a -eq $b ]</td>
</tr>
<tr>
<td style="text-align:left">-ne</td>
<td style="text-align:left">检测两个数是否相等，不相等返回true</td>
<td style="text-align:left">[ $a -ne $b ]</td>
</tr>
<tr>
<td style="text-align:left">-gt</td>
<td style="text-align:left">检测左边的数是否大于右边的数，大于返回true</td>
<td style="text-align:left">[ $a -gt $b ]</td>
</tr>
<tr>
<td style="text-align:left">-lt</td>
<td style="text-align:left">检测左边的数是否小于右边的数，小于返回true</td>
<td style="text-align:left">[ $a -lt $b ]</td>
</tr>
<tr>
<td style="text-align:left">-ge</td>
<td style="text-align:left">检测左边的数是否大于等于右边的数，大于等于则返回true</td>
<td style="text-align:left">[ $a -ge $b ]</td>
</tr>
<tr>
<td style="text-align:left">-le</td>
<td style="text-align:left">检测左边的数是否小于等于右边的数，小于等于则返回true</td>
<td style="text-align:left">[ $a -le $b ]</td>
</tr>
</tbody>
</table>
<h3 id="布尔运算符"><a href="#布尔运算符" class="headerlink" title="布尔运算符"></a>布尔运算符</h3><table>
<thead>
<tr>
<th style="text-align:left">运算符</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">举例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">!</td>
<td style="text-align:left">非运算</td>
<td style="text-align:left">[ !false ] 返回true</td>
</tr>
<tr>
<td style="text-align:left">-o</td>
<td style="text-align:left">或运算</td>
<td style="text-align:left">[ $a -lt 20 -o $b -gt 30 ]</td>
</tr>
<tr>
<td style="text-align:left">-a</td>
<td style="text-align:left">与运算</td>
<td style="text-align:left">[ $a -lt 20 -a $b -gt 30 ]</td>
</tr>
<tr>
<td style="text-align:left">&amp;&amp;</td>
<td style="text-align:left">逻辑与运算符</td>
<td style="text-align:left">[[ $a -lt 100 &amp;&amp; $b -lt 20 ]]</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">逻辑或运算符</td>
<td style="text-align:left">[[ $a -lt 100</td>
<td></td>
<td>$b -lt 20 ]]</td>
</tr>
</tbody>
</table>
<h3 id="字符串运算符"><a href="#字符串运算符" class="headerlink" title="字符串运算符"></a>字符串运算符</h3><table>
<thead>
<tr>
<th style="text-align:left">运算符</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">举例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">=</td>
<td style="text-align:left">检测两个字符串是否相等，相等返回true</td>
<td style="text-align:left">[ $a = $b ]</td>
</tr>
<tr>
<td style="text-align:left">!=</td>
<td style="text-align:left">检测两个字符串是否相等，不相等返回true</td>
<td style="text-align:left">[ $a != $b ]</td>
</tr>
<tr>
<td style="text-align:left">-z</td>
<td style="text-align:left">检测字符串长度是否为0，为0返回true</td>
<td style="text-align:left">[ -z $a ]</td>
</tr>
<tr>
<td style="text-align:left">-n</td>
<td style="text-align:left">检测字符串长度是否为0，不为0返回true</td>
<td style="text-align:left">[ -n $a ]</td>
</tr>
<tr>
<td style="text-align:left">str</td>
<td style="text-align:left">检测字符串是否为空，不为空返回true</td>
<td style="text-align:left">[ $a ]</td>
</tr>
</tbody>
</table>
<h3 id="文件测试运算符"><a href="#文件测试运算符" class="headerlink" title="文件测试运算符"></a>文件测试运算符</h3><table>
<thead>
<tr>
<th style="text-align:left">操作符</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">举例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">-b file</td>
<td style="text-align:left">检测文件是否是二进制文件，是则返回true</td>
<td style="text-align:left">[ -b $file ]</td>
</tr>
<tr>
<td style="text-align:left">-c file</td>
<td style="text-align:left">检测文件是否是字符文件，是则返回ture</td>
<td style="text-align:left">[ -b $file ]</td>
</tr>
<tr>
<td style="text-align:left">-d file</td>
<td style="text-align:left">检测文件是否是目录，是则返回true</td>
<td style="text-align:left">[ -d $file ]</td>
</tr>
<tr>
<td style="text-align:left">-f file</td>
<td style="text-align:left">检测文件是否是普通文件（既不是目录，也不是设备文件）</td>
<td style="text-align:left">[ -f $file ]</td>
</tr>
<tr>
<td style="text-align:left">-g file</td>
<td style="text-align:left">检测文件是否设置了SGID位，是则返回true</td>
<td style="text-align:left">[ -g $file ]</td>
</tr>
<tr>
<td style="text-align:left">-k file</td>
<td style="text-align:left">检测文件是否设置了粘着位(Sticky Bit)，是则返回true</td>
<td style="text-align:left">[ -k $file ]</td>
</tr>
<tr>
<td style="text-align:left">-p file</td>
<td style="text-align:left">检测文件是否是有名管道，是则返回true</td>
<td style="text-align:left">[ -p $file ]</td>
</tr>
<tr>
<td style="text-align:left">-u file</td>
<td style="text-align:left">检测文件是否设置了 SUID 位，是则返回true</td>
<td style="text-align:left">[ -u $file ]</td>
</tr>
<tr>
<td style="text-align:left">-r file</td>
<td style="text-align:left">检测文件是否可读，是则返回true</td>
<td style="text-align:left">[ -r $file ]</td>
</tr>
<tr>
<td style="text-align:left">-w file</td>
<td style="text-align:left">检测文件是否可写，是则返回true</td>
<td style="text-align:left">[ -w $file ]</td>
</tr>
<tr>
<td style="text-align:left">-x file</td>
<td style="text-align:left">检测文件是否可执行，是则返回true</td>
<td style="text-align:left">[ -x $file ]</td>
</tr>
<tr>
<td style="text-align:left">-s file</td>
<td style="text-align:left">检测文件是否为空，不为空则返回true</td>
<td style="text-align:left">[ -s $file ]</td>
</tr>
<tr>
<td style="text-align:left">-e file</td>
<td style="text-align:left">检测文件是否存在，存在则返回true</td>
<td style="text-align:left">[ -e $file ]</td>
</tr>
</tbody>
</table>
<h2 id="Shell-echo命令"><a href="#Shell-echo命令" class="headerlink" title="Shell echo命令"></a>Shell echo命令</h2><p>Shell的echo命令用于字符串的输出。命令格式：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> string</div></pre></td></tr></table></figure></p>
<p>可以使用echo实现更加复杂的输出格式控制。</p>
<h3 id="显示普通字符串"><a href="#显示普通字符串" class="headerlink" title="显示普通字符串"></a>显示普通字符串</h3><p>显示普通的字符串，如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> <span class="string">"It is a test"</span></div></pre></td></tr></table></figure></p>
<p>这里的双引号完全可以省略：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> It is a <span class="built_in">test</span></div></pre></td></tr></table></figure></p>
<h3 id="显示变量"><a href="#显示变量" class="headerlink" title="显示变量"></a>显示变量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$name</span> It is a test"</span></div><div class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;name&#125;</span> It is a test"</span></div></pre></td></tr></table></figure>
<h3 id="显示换行"><a href="#显示换行" class="headerlink" title="显示换行"></a>显示换行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> <span class="_">-e</span> <span class="string">"OK! \n"</span></div></pre></td></tr></table></figure>
<h3 id="显示结果定向到文件"><a href="#显示结果定向到文件" class="headerlink" title="显示结果定向到文件"></a>显示结果定向到文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> <span class="string">"It is a test"</span> &gt; myfile</div></pre></td></tr></table></figure>
<h3 id="原样输出字符串，不进行转义或取变量-用单引号"><a href="#原样输出字符串，不进行转义或取变量-用单引号" class="headerlink" title="原样输出字符串，不进行转义或取变量(用单引号)"></a>原样输出字符串，不进行转义或取变量(用单引号)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> <span class="string">'$name\"'</span></div></pre></td></tr></table></figure>
<h3 id="显示命令执行结果"><a href="#显示命令执行结果" class="headerlink" title="显示命令执行结果"></a>显示命令执行结果</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> `date`</div></pre></td></tr></table></figure>
<h2 id="Shell-printf命令"><a href="#Shell-printf命令" class="headerlink" title="Shell printf命令"></a>Shell printf命令</h2><p>printf是标准所定义，因此使用它比使用echo移植性好。printf使用引用文本或空格分隔的参数，外面可以在printf中使用格式化字符串，还可以定制字符串的宽度、左右对齐方式等。默认printf不会像echo那样自动添加换行，需要手动添加\n。<br>printf语法：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">printf</span> format-string [arguments ... ]</div></pre></td></tr></table></figure></p>
<p>其中 format-string为格式控制字符串    ；arguments为参数列表。<br>示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"> </div><div class="line"><span class="built_in">printf</span> <span class="string">"%-10s %-8s %-4s\n"</span> 姓名 性别 体重kg  </div><div class="line"><span class="built_in">printf</span> <span class="string">"%-10s %-8s %-4.2f\n"</span> 郭靖 男 66.1234 </div><div class="line"><span class="built_in">printf</span> <span class="string">"%-10s %-8s %-4.2f\n"</span> 杨过 男 48.6543 </div><div class="line"><span class="built_in">printf</span> <span class="string">"%-10s %-8s %-4.2f\n"</span> 郭芙 女 47.9876</div></pre></td></tr></table></figure></p>
<p>%s %c %d %f都是格式替代符。%-10s 指一个宽度为10个字符（-表示左对齐，没有则表示右对齐），任何字符都会被显示在10个字符宽的字符内，如果不足则自动以空格填充，超过也会将内容全部显示出来。<br>%-4.2f 指格式化为小数，其中.2指保留2位小数。</p>
<p>printf的转义序列</p>
<table>
<thead>
<tr>
<th style="text-align:left">序列</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">\a</td>
<td style="text-align:left">警告字符，通常为ASCII的BEL字符</td>
</tr>
<tr>
<td style="text-align:left">\b</td>
<td style="text-align:left">后退</td>
</tr>
<tr>
<td style="text-align:left">\c</td>
<td style="text-align:left">抑制（不显示）输出结果中任何结尾的换行符（只在%b格式指示符控制下的参数字符串有效），而且任何留在参数里的字符、任何接下来的参数以及任何留在格式字符串中的字符都会被忽略</td>
</tr>
<tr>
<td style="text-align:left">\f</td>
<td style="text-align:left">换页</td>
</tr>
<tr>
<td style="text-align:left">\n</td>
<td style="text-align:left">换行</td>
</tr>
<tr>
<td style="text-align:left">\r</td>
<td style="text-align:left">回车</td>
</tr>
<tr>
<td style="text-align:left">\t</td>
<td style="text-align:left">水平制表符</td>
</tr>
<tr>
<td style="text-align:left">\v</td>
<td style="text-align:left">垂直制表符</td>
</tr>
<tr>
<td style="text-align:left">\</td>
<td style="text-align:left">转义的饭斜杠</td>
</tr>
<tr>
<td style="text-align:left">\ddd</td>
<td style="text-align:left">表示1到3位八进制的字符。仅在格式字符串中有效</td>
</tr>
<tr>
<td style="text-align:left">\0ddd</td>
<td style="text-align:left">表示1到3位的八进制字符</td>
</tr>
</tbody>
</table>
<h2 id="Shell-test命令"><a href="#Shell-test命令" class="headerlink" title="Shell test命令"></a>Shell test命令</h2><p>test命令用于检测某个条件是否成立，他可以进行数值、字符和文件三个方面的测试：</p>
<h3 id="数值测试"><a href="#数值测试" class="headerlink" title="数值测试"></a>数值测试</h3><table>
<thead>
<tr>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">-eq</td>
<td style="text-align:left">是否等于</td>
</tr>
<tr>
<td style="text-align:left">-ne</td>
<td style="text-align:left">是否不等于</td>
</tr>
<tr>
<td style="text-align:left">-gt</td>
<td style="text-align:left">是否大于</td>
</tr>
<tr>
<td style="text-align:left">-ge</td>
<td style="text-align:left">是否大于等于</td>
</tr>
<tr>
<td style="text-align:left">-lt</td>
<td style="text-align:left">是否小于</td>
</tr>
<tr>
<td style="text-align:left">-le</td>
<td style="text-align:left">是否小于等于</td>
</tr>
</tbody>
</table>
<p>示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> <span class="built_in">test</span> <span class="variable">$&#123;num1&#125;</span> <span class="_">-eq</span> <span class="variable">$&#123;num2&#125;</span></div><div class="line"><span class="keyword">then</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"eq"</span></div><div class="line"><span class="keyword">else</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"not eq"</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure></p>
<h3 id="字符串测试"><a href="#字符串测试" class="headerlink" title="字符串测试"></a>字符串测试</h3><p>字符串测试可用的操作符，详见”Shell基本运算符”章节。<br>示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> <span class="built_in">test</span> <span class="variable">$str1</span> = <span class="variable">$str2</span></div><div class="line"><span class="keyword">then</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"eq"</span></div><div class="line"><span class="keyword">else</span></div><div class="line">	<span class="built_in">echo</span> <span class="string">"not eq"</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure></p>
<h3 id="文件测试"><a href="#文件测试" class="headerlink" title="文件测试"></a>文件测试</h3><p>文件测试可用的操作符，详见”Shell基本运算符”章节。<br>示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> <span class="built_in">test</span> <span class="_">-e</span> ./notFile -o <span class="_">-e</span> ./bash</div><div class="line"><span class="keyword">then</span></div><div class="line">	<span class="built_in">echo</span> <span class="string">'samething is here'</span></div><div class="line"><span class="keyword">else</span></div><div class="line">	<span class="built_in">echo</span> <span class="string">'nothing is here'</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure></p>
<h2 id="Shell-流控制"><a href="#Shell-流控制" class="headerlink" title="Shell 流控制"></a>Shell 流控制</h2><p>和其他语言的流控制不同，shell的流控制语句不能为空，如果为空就不要写。</p>
<h3 id="if-else"><a href="#if-else" class="headerlink" title="if else"></a>if else</h3><h4 id="if语句的语法格式"><a href="#if语句的语法格式" class="headerlink" title="if语句的语法格式"></a>if语句的语法格式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> condition</div><div class="line"><span class="keyword">then</span></div><div class="line">	<span class="built_in">command</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure>
<p>如果写成一行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> [ condition ]; <span class="keyword">then</span> <span class="built_in">command</span>; <span class="keyword">fi</span></div></pre></td></tr></table></figure></p>
<h4 id="if-else语句的语法格式"><a href="#if-else语句的语法格式" class="headerlink" title="if else语句的语法格式"></a>if else语句的语法格式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> condition</div><div class="line"><span class="keyword">then</span></div><div class="line">	<span class="built_in">command</span>1</div><div class="line">	<span class="built_in">command</span>2</div><div class="line"><span class="keyword">else</span></div><div class="line">	<span class="built_in">command</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure>
<h4 id="if-else-if-else语句的语法格式"><a href="#if-else-if-else语句的语法格式" class="headerlink" title="if else-if else语句的语法格式"></a>if else-if else语句的语法格式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> condition</div><div class="line"><span class="keyword">then</span></div><div class="line">	<span class="built_in">command</span>1</div><div class="line"><span class="keyword">elif</span></div><div class="line"><span class="keyword">then</span></div><div class="line">	<span class="built_in">command</span>2</div><div class="line"><span class="keyword">else</span></div><div class="line">	<span class="built_in">command</span>3</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure>
<p>if else语句经常与test命令结合使用<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> <span class="built_in">test</span> <span class="variable">$&#123;num1&#125;</span> <span class="_">-eq</span> <span class="variable">$&#123;num2&#125;</span></div><div class="line"><span class="keyword">then</span></div><div class="line">	<span class="built_in">echo</span> <span class="string">"eq"</span></div><div class="line"><span class="keyword">else</span></div><div class="line">	<span class="built_in">echo</span> <span class="string">"not eq"</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure></p>
<h3 id="for循环"><a href="#for循环" class="headerlink" title="for循环"></a>for循环</h3><p>for循环与其他编程语言类似，一般格式为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> var <span class="keyword">in</span> item1 item2 itemN</div><div class="line"><span class="keyword">do</span></div><div class="line">	<span class="built_in">command</span>1</div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure></p>
<p>如果写成一行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> var <span class="keyword">in</span> item1 item2 itemN; <span class="keyword">do</span> <span class="built_in">command</span>; <span class="built_in">command</span>2; <span class="built_in">command</span>N <span class="keyword">done</span>;</div></pre></td></tr></table></figure></p>
<p>循环体中的命令可以是任何有效的shell和语句。in列表是可选的，如果不使用它，for循环使用命令行的位置参数。<br>无限循环：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span>(( ; ; ))</div></pre></td></tr></table></figure></p>
<h3 id="while循环"><a href="#while循环" class="headerlink" title="while循环"></a>while循环</h3><p>while循环用于不断的执行一系列命令，也用于从文件中读取数据。其格式为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> condition</div><div class="line"><span class="keyword">do</span></div><div class="line">	<span class="built_in">command</span></div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure></p>
<p>无限循环：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> :</div><div class="line"><span class="keyword">do</span></div><div class="line">	<span class="built_in">command</span></div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure></p>
<h3 id="until循环"><a href="#until循环" class="headerlink" title="until循环"></a>until循环</h3><p>until循环执行一系列命令，直到条件为true时停止。until循环与while循环在处理方式上刚好相反。一般while循环由于until循环，但在某些情况下，until循环更加有用。<br>until语法格式：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">until condition</div><div class="line"><span class="keyword">do</span></div><div class="line">	<span class="built_in">command</span></div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure></p>
<p>条件可以为任意测试条件，测试发生在循环末尾，因此循环至少执行一次。</p>
<h3 id="case语句"><a href="#case语句" class="headerlink" title="case语句"></a>case语句</h3><p>case语句为多选择语句。可以用case语句匹配一个值与一个模式，如果匹配成功，执行匹配的命令。case语句格式如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">case</span> value <span class="keyword">in</span></div><div class="line">模式1) </div><div class="line">	<span class="built_in">command</span>1</div><div class="line">	;;</div><div class="line">模式2)</div><div class="line">	<span class="built_in">command</span>2</div><div class="line">	;;</div><div class="line">*)</div><div class="line">	;;</div><div class="line"><span class="keyword">esac</span></div></pre></td></tr></table></figure></p>
<p>case工作方式如上所示。取值后面必须为单词in，每一种模式必须以右括号结束。取值可以为变量或常数。匹配发现取值符合某一模式后，其间所有命令开始执行至;;。<br>取值将检测匹配的每一个模式。一旦模式匹配，则执行完全匹配模式响应命令后不再继续匹配其他模式。如果无一匹配模式，使用星号捕获，再执行后面的命令。<br>case的语法和传统的编程语言不太一样，它需要一个esac来结束case语句，每个case分值用右括号来表示，并用两个分号表示break。</p>
<h3 id="跳出循环"><a href="#跳出循环" class="headerlink" title="跳出循环"></a>跳出循环</h3><p>在执行循环的过程中，有时候需要在某种特定情况下终止循环，shell中使用两个命令来实现：continue和break。</p>
<h4 id="break语句"><a href="#break语句" class="headerlink" title="break语句"></a>break语句</h4><p>break语句用来跳出并结束循环（后面的循环都执行了）。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> :</div><div class="line"><span class="keyword">do</span></div><div class="line">	<span class="built_in">command</span></div><div class="line">	<span class="keyword">if</span> <span class="built_in">test</span> <span class="variable">$a</span> -q 1</div><div class="line">		<span class="built_in">break</span></div><div class="line">	<span class="keyword">fi</span></div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure></p>
<h4 id="continue语句"><a href="#continue语句" class="headerlink" title="continue语句"></a>continue语句</h4><p>continue语句也是用来跳出循环，但是只是跳出当前循环，只是continue语句后面的不执行了，但是下一个循环继续。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> :</div><div class="line"><span class="keyword">do</span></div><div class="line">	<span class="built_in">read</span> aNum</div><div class="line">	<span class="keyword">case</span> <span class="variable">$aNum</span> <span class="keyword">in</span></div><div class="line">		1|2|3)</div><div class="line">			<span class="built_in">echo</span> <span class="string">'1'</span></div><div class="line">		;;</div><div class="line">		*)</div><div class="line">			<span class="built_in">continue</span></div><div class="line">		;;</div><div class="line">	<span class="keyword">esac</span></div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure></p>
<h2 id="Shell文件的引入"><a href="#Shell文件的引入" class="headerlink" title="Shell文件的引入"></a>Shell文件的引入</h2><p>shell脚本也可以引入其他的shell脚本，这样可以很方便利用已有代码来提高效率。<br>shell文件引入的语法如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">. filename</div><div class="line">或</div><div class="line"><span class="built_in">source</span> filename</div></pre></td></tr></table></figure></p>
<p>注意，被引入的文件不需要可执行权限。</p>
<h1 id="高级使用"><a href="#高级使用" class="headerlink" title="高级使用"></a>高级使用</h1><h2 id="Shell的函数"><a href="#Shell的函数" class="headerlink" title="Shell的函数"></a>Shell的函数</h2><p>shell可以允许用户定义函数，然后在shell中随便调用。<br>shell中函数的定义格式如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[ <span class="keyword">function</span> ] funcName [ () ]</div><div class="line">&#123;</div><div class="line">	action;</div><div class="line">	[<span class="built_in">return</span> int;]</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>说明：</p>
<blockquote>
<p>定义函数时，关键字function可以省略；而且可以不带任何参数。<br>返回参数，可以显示的使用return语句，如果没有return语句，则最后一条命令的运行结果作为返回值。return后跟数值n(0-255)</p>
</blockquote>
<p>示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#! /bin/bash</span></div><div class="line"></div><div class="line"><span class="function"><span class="title">demoFun</span></span>() &#123;</div><div class="line">	<span class="built_in">echo</span> <span class="string">'this is a demo'</span></div><div class="line">	<span class="built_in">return</span> 1</div><div class="line">&#125;</div><div class="line"></div><div class="line">demoFun</div></pre></td></tr></table></figure></p>
<p>函数的返回值在调用函数后，通过$?来获取。<br>另外，函数在使用前，必须定义。这表示，函数的调用位置必须在函数的定义位置之后。而且函数的调用，只使用函数名即可。</p>
<h3 id="函数的参数"><a href="#函数的参数" class="headerlink" title="函数的参数"></a>函数的参数</h3><p>调用函数时可以向其传递参数。在函数内部，通过$n的形式来获取参数的值，例如$1表示第一个参数，$2表示第二个参数。当参数超过10个时，从第10个开始，需要使用${N}的方式来获取。<br>示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="title">funWithArg</span></span>()</div><div class="line">&#123;</div><div class="line">	<span class="built_in">echo</span> <span class="variable">$1</span></div><div class="line">	<span class="built_in">echo</span> <span class="variable">$&#123;10&#125;</span></div><div class="line">	<span class="built_in">return</span> <span class="variable">$3</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">funWithArg 1 2 3 4 5 6 7 8 9 10</div></pre></td></tr></table></figure></p>
<p>另外，还有几个特殊的字符来表示参数<br>| 参数 | 说明 |<br>|:————–|:———————|<br>| $#            | 传递到函数中的参数的个数 |<br>| $<em>            | 以一个字符串来显示所有传入到函数中的参数 |<br>| $$            | 脚本运行的当前进程ID |<br>| $!            | 后台运行的最后一个进程ID |<br>| $@            | 与$</em>相同，只是每个参数以带引号的方式返回 |<br>| $-            | 显示shell使用的当前选项，与set命令功能相同 |<br>| $?            | 表示函数的返回值 |</p>
<h2 id="时间的使用"><a href="#时间的使用" class="headerlink" title="时间的使用"></a>时间的使用</h2><p>在执行shell调度的时候，难免需要操作时间，来生成特定的时间格式。</p>
<h3 id="date命令"><a href="#date命令" class="headerlink" title="date命令"></a>date命令</h3><p>date命令的功能是显示和设置系统的日期和时间。date命令中的各个选项的含义分别为：</p>
<blockquote>
<p>-d, –date 提供日期字符串作为输入<br>-s, –set 设置日期格式<br>-u, –universal 显示或设置通用时间</p>
</blockquote>
<h3 id="日期格式字符串列表"><a href="#日期格式字符串列表" class="headerlink" title="日期格式字符串列表"></a>日期格式字符串列表</h3><p>注意使用这些符号时，加号和格式符号之间不能有空格，如：date +”%H”。</p>
<table>
<thead>
<tr>
<th style="text-align:left">格式字符串</th>
<th style="text-align:left">表示含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">%H</td>
<td style="text-align:left">小时，24小时制（00 - 23）</td>
</tr>
<tr>
<td style="text-align:left">%I</td>
<td style="text-align:left">小时，12小时制（01 - 12）</td>
</tr>
<tr>
<td style="text-align:left">%k</td>
<td style="text-align:left">小时，24小时制（1 - 24）</td>
</tr>
<tr>
<td style="text-align:left">%l</td>
<td style="text-align:left">小时，12小时制（1 - 12）</td>
</tr>
<tr>
<td style="text-align:left">%M</td>
<td style="text-align:left">分钟（01 - 59）</td>
</tr>
<tr>
<td style="text-align:left">%p</td>
<td style="text-align:left">显示AM或PM</td>
</tr>
<tr>
<td style="text-align:left">%r</td>
<td style="text-align:left">显示时间，以12小时制显示（hh:mm:ss %p)</td>
</tr>
<tr>
<td style="text-align:left">%s</td>
<td style="text-align:left">从1970年1月1日 00:00:00到目前经历的秒数</td>
</tr>
<tr>
<td style="text-align:left">%S</td>
<td style="text-align:left">显示秒（00-59）</td>
</tr>
<tr>
<td style="text-align:left">%T</td>
<td style="text-align:left">显示时间，以24小时制显示（hh:mm:ss）</td>
</tr>
<tr>
<td style="text-align:left">%Z</td>
<td style="text-align:left">显示时区，日期域（CST）</td>
</tr>
<tr>
<td style="text-align:left">%X</td>
<td style="text-align:left">显示时间的格式（%H:%M:%S），如：14时37分21秒</td>
</tr>
<tr>
<td style="text-align:left">%x</td>
<td style="text-align:left">显示日期的格式（%Y/%m/%d）</td>
</tr>
<tr>
<td style="text-align:left">%a</td>
<td style="text-align:left">星期的简称（Sun - Sat）</td>
</tr>
<tr>
<td style="text-align:left">%A</td>
<td style="text-align:left">星期的全称（Sunday - Staturday）</td>
</tr>
<tr>
<td style="text-align:left">%h, %b</td>
<td style="text-align:left">月的简称（Jan - Dec）</td>
</tr>
<tr>
<td style="text-align:left">%B</td>
<td style="text-align:left">月的全称（January - December）</td>
</tr>
<tr>
<td style="text-align:left">%c</td>
<td style="text-align:left">日期和时间（Tue Nov 20 14:12:48 2017）</td>
</tr>
<tr>
<td style="text-align:left">%d</td>
<td style="text-align:left">当日在当月中的天数（01 - 31）</td>
</tr>
<tr>
<td style="text-align:left">%D</td>
<td style="text-align:left">日期(mm/dd/yy)</td>
</tr>
<tr>
<td style="text-align:left">%x, %D</td>
<td style="text-align:left">日期（mm/dd/yy）</td>
</tr>
<tr>
<td style="text-align:left">%j</td>
<td style="text-align:left">一年的第几天（001 - 366）</td>
</tr>
<tr>
<td style="text-align:left">%m</td>
<td style="text-align:left">月份（01 - 12）</td>
</tr>
<tr>
<td style="text-align:left">%w</td>
<td style="text-align:left">当天是本周的第几天（0代表星期天）</td>
</tr>
<tr>
<td style="text-align:left">%W</td>
<td style="text-align:left">本周是一年的第几个星期（00 - 53， 星期一为第一天）</td>
</tr>
<tr>
<td style="text-align:left">%y</td>
<td style="text-align:left">年的最后两位数字（17）</td>
</tr>
<tr>
<td style="text-align:left">%Y</td>
<td style="text-align:left">年的完整表示（2017）</td>
</tr>
</tbody>
</table>
<h3 id="用例"><a href="#用例" class="headerlink" title="用例"></a>用例</h3><p>以24小时制输出当前时间<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ date +<span class="string">"%x %T"</span></div><div class="line">2017/03/03 14:29:14</div></pre></td></tr></table></figure></p>
<p>显示当日的月份，是今年第几周的星期几<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ date +<span class="string">"%B %W %w"</span></div><div class="line">三月 09 5</div></pre></td></tr></table></figure></p>
<h3 id="时间的加减"><a href="#时间的加减" class="headerlink" title="时间的加减"></a>时间的加减</h3><p>时间的加减使用-d参数，配合描述操作来完成的，例如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">date +<span class="string">"%Y%m%d"</span></div><div class="line"></div><div class="line">d=`date <span class="_">-d</span> +1 day +%D`</div><div class="line"><span class="built_in">echo</span> <span class="variable">$d</span></div></pre></td></tr></table></figure></p>
<p>其中”+1 day”就是描述操作，类似的还有”+1month”、”+1year”、”+1week”、”+1minute”和”+1second”等，其中减操作，使用”-“。<br>除了上面的计算，如果两个时间需要计算时间差，那么需要使用以一种变通的方式来解决，将时间转换为秒，然后在计算对应的时间差：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">last_year=`date <span class="_">-d</span> <span class="string">"2016-01-01 00:00:00"</span> +<span class="string">"%s"</span>`</div><div class="line">this_year=`date +%s`</div><div class="line"><span class="built_in">echo</span> <span class="string">"两个时间差"</span>$(((<span class="variable">$&#123;this_year&#125;</span>-<span class="variable">$&#123;last_year&#125;</span>) / 86400))<span class="string">"天"</span></div></pre></td></tr></table></figure></p>
<h2 id="休眠sleep"><a href="#休眠sleep" class="headerlink" title="休眠sleep"></a>休眠sleep</h2><p>在写一些以循环方式运行某些监控脚本时，设置间隔时间是必不可少的，那么就会用到休眠功能：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"><span class="keyword">for</span> ((i=0;<span class="variable">$i</span>&lt;=100;i++))</div><div class="line">  <span class="keyword">do</span></div><div class="line">    sleep 0.1</div><div class="line">  <span class="keyword">done</span></div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;本文记录一些自己在使用shell进行批量操作、任务调度等工作时用到的一些shell的基础知识，在此记录以备翻阅和查找。&lt;/p&gt;
    
    </summary>
    
      <category term="unix" scheme="http://baimoon.github.io/categories/unix/"/>
    
    
      <category term="shell" scheme="http://baimoon.github.io/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Notebook</title>
    <link href="http://baimoon.github.io/2017/02/09/machine-learning-note/"/>
    <id>http://baimoon.github.io/2017/02/09/machine-learning-note/</id>
    <published>2017-02-09T08:00:37.000Z</published>
    <updated>2017-02-16T05:05:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是自己学习机器学习过程的一些笔记。</p>
<p>决策面 -&gt; 线性决策面</p>
<p>学习的过程就是将data -&gt; DS（决策面）</p>
<p>朴素贝叶斯(Naive bayes)</p>
<p>sklearn（scikit-learn）使用入门<br>可以在google中搜索sklearn naive bayes，Gaussian Naive Bayes</p>
<p>高斯朴素贝叶斯的使用<br>sklearn.naive_bayes.GaussianNB<br>例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X = np.array([[<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">-2</span>, <span class="number">-1</span>], [<span class="number">-3</span>, <span class="number">-2</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>]])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>Y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf = GaussianNB()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.fit(X, Y) <span class="comment">#X是特征， Y是标签</span></div><div class="line">GaussianNB(priors=<span class="keyword">None</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(clf.predict([[<span class="number">-0.8</span>, <span class="number">-1</span>]]))</div><div class="line">[<span class="number">1</span>]</div></pre></td></tr></table></figure></p>
<p>评估分类器的分类准确性<br>准确率是指分类器正确分类的数据点个数占测试集中所有被分类点的比率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</div><div class="line"></div><div class="line"><span class="comment">### create classifier</span></div><div class="line">clf = GaussianNB()</div><div class="line"></div><div class="line"><span class="comment">### fit the classifier on the training features and labels</span></div><div class="line">clf.fit(features_train, labels_train)</div><div class="line"></div><div class="line"><span class="comment">### use the trained classifier to predict labels for the test features</span></div><div class="line">pred = clf.predict(features_test)</div><div class="line"></div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</div><div class="line">accuracy = clf.score(features_test, labels_test)</div><div class="line"><span class="keyword">return</span> accuracy</div><div class="line"></div><div class="line"><span class="comment">#或者 </span></div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</div><div class="line"><span class="keyword">print</span> accuracy_score(pred, lables_test)</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是自己学习机器学习过程的一些笔记。&lt;/p&gt;
&lt;p&gt;决策面 -&amp;gt; 线性决策面&lt;/p&gt;
&lt;p&gt;学习的过程就是将data -&amp;gt; DS（决策面）&lt;/p&gt;
&lt;p&gt;朴素贝叶斯(Naive bayes)&lt;/p&gt;
&lt;p&gt;sklearn（scikit-learn）使用入门&lt;b
    
    </summary>
    
      <category term="Maching learning" scheme="http://baimoon.github.io/categories/Maching-learning/"/>
    
    
      <category term="机器学习入门" scheme="http://baimoon.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>Java execute command</title>
    <link href="http://baimoon.github.io/2017/02/08/java-execute-command/"/>
    <id>http://baimoon.github.io/2017/02/08/java-execute-command/</id>
    <published>2017-02-08T03:27:12.000Z</published>
    <updated>2017-02-09T04:13:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>在Java的使用过程中，难免需要去执行linux命令（执行shell也是linux命令），那么应该如何做呢？本文将进行一些演示。</p>
<h1 id="所依赖的相关类"><a href="#所依赖的相关类" class="headerlink" title="所依赖的相关类"></a>所依赖的相关类</h1><p>要在Java中执行linux命令有两种方式，依赖于三个类。我们先介绍这三个类，然后在使用这三个类，组合两种方案来进行说明。</p>
<h2 id="java-lang-Process"><a href="#java-lang-Process" class="headerlink" title="java.lang.Process"></a>java.lang.Process</h2><h3 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h3><p>ProcessBuilder.start()和Runtime.exec方法创建一个本地进程，并返回一个Process子类的实例，该实例用来控制进程并获得相关信息。Process类提供了执行<br>当Process对象没有更多的引用时，不是删除子进程，而是继续异步执行子进程。</p>
<a id="more"></a>
<h3 id="相关API"><a href="#相关API" class="headerlink" title="相关API"></a>相关API</h3><table>
<thead>
<tr>
<th style="text-align:left">方法名</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">abstract void destory()</td>
<td style="text-align:left">销毁子进程</td>
</tr>
<tr>
<td style="text-align:left">abstract int exitValue()</td>
<td style="text-align:left">获取子进程的出口值</td>
</tr>
<tr>
<td style="text-align:left">abstract InputStream getErrorStream()</td>
<td style="text-align:left">获取子进程的错误流</td>
</tr>
<tr>
<td style="text-align:left">abstract InputStream getInputStream()</td>
<td style="text-align:left">获取子进程的输入流</td>
</tr>
<tr>
<td style="text-align:left">abstract OutputStream getOutputStream()</td>
<td style="text-align:left">获取子进程的输出流</td>
</tr>
<tr>
<td style="text-align:left">abstract int waitFor()</td>
<td style="text-align:left">导致当前线程等待，如有必要，一直要等待由该Process对象表示的进程已经终止</td>
</tr>
</tbody>
</table>
<h2 id="java-lang-Runtime"><a href="#java-lang-Runtime" class="headerlink" title="java.lang.Runtime"></a>java.lang.Runtime</h2><h3 id="概括-1"><a href="#概括-1" class="headerlink" title="概括"></a>概括</h3><p>每个Java应用程序都有一个Runtime类实例，是应用程序能够与其运行环境相连接。可以通过getRuntime方法获取当前运行环境。Process类提供了执行从进程输入、执行输出到进程、等待进程完成、检查进程的退出状态以及销毁进程的方法。<br>创建的子进程没有自己的终端或控制台。它的所有标准IO操作都是通过三个流（getOutputStream()、getInputStream()和getErrorStream()）重定向到父进程的。父进程使用这些流来提供到子进程的输入和获得从子进程的输出。因为有些本机平台对标准输入和输出流提供优先的缓冲区大小，如果读写子进程的输入流或输出流迅速出现失败，则可能导致子进程阻塞，甚至发生死锁。<br>当没有Process对象的更多引用时，不是删除子进程，而是继续异步的执行子进程。<br>对于带有Process对象的Java进程，没有必要异步或并发的执行由Process对象表示的进程。</p>
<h3 id="相关API-1"><a href="#相关API-1" class="headerlink" title="相关API"></a>相关API</h3><table>
<thead>
<tr>
<th style="text-align:left">方法名</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">void addShutdownHook(Thread hook)</td>
<td style="text-align:left">注册新的虚拟机关闭钩子</td>
</tr>
<tr>
<td style="text-align:left">int availableProcessors()</td>
<td style="text-align:left">返回Java虚拟机返回可用的处理器数目</td>
</tr>
<tr>
<td style="text-align:left">Process exec(String command)</td>
<td style="text-align:left">在单独的进程中执行字符串指定的命令</td>
</tr>
<tr>
<td style="text-align:left">Process exec(String[] cmdarray)</td>
<td style="text-align:left">在单独的进程中执行指定的命令和参数</td>
</tr>
<tr>
<td style="text-align:left">Process exec(String[] cmdarray, String[] envp)</td>
<td style="text-align:left">在指定的环境的独立进程中执行指定的命令和参数</td>
</tr>
<tr>
<td style="text-align:left">Process exec(String[] cmdarray, String[] envp, File dir)</td>
<td style="text-align:left">在指定的环境和目录的独立进程中执行指定的命令和参数</td>
</tr>
<tr>
<td style="text-align:left">Process exec(String command, String[] envp)</td>
<td style="text-align:left">在指定的环境的独立进程中执行字符串指定的命令</td>
</tr>
<tr>
<td style="text-align:left">Process exec(String command, String[] envp, File dir)</td>
<td style="text-align:left">在指定的环境和目录的独立进程中执行字符串指定的命令</td>
</tr>
<tr>
<td style="text-align:left">void exit(int status)</td>
<td style="text-align:left">通过启动虚拟机的关闭序列，终止当前正在运行的JVM</td>
</tr>
<tr>
<td style="text-align:left">long freeMemory()</td>
<td style="text-align:left">获取JVM当前的空闲内存量</td>
</tr>
<tr>
<td style="text-align:left">void gc()</td>
<td style="text-align:left">运行垃圾回收器</td>
</tr>
<tr>
<td style="text-align:left">Runtime getRuntime()</td>
<td style="text-align:left">返回当前Java应用相关的运行时对象</td>
</tr>
<tr>
<td style="text-align:left">void halt(int status)</td>
<td style="text-align:left">强制终止目前正在运行的JVM</td>
</tr>
<tr>
<td style="text-align:left">void load(String filename)</td>
<td style="text-align:left">加载作为动态库的指定文件名</td>
</tr>
<tr>
<td style="text-align:left">void loadLibrary(String libname)</td>
<td style="text-align:left">加载具有指定库名的动态库</td>
</tr>
<tr>
<td style="text-align:left">long maxMemory()</td>
<td style="text-align:left">返回Java虚拟机试图使用的最大内存量</td>
</tr>
<tr>
<td style="text-align:left">boolean removeShutdownHook(Thread hook)</td>
<td style="text-align:left">取消注册某个先前已注册的虚拟机关闭钩子</td>
</tr>
<tr>
<td style="text-align:left">void runFinalization()</td>
<td style="text-align:left">运行挂起finalization的所有对象的终止方法</td>
</tr>
<tr>
<td style="text-align:left">long totalMemory()</td>
<td style="text-align:left">返回Java虚拟机的内存总量</td>
</tr>
<tr>
<td style="text-align:left">void traceInstructions(boolean on)</td>
<td style="text-align:left">启用/禁止指令跟踪</td>
</tr>
<tr>
<td style="text-align:left">void traceMethodCalls(boolean on)</td>
<td style="text-align:left">启用/禁用方法调用跟踪</td>
</tr>
</tbody>
</table>
<p>这里最需要关注的是exec相关的重载方法和getRuntime()方法。</p>
<h2 id="java-lang-ProcessBuilder"><a href="#java-lang-ProcessBuilder" class="headerlink" title="java.lang.ProcessBuilder"></a>java.lang.ProcessBuilder</h2><h3 id="概括-2"><a href="#概括-2" class="headerlink" title="概括"></a>概括</h3><p>此类用于创建操作系统进程。<br>每个ProcessBuilder实例管理一个进程属性集。start()方法利用这些属性创建一个新的Process实例。start()方法可以从同一实例反复调用，以利用相同的或相关的属性创建新的子进程。<br>每个进程生成器管理这些进程属性：</p>
<blockquote>
<p>命令是一个字符串列表，他表示要调用的外部程序文件及参数（如果有）。再次，表示有效的操作系统命令的字符串列表是依赖于系统的。<br>环境是从变量到值的依赖于系统的映射。初始值是当前进程环境的一个副本（参阅System.getenv()）。<br>工作目录。默认值是当前进程的当前工作目录，通常根据系统属性user.dir来命名。<br>redirectErrorStream属性。最初，该属性为false，意思是子进程的标准输出和错误输出被发送给两个独立的流，这些流可以通过Process.getEnputStream()和Process.getErrorStream()方法来访问。如果将值设置为true，标准错误将与标准输出合并。这使得关联错误消息和响应的输出变得更容易。再次情况下，合并的数据可以从Process.getInputStream()返回的流中读取，而从Process.getErrorStream()返回的流中读取将直接到达文件尾。</p>
</blockquote>
<p>修改进程构建器的属性将影响后续由该对象的start()方法启动的进程，但不会影响以前启动的进程或Java自身的进程。<br>注意，此类不是同步的。如u哦多个线程同时访问一个ProcessBuilder，而其中至少一个线程从结构上修改了其中一个属性，它必须保持外部同步。</p>
<h3 id="相关API-2"><a href="#相关API-2" class="headerlink" title="相关API"></a>相关API</h3><table>
<thead>
<tr>
<th style="text-align:left">方法名</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">List<string> command()</string></td>
<td style="text-align:left">返回此进程生成器的操作系统程序和参数</td>
</tr>
<tr>
<td style="text-align:left">ProcessBuilder command(List<string> command)</string></td>
<td style="text-align:left">设置此进程生成器的操作系统程序和参数</td>
</tr>
<tr>
<td style="text-align:left">ProcessBuilder command(String… command)</td>
<td style="text-align:left">设置此进程生成器的操作系统程序和参数</td>
</tr>
<tr>
<td style="text-align:left">File directory()</td>
<td style="text-align:left">返回次进程生成器的工作目录</td>
</tr>
<tr>
<td style="text-align:left">ProcessBuilder directory(File directory)</td>
<td style="text-align:left">设置此进程生成器的工作目录</td>
</tr>
<tr>
<td style="text-align:left">Map<string, string=""> environment</string,></td>
<td style="text-align:left">返回此进程生成器环境的字符串映射试图</td>
</tr>
<tr>
<td style="text-align:left">boolean redirectErrorStream()</td>
<td style="text-align:left">返回进程生成器是否合并标准错误和标准输出</td>
</tr>
<tr>
<td style="text-align:left">ProcessBuilder redirectErrorStream(boolean redirectErrorStream)</td>
<td style="text-align:left">设置此进程生成器是否合并标准错误和标准输出</td>
</tr>
<tr>
<td style="text-align:left">Process start()</td>
<td style="text-align:left">使用此进程生成器的属性启动一个新的进程</td>
</tr>
</tbody>
</table>
<h1 id="相关示例"><a href="#相关示例" class="headerlink" title="相关示例"></a>相关示例</h1><h2 id="使用Runtime和Process执行命令"><a href="#使用Runtime和Process执行命令" class="headerlink" title="使用Runtime和Process执行命令"></a>使用Runtime和Process执行命令</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">Runtime r = Runtime.getRuntime();</div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">    File f = <span class="keyword">new</span> File(<span class="string">"/Users/"</span>);</div><div class="line">    Process p = r.exec(<span class="keyword">new</span> String[]&#123;<span class="string">"ls"</span>, <span class="string">"-h"</span>&#125;, <span class="keyword">new</span> String[]&#123;&#125;, f);</div><div class="line">    p.waitFor();</div><div class="line">    InputStream is = p.getInputStream();</div><div class="line">    OutputStream os = p.getOutputStream();</div><div class="line">    BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(is));</div><div class="line">    String info = br.readLine();</div><div class="line">    <span class="keyword">while</span>(info != <span class="keyword">null</span>) &#123;</div><div class="line">        System.out.println(info);</div><div class="line">        info = br.readLine();</div><div class="line">    &#125;</div><div class="line">    p.destroy();</div><div class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">    e.printStackTrace();</div><div class="line">&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">    e.printStackTrace();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="使用ProcessBuilder和Process执行命令"><a href="#使用ProcessBuilder和Process执行命令" class="headerlink" title="使用ProcessBuilder和Process执行命令"></a>使用ProcessBuilder和Process执行命令</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">    ProcessBuilder pb = <span class="keyword">new</span> ProcessBuilder(<span class="string">"ls"</span>, <span class="string">"-h"</span>);</div><div class="line"></div><div class="line">    Process p = pb.start();</div><div class="line">    p.waitFor();</div><div class="line">    InputStream is = p.getInputStream();</div><div class="line">    OutputStream os = p.getOutputStream();</div><div class="line">    BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(is));</div><div class="line">    String info = br.readLine();</div><div class="line">    <span class="keyword">while</span>(info != <span class="keyword">null</span>) &#123;</div><div class="line">        System.out.println(info);</div><div class="line">        info = br.readLine();</div><div class="line">    &#125;</div><div class="line">    p.destroy();</div><div class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">    e.printStackTrace();</div><div class="line">&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">    e.printStackTrace();</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Java的使用过程中，难免需要去执行linux命令（执行shell也是linux命令），那么应该如何做呢？本文将进行一些演示。&lt;/p&gt;
&lt;h1 id=&quot;所依赖的相关类&quot;&gt;&lt;a href=&quot;#所依赖的相关类&quot; class=&quot;headerlink&quot; title=&quot;所依赖的相关类&quot;&gt;&lt;/a&gt;所依赖的相关类&lt;/h1&gt;&lt;p&gt;要在Java中执行linux命令有两种方式，依赖于三个类。我们先介绍这三个类，然后在使用这三个类，组合两种方案来进行说明。&lt;/p&gt;
&lt;h2 id=&quot;java-lang-Process&quot;&gt;&lt;a href=&quot;#java-lang-Process&quot; class=&quot;headerlink&quot; title=&quot;java.lang.Process&quot;&gt;&lt;/a&gt;java.lang.Process&lt;/h2&gt;&lt;h3 id=&quot;概括&quot;&gt;&lt;a href=&quot;#概括&quot; class=&quot;headerlink&quot; title=&quot;概括&quot;&gt;&lt;/a&gt;概括&lt;/h3&gt;&lt;p&gt;ProcessBuilder.start()和Runtime.exec方法创建一个本地进程，并返回一个Process子类的实例，该实例用来控制进程并获得相关信息。Process类提供了执行&lt;br&gt;当Process对象没有更多的引用时，不是删除子进程，而是继续异步执行子进程。&lt;/p&gt;
    
    </summary>
    
      <category term="Java" scheme="http://baimoon.github.io/categories/Java/"/>
    
    
      <category term="Java执行命令行" scheme="http://baimoon.github.io/tags/Java%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4%E8%A1%8C/"/>
    
  </entry>
  
  <entry>
    <title>Python Datetime</title>
    <link href="http://baimoon.github.io/2017/02/04/python-datetime/"/>
    <id>http://baimoon.github.io/2017/02/04/python-datetime/</id>
    <published>2017-02-04T10:38:47.000Z</published>
    <updated>2017-02-06T08:35:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>在使用python写调度任务的时候，离不开的必然有日期和时间的处理；最常见的有根据字符串生成时间、将时间生成指定格式的字符串、日期时间的计算（加减）等等。在python中对日期时间进行操作的包为datetime。下面就对该包的一些常用操作和对应的参数进行介绍。</p>
<a id="more"></a>
<h1 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h1><h2 id="获取当前的日期时间"><a href="#获取当前的日期时间" class="headerlink" title="获取当前的日期时间"></a>获取当前的日期时间</h2><p>使用datetime获取当前日期时间的方法是datetime.datetime.now()：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> datetime</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> datetime.datetime.now()</div><div class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-06</span> <span class="number">12</span>:<span class="number">37</span>:<span class="number">34.767374</span></div></pre></td></tr></table></figure></p>
<h2 id="将日期时间与字符串之间的互转"><a href="#将日期时间与字符串之间的互转" class="headerlink" title="将日期时间与字符串之间的互转"></a>将日期时间与字符串之间的互转</h2><p>将日期时间转换为字符串的方法是datetime.datetime.strftime()：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> datetime</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> datetime.datetime.strftime(datetime.datetime.now(), <span class="string">'%Y-%m-%d %H:%M:%S'</span>)</div><div class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-06</span> <span class="number">12</span>:<span class="number">43</span>:<span class="number">20</span></div></pre></td></tr></table></figure></p>
<p>上面的代码用来将当前时间转换为“年-月-日 时:分:秒”的格式。strftime方法接收两个参数：datetime对象和输出格式。对于输出格式中的各个参数的意义，请参考本节后面的表格。</p>
<p>将字符串转换为日期时间的方法是datetime.datetime.strptime()：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> datetime</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> datetime.datetime.strptime(<span class="string">'2017-02-06 12:43:20'</span>, <span class="string">'%Y-%m-%d %H:%M:%S'</span>)</div><div class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-06</span> <span class="number">12</span>:<span class="number">43</span>:<span class="number">20</span></div></pre></td></tr></table></figure></p>
<p>上面的代码用来将字符串转换为datetime。strptime方法有两个参数：时间字符串和时间字符串的格式。对于格式的具体意义，参考本节后面的表格。</p>
<table>
<thead>
<tr>
<th style="text-align:left">格式</th>
<th style="text-align:left">意义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">%a</td>
<td style="text-align:left">Abbreviated weekday name</td>
</tr>
<tr>
<td style="text-align:left">%A</td>
<td style="text-align:left">Full weekday name</td>
</tr>
<tr>
<td style="text-align:left">%b</td>
<td style="text-align:left">Abbreviated month name</td>
</tr>
<tr>
<td style="text-align:left">%B</td>
<td style="text-align:left">Full month name</td>
</tr>
<tr>
<td style="text-align:left">%c</td>
<td style="text-align:left">Date and time representation appropriate for locale</td>
</tr>
<tr>
<td style="text-align:left">%d</td>
<td style="text-align:left">Day of month as decimal number</td>
</tr>
<tr>
<td style="text-align:left">%H</td>
<td style="text-align:left">Hour in 24-hour format (00 - 23)</td>
</tr>
<tr>
<td style="text-align:left">%I</td>
<td style="text-align:left">Hour in 12-hour format (01 - 12)</td>
</tr>
<tr>
<td style="text-align:left">%j</td>
<td style="text-align:left">Day of year as decimal number (001 - 366)</td>
</tr>
<tr>
<td style="text-align:left">%m</td>
<td style="text-align:left">Month as decimal number (01 - 12)</td>
</tr>
<tr>
<td style="text-align:left">%M</td>
<td style="text-align:left">Minute as decimal number (00 - 59)</td>
</tr>
<tr>
<td style="text-align:left">%p</td>
<td style="text-align:left">Current locale’s A.M./P.M. indicator for 12-hour clock</td>
</tr>
<tr>
<td style="text-align:left">%S</td>
<td style="text-align:left">Second as decimal number (00 - 59)</td>
</tr>
<tr>
<td style="text-align:left">%U</td>
<td style="text-align:left">Week of year as decimal number, with Sunday as first day of week (00 - 51)</td>
</tr>
<tr>
<td style="text-align:left">%w</td>
<td style="text-align:left">Weekday as decimal number (0 - 6; Sunday is 0)</td>
</tr>
<tr>
<td style="text-align:left">%W</td>
<td style="text-align:left">Week of year as decimal number, with Monday as first day of week (00 - 51)</td>
</tr>
<tr>
<td style="text-align:left">%x</td>
<td style="text-align:left">Date representation for current locale</td>
</tr>
<tr>
<td style="text-align:left">%X</td>
<td style="text-align:left">Time representation for current locale</td>
</tr>
<tr>
<td style="text-align:left">%y</td>
<td style="text-align:left">Year without century, as decimal number (00 - 99)</td>
</tr>
<tr>
<td style="text-align:left">%Y</td>
<td style="text-align:left">Year with century, as decimal number</td>
</tr>
<tr>
<td style="text-align:left">%z, %Z</td>
<td style="text-align:left">Time-zone name or abbreviation; no characters if time zone is unknown</td>
</tr>
</tbody>
</table>
<h2 id="日期时间的运算"><a href="#日期时间的运算" class="headerlink" title="日期时间的运算"></a>日期时间的运算</h2><h3 id="计算两个时间之间的时间差"><a href="#计算两个时间之间的时间差" class="headerlink" title="计算两个时间之间的时间差"></a>计算两个时间之间的时间差</h3><p>两个datetime之间可以直接进行减运算<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> datetime</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>before = datetime.datetime.strptime(<span class="string">'2017-02-06 12:43:20'</span>, <span class="string">'%Y-%m-%d %H:%M:%S'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>datetime.datetime.now() - before</div><div class="line">datetime.timedelta(<span class="number">0</span>, <span class="number">10723</span>, <span class="number">342911</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>(datetime.datetime.now() - before).days</div><div class="line"><span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>通过上面的代码可以看到，两个datetime之间的减运算会得到一个datetime.timedelta对象，调用datetime.timedelta的相关属性则可以获取需要的差值信息。通过datetime.timedelta可以获取的时间查信息如下：</p>
<table>
<thead>
<tr>
<th style="text-align:left">属性</th>
<th style="text-align:left">意义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">days</td>
<td style="text-align:left">datetime.timedelta用天数表示</td>
</tr>
<tr>
<td style="text-align:left">seconds</td>
<td style="text-align:left">datetime.timedelta用秒数表示</td>
</tr>
<tr>
<td style="text-align:left">microseconds</td>
<td style="text-align:left">datetime.timedelta用毫秒数表示</td>
</tr>
</tbody>
</table>
<h3 id="对已有的时间进行加减操作"><a href="#对已有的时间进行加减操作" class="headerlink" title="对已有的时间进行加减操作"></a>对已有的时间进行加减操作</h3><p>对datetime的加减操作，也需要借助datetime.timedelta对象，对已有的datetime对象加上或减去datetime.timedelta对象，则获得一个新的datetime：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> datetime</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>starttime = datetime.datetime.now()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> starttime - datetime.timedelta(hours=<span class="number">1</span>)</div><div class="line"><span class="number">2017</span><span class="number">-02</span><span class="number">-06</span> <span class="number">14</span>:<span class="number">48</span>:<span class="number">39.577586</span></div></pre></td></tr></table></figure></p>
<p>上面的代码在进行时间的减法操作之前也是先构建了一个datetime.timedelta对象，构建datetime.timedelta对象时，可以使用的参数如下：</p>
<table>
<thead>
<tr>
<th style="text-align:left">属性</th>
<th style="text-align:left">意义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">weeks</td>
<td style="text-align:left">datetime.timedelta包含的周数</td>
</tr>
<tr>
<td style="text-align:left">days</td>
<td style="text-align:left">datetime.timedelta包含的天数</td>
</tr>
<tr>
<td style="text-align:left">hours</td>
<td style="text-align:left">datetime.timedelta包含的小时数</td>
</tr>
<tr>
<td style="text-align:left">minutes</td>
<td style="text-align:left">datetime.timedelta包含的分钟数</td>
</tr>
<tr>
<td style="text-align:left">seconds</td>
<td style="text-align:left">datetime.timedelta包含的秒数</td>
</tr>
<tr>
<td style="text-align:left">microseconds</td>
<td style="text-align:left">datetime.timedelta包含的毫秒数</td>
</tr>
</tbody>
</table>
<h1 id="使用实例"><a href="#使用实例" class="headerlink" title="使用实例"></a>使用实例</h1><h2 id="查询以指定的格式输出当前时间"><a href="#查询以指定的格式输出当前时间" class="headerlink" title="查询以指定的格式输出当前时间"></a>查询以指定的格式输出当前时间</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> datetime</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>datetime.datetime.now().strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>)</div><div class="line"><span class="string">'2017-02-06 15:09:15'</span></div></pre></td></tr></table></figure>
<p>这种方法要比使用datetime.datetime.strftime(datetime, ‘%Y-%m-%d %H:%M:%S’)更加方便。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在使用python写调度任务的时候，离不开的必然有日期和时间的处理；最常见的有根据字符串生成时间、将时间生成指定格式的字符串、日期时间的计算（加减）等等。在python中对日期时间进行操作的包为datetime。下面就对该包的一些常用操作和对应的参数进行介绍。&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="http://baimoon.github.io/categories/python/"/>
    
    
      <category term="日期时间处理" scheme="http://baimoon.github.io/tags/%E6%97%A5%E6%9C%9F%E6%97%B6%E9%97%B4%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Python Subprocess</title>
    <link href="http://baimoon.github.io/2017/02/04/python-subprocess/"/>
    <id>http://baimoon.github.io/2017/02/04/python-subprocess/</id>
    <published>2017-02-04T10:38:47.000Z</published>
    <updated>2017-02-05T10:19:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>在平时python的使用过程中，难免会遇到调用服务器命令的时候。直接调用普通的命令基本上都没有什么问题，令人比较麻烦的是带有控制台的命令，例如python、beeline、spark-shell。虽然向python、spark都有相关的脚本文件或者jar来避免直接使用控制台命令的调用，然后有些时候还是不免会用到控制台的方式，那么对于带有控制台的命令行应该如何实现呢？本文将使用subprocess，并以beeline为背景来实现使用python执行带有控制台的命令行命令。<br>首先看看参考代码，代码是以执行Hive的beeline命令行为例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">beeline</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"# 1 建立连接"</span></div><div class="line">        self.p = subprocess.Popen([<span class="string">'apache-hive-0.14.0-bin/bin/beeline'</span>], stdin=subprocess.PIPE,</div><div class="line">                             stdout=subprocess.PIPE)</div><div class="line">        <span class="keyword">print</span> &gt;&gt; self.p.stdin, <span class="string">'!connect jdbc:hive2://hdfs001:2181,hdfs002:2181,hdfs003:2181,hdfs004:2181,hdfs005:2181/;serviceDiscoveryMode=zookeeper userName password\n'</span></div><div class="line">        self.p.stdin.flush()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">submit</span><span class="params">(self, hql)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"# 2 输入命令"</span></div><div class="line">        <span class="keyword">print</span> &gt;&gt; self.p.stdin, hql</div><div class="line">        self.p.stdin.flush()</div><div class="line"></div><div class="line">        <span class="keyword">print</span> <span class="string">"# 3 等待关闭"</span></div><div class="line">        <span class="keyword">print</span> &gt;&gt; self.p.stdin, <span class="string">"!q"</span></div><div class="line">        self.p.wait()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hadoop_get</span><span class="params">(self, from_, to_)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"# 4 下载数据"</span></div><div class="line">        (status, output) = commands.getstatusoutput(<span class="string">" "</span>.join((<span class="string">"hadoop-2.6.0/bin/hadoop fs -text"</span>, from_+<span class="string">'*'</span>, <span class="string">'&gt;'</span>, to_)))</div><div class="line">        <span class="keyword">print</span> status, output</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">queryDataByDate</span><span class="params">(start_date, end_date, local_path)</span>:</span></div><div class="line">    sql = <span class="string">"""</span></div><div class="line">    create table database.table_%s_%s</div><div class="line">       ROW FORMAT DELIMITED</div><div class="line">       FIELDS TERMINATED BY '-@-'</div><div class="line">       NULL DEFINED AS '...'</div><div class="line">    STORED AS TEXTFILE</div><div class="line">    AS</div><div class="line">    SELECT * FROM DB.TABLE_NAME;</div><div class="line">"""</div><div class="line">    b = beeline()</div><div class="line">    s = sql % (start_date, end_date, start_date, end_date)</div><div class="line">    b.submit((sql % (start_date, end_date, start_date, end_date)))</div><div class="line"></div><div class="line">    fileName = <span class="string">'feed_%s_%s'</span> % (start_date, end_date)</div><div class="line">    b.hadoop_get((<span class="string">"HDFS_PATH/%s/"</span> % (fileName)), (<span class="string">"LOCAL_PATH/%s"</span> % fileName))</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="keyword">if</span> len(sys.argv) &lt; <span class="number">2</span>:</div><div class="line">        <span class="keyword">print</span> <span class="string">"请输入要获取feed的开始日期和结束日志，如：20160105"</span></div><div class="line">        exit(<span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"正在执行%s文件，来查询%s-%s之间的数据:"</span> % (sys.argv[<span class="number">0</span>], sys.argv[<span class="number">1</span>], sys.argv[<span class="number">2</span>])</div><div class="line">    queryDataByDate(sys.argv[<span class="number">1</span>], sys.argv[<span class="number">2</span>], <span class="string">"/data/"</span>)</div></pre></td></tr></table></figure></p>
<p>该代码块的主要流程是，在初始化beeline对象时调用beeline命令，并进行连接（<strong>init</strong>方法中实现了全部的操作）;然后是提交需要beeline执行的sql（submit方法中实现）;最后是将sql执行的结果从HDFS中取到本地（hadoop_get方法中实现）。queryDataByData方法就是对beeline类中各个方法的一个集成调用。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在平时python的使用过程中，难免会遇到调用服务器命令的时候。直接调用普通的命令基本上都没有什么问题，令人比较麻烦的是带有控制台的命令，例如python、beeline、spark-shell。虽然向python、spark都有相关的脚本文件或者jar来避免直接使用控制台
    
    </summary>
    
      <category term="python" scheme="http://baimoon.github.io/categories/python/"/>
    
    
      <category term="python subprocess" scheme="http://baimoon.github.io/tags/python-subprocess/"/>
    
  </entry>
  
  <entry>
    <title>Double-Array trie</title>
    <link href="http://baimoon.github.io/2017/01/13/doubleArray-trie/"/>
    <id>http://baimoon.github.io/2017/01/13/doubleArray-trie/</id>
    <published>2017-01-13T08:01:54.000Z</published>
    <updated>2017-01-16T09:52:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要用来学习Double-Array trie的相关知识。</p>
<p><a href="https://github.com/digitalstain/DoubleArrayTrie" title="Double-Array trie" target="_blank" rel="external">源码的github地址</a></p>
<p>Double-Array trie是Trie结构的压缩形式，仅用两个数组来表示Trie，这个结构有效的结合数字搜索树(Digital Search Tree)检索时间高效的特点和链式表示的Trie空间结构紧凑的特点。双数组Trie的本质是一个确定有限状态自动机(DFA)，每个节点代表自动机的一个状态，根据不同的变量，进行状态转移，当到达结束状态或无法转移时，完成一次查询操作。在双数组所有键中，包含的字符之间的联系都是通过简单的数学加法运算表示的，不仅提高了检索速度，而且省去了链式结构中使用的大量指针，节省了存储空间。</p>
<p>在了解Double-Array trie之前，我们先了解一下“确定有限状态自动机”。在数学理论中，确定有限状态自动机或确定有限自动机（deterministic finite automation, DFA）是一个能实现状态转移的自动机。对于一个给定的属于该自动机的状态和一个属于该自动机字母表的字符，它能够根据实现给定的函数转移到下一个状态。简单的说，就是当前状态根据一个公式和状态的确定值，能够到达另外一个状态，而且要到达的状态是确定的。如图：<br><a href="&quot;确定有限自动机&quot;">确定有限自动状态机</a><br>图中的每个字代表一个状态，并且每个状态都有一个固定的变量。</p>
<p>在了解了确定优先状态自动机之后，我们来了解一下Double-Array trie。Double-Array trie的核心是使用两个整型数组base和check来分别存储状态以及前驱状态。说的简单一些，base用来存储状态，check用来验证。在状态的转移过程中，有如下公式：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">check[t]=s</div><div class="line">base[s]+c=t <span class="comment">//其中t和s是数组下标</span></div></pre></td></tr></table></figure></p>
<p>上面的公式表示 base[s]的值 + 状态的变量 = t下标，check[t]的值 = s下标。</p>
<p>举例来说明：</p>
<p>在学习Douoble-Array trie和看DoubleArrayTrie源码的时候，参考了以下文章，在此表示感谢：<br><a href="http://www.hankcs.com/program/java/%E5%8F%8C%E6%95%B0%E7%BB%84trie%E6%A0%91doublearraytriejava%E5%AE%9E%E7%8E%B0.html" title="双数组Trie树(DoubleArrayTrie)Java实现" target="_blank" rel="external">双数组Trie树(DoubleArrayTrie)Java实现</a><br><a href="http://www.cnblogs.com/zhangchaoyang/articles/4508266.html" title="Double Array Trie" target="_blank" rel="external">Double Array Trie</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要用来学习Double-Array trie的相关知识。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/digitalstain/DoubleArrayTrie&quot; title=&quot;Double-Array trie&quot; target=&quot;_blank
    
    </summary>
    
      <category term="数据结构" scheme="http://baimoon.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="数据结构-树" scheme="http://baimoon.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>JPinYin</title>
    <link href="http://baimoon.github.io/2016/12/30/JPinYin/"/>
    <id>http://baimoon.github.io/2016/12/30/JPinYin/</id>
    <published>2016-12-30T08:28:15.000Z</published>
    <updated>2017-01-15T13:54:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍JPinYin的使用和配置，<a href="https://github.com/stuxuhai/jpinyin" title="Jpinyin" target="_blank" rel="external">github的地址</a>。</p>
<h1 id="简单介绍"><a href="#简单介绍" class="headerlink" title="简单介绍"></a>简单介绍</h1><p>Jpinyin是一个开源的用于将汉字转换为拼音的java库。</p>
<h2 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h2><p>1、准确、完善的字库：Unicode编码从4E00-9FA5范围及3007(〇)的20903个汉字中，除了46个异体字（不存在标准拼音）Jpinyin都能转换。<br>2、拼音转换速度快：经测试，转换Unicode编码范围的20902个汉字，Jpinyin耗时约为100毫秒。<br>3、支持多种拼音格式：Jpinyin支持多种拼音输出格式：带声调、不带声调、数字表示声调以及拼音首字母格式输出。<br>4、常见多音字识别：Jpinyin支持常见多音字的识别，其中包括词组、成语、地名等；<br>5、简繁体中文互转。<br>6、支持用户自定义字典。</p>
<h2 id="Maven依赖"><a href="#Maven依赖" class="headerlink" title="Maven依赖"></a>Maven依赖</h2><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">   &lt;groupId&gt;com.github.stuxuhai&lt;/groupId&gt;</div><div class="line">   &lt;artifactId&gt;jpinyin&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;1.1.8&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure>
<h2 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">String str = <span class="string">"你好世界"</span>;</div><div class="line">PinyinHelper.convertToPinyinString(str, <span class="string">","</span>, PinyinFormat.WITH_TONE_MARK); <span class="comment">// nǐ,hǎo,shì,jiè</span></div><div class="line">PinyinHelper.convertToPinyinString(str, <span class="string">","</span>, PinyinFormat.WITH_TONE_NUMBER); <span class="comment">// ni3,hao3,shi4,jie4</span></div><div class="line">PinyinHelper.convertToPinyinString(str, <span class="string">","</span>, PinyinFormat.WITHOUT_TONE); <span class="comment">// ni,hao,shi,jie</span></div><div class="line">PinyinHelper.getShortPinyin(str); <span class="comment">// nhsj</span></div><div class="line">PinyinHelper.addPinyinDict(<span class="string">"user.dict"</span>);  <span class="comment">// 添加用户自定义字典</span></div></pre></td></tr></table></figure>
<a id="more"></a>
<h1 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h1><p>下面的部分，试着从源码来理解Jpinyin，以便更好的使用它。</p>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p><img src="http://oaavtz33a.bkt.clouddn.com/Jpinyin_java_source.png" alt="源代码部分" title="图-1 java代码"><br>根据(图-1)中可以看出，Jpinyin的实现主要由6个Java类来实现，其中PinyinException是一个异常类，定义了Pinyin异常；PinyinFormat为枚举类，用于定义汉字转拼音的格式（前面提到的：带声调、不带声调和数字表示声调三种），主要作为PinyinHelper类中方法的参数；PinyinHelper类，用来将汉字转换为不同格式的拼音，以及将汉字转换为拼音首字母缩写；PinyinResource类，用来加载拼音资源，主要是提供了从文件加载字典数据，并将数据以字典(Map)类型的数据结构保存在内存中；ChinesHelper类，用来是实现汉字繁简互转的功能；DoubleArrayTrie类，是Double-Array trie的实现，用来将词组装配成一棵，并使用这棵树来检测词组。</p>
<p><img src="http://oaavtz33a.bkt.clouddn.com/Jpinyin_resource.png" alt="资源部分" title="图-2 资源"><br>根据(图-2)中可以看出，Jpinyin的资源中有三个数据文件，分别为chinese.dict、mutil_pinyin.dict和pinyin.dict。其中，chinese.dict存储的是繁体字与简体字之间的对应关系；mutil_pinyin.dict存储的是词组；pinyin.dict存储的是单字的读音（如果是多音字则对一个多个拼音，多个拼音之间使用逗号分隔，如：重=zhòng,chóng）。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>了解了项目工程的结构，我们来具体的看一下Jpinyin的实现</p>
<h3 id="PinyinException类"><a href="#PinyinException类" class="headerlink" title="PinyinException类"></a>PinyinException类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PinyinException</span> <span class="keyword">extends</span> <span class="title">Exception</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">PinyinException</span><span class="params">(String message)</span> </span>&#123;</div><div class="line">        <span class="keyword">super</span>(message);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>该类主要就是定义了一个异常类，用于区分普通异常和该项目的异常。</p>
<h3 id="pinyinFormat类"><a href="#pinyinFormat类" class="headerlink" title="pinyinFormat类"></a>pinyinFormat类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">enum</span> PinyinFormat &#123;</div><div class="line">    WITH_TONE_MARK, WITHOUT_TONE, WITH_TONE_NUMBER;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>该类为枚举类型，用来定义拼音的格式。主要作为PyinHelper类中方法的参数，用来在汉字转换为拼音时的格式，如：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">PinyinHelper.convertToPinyinString(str, <span class="string">","</span>, PinyinFormat.WITH_TONE_MARK);</div></pre></td></tr></table></figure></p>
<p>其中WITH_TONE_MARK表示带声调的拼音格式，如nǐ,hǎo,shì,jiè；WITHOUT_TONE表示不带声调的拼音格式，如ni,hao,shi,jie；WITH_TONE_NUMBER表示使用数字表示声调的拼音格式，如ni3,hao3,shi4,jie4。</p>
<h3 id="PinyinResource类"><a href="#PinyinResource类" class="headerlink" title="PinyinResource类"></a>PinyinResource类</h3><p>PinyinResource类主要用来加载资源文件（resource/data目录中的数据文件），并将资源文件以Map为数据结构保存到内存中。该类提供了6个方法：<br>两个方法用于获取资源的输入，并以UTF-8格式读入：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//以当前类的类加载器的输入来获取资源文件的输入</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> Reader <span class="title">newClassPathReader</span><span class="params">(String classpath)</span> </span>&#123;</div><div class="line">    InputStream is = PinyinResource.class.getResourceAsStream(classpath);</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">return</span> <span class="keyword">new</span> InputStreamReader(is, <span class="string">"UTF-8"</span>);</div><div class="line">    &#125; <span class="keyword">catch</span> (UnsupportedEncodingException e) &#123;</div><div class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">//以文件的输入来获取资源文件的输入</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> Reader <span class="title">newFileReader</span><span class="params">(String path)</span> <span class="keyword">throws</span> FileNotFoundException </span>&#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">return</span> <span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(path), <span class="string">"UTF-8"</span>);</div><div class="line">    &#125; <span class="keyword">catch</span> (UnsupportedEncodingException e) &#123;</div><div class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>一个资源加载方法，并以Map作为数据结构进行存储：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//根据输入源，将资源文件加载到内存中（以Map作为数据结构）</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> Map&lt;String, String&gt; <span class="title">getResource</span><span class="params">(Reader reader)</span> </span>&#123;</div><div class="line">    Map&lt;String, String&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;String, String&gt;();</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        BufferedReader br = <span class="keyword">new</span> BufferedReader(reader);</div><div class="line">        String line = <span class="keyword">null</span>;</div><div class="line">        <span class="comment">//循环读取资源文件中的每一行</span></div><div class="line">        <span class="keyword">while</span> ((line = br.readLine()) != <span class="keyword">null</span>) &#123;</div><div class="line">        	<span class="comment">//由此可以看出，资源文件的格式必须是key=value，并且数据以key和value的方式加载到内存中</span></div><div class="line">            String[] tokens = line.trim().split(<span class="string">"="</span>);</div><div class="line">            map.put(tokens[<span class="number">0</span>], tokens[<span class="number">1</span>]);</div><div class="line">        &#125;</div><div class="line">        br.close();</div><div class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> map;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>三个资源加载调用方法，分别加载pinyin.dict、mutil_pinyin.dict、chinese.dict：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> Map&lt;String, String&gt; <span class="title">getPinyinResource</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> getResource(newClassPathReader(<span class="string">"/data/pinyin.dict"</span>));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> Map&lt;String, String&gt; <span class="title">getMutilPinyinResource</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> getResource(newClassPathReader(<span class="string">"/data/mutil_pinyin.dict"</span>));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> Map&lt;String, String&gt; <span class="title">getChineseResource</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> getResource(newClassPathReader(<span class="string">"/data/chinese.dict"</span>));</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="ChineseHelper类"><a href="#ChineseHelper类" class="headerlink" title="ChineseHelper类"></a>ChineseHelper类</h3><p>ChineseHelper类主要用于繁体字和简体字之间的转换、体字的检测，并提供了一个方法用来扩展用户自定义的繁体字对应简体字的字库（字库的格式必须是key=value，因为它使用的是PinyinResource中的资源加载方式）。<br>首先是两个类变量：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//汉字的正则表达式，这个值范围内的表示为汉字</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String CHINESE_REGEX = <span class="string">"[\\u4e00-\\u9fa5]"</span>;</div><div class="line"><span class="comment">//加载资源文件“/data/chinese.dict”中的数据，由这里看，好像由繁体转换为简体速度应该会很快</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;String, String&gt; CHINESE_MAP = PinyinResource.getChineseResource();</div></pre></td></tr></table></figure></p>
<p>简体字和繁体字之间的互转，包括单个字和多个字：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//将繁体字转换为简体字，因为CHINESE_MAP就是繁体字-&gt;简体字的映射，因此直接在该Map中查找</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">char</span> <span class="title">convertToSimplifiedChinese</span><span class="params">(<span class="keyword">char</span> c)</span> </span>&#123;</div><div class="line">	<span class="comment">//因为CHINESE_MAP中存储的是字符串，因此需要将字节转换为字符串，然后进行查找</span></div><div class="line">    String simplifiedChinese = CHINESE_MAP.get(String.valueOf(c));</div><div class="line">    <span class="keyword">if</span> (simplifiedChinese != <span class="keyword">null</span>) &#123;</div><div class="line">    	<span class="comment">//要求返回的是char类型</span></div><div class="line">        <span class="keyword">return</span> simplifiedChinese.charAt(<span class="number">0</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> c;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/* 将简体字转换为繁体字，该方法效率很低，因为CHINESE_MAP是繁体字到简体字的转换，</span></div><div class="line"> * 而没有简体字到繁体字之间的映射数据，因此只能对CHINESE_MAP进行遍历，然后匹配每个entry的值来判断</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">char</span> <span class="title">convertToTraditionalChinese</span><span class="params">(<span class="keyword">char</span> c)</span> </span>&#123;</div><div class="line">	<span class="comment">//因为CHINESE_MAP中存储的是字符串，因此需要将字符转换为字符串</span></div><div class="line">    String simplifiedChinese = String.valueOf(c);</div><div class="line">    <span class="comment">//循环CHINESE_MAP中的所有entry，并对entry的value进行判断</span></div><div class="line">    <span class="keyword">for</span> (Entry&lt;String, String&gt; entry : CHINESE_MAP.entrySet()) &#123;</div><div class="line">        <span class="keyword">if</span> (entry.getValue().equals(simplifiedChinese)) &#123;</div><div class="line">            <span class="keyword">return</span> entry.getKey().charAt(<span class="number">0</span>);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> c;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//将繁体字符串转换为简体字符串，其实现就是循环字符串的每个字符的转换，然后拼接在一起</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">convertToSimplifiedChinese</span><span class="params">(String str)</span> </span>&#123;</div><div class="line">    StringBuilder sb = <span class="keyword">new</span> StringBuilder();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, len = str.length(); i &lt; len; i++) &#123;</div><div class="line">        <span class="keyword">char</span> c = str.charAt(i);</div><div class="line">        sb.append(convertToSimplifiedChinese(c));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> sb.toString();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/* 将简体字符串转换为繁体字符串，效率很低，每个字的转换都需要遍历整个CHINESE_MAP，如果频繁使用需要优化</span></div><div class="line"> * 其实现就是循环简体字符串的每个字符，然后拼接在一起</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">convertToTraditionalChinese</span><span class="params">(String str)</span> </span>&#123;</div><div class="line">    StringBuilder sb = <span class="keyword">new</span> StringBuilder();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, len = str.length(); i &lt; len; i++) &#123;</div><div class="line">        <span class="keyword">char</span> c = str.charAt(i);</div><div class="line">        sb.append(convertToTraditionalChinese(c));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> sb.toString();</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>字体检测：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* 检测是否是繁体字，因为该方法是从CHINESE_MAP中的keys中查找，如果chinese.dict数据文件中有遗漏，则检测是不准的</span></div><div class="line"> * 其实现就是判断CHINESE_MAP中是否包含了指定的key</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isTraditionalChinese</span><span class="params">(<span class="keyword">char</span> c)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> CHINESE_MAP.containsKey(String.valueOf(c));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//判断给定的字符是否是中文，其实现是判断字符是否在CHINESE_REGEX所指定的范围内，或者是否等于'〇'</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isChinese</span><span class="params">(<span class="keyword">char</span> c)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="string">'〇'</span> == c || String.valueOf(c).matches(CHINESE_REGEX);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//判断给定的字符串是否包含中文，循环字符串中的每个字符，逐个判断</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">containsChinese</span><span class="params">(String str)</span> </span>&#123;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, len = str.length(); i &lt; len; i++) &#123;</div><div class="line">        <span class="keyword">if</span> (isChinese(str.charAt(i))) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>添加用户自定义的繁体字对应简体字的字库：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*将指定路径的数据资源添加到繁体字与简体字映射关系的数据结构中，要求path指定的数据文件必须是key=value格式，并且key为繁体字</span></div><div class="line"> * 其实现就是将要添加的数据资源的Map添加到已有映射关系的Map中</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">addChineseDict</span><span class="params">(String path)</span> <span class="keyword">throws</span> FileNotFoundException </span>&#123;</div><div class="line">    CHINESE_MAP.putAll(PinyinResource.getResource(PinyinResource.newFileReader(path)));</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="PinyinHelper类"><a href="#PinyinHelper类" class="headerlink" title="PinyinHelper类"></a>PinyinHelper类</h3><p>PinyinHelper类主要用于将中文的汉字转换为拼音（支持三种格式），并提供了添加用户自定义的汉字与拼音关系（单字和词组）的资源库的功能。<br>首先是类变量：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> List&lt;String&gt; dict = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line"></div><div class="line"><span class="comment">//存储了汉字到拼音之间的映射关系</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;String, String&gt; PINYIN_TABLE = PinyinResource.getPinyinResource();</div><div class="line"></div><div class="line"><span class="comment">//存储了词组（汉字）到拼音之间的映射关系</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;String, String&gt; MUTIL_PINYIN_TABLE = PinyinResource.getMutilPinyinResource();</div><div class="line"></div><div class="line"><span class="comment">//一个double数组结构------------------------------干啥用的呢</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> DoubleArrayTrie DOUBLE_ARRAY_TRIE = <span class="keyword">new</span> DoubleArrayTrie();</div><div class="line"></div><div class="line"><span class="comment">//拼音的默认分隔符，用户在调用转换的时候可以自定义</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String PINYIN_SEPARATOR = <span class="string">","</span>; <span class="comment">// 拼音分隔符</span></div><div class="line"></div><div class="line"><span class="comment">//没有在4E00-9FA5范围之间的第20903个汉字</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">char</span> CHINESE_LING = <span class="string">'〇'</span>;</div><div class="line"></div><div class="line"><span class="comment">//</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ALL_UNMARKED_VOWEL = <span class="string">"aeiouv"</span>;</div><div class="line"></div><div class="line"><span class="comment">//所有可以标声调的字母</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ALL_MARKED_VOWEL = <span class="string">"āáǎàēéěèīíǐìōóǒòūúǔùǖǘǚǜ"</span>; <span class="comment">// 所有带声调的拼音字母</span></div></pre></td></tr></table></figure></p>
<p>静态块：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> &#123;</div><div class="line">	<span class="comment">//循环词组或短语库中的所有汉字，添加dict（ArrayList）中</span></div><div class="line">	<span class="keyword">for</span> (String word : MUTIL_PINYIN_TABLE.keySet()) &#123;</div><div class="line">		dict.add(word);</div><div class="line">	&#125;</div><div class="line">	<span class="comment">//将所有的词组进行排序，并使用排序后的词组集合构建一棵前缀树</span></div><div class="line">	Collections.sort(dict);</div><div class="line">	DOUBLE_ARRAY_TRIE.build(dict);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>将声调格式的拼音转换为以数字表示声调的拼音<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] convertWithToneNumber(String pinyinArrayString) &#123;</div><div class="line">	<span class="comment">//将拼音字符串按照指定的分隔符进行拆分，拆分后的每个数组项是一个单独的拼音</span></div><div class="line">	String[] pinyinArray = pinyinArrayString.split(PINYIN_SEPARATOR);</div><div class="line"></div><div class="line">	<span class="comment">//循环拼音字符串的每个拼音</span></div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = pinyinArray.length - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</div><div class="line"></div><div class="line">		<span class="comment">//用来记录是否有带声调的拼音字母，如果为false，可以是轻声的读音</span></div><div class="line">		<span class="keyword">boolean</span> hasMarkedChar = <span class="keyword">false</span>;</div><div class="line"></div><div class="line">		<span class="comment">//将ü替换为v，因为在英文键盘中，是没有ü的，而是使用v代替</span></div><div class="line">		<span class="comment">//这里的pinyinArray是拼音字符串中单个字的拼音</span></div><div class="line">		String originalPinyin = pinyinArray[i].replace(<span class="string">"ü"</span>, <span class="string">"v"</span>); <span class="comment">// 将拼音中的ü替换为v，注意这里没有替换ǖǘǚǜ</span></div><div class="line"></div><div class="line">		<span class="comment">//循环单个字的拼音中的每个字母</span></div><div class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> j = originalPinyin.length() - <span class="number">1</span>; j &gt;= <span class="number">0</span>; j--) &#123;</div><div class="line"></div><div class="line">			<span class="keyword">char</span> originalChar = originalPinyin.charAt(j);</div><div class="line"></div><div class="line">			<span class="comment">// 搜索带声调的拼音字母，如果存在则替换为对应不带声调的英文字母</span></div><div class="line">			<span class="comment">// 这里之所以使用(&lt; 'a' || &gt; 'z')这个条件来判断，是因为"āáǎàēéěèīíǐìōóǒòūúǔùǖǘǚǜ"没有在a-z之间，而其他的拼音均在，所以如果有</span></div><div class="line">			<span class="comment">// 不在这个范围内的字符，则表示有带声调的字母</span></div><div class="line">			<span class="keyword">if</span> (originalChar &lt; <span class="string">'a'</span> || originalChar &gt; <span class="string">'z'</span>) &#123;</div><div class="line"></div><div class="line">				<span class="comment">//查找声调字母在āáǎàēéěèīíǐìōóǒòūúǔùǖǘǚǜ中的位置，通过位置可以在下面可以计算出声调的值（是一声、二声、三声还是四声）</span></div><div class="line">				<span class="keyword">int</span> indexInAllMarked = ALL_MARKED_VOWEL.indexOf(originalChar);</div><div class="line"></div><div class="line">				<span class="comment">//利用上面索引位置与4取模，可以可以计算声调，但是声调是从1到4，因此需要加1</span></div><div class="line">				<span class="keyword">int</span> toneNumber = indexInAllMarked % <span class="number">4</span> + <span class="number">1</span>; <span class="comment">// 声调数</span></div><div class="line"></div><div class="line">				<span class="comment">//计算需要将声调字母替换成为的不带声调的字母，这里使用了一个计算公式，将ALL_MARKED_VOWEL中的字母转化为ALL_UNMARKED_VOWEL中的一个</span></div><div class="line">				<span class="keyword">char</span> replaceChar = ALL_UNMARKED_VOWEL.charAt((indexInAllMarked - indexInAllMarked % <span class="number">4</span>) / <span class="number">4</span>);</div><div class="line"></div><div class="line">				<span class="comment">//对字拼音中的声调字母进行替换，并在末尾添加声调所对应的数字</span></div><div class="line">				pinyinArray[i] = originalPinyin.replace(String.valueOf(originalChar), String.valueOf(replaceChar))</div><div class="line">						+ toneNumber;</div><div class="line">				<span class="comment">//替换成功则退出循环，因为拼音中只可能有一个声调，因此可以避免无谓的循环</span></div><div class="line">				hasMarkedChar = <span class="keyword">true</span>;</div><div class="line">				<span class="keyword">break</span>;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="comment">//如果在上面没有发生带声调字母的替换，则标记为5，表示轻声</span></div><div class="line">		<span class="keyword">if</span> (!hasMarkedChar) &#123;</div><div class="line">			<span class="comment">// 找不到带声调的拼音字母说明是轻声，用数字5表示</span></div><div class="line">			pinyinArray[i] = originalPinyin + <span class="string">"5"</span>;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">return</span> pinyinArray;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>将带声调的拼音转换为不带声调的拼音，并以数组的形式返回，每个数组项是一个字的拼音<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] convertWithoutTone(String pinyinArrayString) &#123;</div><div class="line">	String[] pinyinArray;</div><div class="line">	<span class="comment">//循环"āáǎàēéěèīíǐìōóǒòūúǔùǖǘǚǜ"中的每个字母</span></div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = ALL_MARKED_VOWEL.length() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</div><div class="line">		<span class="comment">//计算"āáǎàēéěèīíǐìōóǒòūúǔùǖǘǚǜ"中的每个字母与"aeiouv"中字母的对应关系，并用"aeiouv"的字母来替换"āáǎàēéěèīíǐìōóǒòūúǔùǖǘǚǜ"中的字母</span></div><div class="line">		<span class="keyword">char</span> originalChar = ALL_MARKED_VOWEL.charAt(i);</div><div class="line">		<span class="keyword">char</span> replaceChar = ALL_UNMARKED_VOWEL.charAt((i - i % <span class="number">4</span>) / <span class="number">4</span>);</div><div class="line">		pinyinArrayString = pinyinArrayString.replace(String.valueOf(originalChar), String.valueOf(replaceChar));</div><div class="line">	&#125;</div><div class="line">	<span class="comment">// 将拼音中的ü替换为v</span></div><div class="line">	pinyinArray = pinyinArrayString.replace(<span class="string">"ü"</span>, <span class="string">"v"</span>).split(PINYIN_SEPARATOR);</div><div class="line">	<span class="keyword">return</span> pinyinArray;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>将拼音字符串按照给定的格式进行格式化，并以数组的形式返回，需要注意的是，这个方法只是提供格式化，提供的字符串已经是拼音了<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] formatPinyin(String pinyinString, PinyinFormat pinyinFormat) &#123;</div><div class="line">	<span class="comment">//如果指定为带声调的字母的格式，则直接将字符串按照给定的分隔符进行拆分</span></div><div class="line">	<span class="keyword">if</span> (pinyinFormat == PinyinFormat.WITH_TONE_MARK) &#123;</div><div class="line">		<span class="keyword">return</span> pinyinString.split(PINYIN_SEPARATOR);</div><div class="line">	&#125; </div><div class="line"></div><div class="line">	<span class="comment">//如果指定为以数字表示声调的格式，则调用convertWithToneNumber方法进行格式化</span></div><div class="line">	<span class="keyword">else</span> <span class="keyword">if</span> (pinyinFormat == PinyinFormat.WITH_TONE_NUMBER) &#123;</div><div class="line">		<span class="keyword">return</span> convertWithToneNumber(pinyinString);</div><div class="line">	&#125; </div><div class="line"></div><div class="line">	<span class="comment">//如果指定为不带声调格式的拼音，则调用converWithoutTone方法进行格式化</span></div><div class="line">	<span class="keyword">else</span> <span class="keyword">if</span> (pinyinFormat == PinyinFormat.WITHOUT_TONE) &#123;</div><div class="line">		<span class="keyword">return</span> convertWithoutTone(pinyinString);</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> <span class="keyword">new</span> String[<span class="number">0</span>];</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>将单个汉字以pinyinFormat指定的格式进行转换，以数组的方式返回，是为了兼容多音字，多音字会返回多个拼音<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> String[] convertToPinyinArray(<span class="keyword">char</span> c, PinyinFormat pinyinFormat) &#123;</div><div class="line"></div><div class="line">	<span class="comment">//获取c字符的拼音，PINYIN_TABLE表示的是/data/pinyin.dict中的数据，是单个字与拼音的对应关系</span></div><div class="line">	String pinyin = PINYIN_TABLE.get(String.valueOf(c));</div><div class="line"></div><div class="line">	<span class="comment">//如果有对应的拼音信息，则将拼音按照指定的格式进行格式化（调用formatPinyin方法来实现）</span></div><div class="line">	<span class="keyword">if</span> ((pinyin != <span class="keyword">null</span>) &amp;&amp; (!<span class="string">"null"</span>.equals(pinyin))) &#123;</div><div class="line"></div><div class="line">		<span class="comment">//这里使用set来接收，是为了防止有多音字的情况，多音字会返回多个拼音</span></div><div class="line">		Set&lt;String&gt; set = <span class="keyword">new</span> LinkedHashSet&lt;String&gt;();</div><div class="line">		<span class="keyword">for</span> (String str : formatPinyin(pinyin, pinyinFormat)) &#123;</div><div class="line">			set.add(str);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="comment">//以数组的方式返回字的拼音，因为可能是多音字</span></div><div class="line">		<span class="keyword">return</span> set.toArray(<span class="keyword">new</span> String[set.size()]);</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> <span class="keyword">new</span> String[<span class="number">0</span>];</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>将字转换为带有声调的拼音，通过调用convertToPinyinArray<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> String[] convertToPinyinArray(<span class="keyword">char</span> c) &#123;</div><div class="line">	<span class="keyword">return</span> convertToPinyinArray(c, PinyinFormat.WITH_TONE_MARK);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>将字符串转换为指定格式的拼音字符串，并且品字符串中各个字的拼音之间使用separator指定的分隔符分隔<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">convertToPinyinString</span><span class="params">(String str, String separator, PinyinFormat pinyinFormat)</span> <span class="keyword">throws</span> PinyinException </span>&#123;</div><div class="line">	<span class="comment">//首先将字符串转换简体汉字</span></div><div class="line">	str = ChineseHelper.convertToSimplifiedChinese(str);</div><div class="line"></div><div class="line">	StringBuilder sb = <span class="keyword">new</span> StringBuilder();</div><div class="line">	<span class="keyword">int</span> i = <span class="number">0</span>;</div><div class="line">	<span class="keyword">int</span> strLen = str.length();</div><div class="line">	<span class="comment">//从字符串第一个字符开始循环</span></div><div class="line">	<span class="keyword">while</span> (i &lt; strLen) &#123;</div><div class="line"></div><div class="line">		<span class="comment">//获取字符串的第一个字符</span></div><div class="line">		String substr = str.substring(i);</div><div class="line"></div><div class="line">		<span class="comment">//-----这里应该是使用的前缀树进行查找，这里查找的应该是词组或这短语</span></div><div class="line">		List&lt;Integer&gt; commonPrefixList = DOUBLE_ARRAY_TRIE.commonPrefixSearch(substr);</div><div class="line"></div><div class="line">		<span class="comment">//如果通过上面的方法没有查找到以该字开头的词组，则进行单字的转换</span></div><div class="line">		<span class="keyword">if</span> (commonPrefixList.size() == <span class="number">0</span>) &#123;</div><div class="line"></div><div class="line">			<span class="comment">//获取要进行转换的字符</span></div><div class="line">			<span class="keyword">char</span> c = str.charAt(i);</div><div class="line"></div><div class="line">			<span class="comment">// 判断是否为汉字或者〇，如果是，则调用上面的convertToPinyinArray方法将字符转化为拼音，由于存在多音字，转换后的结果可能为数组</span></div><div class="line">			<span class="keyword">if</span> (ChineseHelper.isChinese(c) || c == CHINESE_LING) &#123;</div><div class="line">				String[] pinyinArray = convertToPinyinArray(c, pinyinFormat);</div><div class="line"></div><div class="line">				<span class="comment">//单字转换拼音时，如果结果不为空，则将拼音添加到拼音字符串的后面，如果返回的结果表示是多音字，则只取第一个读音</span></div><div class="line">				<span class="keyword">if</span> (pinyinArray != <span class="keyword">null</span>) &#123;</div><div class="line">					<span class="keyword">if</span> (pinyinArray.length &gt; <span class="number">0</span>) &#123;</div><div class="line">						sb.append(pinyinArray[<span class="number">0</span>]);</div><div class="line">					&#125; </div><div class="line">					<span class="comment">// 如果在单个字中没有找到对应的拼音，这里只会抛出异常信息，应该想办法收集没有拼音的汉字，并进行补充</span></div><div class="line">					<span class="keyword">else</span> &#123;</div><div class="line">						<span class="keyword">throw</span> <span class="keyword">new</span> PinyinException(<span class="string">"Can't convert to pinyin: "</span> + c);</div><div class="line">					&#125;</div><div class="line">				&#125; </div><div class="line">				<span class="comment">// 如果转换拼音失败，则表示字符可能46个异体字中的一个，直接添加到拼音字符串的后面</span></div><div class="line">				<span class="comment">//  -- 这里存在一个问题：假设任何符号都有拼音对应的意义，应该想办法收集起来，将这些符号转换为拼音</span></div><div class="line">				<span class="keyword">else</span> &#123;</div><div class="line">					sb.append(str.charAt(i));</div><div class="line">				&#125;</div><div class="line">			&#125; </div><div class="line">			<span class="comment">//如果不是汉字，则直接添加拼音字符串的后面</span></div><div class="line">			/  -- 这里存在一个问题：假设任何符号都有拼音对应的意义，应该想办法收集起来，将这些符号转换为拼音</div><div class="line">			<span class="keyword">else</span> &#123;</div><div class="line">				sb.append(c);</div><div class="line">			&#125;</div><div class="line">			<span class="comment">//将转换的索引加一</span></div><div class="line">			i++;</div><div class="line">		&#125; </div><div class="line">		<span class="comment">//表示通过前缀树，已经查找到了词组或短语</span></div><div class="line">		<span class="keyword">else</span> &#123;</div><div class="line"></div><div class="line">			<span class="comment">//获取从前缀树中查找到的词组或短语</span></div><div class="line">			String words = dict.get(commonPrefixList.get(commonPrefixList.size() - <span class="number">1</span>));</div><div class="line"></div><div class="line">			<span class="comment">//将查找到的词组到词组拼音表中获取对应的拼音，并调用formatPinyin方法，按照指定的格式对拼音进行格式化</span></div><div class="line">			String[] pinyinArray = formatPinyin(MUTIL_PINYIN_TABLE.get(words), pinyinFormat);</div><div class="line"></div><div class="line">			<span class="comment">//将格式化后的拼音添加到拼音字符串中，并且每个字的拼音之间使用分隔符进行分隔</span></div><div class="line">			<span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>, l = pinyinArray.length; j &lt; l; j++) &#123;</div><div class="line">				sb.append(pinyinArray[j]);</div><div class="line">				<span class="keyword">if</span> (j &lt; l - <span class="number">1</span>) &#123;</div><div class="line">					sb.append(separator);</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			<span class="comment">//将汉字循环的索引进行前进</span></div><div class="line">			i += words.length();</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="comment">//转换一部分拼音后，使用指定的分隔符进行分隔</span></div><div class="line">		<span class="keyword">if</span> (i &lt; strLen) &#123;</div><div class="line">			sb.append(separator);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> sb.toString();</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>将汉字字符串转换为带声调格式的拼音，调用convertToPinyinString方法，并指定转换的格式<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">convertToPinyinString</span><span class="params">(String str, String separator)</span> <span class="keyword">throws</span> PinyinException </span>&#123;</div><div class="line">	<span class="keyword">return</span> convertToPinyinString(str, separator, PinyinFormat.WITH_TONE_MARK);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>判断一个汉字是否是多音字，其实现就是将单个汉字转换为拼音数组，并判断返回的数组的元素的个数<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">hasMultiPinyin</span><span class="params">(<span class="keyword">char</span> c)</span> </span>&#123;</div><div class="line"></div><div class="line">	<span class="comment">//如果是多音字，convertToPinyinArray会返回一个长度大于1的数组</span></div><div class="line">	String[] pinyinArray = convertToPinyinArray(c);</div><div class="line">	<span class="keyword">if</span> (pinyinArray != <span class="keyword">null</span> &amp;&amp; pinyinArray.length &gt; <span class="number">1</span>) &#123;</div><div class="line">		<span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>将汉字字符串转换拼音首字母<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getShortPinyin</span><span class="params">(String str)</span> <span class="keyword">throws</span> PinyinException </span>&#123;</div><div class="line">	<span class="comment">//作为汉字字符串转换拼音时，拼音的分隔符</span></div><div class="line">	String separator = <span class="string">"#"</span>; <span class="comment">// 使用#作为拼音分隔符</span></div><div class="line"></div><div class="line">	<span class="comment">//该对象用来存储需要转换的汉字字符串，会去除汉字字符串中前面不属于汉字的那些字符，如 "*你好"，sb在计算完成之后应该为"你好"</span></div><div class="line">	StringBuilder sb = <span class="keyword">new</span> StringBuilder();</div><div class="line"></div><div class="line">	<span class="comment">//用来存储字符串拼音的首字母</span></div><div class="line">	<span class="keyword">char</span>[] charArray = <span class="keyword">new</span> <span class="keyword">char</span>[str.length()];</div><div class="line"></div><div class="line">	<span class="comment">//循环汉字字符串中的每个汉字，该部分代码逻辑比较复杂</span></div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, len = str.length(); i &lt; len; i++) &#123;</div><div class="line">		<span class="keyword">char</span> c = str.charAt(i);</div><div class="line"></div><div class="line">		<span class="comment">// 首先判断是否为汉字或者〇，不是的话直接将该字符返回，还可能是其他的字符，这里应该做一个收集并进行分析以便丰富词库</span></div><div class="line">		<span class="keyword">if</span> (!ChineseHelper.isChinese(c) &amp;&amp; c != CHINESE_LING) &#123;</div><div class="line">			charArray[i] = c;</div><div class="line">		&#125; </div><div class="line"></div><div class="line">		<span class="comment">//到这里，则表示字符是汉字或着〇</span></div><div class="line">		<span class="keyword">else</span> &#123;</div><div class="line">			<span class="keyword">int</span> j = i + <span class="number">1</span>;</div><div class="line">			sb.append(c);</div><div class="line"></div><div class="line">			<span class="comment">// 从str第一个汉字字符之后，连续提取汉字字符，并将汉字添加到sb中，遇到非汉字后，则终止该循环</span></div><div class="line">			<span class="keyword">while</span> (j &lt; len &amp;&amp; (ChineseHelper.isChinese(str.charAt(j)) || str.charAt(j) == CHINESE_LING)) &#123;</div><div class="line">				sb.append(str.charAt(j));</div><div class="line">				j++;</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			<span class="comment">//将上面提取出来的汉字字符串转换为拼音（没有声调的拼音）</span></div><div class="line">			String hanziPinyin = convertToPinyinString(sb.toString(), separator, PinyinFormat.WITHOUT_TONE);</div><div class="line"></div><div class="line">			<span class="comment">//将拼音按照指定的分隔符进行拆分，然后提取才分后，各个数组元素的首字母</span></div><div class="line">			String[] pinyinArray = hanziPinyin.split(separator);</div><div class="line">			<span class="keyword">for</span> (String string : pinyinArray) &#123;</div><div class="line">				charArray[i] = string.charAt(<span class="number">0</span>);</div><div class="line">				i++;</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			<span class="comment">//如果在while循环中遇到了非汉字后，会走到这里从而再次进入for循环</span></div><div class="line">			i--;</div><div class="line">			sb.setLength(<span class="number">0</span>);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="comment">//返回拼音首字母缩写</span></div><div class="line">	<span class="keyword">return</span> String.valueOf(charArray);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>添加用户自定义汉字与拼音的对应关系数据，要求添加的文件的格式必须是key=value，其中key是汉字，value是拼音。因为使用的统一的资源加载器。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">addPinyinDict</span><span class="params">(String path)</span> <span class="keyword">throws</span> FileNotFoundException </span>&#123;</div><div class="line">	PINYIN_TABLE.putAll(PinyinResource.getResource(PinyinResource.newFileReader(path)));</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>添加用户自定义的词组与拼音的对应关系数据，要求数据文件的格式必须是key=value，其中key是词组，value是对应的拼音。因为它使用的是统一的资源加载器。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">addMutilPinyinDict</span><span class="params">(String path)</span> <span class="keyword">throws</span> FileNotFoundException </span>&#123;</div><div class="line">	<span class="comment">//将自定义数据的汉字词组与拼音的对应关系添加到MUTIL_PINYIN_TABLE中，以便根据词组查找对应的拼音</span></div><div class="line">	MUTIL_PINYIN_TABLE.putAll(PinyinResource.getResource(PinyinResource.newFileReader(path)));</div><div class="line"></div><div class="line">	<span class="comment">//将ArrayList清空，该ArrayList用来构建前缀树</span></div><div class="line">	dict.clear();</div><div class="line"></div><div class="line">	<span class="comment">//清空前缀树中的数据</span></div><div class="line">	DOUBLE_ARRAY_TRIE.clear();</div><div class="line"></div><div class="line">	<span class="comment">//重新构建前缀树</span></div><div class="line">	<span class="keyword">for</span> (String word : MUTIL_PINYIN_TABLE.keySet()) &#123;</div><div class="line">		dict.add(word);</div><div class="line">	&#125;</div><div class="line">	Collections.sort(dict);</div><div class="line">	DOUBLE_ARRAY_TRIE.build(dict);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="DoubleArrayTrie类"><a href="#DoubleArrayTrie类" class="headerlink" title="DoubleArrayTrie类"></a>DoubleArrayTrie类</h3><p>DoubleArrayTrie类是一个前缀树的实现，主要用来查找词组或短语：<br>累的定义如下，首先是类变量和类的私有类：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//用来从文件中读取trie数据的缓冲区大小</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> BUF_SIZE = <span class="number">16384</span>;</div><div class="line"></div><div class="line"><span class="comment">//定义了数据单元的大小，主要在将Double-Array trie从文件中加载的时候使用，来判断trie的size</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> UNIT_SIZE = <span class="number">8</span>; <span class="comment">// size of int + int</span></div><div class="line"></div><div class="line"><span class="comment">//私有类，作为Double-Array trie节点的对象</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> code;</div><div class="line"></div><div class="line">    <span class="comment">//表示节点的深度</span></div><div class="line">    <span class="keyword">int</span> depth;</div><div class="line"></div><div class="line">    <span class="comment">// 与right配合，标记出子节点的范围</span></div><div class="line">    <span class="keyword">int</span> left;</div><div class="line"></div><div class="line">    <span class="comment">// 与left配合，标记出子节点的范围</span></div><div class="line">    <span class="keyword">int</span> right;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="comment">//双数组树的核心check数组和base数组</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">int</span> check[];</div><div class="line"><span class="keyword">private</span> <span class="keyword">int</span> base[];</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">boolean</span> used[];</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">int</span> size;</div><div class="line"></div><div class="line"><span class="comment">//核心数组check和base所分配的大小</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">int</span> allocSize;</div><div class="line"></div><div class="line"><span class="comment">//用来生成Double-Array trie的数据，是完整的数据集</span></div><div class="line"><span class="keyword">private</span> List&lt;String&gt; key;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">int</span> keySize;</div><div class="line"><span class="keyword">private</span> <span class="keyword">int</span> length[];</div><div class="line"><span class="keyword">private</span> <span class="keyword">int</span> value[];</div><div class="line"><span class="keyword">private</span> <span class="keyword">int</span> progress;</div><div class="line"></div><div class="line"><span class="comment">//插入节点时使用的位置起始点</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">int</span> nextCheckPos;</div><div class="line"></div><div class="line"><span class="comment">// boolean no_delete_;</span></div><div class="line"><span class="keyword">int</span> error_;</div></pre></td></tr></table></figure></p>
<p>重新分配Double-Array trie内部核心数组check和base的方法，根据指定的大小生成新的数组，并将原来数组的数据完整的拷贝的新数组中：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * 重新将数据进行扩容</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">resize</span><span class="params">(<span class="keyword">int</span> newSize)</span> </span>&#123;</div><div class="line">    <span class="comment">//以指定的大小来生成所需大小的数组</span></div><div class="line">    <span class="keyword">int</span>[] base2 = <span class="keyword">new</span> <span class="keyword">int</span>[newSize];</div><div class="line">    <span class="keyword">int</span>[] check2 = <span class="keyword">new</span> <span class="keyword">int</span>[newSize];</div><div class="line">    <span class="keyword">boolean</span> used2[] = <span class="keyword">new</span> <span class="keyword">boolean</span>[newSize];</div><div class="line"></div><div class="line">    <span class="comment">//判断数据数组分配的大小是否有效，如果有，则将原来分配的数据数组的内容拷贝到新分配的数组中</span></div><div class="line">    <span class="keyword">if</span> (allocSize &gt; <span class="number">0</span>) &#123;</div><div class="line">        System.arraycopy(base, <span class="number">0</span>, base2, <span class="number">0</span>, allocSize);</div><div class="line">        System.arraycopy(check, <span class="number">0</span>, check2, <span class="number">0</span>, allocSize);</div><div class="line">        System.arraycopy(used2, <span class="number">0</span>, used2, <span class="number">0</span>, allocSize);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//更改Double-Array trie内部check、base和used的引用关系</span></div><div class="line">    base = base2;</div><div class="line">    check = check2;</div><div class="line">    used = used2;</div><div class="line"></div><div class="line">    <span class="comment">// 记录当前数据数组的分配大小</span></div><div class="line">    <span class="keyword">return</span> allocSize = newSize;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>根据父节点提取子节点数据的方法，并返回子节点的个数，所有子节点的数据通过参数siblings进行返回：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">fetch</span><span class="params">(Node parent, List&lt;Node&gt; siblings)</span> </span>&#123;</div><div class="line">    <span class="comment">//如果发生错误，直接返回，如果keys（数据集）没有排序，error的值为-3，因为没有排序的话，在构建树数据的时候会乱，因此需要数据集是排序后的</span></div><div class="line">    <span class="keyword">if</span> (error_ &lt; <span class="number">0</span>)</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line"></div><div class="line">    <span class="comment">//用来存储前一个节点的code值</span></div><div class="line">    <span class="keyword">int</span> prev = <span class="number">0</span>;</div><div class="line"></div><div class="line">    <span class="comment">//其实就是查找具有相同前缀的一组词</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = parent.left; i &lt; parent.right; i++) &#123;</div><div class="line">        <span class="keyword">if</span> ((length != <span class="keyword">null</span> ? length[i] : key.get(i).length()) &lt; parent.depth)</div><div class="line">            <span class="keyword">continue</span>;</div><div class="line"></div><div class="line">        <span class="comment">//获取下一个词语</span></div><div class="line">        String tmp = key.get(i);</div><div class="line"></div><div class="line">        <span class="comment">//用来记录当前的层次深度</span></div><div class="line">        <span class="keyword">int</span> cur = <span class="number">0</span>;</div><div class="line"></div><div class="line">        <span class="comment">//当前的层次深度是父级深度+1</span></div><div class="line">        <span class="keyword">if</span> ((length != <span class="keyword">null</span> ? length[i] : tmp.length()) != parent.depth)</div><div class="line">            <span class="comment">//注意这里使用的tmp，tmp是父节点的字节点的第i个词，因为所有的词都是从根节点过来的，因此</span></div><div class="line">            <span class="comment">//词中的第N个字，就位于树中的第N层，因为数组和树都是从0开始算，因此当前层级=当前层级-1=父级的层级</span></div><div class="line">            <span class="comment">//其实这里就是获取节点的常量值--使用的是char的值</span></div><div class="line">            cur = (<span class="keyword">int</span>) tmp.charAt(parent.depth) + <span class="number">1</span>;</div><div class="line"></div><div class="line">        <span class="comment">//if代码块是要求key或length中的数据应该是排序后（升序）的数据，否则就会出错，error_是类全局错误，如果error_小于0，则所有的方法都会出错而退出</span></div><div class="line">        <span class="keyword">if</span> (prev &gt; cur) &#123;</div><div class="line">            error_ = -<span class="number">3</span>;</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">/**</span></div><div class="line">         * 该部分代码用来判断是否找到了新的节点，前节点值与当前节点值不相同或第一个节点(siblings.size == 0)，表示找到了新节点。新节点使用Node来表示</div><div class="line">         */</div><div class="line">        <span class="keyword">if</span> (cur != prev || siblings.size() == <span class="number">0</span>) &#123;</div><div class="line">            <span class="comment">//生成一个新的树节点</span></div><div class="line">            Node tmp_node = <span class="keyword">new</span> Node();</div><div class="line"></div><div class="line">            <span class="comment">//depth表示节点从根节点开始算，所处层次</span></div><div class="line">            tmp_node.depth = parent.depth + <span class="number">1</span>;</div><div class="line"></div><div class="line">            <span class="comment">//用来表示当前节点字的code值</span></div><div class="line">            tmp_node.code = cur;</div><div class="line"></div><div class="line">            <span class="comment">//用来记录子节点在数据集中的左边界</span></div><div class="line">            tmp_node.left = i;</div><div class="line"></div><div class="line">            <span class="comment">//用来记录子节点在数据集中的右边界</span></div><div class="line">            <span class="keyword">if</span> (siblings.size() != <span class="number">0</span>)</div><div class="line">                siblings.get(siblings.size() - <span class="number">1</span>).right = i;</div><div class="line"></div><div class="line">            <span class="comment">//siblings中存储的是去重后的字节点，但是每个字节点中包含有取词范围</span></div><div class="line">            siblings.add(tmp_node);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        prev = cur;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//如果循环到了结尾，则parent最后一个字节点的最右边的界限为父节点的界限，将上面Mark-1中最后一个节点Node.right进行填充</span></div><div class="line">    <span class="keyword">if</span> (siblings.size() != <span class="number">0</span>)</div><div class="line">        siblings.get(siblings.size() - <span class="number">1</span>).right = parent.right;</div><div class="line"></div><div class="line">    <span class="comment">//返回父节点拥有子节点的个数</span></div><div class="line">    <span class="keyword">return</span> siblings.size();</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>Double-Array trie的构建方法，通过上面的fetch获取子节点，然后利用本方法来构建两个数组的数据：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">insert</span><span class="params">(List&lt;Node&gt; siblings)</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (error_ &lt; <span class="number">0</span>)</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line"></div><div class="line">    <span class="comment">//用来寻找base[i] + c(常量) = t的那个初始值，这个值会保存在base[i]中</span></div><div class="line">    <span class="comment">//注意这里是一个局部变量</span></div><div class="line">    <span class="keyword">int</span> begin = <span class="number">0</span>;</div><div class="line"></div><div class="line">    <span class="comment">//nextCheckPost用来存储下一个检查点的值，根节点进入的时候，值为0</span></div><div class="line">    <span class="comment">//pos表示从这个位置开始进行检查</span></div><div class="line">    <span class="comment">//这里为什么要做这个检查呢？</span></div><div class="line">    <span class="comment">//因为，在双数组树中，每个字都有一个变量（一个固定的值），Node.code就是这个变量的值，在对根节点进行定位的时候，下标就是这个变量值来定位的</span></div><div class="line">    <span class="comment">//注意这里是一个局部变量</span></div><div class="line">    <span class="keyword">int</span> pos = ((siblings.get(<span class="number">0</span>).code + <span class="number">1</span> &gt; nextCheckPos) ? siblings.get(<span class="number">0</span>).code + <span class="number">1</span> : nextCheckPos) - <span class="number">1</span>;</div><div class="line"></div><div class="line">    <span class="comment">//用来计算check数组中，从pos开始到pos + siblings.size()之间，非0元素的个数，并用该值去计算 nextCheckPos的值</span></div><div class="line">    <span class="keyword">int</span> nonzero_num = <span class="number">0</span>;</div><div class="line"></div><div class="line">    <span class="comment">//用来表示是否是对siblings中的元素第一次找到check[pos]=0的pos值</span></div><div class="line">    <span class="keyword">int</span> first = <span class="number">0</span>;</div><div class="line"></div><div class="line">    <span class="comment">//如果pos大于allocSize，则表示"状态"（每个字称为状态）超出了check数组和base数组的界限了，需要扩容</span></div><div class="line">    <span class="keyword">if</span> (allocSize &lt;= pos)</div><div class="line">        resize(pos + <span class="number">1</span>);</div><div class="line"></div><div class="line">    <span class="comment">//这段代码就是为siblings中的Node寻找在check数组中那些连续的空间的起始值</span></div><div class="line">    outer: <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line"></div><div class="line">        <span class="comment">//增加下标值，用来寻找满足所有子节点在base数组中能够连续存储的起始位置</span></div><div class="line">        pos++;</div><div class="line"></div><div class="line">        <span class="comment">//检查下标是否超出了check数组和base数组的界限</span></div><div class="line">        <span class="keyword">if</span> (allocSize &lt;= pos)</div><div class="line">            resize(pos + <span class="number">1</span>);</div><div class="line"></div><div class="line">        <span class="comment">//判断check中下标位置的值是否为0，</span></div><div class="line">        <span class="comment">// 如果不是0，则表示该位置已经被使用，因此需要增加回到开头，对pos进行前进</span></div><div class="line">        <span class="comment">// 为0则表示可以用</span></div><div class="line">        <span class="keyword">if</span> (check[pos] != <span class="number">0</span>) &#123;</div><div class="line">            nonzero_num++;</div><div class="line">            <span class="keyword">continue</span>;</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (first == <span class="number">0</span>) &#123;</div><div class="line">            <span class="comment">//nextCheckPos用来表示，下一次应该从这个位置开始检查check</span></div><div class="line">            <span class="comment">//另外为啥要从nextCheckPos这个位置开始呢，因为构建双数组树的数据集是已经排过序的了，</span></div><div class="line">            <span class="comment">// 所以以后的值都会比最最第一个字的code大，因此无论怎么加，都会比这个值要大</span></div><div class="line">            nextCheckPos = pos;</div><div class="line">            <span class="comment">//已经寻找第一个check[pos]为0的pos位置</span></div><div class="line">            first = <span class="number">1</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//pos-(siblings.get(0).code)(这个值相当于字的变量值)，也就是说base中要存储的值可能为begin，最起码要从这个值开始尝试</span></div><div class="line">        <span class="comment">//begin相当于base[s] + c = t公式中，base[s]中存储的值，用来计算当前字应该存储的下标t</span></div><div class="line">        begin = pos - siblings.get(<span class="number">0</span>).code;</div><div class="line"></div><div class="line">        <span class="comment">//继续判断check数组和base数组是否能够容纳当前字，如果不能够容纳，则需要根据 字的个数与progress的比值来扩容</span></div><div class="line">        <span class="keyword">if</span> (allocSize &lt;= (begin + siblings.get(siblings.size() - <span class="number">1</span>).code)) &#123;</div><div class="line">            <span class="comment">// progress can be zero</span></div><div class="line">            <span class="comment">// 通过keySize和progress，可以计算base和check扩容的比率</span></div><div class="line">            <span class="keyword">double</span> l = (<span class="number">1.05</span> &gt; <span class="number">1.0</span> * keySize / (progress + <span class="number">1</span>)) ? <span class="number">1.05</span> : <span class="number">1.0</span> * keySize / (progress + <span class="number">1</span>);</div><div class="line">            resize((<span class="keyword">int</span>) (allocSize * l));</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//判断used[begin]位置是否为true，used的作用是什么呢？？？？总之如果used[begin]==true就需要进行循环</span></div><div class="line">        <span class="keyword">if</span> (used[begin])</div><div class="line">            <span class="keyword">continue</span>;</div><div class="line"></div><div class="line">        <span class="comment">//这一段代码是在寻找 base[s] + c = t公式中，base[s]的值，使得siblings中的所有Node都满足check[begin + Node.code] = 0</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; siblings.size(); i++)</div><div class="line"></div><div class="line">            <span class="comment">//在循环判断的过程中，只要siblings中的某个Node不满足check[begin + Node.code] = 0，则说明begin不好使，需要回到outer继续寻找</span></div><div class="line">            <span class="keyword">if</span> (check[begin + siblings.get(i).code] != <span class="number">0</span>)</div><div class="line">                <span class="keyword">continue</span> outer;</div><div class="line"></div><div class="line">        <span class="comment">//如果程序运行到了这里，则表示找到了使得siblings中的所有Node都满足check[begin + Node.code] = 0的begin</span></div><div class="line">        <span class="keyword">break</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//------------如果运行到了这里，则表示base[s] + c = t，中base[s]的值已经找到，即为begin-----------------</span></div><div class="line">    <span class="comment">// -- Simple heuristics --</span></div><div class="line">    <span class="comment">// if the percentage of non-empty contents in check between the</span></div><div class="line">    <span class="comment">// index</span></div><div class="line">    <span class="comment">// 'next_check_pos' and 'check' is greater than some constant value</span></div><div class="line">    <span class="comment">// (e.g. 0.9),</span></div><div class="line">    <span class="comment">// new 'next_check_pos' index is written by 'check'.</span></div><div class="line">    <span class="comment">//这块代码的意思就是计算当前位置和之前位置之间check中有多少位置已经不是0了，</span></div><div class="line">    <span class="comment">// 如果比例超过了一个比率，就说明干[nextCheckPos, pos]区间几乎不可以使用了，</span></div><div class="line">    <span class="comment">// 如果比例低于这个比率，则说明还有可以使用的空间</span></div><div class="line">    <span class="keyword">if</span> (<span class="number">1.0</span> * nonzero_num / (pos - nextCheckPos + <span class="number">1</span>) &gt;= <span class="number">0.95</span>)</div><div class="line">        nextCheckPos = pos;</div><div class="line"></div><div class="line">    <span class="comment">//干啥的？？？--似乎没怎么用到</span></div><div class="line">    used[begin] = <span class="keyword">true</span>;</div><div class="line"></div><div class="line">    <span class="comment">//size用来记录了数组的有效长度（默认数组中的值为0，有效长度就是最后一个不是0的位置）</span></div><div class="line">    <span class="comment">//begin代表了Node在check中寻找位置时的那个常量</span></div><div class="line">    size = (size &gt; begin + siblings.get(siblings.size() - <span class="number">1</span>).code + <span class="number">1</span>) ? size : begin + siblings.get(siblings.size() - <span class="number">1</span>).code + <span class="number">1</span>;</div><div class="line"></div><div class="line">    <span class="comment">//上面寻找到了常量，siblings中的字在check数组中对应位置的值设置为begin，begin</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; siblings.size(); i++)</div><div class="line">        check[begin + siblings.get(i).code] = begin;</div><div class="line"></div><div class="line">    <span class="comment">//循环siblings中的所有Node，每个Node称为当前节点</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; siblings.size(); i++) &#123;</div><div class="line">        List&lt;Node&gt; new_siblings = <span class="keyword">new</span> ArrayList&lt;Node&gt;();</div><div class="line"></div><div class="line">        <span class="comment">//获取当前节点的子节点，返回0则表示当前节点没有子节点，如果有字节点则插入子节点，这是一个迭代处理</span></div><div class="line">        <span class="keyword">if</span> (fetch(siblings.get(i), new_siblings) == <span class="number">0</span>) &#123;</div><div class="line">            <span class="comment">//base用来记录什么？？？？？？？</span></div><div class="line">            System.out.println(value);</div><div class="line">            base[begin + siblings.get(i).code] = (value != <span class="keyword">null</span>) ? (-value[siblings.get(i).left] - <span class="number">1</span>) : (-siblings.get(i).left - <span class="number">1</span>);</div><div class="line"></div><div class="line">            <span class="keyword">if</span> (value != <span class="keyword">null</span> &amp;&amp; (-value[siblings.get(i).left] - <span class="number">1</span>) &gt;= <span class="number">0</span>) &#123;</div><div class="line">                error_ = -<span class="number">2</span>;</div><div class="line">                <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            progress++;</div><div class="line">            <span class="comment">// if (progress_func_) (*progress_func_) (progress,</span></div><div class="line">            <span class="comment">// keySize);</span></div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="comment">//为当前节点插入子节点，并记录子节点的位置</span></div><div class="line">            <span class="keyword">int</span> h = insert(new_siblings);</div><div class="line">            base[begin + siblings.get(i).code] = h;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> begin;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>Double-Array trie类的默认构造方法：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">DoubleArrayTrie</span><span class="params">()</span> </span>&#123;</div><div class="line">    check = <span class="keyword">null</span>;</div><div class="line">    base = <span class="keyword">null</span>;</div><div class="line">    used = <span class="keyword">null</span>;</div><div class="line">    size = <span class="number">0</span>;</div><div class="line">    allocSize = <span class="number">0</span>;</div><div class="line">    <span class="comment">// no_delete_ = false;</span></div><div class="line">    error_ = <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line">``</div><div class="line">用来清理Double-Array trie的方法：</div><div class="line">``` <span class="function">Java</span></div><div class="line"><span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> &#123;</div><div class="line">    <span class="comment">// if (! no_delete_)</span></div><div class="line">    check = <span class="keyword">null</span>;</div><div class="line">    base = <span class="keyword">null</span>;</div><div class="line">    used = <span class="keyword">null</span>;</div><div class="line">    allocSize = <span class="number">0</span>;</div><div class="line">    size = <span class="number">0</span>;</div><div class="line">    <span class="comment">// no_delete_ = false;</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>计算数组存储的基本单元的大小：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getUnitSize</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> UNIT_SIZE;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>计算Double-Array tire的有效大小，和分配的大小不同：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getSize</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> size;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>计算Double-Array trie所占用的有效空间：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getTotalSize</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> size * UNIT_SIZE;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>计算Double-Array trie中核心数组check和base中真正存储了数据的数据单元个数：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getNonzeroSize</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> result = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++)</div><div class="line">        <span class="keyword">if</span> (check[i] != <span class="number">0</span>)</div><div class="line">            result++;</div><div class="line">    <span class="keyword">return</span> result;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>构建Double-Array trie的方法，key是一个挣序排序后的集合，集合中包含了所需的所有数据：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">build</span><span class="params">(List&lt;String&gt; key)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> build(key, <span class="keyword">null</span>, <span class="keyword">null</span>, key.size());</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>构建Double-Array tire的方法，本方法为实际操作方法，并且提供了更加丰富的参数：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">build</span><span class="params">(List&lt;String&gt; _key, <span class="keyword">int</span> _length[], <span class="keyword">int</span> _value[], <span class="keyword">int</span> _keySize)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (_keySize &gt; _key.size() || _key == <span class="keyword">null</span>)</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line"></div><div class="line">    <span class="comment">// progress_func_ = progress_func;</span></div><div class="line">    key = _key;</div><div class="line">    length = _length;</div><div class="line">    keySize = _keySize;</div><div class="line">    value = _value;</div><div class="line">    progress = <span class="number">0</span>;</div><div class="line"></div><div class="line">    resize(<span class="number">65536</span> * <span class="number">32</span>); </div><div class="line"></div><div class="line">    base[<span class="number">0</span>] = <span class="number">1</span>;</div><div class="line">    nextCheckPos = <span class="number">0</span>;</div><div class="line"></div><div class="line">    <span class="comment">//生成double-array trie的根节点，根节点的left=0，right=keySize, depth=0，left和right指定的字节点的范围，因为是根节点，所以是所有数据</span></div><div class="line">    Node root_node = <span class="keyword">new</span> Node();</div><div class="line">    root_node.left = <span class="number">0</span>;</div><div class="line">    root_node.right = keySize;</div><div class="line">    root_node.depth = <span class="number">0</span>;</div><div class="line"></div><div class="line">    <span class="comment">//siblings用来存储根节点的字节点（去重后的）</span></div><div class="line">    List&lt;Node&gt; siblings = <span class="keyword">new</span> ArrayList&lt;Node&gt;();</div><div class="line"></div><div class="line">    <span class="comment">//查找根节点的子节点（去重）</span></div><div class="line">    fetch(root_node, siblings);</div><div class="line"></div><div class="line">    <span class="comment">//构建根节点的子节点，内部有迭代查询，当根节点的字节点构造完成后，整棵树就构造完成了</span></div><div class="line">    insert(siblings);</div><div class="line"></div><div class="line">    <span class="comment">// size += (1 &lt;&lt; 8 * 2) + 1; // ???</span></div><div class="line">    <span class="comment">// if (size &gt;= allocSize) resize (size);</span></div><div class="line"></div><div class="line">    used = <span class="keyword">null</span>;</div><div class="line">    key = <span class="keyword">null</span>;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> error_;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>从文件中加载已有的Double-Array trie的数据，并重新构建成可用的Double-Array trie：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(String fileName)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    File file = <span class="keyword">new</span> File(fileName);</div><div class="line"></div><div class="line">    <span class="comment">//base和check是分别作为int写到文件的，因此两个int代表一个数据，文件的大小与两个int大小的比值就是数据的个数</span></div><div class="line">    size = (<span class="keyword">int</span>) file.length() / UNIT_SIZE;</div><div class="line"></div><div class="line">    <span class="comment">//实例化空的check和base，数据为空，但是不是null</span></div><div class="line">    check = <span class="keyword">new</span> <span class="keyword">int</span>[size];</div><div class="line">    base = <span class="keyword">new</span> <span class="keyword">int</span>[size];</div><div class="line"></div><div class="line">    DataInputStream is = <span class="keyword">null</span>;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="comment">//打开文件，并将读取的数据填充到base和check中</span></div><div class="line">        is = <span class="keyword">new</span> DataInputStream(<span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> FileInputStream(file), BUF_SIZE));</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</div><div class="line">            base[i] = is.readInt();</div><div class="line">            check[i] = is.readInt();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">    <span class="comment">//关闭输入流</span></div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">        <span class="keyword">if</span> (is != <span class="keyword">null</span>)</div><div class="line">            is.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>将现有的Double-Array trie保存到文件：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">save</span><span class="params">(String fileName)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    DataOutputStream out = <span class="keyword">null</span>;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="comment">//打开文件的输入流</span></div><div class="line">        out = <span class="keyword">new</span> DataOutputStream(<span class="keyword">new</span> BufferedOutputStream(<span class="keyword">new</span> FileOutputStream(fileName)));</div><div class="line"></div><div class="line">        <span class="comment">//将base和check对应元素的值作为一条数据写出到文件</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</div><div class="line">            out.writeInt(base[i]);</div><div class="line">            out.writeInt(check[i]);</div><div class="line">        &#125;</div><div class="line">        out.close();</div><div class="line"></div><div class="line">    <span class="comment">//关闭输出流</span></div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">        <span class="keyword">if</span> (out != <span class="keyword">null</span>)</div><div class="line">            out.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>在Double-Array trie中精确匹配搜索给定的词组：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">exactMatchSearch</span><span class="params">(String key)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> exactMatchSearch(key, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>在Double-Array trie中精确匹配搜索，该方法为实际搜索方法：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">exactMatchSearch</span><span class="params">(String key, <span class="keyword">int</span> pos, <span class="keyword">int</span> len, <span class="keyword">int</span> nodePos)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (len &lt;= <span class="number">0</span>)</div><div class="line">        len = key.length();</div><div class="line">    <span class="keyword">if</span> (nodePos &lt;= <span class="number">0</span>)</div><div class="line">        nodePos = <span class="number">0</span>;</div><div class="line"></div><div class="line">    <span class="keyword">int</span> result = -<span class="number">1</span>;</div><div class="line"></div><div class="line">    <span class="keyword">char</span>[] keyChars = key.toCharArray();</div><div class="line"></div><div class="line">    <span class="comment">//获取第一个下标值，应该为1</span></div><div class="line">    <span class="keyword">int</span> b = base[nodePos];</div><div class="line">    <span class="keyword">int</span> p;</div><div class="line"></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = pos; i &lt; len; i++) &#123;</div><div class="line">        p = b + (<span class="keyword">int</span>) (keyChars[i]) + <span class="number">1</span>;</div><div class="line">        <span class="keyword">if</span> (b == check[p])</div><div class="line">            b = base[p];</div><div class="line">        <span class="keyword">else</span></div><div class="line">            <span class="keyword">return</span> result;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    p = b;</div><div class="line">    <span class="keyword">int</span> n = base[p];</div><div class="line">    <span class="keyword">if</span> (b == check[p] &amp;&amp; n &lt; <span class="number">0</span>) &#123;</div><div class="line">        result = -n - <span class="number">1</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> result;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>该方法主要用来精确匹配搜索的，理解该方法也是理解inster方法的关键，接下来我们将详细介绍并举例该方法。<br>……<br>常用前缀搜索方法：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">commonPrefixSearch</span><span class="params">(String key)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> commonPrefixSearch(key, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>常用前缀搜索方法的具体实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">commonPrefixSearch</span><span class="params">(String key, <span class="keyword">int</span> pos, <span class="keyword">int</span> len, <span class="keyword">int</span> nodePos)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (len &lt;= <span class="number">0</span>)</div><div class="line">        len = key.length();</div><div class="line">    <span class="keyword">if</span> (nodePos &lt;= <span class="number">0</span>)</div><div class="line">        nodePos = <span class="number">0</span>;</div><div class="line"></div><div class="line">    List&lt;Integer&gt; result = <span class="keyword">new</span> ArrayList&lt;Integer&gt;();</div><div class="line"></div><div class="line">    <span class="comment">//将要搜索的字符串拆成字符数组</span></div><div class="line">    <span class="keyword">char</span>[] keyChars = key.toCharArray();</div><div class="line"></div><div class="line">    <span class="keyword">int</span> b = base[nodePos];</div><div class="line">    <span class="keyword">int</span> n;</div><div class="line">    <span class="keyword">int</span> p;</div><div class="line"></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = pos; i &lt; len; i++) &#123;</div><div class="line">        p = b;</div><div class="line">        n = base[p];</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (b == check[p] &amp;&amp; n &lt; <span class="number">0</span>) &#123;</div><div class="line">            result.add(-n - <span class="number">1</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        p = b + (<span class="keyword">int</span>) (keyChars[i]) + <span class="number">1</span>;</div><div class="line">        <span class="keyword">if</span> (b == check[p])</div><div class="line">            b = base[p];</div><div class="line">        <span class="keyword">else</span></div><div class="line">            <span class="keyword">return</span> result;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    p = b;</div><div class="line">    n = base[p];</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (b == check[p] &amp;&amp; n &lt; <span class="number">0</span>) &#123;</div><div class="line">        result.add(-n - <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> result;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>理解该方法也是理解前缀搜索的关键，更加有助于理解insert方法的实现。……</p>
<p>调试方法，会将Double-Array trie的关键数组check和base打印出来：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">dump</span><span class="params">()</span> </span>&#123;</div><div class="line">    System.out.print(<span class="string">"index:\t\t"</span>);</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</div><div class="line">        System.out.print(i + <span class="string">"\t\t"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    System.out.print(<span class="string">"\r\nbase:\t\t"</span>);</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</div><div class="line">        System.out.print(base[i] + <span class="string">"\t\t"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    System.out.print(<span class="string">"\r\ncheck:\t\t"</span>);</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</div><div class="line">        System.out.print(check[i] + <span class="string">"\t\t"</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过源码分析，Jpinyin可以方便的将汉字转换为拼音，也可以方便的将繁体字转换为简体字，但是不足之处是，在将简体字转换为繁体子的时候，效率会非常的低。另外，对于特殊的46个异体字，没有明确的指出，从而加以区分。<br>单单从汉字转拼音的角度来说，该功能比较好用，但是如果考虑到自然语言处理的应用，还是有很多不足的地方。例如，表情符号也可以代表汉字，也就对应着拼音；unicode其实也是汉字的另一种表示，也应该有对应的拼音；等等。</p>
<h1 id="其他备注"><a href="#其他备注" class="headerlink" title="其他备注"></a>其他备注</h1><p>moji表情一般是两个unicode，但也有一部分是一个unicode<br>String pattern = “[\ud83c\udc00-\ud83c\udfff]|[\ud83d\udc00-\ud83d\udfff]|[\u2600-\u27ff]”</p>
<p><a href="http://oaavtz33a.bkt.clouddn.com/emoji_unicode.txt" title="moji表情与unicode的对应" target="_blank" rel="external">moji表情与unicode的对应</a><br>moji表情和unicode字符串之间的相互转换可以使用apache的commons-lang包中StringEscapeUtils类的escapeJava(String)和unescapeJava(String)来进行相互转换。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍JPinYin的使用和配置，&lt;a href=&quot;https://github.com/stuxuhai/jpinyin&quot; title=&quot;Jpinyin&quot;&gt;github的地址&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id=&quot;简单介绍&quot;&gt;&lt;a href=&quot;#简单介绍&quot; class=&quot;headerlink&quot; title=&quot;简单介绍&quot;&gt;&lt;/a&gt;简单介绍&lt;/h1&gt;&lt;p&gt;Jpinyin是一个开源的用于将汉字转换为拼音的java库。&lt;/p&gt;
&lt;h2 id=&quot;主要特性&quot;&gt;&lt;a href=&quot;#主要特性&quot; class=&quot;headerlink&quot; title=&quot;主要特性&quot;&gt;&lt;/a&gt;主要特性&lt;/h2&gt;&lt;p&gt;1、准确、完善的字库：Unicode编码从4E00-9FA5范围及3007(〇)的20903个汉字中，除了46个异体字（不存在标准拼音）Jpinyin都能转换。&lt;br&gt;2、拼音转换速度快：经测试，转换Unicode编码范围的20902个汉字，Jpinyin耗时约为100毫秒。&lt;br&gt;3、支持多种拼音格式：Jpinyin支持多种拼音输出格式：带声调、不带声调、数字表示声调以及拼音首字母格式输出。&lt;br&gt;4、常见多音字识别：Jpinyin支持常见多音字的识别，其中包括词组、成语、地名等；&lt;br&gt;5、简繁体中文互转。&lt;br&gt;6、支持用户自定义字典。&lt;/p&gt;
&lt;h2 id=&quot;Maven依赖&quot;&gt;&lt;a href=&quot;#Maven依赖&quot; class=&quot;headerlink&quot; title=&quot;Maven依赖&quot;&gt;&lt;/a&gt;Maven依赖&lt;/h2&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&amp;lt;dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;   &amp;lt;groupId&amp;gt;com.github.stuxuhai&amp;lt;/groupId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;   &amp;lt;artifactId&amp;gt;jpinyin&amp;lt;/artifactId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;lt;version&amp;gt;1.1.8&amp;lt;/version&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;lt;/dependency&amp;gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;用法&quot;&gt;&lt;a href=&quot;#用法&quot; class=&quot;headerlink&quot; title=&quot;用法&quot;&gt;&lt;/a&gt;用法&lt;/h2&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;String str = &lt;span class=&quot;string&quot;&gt;&quot;你好世界&quot;&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;PinyinHelper.convertToPinyinString(str, &lt;span class=&quot;string&quot;&gt;&quot;,&quot;&lt;/span&gt;, PinyinFormat.WITH_TONE_MARK); &lt;span class=&quot;comment&quot;&gt;// nǐ,hǎo,shì,jiè&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;PinyinHelper.convertToPinyinString(str, &lt;span class=&quot;string&quot;&gt;&quot;,&quot;&lt;/span&gt;, PinyinFormat.WITH_TONE_NUMBER); &lt;span class=&quot;comment&quot;&gt;// ni3,hao3,shi4,jie4&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;PinyinHelper.convertToPinyinString(str, &lt;span class=&quot;string&quot;&gt;&quot;,&quot;&lt;/span&gt;, PinyinFormat.WITHOUT_TONE); &lt;span class=&quot;comment&quot;&gt;// ni,hao,shi,jie&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;PinyinHelper.getShortPinyin(str); &lt;span class=&quot;comment&quot;&gt;// nhsj&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;PinyinHelper.addPinyinDict(&lt;span class=&quot;string&quot;&gt;&quot;user.dict&quot;&lt;/span&gt;);  &lt;span class=&quot;comment&quot;&gt;// 添加用户自定义字典&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://baimoon.github.io/categories/NLP/"/>
    
    
      <category term="汉字转拼音" scheme="http://baimoon.github.io/tags/%E6%B1%89%E5%AD%97%E8%BD%AC%E6%8B%BC%E9%9F%B3/"/>
    
  </entry>
  
  <entry>
    <title>scikit image - a crash course on NumPy for images</title>
    <link href="http://baimoon.github.io/2016/12/19/scikitImage-ACrashCourseOnNumPyForImages/"/>
    <id>http://baimoon.github.io/2016/12/19/scikitImage-ACrashCourseOnNumPyForImages/</id>
    <published>2016-12-19T04:43:26.000Z</published>
    <updated>2016-12-22T07:23:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文用来学习scikit-image的官方文档的<a href="http://scikit-image.org/docs/stable/user_guide/numpy_images.html" title="a crash course on NumPy for images" target="_blank" rel="external">a crash course on NumPy for images</a>，<a href="http://scikit-image.org/docs/stable/user_guide/numpy_images.html" title="a crash course on NumPy for images" target="_blank" rel="external">原链接</a></p>
<h1 id="A-crash-course-on-NumPy-for-images"><a href="#A-crash-course-on-NumPy-for-images" class="headerlink" title="A crash course on NumPy for images"></a>A crash course on NumPy for images</h1><p>scikit-image是以NumPy数组的方式来操作图像。因此图像很大一部分的操作将是使用NumPy：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> skimage <span class="keyword">import</span> data</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera = data.camera()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>type(camera)</div><div class="line">&lt;type <span class="string">'numpy.ndarray'</span>&gt;</div></pre></td></tr></table></figure></p>
<p>检索图像的几何以及像素数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.shape</div><div class="line">(<span class="number">512</span>, <span class="number">512</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.size</div><div class="line"><span class="number">262144</span></div></pre></td></tr></table></figure></p>
<p>检索关于灰度值的统计信息：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.min(), camera.max()</div><div class="line">(<span class="number">0</span>, <span class="number">255</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.mean()</div><div class="line"><span class="number">118.31400299072266</span></div></pre></td></tr></table></figure></p>
<p>代表图片的NumPy数组可以是浮点数类型的不同整数。查看<a href="http://scikit-image.org/docs/stable/user_guide/data_types.html#data-types" title="Image data type and what the mean" target="_blank" rel="external">Image data type and what the mean</a>获取关于这些类型的更多信息，以及scikit-image如何处理它们。</p>
<a id="more"></a>
<h2 id="NumPy-indexing"><a href="#NumPy-indexing" class="headerlink" title="NumPy indexing"></a>NumPy indexing</h2><p>NumPy indexing能够被用来查找像素值和修改像素值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Get the value of the pixel on the 10th row and 20th column</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera[<span class="number">10</span>, <span class="number">20</span>]</div><div class="line"><span class="number">153</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Set to black the pixel on the 3rd row and 10th column</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera[<span class="number">3</span>, <span class="number">10</span>] = <span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>注意：在NumPy中，第一个维度(camera.shape[0])对应于行，第二个维度(camera.shape[1])对应于列，使用原点(camerap[0, 0])对应左上角。这符合矩阵/线性代数符号，但是与笛卡尔(x, y)坐标相反。查看<a href="#cc">Coordinate conventions笛卡尔惯例</a>获取更多细节。</p>
<p>除了某个单独节点，还可以访问或修改整个像素集合，使用的是不同的NumPy的索引功能。<br>切片：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Set to black the ten first lines</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera[:<span class="number">10</span>] = <span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>mask(那些使用boolean遮盖的索引)：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>mask = camera &lt; <span class="number">87</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Set to "white" (255) pixels where mask is True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera[mask] = <span class="number">255</span></div></pre></td></tr></table></figure></p>
<p>花式索引（使用索引集的索引）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>inds_r = np.arange(len(camera))</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>inds_c = <span class="number">4</span> * inds_r % len(camera)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera[inds_r, inds_c] = <span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>对于选择一组像素来进行进一步的操作来说，使用mask是尤其有用的。mask可以是任何相同形状的boolean数组。这能够被用来定义一个有趣的区域，就像一块磁盘：？？？这一段怎么理解<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>nrows, ncols = camera.shape</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>row, col = np.ogrid[:nrows, :ncols]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>cnt_row, cnt_col = nrows / <span class="number">2</span>, ncols / <span class="number">2</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>outer_disk_mask = ((row - cnt_row)**<span class="number">2</span> + (col - cnt_col)**<span class="number">2</span> &gt; (nrows / <span class="number">2</span>)**<span class="number">2</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera[outer_disk_mask] = <span class="number">0</span></div></pre></td></tr></table></figure></p>
<p><img src="http://oaavtz33a.bkt.clouddn.com/plot_camera_numpy_11.png" alt="plot_camera_numpy_11" title="plot camera numpy"><br>布尔类型的算数能够被用来定义更加复杂的masks：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>lower_half = row &gt; cnt_row</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>lower_half_disk = np.logical_and(lower_half, outer_disk_mask)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera = data.camera()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera[lower_half_disk] = <span class="number">0</span></div></pre></td></tr></table></figure></p>
<h1 id="Color-images"><a href="#Color-images" class="headerlink" title="Color images"></a>Color images</h1><p>以上所有都是色彩图像的真实情况：一张色彩图片就是一个NumPy数组，并且带有额外的通道信息：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>cat = data.chelsea()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>type(cat)</div><div class="line">&lt;type <span class="string">'numpy.ndarray'</span>&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>cat.shape</div><div class="line">(<span class="number">300</span>, <span class="number">451</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure></p>
<p>这展示了cat是一个300*451像素的图片，并且使用了三个通道(红、绿和蓝)。像之前一样，我们能够得到并设置像素值：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>cat[<span class="number">10</span>, <span class="number">20</span>]</div><div class="line">array([<span class="number">151</span>, <span class="number">129</span>, <span class="number">115</span>], dtype=uint8)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># set the pixel at row 50, column 60 to black</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>cat[<span class="number">50</span>, <span class="number">60</span>] = <span class="number">0</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># set the pixel at row 50, column 61 to green</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>cat[<span class="number">50</span>, <span class="number">61</span>] = [<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>] <span class="comment"># [red, green, blue]</span></div></pre></td></tr></table></figure></p>
<p><img src="http://oaavtz33a.bkt.clouddn.com/numpy_images-1.png" alt="numpy images 1" title="numpy images 1"></p>
<h1 id="Coordinate-conventiions-坐标惯例"><a href="#Coordinate-conventiions-坐标惯例" class="headerlink" title="Coordinate conventiions (坐标惯例)"></a>Coordinate conventiions (坐标惯例)</h1><p>因为我们使用NumPy数组来代表图片，因此我们的坐标必须响应的匹配。两维(2D)的灰度图片(类似上面的camera)是通过行和列来进行索引的（缩写为row, col或 r, c），使用最低的两个元素(0, 0)来表示左上角。在library的其他部分中，你将会看到rr和cc，用来引用行和列的坐标列表。我们将其与(x,y)区分开来，(x, y)通常标识标准的笛卡尔坐标，其中x是标准水平坐标，y是垂直坐标，原点为右下。<br>在彩色（或多个通道）图片的例子中，最后一个维度包含了色彩信息，并使用channel或ch来表示。<br>最后，对于3D图片，诸如视频、磁共振丞相扫描或共聚焦显微镜，我们</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文用来学习scikit-image的官方文档的&lt;a href=&quot;http://scikit-image.org/docs/stable/user_guide/numpy_images.html&quot; title=&quot;a crash course on NumPy for images&quot;&gt;a crash course on NumPy for images&lt;/a&gt;，&lt;a href=&quot;http://scikit-image.org/docs/stable/user_guide/numpy_images.html&quot; title=&quot;a crash course on NumPy for images&quot;&gt;原链接&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;A-crash-course-on-NumPy-for-images&quot;&gt;&lt;a href=&quot;#A-crash-course-on-NumPy-for-images&quot; class=&quot;headerlink&quot; title=&quot;A crash course on NumPy for images&quot;&gt;&lt;/a&gt;A crash course on NumPy for images&lt;/h1&gt;&lt;p&gt;scikit-image是以NumPy数组的方式来操作图像。因此图像很大一部分的操作将是使用NumPy：&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; skimage &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; data&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;camera = data.camera()&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;type(camera)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;lt;type &lt;span class=&quot;string&quot;&gt;&#39;numpy.ndarray&#39;&lt;/span&gt;&amp;gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;检索图像的几何以及像素数：&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;camera.shape&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;(&lt;span class=&quot;number&quot;&gt;512&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;512&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;camera.size&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;number&quot;&gt;262144&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;检索关于灰度值的统计信息：&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;camera.min(), camera.max()&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;255&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;camera.mean()&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;number&quot;&gt;118.31400299072266&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;代表图片的NumPy数组可以是浮点数类型的不同整数。查看&lt;a href=&quot;http://scikit-image.org/docs/stable/user_guide/data_types.html#data-types&quot; title=&quot;Image data type and what the mean&quot;&gt;Image data type and what the mean&lt;/a&gt;获取关于这些类型的更多信息，以及scikit-image如何处理它们。&lt;/p&gt;
    
    </summary>
    
      <category term="图像处理" scheme="http://baimoon.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>scikit image - getting started</title>
    <link href="http://baimoon.github.io/2016/12/16/scikitImage-gettingStarted/"/>
    <id>http://baimoon.github.io/2016/12/16/scikitImage-gettingStarted/</id>
    <published>2016-12-16T08:42:31.000Z</published>
    <updated>2016-12-19T04:42:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文用来学习scikit-image的官方文档的入门手册，<a href="http://scikit-image.org/docs/stable/user_guide/getting_started.html" title="Getting started" target="_blank" rel="external">原链接</a></p>
<h1 id="Getting-started"><a href="#Getting-started" class="headerlink" title="Getting started"></a>Getting started</h1><p>scikit-image是一个图像处理的Python包，它使用numpy数组来工作。这个包作为skimage被引入：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> skimage</div></pre></td></tr></table></figure></p>
<p>skimage的大多数函数将在子模块中找到：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> skimage <span class="keyword">import</span> data</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera = data.camera()</div></pre></td></tr></table></figure></p>
<p>一个包含子模块和函数的web页面可以在<a href="http://scikit-image.org/docs/stable/api/api.html" title="API Reference" target="_blank" rel="external">API reference</a>中找到。<br>在scikit-image中，图片相当于一个NumPy数组，例如，一个2-D的数组表示了一个灰度的2-D图片<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>type(camera)</div><div class="line">&lt;type <span class="string">'numpy.ndarray'</span>&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># An image with 512 rows and 512 columns</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.shape</div><div class="line">(<span class="number">512</span>, <span class="number">512</span>)</div></pre></td></tr></table></figure></p>
<p>skimage.data模块提供了一组返回示例图片的函数，这些图片可以用来快速学习scikit-image的函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>coins = data.coins()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> skimage <span class="keyword">import</span> filters</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>threshold_value = filters.threshold_otsu(coins)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>threshold_value</div><div class="line"><span class="number">107</span></div></pre></td></tr></table></figure></p>
<p>当然，还可以使用skimage.io.imread()从图片文件来加载自己的图片信息，加载后的图片也是作为一个NumPy数组：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> os</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>filename = os.path.join(skimage.data_dir, <span class="string">'moon.png'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> skimage <span class="keyword">import</span> io</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>moon = io.imread(filename)</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文用来学习scikit-image的官方文档的入门手册，&lt;a href=&quot;http://scikit-image.org/docs/stable/user_guide/getting_started.html&quot; title=&quot;Getting started&quot; target
    
    </summary>
    
      <category term="图像处理" scheme="http://baimoon.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>Log4j Configuration</title>
    <link href="http://baimoon.github.io/2016/11/10/log4j-configuration/"/>
    <id>http://baimoon.github.io/2016/11/10/log4j-configuration/</id>
    <published>2016-11-10T07:25:48.000Z</published>
    <updated>2016-12-29T03:32:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文用来学习关于Log4j的配置（通过配置文件的方式），<a href="http://logging.apache.org/log4j/2.x/manual/configuration.html" title="Configuration" target="_blank" rel="external">原文档连接</a></p>
<h1 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h1><p>将日志请求插入到application代码中需要相当多的计划和努力。观察表明，大约百分之四的代码用于记录日志。因此，即使是一般大小的application也将会有数以千计的日志片段嵌套在代码中。考虑到这个数量，如何不需要手动修改就能管理这些日志片段就显得十分重要。<br>配置Log4j 2版本，能够通过下面四种方法中的任意一种来完成：<br>1、通过一个以XML、JSON、YAML或properties格式的配置文件。<br>2、编程方式，通过创建一个ConfigurationFactory和Configuration实现。<br>3、编程方式，通过调用在Configuration接口中的APIs来添加组件到默认配置。<br>4、编程方式，通过调用内部Logger类的方法。<br>本文主要关注通过一个配置文件来配置Log4j。对于通过编程方式来配置Log4j，可以在<a href="http://logging.apache.org/log4j/2.x/manual/extending.html" title="Extending Log4j 2" target="_blank" rel="external">Extending Log4j 2</a>和<a href="http://logging.apache.org/log4j/2.x/manual/customconfig.html" title="Programmatic Log4j Configuration" target="_blank" rel="external">Programmatic Log4j Configuration</a>。<br>注意，不同于Log4j 1.x，Log4j 2的公共API没有公开关于添加、修改和移除appenders和filter的方法，或者以任何方式来操作配置。</p>
<a id="more"></a>
<h1 id="Automatic-Configuation"><a href="#Automatic-Configuation" class="headerlink" title="Automatic Configuation"></a>Automatic Configuation</h1><p>Log4j有在初始化期间进行自我配置的能力。当Log4j启动时，它将安装所有的ConfigurationFactory插件，并且按照权重从高到底对插件进行整理。正如之前所说的，Log4j包含四种ConfigurationFactory实现：JSON、YAML、properties和XML。<br>1、Log4j将会检查”log4j.configurationFile”系统属性，如果设置了该属性，将使用与文件扩展名匹配的ConfigurationFactory来尝试加载配置。<br>2、如果没有设置系统属性，properties ConfigurationFactory将在classpath中查找log4j2-test.properties文件。<br>3、如果没有发现properties的配置文件，将会使用YAML ConfigurationFactory在classpath中查找log4j2-test.yaml文件或log4j2-test.yml文件。<br>4、如果没有发现YAML配置文件，将会使用JSON ConfiguationFactory在classpath中查找log4j2-test.json文件或log4j2-test.jsn文件。<br>5、如果没有发现JSON配置文件，将会使用XML ConfigurationFactory在classpath中查找log4j2-test.xml文件。<br>6、如果没有发现任何测试的配置文件，那么properties ConfigurationFactory将会在classpath中查找log4j2.properties文件。<br>7、如果没有任何properties配置文件被装载，那么YAML ConfigurationFactory将会在classpath中查找log4j2.yaml文件或log4j2.yml文件。<br>8、如果没有任何YAML配置文件被装载，那么JSON ConfigurationFactory将在classpath中查找log4j2.json文件或log4j2.jsn文件。<br>9、如果没有任何JSON配置文件被装载，那么XML ConfigurationFactory将在classpath中查找log4j2.xml文件。<br>10、如果没有任何配置文件被装载，那么将使用DefaultConfiguration。这将使被记录的日志输出到控制台。</p>
<p>一个用例，例子将解释名为MyApp的application将如何使用log4j。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> com.foo.Bar;</div><div class="line"> </div><div class="line"><span class="comment">// Import log4j classes.</span></div><div class="line"><span class="keyword">import</span> org.apache.logging.log4j.Logger;</div><div class="line"><span class="keyword">import</span> org.apache.logging.log4j.LogManager;</div><div class="line"> </div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyApp</span> </span>&#123;</div><div class="line"> </div><div class="line">    <span class="comment">// Define a static logger variable so that it references the</span></div><div class="line">    <span class="comment">// Logger instance named "MyApp".</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = LogManager.getLogger(MyApp.class);</div><div class="line"> </div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">final</span> String... args)</span> </span>&#123;</div><div class="line"> </div><div class="line">        <span class="comment">// Set up a simple configuration that logs on the console.</span></div><div class="line"> </div><div class="line">        logger.trace(<span class="string">"Entering application."</span>);</div><div class="line">        Bar bar = <span class="keyword">new</span> Bar();</div><div class="line">        <span class="keyword">if</span> (!bar.doIt()) &#123;</div><div class="line">            logger.error(<span class="string">"Didn't do it."</span>);</div><div class="line">        &#125;</div><div class="line">        logger.trace(<span class="string">"Exiting application."</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>MyApp通过导入log4j相关的类开始。然后使用类的完全限定名定义了一个名为MyApp的静态logger变量。<br>MyApp使用在com.foo包中定义的Bar类。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> com.foo;</div><div class="line"><span class="keyword">import</span> org.apache.logging.log4j.Logger;</div><div class="line"><span class="keyword">import</span> org.apache.logging.log4j.LogManager;</div><div class="line"> </div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Bar</span> </span>&#123;</div><div class="line">  <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = LogManager.getLogger(Bar.class.getName());</div><div class="line"> </div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">doIt</span><span class="params">()</span> </span>&#123;</div><div class="line">    logger.entry();</div><div class="line">    logger.error(<span class="string">"Did it again!"</span>);</div><div class="line">    <span class="keyword">return</span> logger.exit(<span class="keyword">false</span>);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>如果没有装配到任何配置文件，Log4j将提供默认配置。默认配置，在类DefaultConfiguration中提供，将会启动：<br>1、一个附加到root logger的ConsoleAppender。<br>2、一个附加到ConsoleAppender的格式为”%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n”的PatternLayout。<br>注意，默认分配给Log4j的root logger的是 Level.ERROR。<br>MyApp的输出类似：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="number">17</span>:<span class="number">13</span>:<span class="number">01.540</span> [main] ERROR com.foo.Bar - Did it again!</div><div class="line"><span class="number">17</span>:<span class="number">13</span>:<span class="number">01.540</span> [main] ERROR MyApp - Didn<span class="string">'t do it.</span></div></pre></td></tr></table></figure></p>
<p>正如之前描述的，Log4j将首先尝试从配置文件配置它自己。与默认配置相同的配置看起来是这样的：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</div><div class="line">&lt;Configuration status="WARN"&gt;</div><div class="line">  &lt;Appenders&gt;</div><div class="line">    &lt;Console name="Console" target="SYSTEM_OUT"&gt;</div><div class="line">      &lt;PatternLayout pattern="%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n"/&gt;</div><div class="line">    &lt;/Console&gt;</div><div class="line">  &lt;/Appenders&gt;</div><div class="line">  &lt;Loggers&gt;</div><div class="line">    &lt;Root level="error"&gt;</div><div class="line">      &lt;AppenderRef ref="Console"/&gt;</div><div class="line">    &lt;/Root&gt;</div><div class="line">  &lt;/Loggers&gt;</div><div class="line">&lt;/Configuration&gt;</div></pre></td></tr></table></figure></p>
<p>一旦将上面的配置作为log4j2.xml放到classpath下，你将会得到与上面所列出的相同的结果。将root的级别修改为trace将得到类似的结果：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="number">17</span>:<span class="number">13</span>:<span class="number">01.540</span> [main] TRACE MyApp - Entering application.</div><div class="line"><span class="number">17</span>:<span class="number">13</span>:<span class="number">01.540</span> [main] TRACE com.foo.Bar - entry</div><div class="line"><span class="number">17</span>:<span class="number">13</span>:<span class="number">01.540</span> [main] ERROR com.foo.Bar - Did it again!</div><div class="line"><span class="number">17</span>:<span class="number">13</span>:<span class="number">01.540</span> [main] TRACE com.foo.Bar - <span class="function">exit <span class="title">with</span> <span class="params">(<span class="keyword">false</span>)</span></span></div><div class="line">17:13:01.540 [main] ERROR MyApp - Didn't do it.</div><div class="line">17:13:01.540 [main] TRACE MyApp - Exiting application.</div></pre></td></tr></table></figure></p>
<p>注意，在使用默认配置时，记录日志的状态将被禁用。</p>
<h1 id="Additivity"><a href="#Additivity" class="headerlink" title="Additivity"></a>Additivity</h1><p>获取希望消除来自除了com.foo.Bar之外的所有TRACE输出。简单的修改日志级别不能达到这个目的。而是需要在配置中增加一个新的logger：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&lt;Logger name="com.foo.Bar" level="TRACE"/&gt;</div><div class="line">&lt;Root level="ERROR"&gt;</div><div class="line">  &lt;AppenderRef ref="STDOUT"&gt;</div><div class="line">&lt;/Root&gt;</div></pre></td></tr></table></figure></p>
<p>使用这个配置，来自com.foo.Bar的所有日志事件将被记录，而来只有自其他组件的事件，只有error的会被记录。<br>在之前的例子中，所有来自com.foo.Bar的事件将继续被写到控制台。这是因为com.foo.Bar的logger还没有添加任何appender，而它的父级添加了。实际上，下面的配置<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</div><div class="line">&lt;Configuration status="WARN"&gt;</div><div class="line">  &lt;Appenders&gt;</div><div class="line">    &lt;Console name="Console" target="SYSTEM_OUT"&gt;</div><div class="line">      &lt;PatternLayout pattern="%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n"/&gt;</div><div class="line">    &lt;/Console&gt;</div><div class="line">  &lt;/Appenders&gt;</div><div class="line">  &lt;Loggers&gt;</div><div class="line">    &lt;Logger name="com.foo.Bar" level="trace"&gt;</div><div class="line">      &lt;AppenderRef ref="Console"/&gt;</div><div class="line">    &lt;/Logger&gt;</div><div class="line">    &lt;Root level="error"&gt;</div><div class="line">      &lt;AppenderRef ref="Console"/&gt;</div><div class="line">    &lt;/Root&gt;</div><div class="line">  &lt;/Loggers&gt;</div><div class="line">&lt;/Configuration&gt;</div></pre></td></tr></table></figure></p>
<p>结果如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="number">17</span>:<span class="number">13</span>:<span class="number">01.540</span> [main] TRACE com.foo.Bar - entry</div><div class="line"><span class="number">17</span>:<span class="number">13</span>:<span class="number">01.540</span> [main] TRACE com.foo.Bar - entry</div><div class="line"><span class="number">17</span>:<span class="number">13</span>:<span class="number">01.540</span> [main] ERROR com.foo.Bar - Did it again!</div><div class="line"><span class="number">17</span>:<span class="number">13</span>:<span class="number">01.540</span> [main] TRACE com.foo.Bar - exit (<span class="keyword">false</span>)</div><div class="line"><span class="number">17</span>:<span class="number">13</span>:<span class="number">01.540</span> [main] TRACE com.foo.Bar - exit (<span class="keyword">false</span>)</div><div class="line"><span class="number">17</span>:<span class="number">13</span>:<span class="number">01.540</span> [main] ERROR MyApp - Didn<span class="string">'t do it.</span></div></pre></td></tr></table></figure></p>
<p>注意，来自com.foo.Bar的message出现了两次。这是因为与名为com.foo.Bar的logger关联的appender首先被使用，它将第一个实例（记录）写到控制台。接着，com.foo.Bar的父级，在本例中是root logger，被使用。接着事件传递到它的appender，也将日志写到控制台，就是控制台中的第二个实例（记录）。虽然附加性是一个相当方便的特性（例如在前面的例子中，不需要配置配置appender），但是在很多的例子中，这个行为被认为是讨厌的，并且会通过在logger上设置附加性的属性为false对其进行禁用：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</div><div class="line">&lt;Configuration status="WARN"&gt;</div><div class="line">  &lt;Appenders&gt;</div><div class="line">    &lt;Console name="Console" target="SYSTEM_OUT"&gt;</div><div class="line">      &lt;PatternLayout pattern="%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n"/&gt;</div><div class="line">    &lt;/Console&gt;</div><div class="line">  &lt;/Appenders&gt;</div><div class="line">  &lt;Loggers&gt;</div><div class="line">    &lt;Logger name="com.foo.Bar" level="trace" additivity="false"&gt;</div><div class="line">      &lt;AppenderRef ref="Console"/&gt;</div><div class="line">    &lt;/Logger&gt;</div><div class="line">    &lt;Root level="error"&gt;</div><div class="line">      &lt;AppenderRef ref="Console"/&gt;</div><div class="line">    &lt;/Root&gt;</div><div class="line">  &lt;/Loggers&gt;</div><div class="line">&lt;/Configuration&gt;</div></pre></td></tr></table></figure></p>
<p>一旦一条事件到达附加性被设置为false的logger，将不会传递给它的父级logger，而不管父级logger的附加性设置为true还是false。</p>
<h1 id="Automatic-Reconfiguration"><a href="#Automatic-Reconfiguration" class="headerlink" title="Automatic Reconfiguration"></a>Automatic Reconfiguration</h1><p>当我根据一个文件来配置log4j，log4j能够自动的发现配置文件的变更并重新配置它自己。如果monitorInterval属性被指定并设置为非零值，那么配置文集将被检查，下一次日志事件被评估并且被记录，并且会因为上一次的检查而被推迟。这个例子展示了如何配置这个属性，因此那个配置文件将会在至少30秒后才被检查是否有更改。最小的间隔为5秒。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</div><div class="line">&lt;Configuration monitorInterval="30"&gt;</div><div class="line">...</div><div class="line">&lt;/Configuration&gt;</div></pre></td></tr></table></figure></p>
<h1 id="Chainsaw-can-automatically-process-your-log-files-Advertising-appender-configurations"><a href="#Chainsaw-can-automatically-process-your-log-files-Advertising-appender-configurations" class="headerlink" title="Chainsaw can automatically process your log files (Advertising appender configurations)"></a>Chainsaw can automatically process your log files (Advertising appender configurations)</h1><p>log4j提供了为基于文件的appender和基于socket的appender”公布”appender配置细节的能力。例如，对于基于文件的appender，文件的位置和文件内的模式布局都包含在公布的信息中。其他外部系统能够发现这些公布的信息并使用这些信息聪明的处理日志文件。<br>这个机制通过一个广告来实现，而广告的格式是通过每个Advertiser实现来指定的。一个外部系统想要使用特定的Advertiser的实现来工作，就必须理解如何装载已公布的配置以及公布信息的格式。例如，一个”database” Advertiser可能需要在一个数据库表格中存储配置信息。一个外部系统能够读取数据库表格，以便发现文件的位置和文件的格式。<br>Log4j提供了一个Advertiser实现，一个’多路广播DNS’Advertiser，它使用<a href="http://jmdns.sourceforge.net/" target="_blank" rel="external">http://jmdns.sourceforge.net</a>库，通过IP的多路广播来公布appender配置细节。<br>Chainsaw自动发现log4j的由多路广播DNS生成的公布信息并将这些发现的公布信息放到Chainsaw的Zeroconf tab中（如果jmdns库包含在Chainsaw的classpath中）。要开始格式化并tail一个在公布信息中的日志文件，只需要双击Chainsaw的Zeroconf tab中的公布项。当前，Chainsaw只支持FileAppender的公告信息。<br>要公布一个appender配置：<br>1、添加来自<a href="http://jmdns.sourceforge.net/" target="_blank" rel="external">http://jmdns.sourceforge.net</a>的JmDns库到application的classpath中。<br>2、设置配置项的’advertiser’属性为’multicastdns’。<br>3、在appender项的’advertiser’属性为’true’。<br>4、如果公布一个基于FileAppender的配置，设置appender项的’advertiserURI’为一个合适的URI。<br>基于FileAppender配置要求在appender上指定一个额外的’advertiseURI’属性。这个’advertiseURI’属性为Chainsaw提供如何访问对应文件的信息。例如，这个文件对于Chainsaw来说，可能通过指定一个通用的VFS(<a href="http://commons.apache.org/proper/commons-vfs/" target="_blank" rel="external">http://commons.apache.org/proper/commons-vfs/</a>)sftp://URI使用ssh/sftp可访问的，如果文件可以通过一个web服务来访问，可能需要使用<a href="http://URI，如果是从本地运行的Chainsaw实例来访问文件，可能需要指定file://URI。" target="_blank" rel="external">http://URI，如果是从本地运行的Chainsaw实例来访问文件，可能需要指定file://URI。</a><br>这里有一个例子，启用了公布功能的appender的配置，可以被本地运行的Chainsaw使用来自动的tail日志文件（注意file://advertiseURI）：<br><strong><em>请注意，你必须添加来自<a href="http://jmdns.sourceforge.net" target="_blank" rel="external">http://jmdns.sourceforge.net</a>的JmDns库到你application的classpath中，以便使用’nulticastdns’ advertiser来公布信息。</em></strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</div><div class="line">&lt;Configuration advertiser="multicastdns"&gt;</div><div class="line">...</div><div class="line">&lt;/Configuration&gt;</div><div class="line">&lt;Appenders&gt;</div><div class="line">  &lt;File name="File1" fileName="output.log" bufferedIO="false" advertiseURI="file://path/to/output.log" advertise="true"&gt;</div><div class="line">  ...</div><div class="line">  &lt;/File&gt;</div><div class="line">&lt;/Appenders&gt;</div></pre></td></tr></table></figure></p>
<h1 id="Configuration-Syntax"><a href="#Configuration-Syntax" class="headerlink" title="Configuration Syntax"></a>Configuration Syntax</h1><p>正如前面已经展示的以及后面将要展示的，Log4j允许你们轻松的定义日志记录的行为，而不需要修改你的application。可以为application的部分代码禁用日志记录，只在满足特定条件时才记录日志，例如特定用户执行的操作，转发输出到Flume或输出到日志报告系统等。<br>在XML文件中配置项有如下属性：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Attribute Name</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">advertiser</td>
<td style="text-align:left">(可选) Advertiser插件的名称，这个插件用来公布独特的FileAppender或SocketAppender配置。提供的唯一的Advertiser插件是’multicastdns’</td>
</tr>
<tr>
<td style="text-align:left">dest</td>
<td style="text-align:left">如果是”err”则输出标准错误，或者是一个文件路径或URL</td>
</tr>
<tr>
<td style="text-align:left">monitorInterval</td>
<td style="text-align:left">以秒为单位，时间的总数，在检查配置文件是否变更之前等待的时间</td>
</tr>
<tr>
<td style="text-align:left">name</td>
<td style="text-align:left">配置的名称</td>
</tr>
<tr>
<td style="text-align:left">packages</td>
<td style="text-align:left">用于搜索插件的以逗号分隔的包名称的列表。每个类加载器只加载插件一次，因此更改这个值可能对重新配置没有任何影响</td>
</tr>
<tr>
<td style="text-align:left">schema</td>
<td style="text-align:left">用来标识类加载器加载XML的模式，用来验证配置信息。只有当strict被设置为true时有效。如果没有设置，则验证不会发生。</td>
</tr>
<tr>
<td style="text-align:left">shutdownHook</td>
<td style="text-align:left">指定当JVM关闭时Log4j是否也自动关闭关闭。该功能是默认启用的，但是可以通过设置为”disable” 来禁用该功能。</td>
</tr>
<tr>
<td style="text-align:left">status</td>
<td style="text-align:left">应该被记录到控制台的内部Log4j事件的级别。该属性有效的值为”trace”、”debug”、”info”、”warn”、”error”和”fatal”。Log4j会将有关初始化、反转和其他内部操作的详细信息记录到状态logger。</td>
</tr>
<tr>
<td style="text-align:left">strict</td>
<td style="text-align:left">启用严格的XML格式。在JSON配置中不支持。</td>
</tr>
<tr>
<td style="text-align:left">verbose</td>
<td style="text-align:left">在加载插件的时候启用诊断信息。</td>
</tr>
</tbody>
</table>
<p>Log4j能够使用两种XML特色来进行配置：简洁的和严格的。简洁的格式是配置更加容易，因为元素名称匹配它们代表的组件，然而它不能使用XML模式进行验证。例如，ConsoleAppender通过宣告一个名为Console的XML元素，将它自己配置在它父级appender元素下。然而，元素和属性的名称大小写不敏感。另外，属性可以做一个XML属性被指定也可以作为一个没有属性但是有一个字符串值的XML元素来指定。因此：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;PatternLayout pattern=<span class="string">"%m%n"</span>/&gt;</div></pre></td></tr></table></figure></p>
<p>和<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&lt;PatternLayout&gt;</div><div class="line">  &lt;Pattern&gt;%m%n&lt;/Pattern&gt;</div><div class="line">&lt;/PatternLayout&gt;</div></pre></td></tr></table></figure></p>
<p>是相同的。<br>下面的文件相当于一个XML结构的配置，但是注意斜体的元素代表了简洁的元素名称。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;;</div><div class="line">&lt;Configuration&gt;</div><div class="line">  &lt;Properties&gt;</div><div class="line">    &lt;Property name="name1"&gt;value&lt;/property&gt;</div><div class="line">    &lt;Property name="name2" value="value2"/&gt;</div><div class="line">  &lt;/Properties&gt;</div><div class="line">  &lt;filter  ... /&gt;</div><div class="line">  &lt;Appenders&gt;</div><div class="line">    &lt;appender ... &gt;</div><div class="line">      &lt;filter  ... /&gt;</div><div class="line">    &lt;/appender&gt;</div><div class="line">    ...</div><div class="line">  &lt;/Appenders&gt;</div><div class="line">  &lt;Loggers&gt;</div><div class="line">    &lt;Logger name="name1"&gt;</div><div class="line">      &lt;filter  ... /&gt;</div><div class="line">    &lt;/Logger&gt;</div><div class="line">    ...</div><div class="line">    &lt;Root level="level"&gt;</div><div class="line">      &lt;AppenderRef ref="name"/&gt;</div><div class="line">    &lt;/Root&gt;</div><div class="line">  &lt;/Loggers&gt;</div><div class="line">&lt;/Configuration&gt;</div></pre></td></tr></table></figure></p>
<p>查看本页面中更过关于appender、filter和logger的声明的例子。</p>
<h2 id="Strict-XML"><a href="#Strict-XML" class="headerlink" title="Strict XML"></a>Strict XML</h2><p>除了上面简洁的XML格式，Log4j允许配置被指定为一个更加“通用”的XML方式，这种方式可以使用XML模式对配置进行验证。这是通过使用下面所示的对象类型来替换友好的元素名称来实现的。例如，使用一个带有”Console”的类型属性的appender元素来替换使用元素名的Console来配置ConsoleAppender。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;;</div><div class="line">&lt;Configuration&gt;</div><div class="line">  &lt;Properties&gt;</div><div class="line">    &lt;Property name="name1"&gt;value&lt;/property&gt;</div><div class="line">    &lt;Property name="name2" value="value2"/&gt;</div><div class="line">  &lt;/Properties&gt;</div><div class="line">  &lt;Filter type="type" ... /&gt;</div><div class="line">  &lt;Appenders&gt;</div><div class="line">    &lt;Appender type="type" name="name"&gt;</div><div class="line">      &lt;Filter type="type" ... /&gt;</div><div class="line">    &lt;/Appender&gt;</div><div class="line">    ...</div><div class="line">  &lt;/Appenders&gt;</div><div class="line">  &lt;Loggers&gt;</div><div class="line">    &lt;Logger name="name1"&gt;</div><div class="line">      &lt;Filter type="type" ... /&gt;</div><div class="line">    &lt;/Logger&gt;</div><div class="line">    ...</div><div class="line">    &lt;Root level="level"&gt;</div><div class="line">      &lt;AppenderRef ref="name"/&gt;</div><div class="line">    &lt;/Root&gt;</div><div class="line">  &lt;/Loggers&gt;</div><div class="line">&lt;/Configuration&gt;</div></pre></td></tr></table></figure></p>
<p>下面是使用了严格格式的配置示例：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</div><div class="line">&lt;Configuration status="debug" strict="true" name="XMLConfigTest"</div><div class="line">               packages="org.apache.logging.log4j.test"&gt;</div><div class="line">  &lt;Properties&gt;</div><div class="line">    &lt;Property name="filename"&gt;target/test.log&lt;/Property&gt;</div><div class="line">  &lt;/Properties&gt;</div><div class="line">  &lt;Filter type="ThresholdFilter" level="trace"/&gt;</div><div class="line"> </div><div class="line">  &lt;Appenders&gt;</div><div class="line">    &lt;Appender type="Console" name="STDOUT"&gt;</div><div class="line">      &lt;Layout type="PatternLayout" pattern="%m MDC%X%n"/&gt;</div><div class="line">      &lt;Filters&gt;</div><div class="line">        &lt;Filter type="MarkerFilter" marker="FLOW" onMatch="DENY" onMismatch="NEUTRAL"/&gt;</div><div class="line">        &lt;Filter type="MarkerFilter" marker="EXCEPTION" onMatch="DENY" onMismatch="ACCEPT"/&gt;</div><div class="line">      &lt;/Filters&gt;</div><div class="line">    &lt;/Appender&gt;</div><div class="line">    &lt;Appender type="Console" name="FLOW"&gt;</div><div class="line">      &lt;Layout type="PatternLayout" pattern="%C&#123;1&#125;.%M %m %ex%n"/&gt;&lt;!-- class and line number --&gt;</div><div class="line">      &lt;Filters&gt;</div><div class="line">        &lt;Filter type="MarkerFilter" marker="FLOW" onMatch="ACCEPT" onMismatch="NEUTRAL"/&gt;</div><div class="line">        &lt;Filter type="MarkerFilter" marker="EXCEPTION" onMatch="ACCEPT" onMismatch="DENY"/&gt;</div><div class="line">      &lt;/Filters&gt;</div><div class="line">    &lt;/Appender&gt;</div><div class="line">    &lt;Appender type="File" name="File" fileName="$&#123;filename&#125;"&gt;</div><div class="line">      &lt;Layout type="PatternLayout"&gt;</div><div class="line">        &lt;Pattern&gt;%d %p %C&#123;1.&#125; [%t] %m%n&lt;/Pattern&gt;</div><div class="line">      &lt;/Layout&gt;</div><div class="line">    &lt;/Appender&gt;</div><div class="line">    &lt;Appender type="List" name="List"&gt;</div><div class="line">    &lt;/Appender&gt;</div><div class="line">  &lt;/Appenders&gt;</div><div class="line"> </div><div class="line">  &lt;Loggers&gt;</div><div class="line">    &lt;Logger name="org.apache.logging.log4j.test1" level="debug" additivity="false"&gt;</div><div class="line">      &lt;Filter type="ThreadContextMapFilter"&gt;</div><div class="line">        &lt;KeyValuePair key="test" value="123"/&gt;</div><div class="line">      &lt;/Filter&gt;</div><div class="line">      &lt;AppenderRef ref="STDOUT"/&gt;</div><div class="line">    &lt;/Logger&gt;</div><div class="line"> </div><div class="line">    &lt;Logger name="org.apache.logging.log4j.test2" level="debug" additivity="false"&gt;</div><div class="line">      &lt;AppenderRef ref="File"/&gt;</div><div class="line">    &lt;/Logger&gt;</div><div class="line"> </div><div class="line">    &lt;Root level="trace"&gt;</div><div class="line">      &lt;AppenderRef ref="List"/&gt;</div><div class="line">    &lt;/Root&gt;</div><div class="line">  &lt;/Loggers&gt;</div><div class="line"> </div><div class="line">&lt;/Configuration&gt;</div></pre></td></tr></table></figure></p>
<h2 id="Configuration-with-JSON"><a href="#Configuration-with-JSON" class="headerlink" title="Configuration with JSON"></a>Configuration with JSON</h2><p>除了XML，Log4j还可以使用JSON进行配置。JSON格式与宽松的XML格式类似。每个关键字代表了插件的名称并且以key/value对作为它的属性。如果一个key包含多个简单的值，那么它将是一个底层插件。在下面的例子中，ThresholdFilter，Console和PatternLayout全都是插件，而Console插件为它的name属性将会被分配一个值为STDOUT的值，ThresholdFilter将被分配一个debug级别。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">&#123; <span class="string">"configuration"</span>: &#123; <span class="string">"status"</span>: <span class="string">"error"</span>, <span class="string">"name"</span>: <span class="string">"RoutingTest"</span>,</div><div class="line">                     <span class="string">"packages"</span>: <span class="string">"org.apache.logging.log4j.test"</span>,</div><div class="line">      <span class="string">"properties"</span>: &#123;</div><div class="line">        <span class="string">"property"</span>: &#123; <span class="string">"name"</span>: <span class="string">"filename"</span>,</div><div class="line">                      <span class="string">"value"</span> : <span class="string">"target/rolling1/rollingtest-$$&#123;sd:type&#125;.log"</span> &#125;</div><div class="line">      &#125;,</div><div class="line">    <span class="string">"ThresholdFilter"</span>: &#123; <span class="string">"level"</span>: <span class="string">"debug"</span> &#125;,</div><div class="line">    <span class="string">"appenders"</span>: &#123;</div><div class="line">      <span class="string">"Console"</span>: &#123; <span class="string">"name"</span>: <span class="string">"STDOUT"</span>,</div><div class="line">        <span class="string">"PatternLayout"</span>: &#123; <span class="string">"pattern"</span>: <span class="string">"%m%n"</span> &#125;</div><div class="line">      &#125;,</div><div class="line">      <span class="string">"List"</span>: &#123; <span class="string">"name"</span>: <span class="string">"List"</span>,</div><div class="line">        <span class="string">"ThresholdFilter"</span>: &#123; <span class="string">"level"</span>: <span class="string">"debug"</span> &#125;</div><div class="line">      &#125;,</div><div class="line">      <span class="string">"Routing"</span>: &#123; <span class="string">"name"</span>: <span class="string">"Routing"</span>,</div><div class="line">        <span class="string">"Routes"</span>: &#123; <span class="string">"pattern"</span>: <span class="string">"$$&#123;sd:type&#125;"</span>,</div><div class="line">          <span class="string">"Route"</span>: [</div><div class="line">            &#123;</div><div class="line">              <span class="string">"RollingFile"</span>: &#123;</div><div class="line">                <span class="string">"name"</span>: <span class="string">"Rolling-$&#123;sd:type&#125;"</span>, <span class="string">"fileName"</span>: <span class="string">"$&#123;filename&#125;"</span>,</div><div class="line">                <span class="string">"filePattern"</span>: <span class="string">"target/rolling1/test1-$&#123;sd:type&#125;.%i.log.gz"</span>,</div><div class="line">                <span class="string">"PatternLayout"</span>: &#123;<span class="string">"pattern"</span>: <span class="string">"%d %p %c&#123;1.&#125; [%t] %m%n"</span>&#125;,</div><div class="line">                <span class="string">"SizeBasedTriggeringPolicy"</span>: &#123; <span class="string">"size"</span>: <span class="string">"500"</span> &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;,</div><div class="line">            &#123; <span class="string">"AppenderRef"</span>: <span class="string">"STDOUT"</span>, <span class="string">"key"</span>: <span class="string">"Audit"</span>&#125;,</div><div class="line">            &#123; <span class="string">"AppenderRef"</span>: <span class="string">"List"</span>, <span class="string">"key"</span>: <span class="string">"Service"</span>&#125;</div><div class="line">          ]</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;,</div><div class="line">    <span class="string">"loggers"</span>: &#123;</div><div class="line">      <span class="string">"logger"</span>: &#123; <span class="string">"name"</span>: <span class="string">"EventLogger"</span>, <span class="string">"level"</span>: <span class="string">"info"</span>, <span class="string">"additivity"</span>: <span class="string">"false"</span>,</div><div class="line">                  <span class="string">"AppenderRef"</span>: &#123; <span class="string">"ref"</span>: <span class="string">"Routing"</span> &#125;&#125;,</div><div class="line">      <span class="string">"root"</span>: &#123; <span class="string">"level"</span>: <span class="string">"error"</span>, <span class="string">"AppenderRef"</span>: &#123; <span class="string">"ref"</span>: <span class="string">"STDOUT"</span> &#125;&#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>注意，在RoutingAppender中，Route元素作为一个数组被声明。这是有效的，因为每个数组元素将是一个Route组件。这种方式对于appender和filter是不好使的，因为其中的每个元素都有一个简洁格式的不同的名字。如果appender或filter声明一个名为type的属性，这个属性包含了appender的类型，那么appender和filter可以作为数组元素来定义。下面的例子说明了如何以数组的方式定义多个logger。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">&#123; "configuration": &#123; "status": "debug", "name": "RoutingTest",</div><div class="line">                      "packages": "org.apache.logging.log4j.test",</div><div class="line">      "properties": &#123;</div><div class="line">        "property": &#123; "name": "filename",</div><div class="line">                      "value" : "target/rolling1/rollingtest-$$&#123;sd:type&#125;.log" &#125;</div><div class="line">      &#125;,</div><div class="line">    "ThresholdFilter": &#123; "level": "debug" &#125;,</div><div class="line">    "appenders": &#123;</div><div class="line">      "appender": [</div><div class="line">         &#123; "type": "Console", "name": "STDOUT", "PatternLayout": &#123; "pattern": "%m%n" &#125;&#125;,</div><div class="line">         &#123; "type": "List", "name": "List", "ThresholdFilter": &#123; "level": "debug" &#125;&#125;,</div><div class="line">         &#123; "type": "Routing",  "name": "Routing",</div><div class="line">          "Routes": &#123; "pattern": "$$&#123;sd:type&#125;",</div><div class="line">            "Route": [</div><div class="line">              &#123;</div><div class="line">                "RollingFile": &#123;</div><div class="line">                  "name": "Rolling-$&#123;sd:type&#125;", "fileName": "$&#123;filename&#125;",</div><div class="line">                  "filePattern": "target/rolling1/test1-$&#123;sd:type&#125;.%i.log.gz",</div><div class="line">                  "PatternLayout": &#123;"pattern": "%d %p %c&#123;1.&#125; [%t] %m%n"&#125;,</div><div class="line">                  "SizeBasedTriggeringPolicy": &#123; "size": "500" &#125;</div><div class="line">                &#125;</div><div class="line">              &#125;,</div><div class="line">              &#123; "AppenderRef": "STDOUT", "key": "Audit"&#125;,</div><div class="line">              &#123; "AppenderRef": "List", "key": "Service"&#125;</div><div class="line">            ]</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      ]</div><div class="line">    &#125;,</div><div class="line">    "loggers": &#123;</div><div class="line">      "logger": [</div><div class="line">        &#123; "name": "EventLogger", "level": "info", "additivity": "false",</div><div class="line">          "AppenderRef": &#123; "ref": "Routing" &#125;&#125;,</div><div class="line">        &#123; "name": "com.foo.bar", "level": "error", "additivity": "false",</div><div class="line">          "AppenderRef": &#123; "ref": "Console" &#125;&#125;</div><div class="line">      ],</div><div class="line">      "root": &#123; "level": "error", "AppenderRef": &#123; "ref": "STDOUT" &#125;&#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>JSON支持使用Jackson Data Processor来格式化JSON文件。如果想要使用JSON作为配置，这些依赖必须要添加到项目中：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.fasterxml.jackson.core<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jackson-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line"> </div><div class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.fasterxml.jackson.core<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jackson-databind<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line"> </div><div class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.fasterxml.jackson.core<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jackson-annotations<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div></pre></td></tr></table></figure></p>
<h2 id="Configuring-loggers"><a href="#Configuring-loggers" class="headerlink" title="Configuring loggers"></a>Configuring loggers</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文用来学习关于Log4j的配置（通过配置文件的方式），&lt;a href=&quot;http://logging.apache.org/log4j/2.x/manual/configuration.html&quot; title=&quot;Configuration&quot;&gt;原文档连接&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Configuration&quot;&gt;&lt;a href=&quot;#Configuration&quot; class=&quot;headerlink&quot; title=&quot;Configuration&quot;&gt;&lt;/a&gt;Configuration&lt;/h1&gt;&lt;p&gt;将日志请求插入到application代码中需要相当多的计划和努力。观察表明，大约百分之四的代码用于记录日志。因此，即使是一般大小的application也将会有数以千计的日志片段嵌套在代码中。考虑到这个数量，如何不需要手动修改就能管理这些日志片段就显得十分重要。&lt;br&gt;配置Log4j 2版本，能够通过下面四种方法中的任意一种来完成：&lt;br&gt;1、通过一个以XML、JSON、YAML或properties格式的配置文件。&lt;br&gt;2、编程方式，通过创建一个ConfigurationFactory和Configuration实现。&lt;br&gt;3、编程方式，通过调用在Configuration接口中的APIs来添加组件到默认配置。&lt;br&gt;4、编程方式，通过调用内部Logger类的方法。&lt;br&gt;本文主要关注通过一个配置文件来配置Log4j。对于通过编程方式来配置Log4j，可以在&lt;a href=&quot;http://logging.apache.org/log4j/2.x/manual/extending.html&quot; title=&quot;Extending Log4j 2&quot;&gt;Extending Log4j 2&lt;/a&gt;和&lt;a href=&quot;http://logging.apache.org/log4j/2.x/manual/customconfig.html&quot; title=&quot;Programmatic Log4j Configuration&quot;&gt;Programmatic Log4j Configuration&lt;/a&gt;。&lt;br&gt;注意，不同于Log4j 1.x，Log4j 2的公共API没有公开关于添加、修改和移除appenders和filter的方法，或者以任何方式来操作配置。&lt;/p&gt;
    
    </summary>
    
      <category term="Logging" scheme="http://baimoon.github.io/categories/Logging/"/>
    
    
      <category term="Log4j 配置" scheme="http://baimoon.github.io/tags/Log4j-%E9%85%8D%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>Log4j Architecture</title>
    <link href="http://baimoon.github.io/2016/11/10/log4j/"/>
    <id>http://baimoon.github.io/2016/11/10/log4j/</id>
    <published>2016-11-10T07:25:48.000Z</published>
    <updated>2016-12-22T07:24:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要是针对Log4j的2.x版本的文档的，<a href="http://logging.apache.org/log4j/2.x/" title="Log4j" target="_blank" rel="external">链接</a></p>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><h2 id="Main-Components"><a href="#Main-Components" class="headerlink" title="Main Components"></a>Main Components</h2><p>Log4j使用的类下面图表中展示。<br><img src="http://oaavtz33a.bkt.clouddn.com/Log4jClasses.jpg" alt="Log4j Classes" title="class Log4j Classes"><br>使用Log4j的applications讲需要使用一个特定的名称向LogManager请求一个Logger。LogManager会定位到适当的LoggerContext，然后从LoggerContext中获取Logger。如果Logger必须被创建，它将与包含了如下内容的LoggerConfig进行关联：1）与Logger相同名称的LoggerConfig；b）父级package的名称的LoggerConfig；3）根级LoggerConfig。LoggerConfig对象根据配置中Logger的声明进行创建。LoggerConfig与日志事件的实际输出源联系在一起。</p>
<a id="more"></a>
<h3 id="Logger-Hierarchy-日志级别"><a href="#Logger-Hierarchy-日志级别" class="headerlink" title="Logger Hierarchy(日志级别)"></a>Logger Hierarchy(日志级别)</h3><p>首先，任何优于简单的System.out.println的日志记录API可以禁用某些日志语句，而其它的打印不受限制。这种能力假定日志空间，那就是，所有可能的记录日志的语句的空间，根据开发者的选择标准被分类。</p>
<p>在Log4 1.x版本中，日志级别通过Loggers之间的关系来维持。在Log4j 2版本中，这个关系不在存在。取而代之的是通过LoggerConfig对象之间的联系来维持。</p>
<p>Logger和LoggerConfig被整体命名。Logger是大小写敏感的，并且他们遵循分层的命名规则：</p>
<blockquote>
<h3 id="分层命名"><a href="#分层命名" class="headerlink" title="分层命名"></a>分层命名</h3><p>如果一个LoggerConfig的名称是后代logger名称的点前缀，那么这个LoggerConfig是另一个LoggerConfig的祖先。如果一个LoggerConfig和后代LoggerConfig之间没有祖先，那么就说这个LoggerConfig是子LoggerConfig的父级。</p>
</blockquote>
<p>例如，名为”com.foo”的LoggerConfig是名为”com.foo.Bar”的LoggerConfig的父级。同样，”java”是”java.util”的父级，并且是”java.util.Vector”的祖先。这个命名结构是大多数开发者所熟悉的。</p>
<p>根LoggerConfig位于LoggerConfig层次的最顶层。它是一个例外，因为它总是存在并且是每个层级的一部分。一个直接连接到根LoggerConfig的Logger可以如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Logger logger = LogManager.getLogger(LogManager.ROOT_LOGGER_NAME);</div></pre></td></tr></table></figure></p>
<p>或者更加简单：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Logger logger = LogManager.getRootLogger();</div></pre></td></tr></table></figure></p>
<p>所有其他的Loggers都能够使用LogManager.getLogger静态方法并传递想要获取的Logger的名称作为参数进行获取。Logging API的更多信息可以在<a href="#" title="Log4j 2 API">Log4j 2 API</a>中找到。</p>
<h3 id="LoggerContext"><a href="#LoggerContext" class="headerlink" title="LoggerContext"></a>LoggerContext</h3><p>LoggerContext为日志系统充当锚点。然而依赖不同的形式，在一个application中可能有多个活跃的LoggerContexts。LoggerContext的更多细节在<a href="http://logging.apache.org/log4j/2.x/manual/logsep.html" title="Logging Separation" target="_blank" rel="external">Log Separation</a>章中介绍。</p>
<h3 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h3><p>每个LoggerContext有一个活跃的配置。配置包含了所有输出源，context-wide过滤器、LoggerConfigs并包含引用。在重新配置两个配置对象期间将会继续存在。一旦所有的Logger被重新分配到新的配置，老的配置将被停止并消失。</p>
<h3 id="Logger"><a href="#Logger" class="headerlink" title="Logger"></a>Logger</h3><p>如前所述，Logger通过调用LogManager.getLogger被创建。Logger本身不直接执行任何action。它简单的有一个名称，并与一个LoggerConfig相关联。它继承AbstractLogger并实现了所要求的方法。因为配置被修改，Logger可能与一个不同的LoggerConfig相关联，从而导致它们的行为被修改。</p>
<h4 id="Retrieving-Loggers"><a href="#Retrieving-Loggers" class="headerlink" title="Retrieving Loggers"></a>Retrieving Loggers</h4><p>使用相同的名称调用LogManager.getLogger方法将总是返回相同Logger对象的引用。<br>例如，在<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Logger x = LogManager.getLogger(<span class="string">"wombat"</span>);</div><div class="line">Logger y = LogManager.getLogger(<span class="string">"wombat"</span>);</div></pre></td></tr></table></figure></p>
<p>x和y涉及的是相同的Logger对象。</p>
<p>log4j环境的配置通常是在application初始化的时候完成的。最好的方式是通过阅读一个配置文件。这将在Configuration章节中讨论。</p>
<p>log4j通过软件组件使得命名Loggers很容易。这可以通过在每个class中实例化一个Logger来完成，使用logger名称等同于class的完整的限定名。这是定义Logger的一个有用且简单的方法。由于日志的输出带有Logger的名称，这个名字的策略是易于识别一个日志message的来源。然而这只是一种可能，是用于带有名称logger的一种常见策略。Log4j不限制logger集合的可能性。开发者可以根据需要命名logger。</p>
<p>因为在类中，常常使用类的名称作为Logger的名称，因此提供了LogManager.getLogger()这个简单的方法来自动的使用类的完整的限定名来命名Logger。</p>
<h3 id="LoggerConfig"><a href="#LoggerConfig" class="headerlink" title="LoggerConfig"></a>LoggerConfig</h3><p>LoggerConfig对象在Loggers在日志配置中被声明时被创建。LoggerConfig包含了一组filter，LogEvents在传递到任何日志存储器之前会被传递到这些filter。它包含了日志存储器的一组引用，这些日志存储器被用来处理event。</p>
<h4 id="Log-Levels"><a href="#Log-Levels" class="headerlink" title="Log Levels"></a>Log Levels</h4><p>LoggerConfig会被分配一个日志级别。这组内建级别包含TRACE、DEBUG、WARN、ERROR和FATAL。Log4j 2还支持自定义日志级别。获取更多隔离尺寸的机制是使用<a href="&quot;&quot;">Markers</a>来代替。</p>
<p><a href="&quot;&quot;">Log4j 1.x</a>和<a href="&quot;&quot;">Logback</a>都有”级别继承”的概念。在Log4j 2中，Loggers和LoggerConfigs是两个不同的对象，因此这个概念的实现是不相同的。每个Logger引用适当的LoggerConfig，LoggerConfig能够引用到它的父级，因此实现了相同的效果。<br>下面五个表格，使用了不同的分配级别，最终的级别将于每个Logger相关联。注意，在所有的这些例子中，如果根LoggerConfig没有配置，将会分配一个默认的级别。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Logger Name</th>
<th style="text-align:left">Assigned LoggerConfig</th>
<th style="text-align:left">LoggerConfig Level</th>
<th style="text-align:left">Logger Level</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">root</td>
<td style="text-align:left">root</td>
<td style="text-align:left">DEBUG</td>
<td style="text-align:left">DEBUG</td>
</tr>
<tr>
<td style="text-align:left">X</td>
<td style="text-align:left">root</td>
<td style="text-align:left">DEBUG</td>
<td style="text-align:left">DEBUG</td>
</tr>
<tr>
<td style="text-align:left">X.Y</td>
<td style="text-align:left">root</td>
<td style="text-align:left">DEBUG</td>
<td style="text-align:left">DEBUG</td>
</tr>
<tr>
<td style="text-align:left">X.Y.Z</td>
<td style="text-align:left">root</td>
<td style="text-align:left">DEBUG</td>
<td style="text-align:left">DEBUG</td>
</tr>
</tbody>
</table>
<p>在上面的例子1中，只有根logger配置了，并且只有一个日志级别。所有其他引用根LoggerConfig的Logger将使用根LoggerConfig的级别。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Logger Name</th>
<th style="text-align:left">Assigned LoggerConfig</th>
<th style="text-align:left">LoggerConfig Level</th>
<th style="text-align:left">Level</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">root</td>
<td style="text-align:left">root</td>
<td style="text-align:left">DEBUG</td>
<td style="text-align:left">DEBUG</td>
</tr>
<tr>
<td style="text-align:left">X</td>
<td style="text-align:left">X</td>
<td style="text-align:left">ERROR</td>
<td style="text-align:left">ERROR</td>
</tr>
<tr>
<td style="text-align:left">X.Y</td>
<td style="text-align:left">X.Y</td>
<td style="text-align:left">INFO</td>
<td style="text-align:left">INFO</td>
</tr>
<tr>
<td style="text-align:left">X.Y.Z</td>
<td style="text-align:left">X.Y.Z</td>
<td style="text-align:left">WARN</td>
<td style="text-align:left">WARN</td>
</tr>
</tbody>
</table>
<p>在上面的例子中，所有的logger都配置了一个LoggerConfig，并且从配置的LoggerConfig中取得它们的级别。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Logger Name</th>
<th style="text-align:left">Assigned LoggerConfig</th>
<th style="text-align:left">LoggerConfig Level</th>
<th style="text-align:left">Level</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">root</td>
<td style="text-align:left">root</td>
<td style="text-align:left">DEBUG</td>
<td style="text-align:left">DEBUG</td>
</tr>
<tr>
<td style="text-align:left">X</td>
<td style="text-align:left">X</td>
<td style="text-align:left">DRROR</td>
<td style="text-align:left">ERROR</td>
</tr>
<tr>
<td style="text-align:left">X.Y</td>
<td style="text-align:left">X</td>
<td style="text-align:left">ERROR</td>
<td style="text-align:left">ERROR</td>
</tr>
<tr>
<td style="text-align:left">X.Y.Z</td>
<td style="text-align:left">X.Y.Z</td>
<td style="text-align:left">WARN</td>
<td style="text-align:left">WARN</td>
</tr>
</tbody>
</table>
<p>在上面的例子中，名为root、X和X.Y.Z的三个logger，每个都有一个与之相同名称的LoggerConfig。名为X.Y的Logger，没有配置与之匹配名称的LoggerConfig，因此使用名为X的Logger的LoggerConfig，因为会匹配与Logger的名称最长匹配的LoggerConfig，名称的匹配是从名字的开始进行匹配。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Logger Name</th>
<th style="text-align:left">Assigned LoggerConfig</th>
<th style="text-align:left">LoggerConfig Level</th>
<th style="text-align:left">level</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">root</td>
<td style="text-align:left">root</td>
<td style="text-align:left">DEBUG</td>
<td style="text-align:left">DEBUG</td>
</tr>
<tr>
<td style="text-align:left">X</td>
<td style="text-align:left">X</td>
<td style="text-align:left">ERROR</td>
<td style="text-align:left">ERROR</td>
</tr>
<tr>
<td style="text-align:left">X.Y</td>
<td style="text-align:left">X.Y</td>
<td style="text-align:left">ERROR</td>
<td style="text-align:left">ERROR</td>
</tr>
<tr>
<td style="text-align:left">X.Y.Z</td>
<td style="text-align:left">X.Y.Z</td>
<td style="text-align:left">ERROR</td>
<td style="text-align:left">ERROR</td>
</tr>
</tbody>
</table>
<p>在上面的例子中，名为root和X的logger，每个都配置了一个与之相同名称的LoggerConfig。名为X.Y和X.Y.Z的logger，没有有配置LoggerConfig，因此它们的级别是来自与之关联的LoggerConfig，名为X，因为X是与它们名字最长匹配的LoggerConfig。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Logger Name</th>
<th style="text-align:left">Assigned LoggerConfig</th>
<th style="text-align:left">LoggerConfig Level</th>
<th style="text-align:left">level</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">root</td>
<td style="text-align:left">root</td>
<td style="text-align:left">DEBUG</td>
<td style="text-align:left">DEBUG</td>
</tr>
<tr>
<td style="text-align:left">X</td>
<td style="text-align:left">X</td>
<td style="text-align:left">ERROR</td>
<td style="text-align:left">ERROR</td>
</tr>
<tr>
<td style="text-align:left">X.Y</td>
<td style="text-align:left">X.Y</td>
<td style="text-align:left">INFO</td>
<td style="text-align:left">INFO</td>
</tr>
<tr>
<td style="text-align:left">X.YZ</td>
<td style="text-align:left">X</td>
<td style="text-align:left">ERROR</td>
<td style="text-align:left">ERROR</td>
</tr>
</tbody>
</table>
<p>在上面的例子中，名为root、X和X.Y的logger，每个都有一个与之相同名称的LoggerConfig。名为X.YZ的logger没有配置LoggerConfig，因此它的级别来自阈值管理的LoggerConfig，名为X，因为X是与它匹配度最长的LoggerConfig。它没有和X.Y关联，因为每个点号后面必须完全匹配。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Logger Name</th>
<th style="text-align:left">Assigned LoggerConfig</th>
<th style="text-align:left">LoggerConfig Level</th>
<th style="text-align:left">Level</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">root</td>
<td style="text-align:left">root</td>
<td style="text-align:left">DEBUG</td>
<td style="text-align:left">DEBUG</td>
</tr>
<tr>
<td style="text-align:left">X</td>
<td style="text-align:left">X</td>
<td style="text-align:left">ERROR</td>
<td style="text-align:left">ERROR</td>
</tr>
<tr>
<td style="text-align:left">X.Y</td>
<td style="text-align:left">X.Y</td>
<td style="text-align:left"></td>
<td style="text-align:left">ERROR</td>
</tr>
<tr>
<td style="text-align:left">X.Y.Z</td>
<td style="text-align:left">X.Y</td>
<td style="text-align:left"></td>
<td style="text-align:left">ERROR</td>
</tr>
</tbody>
</table>
<p>在上面的例子中，名为X.Y的LoggerConfig没有配置级别，因此它的级别从名为X的LoggerConfig继承。名为X.Y.Z使用了名为X.Y的LoggerConfig，因为它没有与之精确匹配的LoggerConfig。它也是从名为X的LoggerConfig那里继承日志级别。<br>下面的表格说明了级别过滤是如何工作的。在这个表格中，列表头中，显示了LogEvent的级别，而行表头中，展示了相关LoggerConfig的关联级别。交叉表示LogEvent是否允许被传递到进一步的处理（Yes表示允许，No表示不允许）。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Event Level</th>
<th style="text-align:left">LoggerConfig Level</th>
<th style="text-align:left"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">TRACE</td>
<td style="text-align:left">DEBUG</td>
<td style="text-align:left">INFO</td>
<td style="text-align:left">WARN</td>
<td style="text-align:left">ERROR</td>
<td style="text-align:left">FATAL</td>
<td style="text-align:left">OFF</td>
</tr>
<tr>
<td style="text-align:left">ALL</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">NO</td>
</tr>
<tr>
<td style="text-align:left">TRACE</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
</tr>
<tr>
<td style="text-align:left">DEBUG</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
</tr>
<tr>
<td style="text-align:left">INFO</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
</tr>
<tr>
<td style="text-align:left">WARN</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
</tr>
<tr>
<td style="text-align:left">ERROR</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
</tr>
<tr>
<td style="text-align:left">FATAL</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">NO</td>
</tr>
<tr>
<td style="text-align:left">OFF</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
</tr>
</tbody>
</table>
<h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><p>除了前面章节中秒数的自动日志级别过滤，Log4j提供了Filter，这些Filter能够在控制被传递到任何LoggerConfig之前被应用、控制被传递到一个LoggerConfig之后但是调用任何Appender之前、控制被传递到一个LoggerConfig之后但是在调用一个指定Appender之前和每个Appender上。以类似防火墙过滤器的方式，每个过滤器返回三个结果中的一个：Accept、Deny和Neutral。Accept的答复意味着没有其他Filter会被调用，和事件被处理。Deny的答复意味着事件应该立即被忽略并且控制应该返回给调用者。Neutral的答复意味着事件应该被传递给其他Filter。如果这里没有其他Filter，事件应该被处理。</p>
<p>虽然一个event可能通过一个Filter被接收，但是这个event可能仍然不给记录。这发生在当这个event被一个前置LoggerConfig Filter接收但是接着被一个LoggerConfig filter忽略或被所有Appenders忽略。</p>
<h3 id="Appender"><a href="#Appender" class="headerlink" title="Appender"></a>Appender</h3><p>基于Logger，选择性的开启或禁用日志记录请求只是图像的一部分。Log4j允许日志记录请求打印到多个目的地。在Log4j中一个输出目的地成为一个Appender。当前支持的appender有，控制台、文件、远程socket服务、Apache Flume、JMS、远程 UNIX系统日志后台和各种数据库APIs。查看这一章的Appenders获取关于不同类型appender的详细信息。一个Logger可以附加多个Appender。</p>
<p>一个Appender可以通过在当前配置上调用<a href="http://logging.apache.org/log4j/2.x/log4j-core/apidocs/org/apache/logging/log4j/core/config/Configuration.html#addLoggerAppenderorg.apache.logging.log4j.core.Logger20org.apache.logging.log4j.core.Appender" title="addLoggerAppender" target="_blank" rel="external">addLoggerAppender</a>方法来添加到一个Logger上。如果一个LoggerConfig没有匹配到Logger（根据Logger的名称匹配），将会创建一个，Appender将被固定到创建的Logger上，然后所有的Logger将会被通知并更新它们的LoggerConfig引用。</p>
<p>对于给定的logger，每个可用的日志记录请求都将被转发给这个logger的LoggerConfig内的所有appender，以及这个LoggerConfig的父级的appender们。换句话说，appenders附加的从LoggerConfig继承中被继承。例如，如果一个控制台appender被添加到根logger，那么所有可用的日志记录请求将至少会输出到控制台。假设将一个文件appender添加到了一个名为C的appender，那么对于C和C的子类的可用的日志记录请求将会输出到一个文件和控制台。通过在配置文件中设置 additivity=”false”来声明Logger不需要额外的添加来重写Logger的默认行为。</p>
<p>控制appender的添加规则汇总如下：</p>
<blockquote>
<p>Appender Additivity<br>    名为L的Logger的一个日志片段的输出将去往与L相关的LoggerConfig中的所有Appender以及这个LoggerConfig上一代的所有Appender。这是appender additivity的含义。<br>    然而，如果与L相关联的LoggerConfig的一个上一代，假设是P，设置附加性标识为false，那么L的输出将直接到达L的LoggerConfig的appenders，以及它的上一代，包括P，但是不包含P的上一代中的任何appenders。<br>    Logger有它自己的附加性标识，默认设置为true。</p>
</blockquote>
<p>下面的表格展示了一个例子：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Logger Name</th>
<th style="text-align:left">Added Appenders</th>
<th style="text-align:left">Additivity Flag</th>
<th style="text-align:left">Output Targets</th>
<th style="text-align:left">Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">root</td>
<td style="text-align:left">A1</td>
<td style="text-align:left">not applicable</td>
<td style="text-align:left">A1</td>
<td style="text-align:left">根logger没有父级，因此附加性应用不到。</td>
</tr>
<tr>
<td style="text-align:left">x</td>
<td style="text-align:left">A-x1, A-x2</td>
<td style="text-align:left">true</td>
<td style="text-align:left">A1, A-x1, A-x2</td>
<td style="text-align:left">名为x的Logger和根Logger的Appdenders。</td>
</tr>
<tr>
<td style="text-align:left">x.y</td>
<td style="text-align:left">none</td>
<td style="text-align:left">true</td>
<td style="text-align:left">A1, A-x1, A-x2</td>
<td style="text-align:left">名为x的Logger和根Logger的Appdenders。通常不配置没有Appender的Logger。</td>
</tr>
<tr>
<td style="text-align:left">x.y.z</td>
<td style="text-align:left">A-xyz1</td>
<td style="text-align:left">true</td>
<td style="text-align:left">A1, A-x1, A-x2, A-xyz1</td>
<td style="text-align:left">名为x.y.z和名为x的Logger的Appender，以及根Logger的Appender。</td>
</tr>
<tr>
<td style="text-align:left">security</td>
<td style="text-align:left">A-sec</td>
<td style="text-align:left">false</td>
<td style="text-align:left">A-sec</td>
<td style="text-align:left">没有appender累加，因为附加标识设置为false</td>
</tr>
<tr>
<td style="text-align:left">security.access</td>
<td style="text-align:left">none</td>
<td style="text-align:left">true</td>
<td style="text-align:left">A-sec</td>
<td style="text-align:left">只有名为security的Logger的appender，因为 security的附加标识设置为false。</td>
</tr>
</tbody>
</table>
<h3 id="Layout"><a href="#Layout" class="headerlink" title="Layout"></a>Layout</h3><p>通常，用户希望能够定制的不只是输出的目的地，还包含输出的格式。这是通过把一个<a href="http://logging.apache.org/log4j/2.x/log4j-core/apidocs/org/apache/logging/log4j/core/Layout.html" title="Layout" target="_blank" rel="external">Layout</a>与一个Appender联系在一起来实现的。这个Layout负责根据用户的意愿格式化LogEvent，因此一个appender会非常注意的发送格式化的输出到它的目的地。<a href="http://logging.apache.org/log4j/2.x/log4j-core/apidocs/org/apache/logging/log4j/core/layout/PatternLayout.html" title="PatternLayout" target="_blank" rel="external">PatternLayout</a>，是Log4j标准分发的一部分，让用户指定输出格式来进行一致性的格式转换，类似于C语言的printf函数。</p>
<p>例如，这个使用了%r[%t] %-5p %c - %m %n模式的PatternLayout将会输出类似的一些东西：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="number">176</span> [main] INFO  org.foo.Bar - Located nearest gas station.</div></pre></td></tr></table></figure></p>
<p>第一个字段是毫秒的数字，表示从程序启动到现在过去的时长。第二个字段是产生日志请求的线程。第三个字段是日志的级别。第四个字段是与日志请求相关联的logger的名称。“-”之后的文本是日志的message。</p>
<p>Log4j为不同的用例带来了很多的<a href="http://logging.apache.org/log4j/2.x/manual/layouts.html" title="Layouts" target="_blank" rel="external">Layouts</a>，例如JSON、XML、HTML和Syslog（包括新的RFC 5424版本）.其他appenders（诸如数据库连接）放在了特定的字段中，而非一个特定的文本布局。</p>
<p>同样重要的是，log4j将根据用户指定的表中来呈现日志消息的内容。例如，如果在你当前的项目中需要使用一个对象类型0ranges，并需要常常对它进行记录，那么你可以创建一个OrangeMessage来接收Orange实例，并传递到Log4j，因此哪个Orange对象能在请求的时候被格式化为一个合适的字节数组。</p>
<h3 id="StrSubstitutor-and-StrLookup"><a href="#StrSubstitutor-and-StrLookup" class="headerlink" title="StrSubstitutor and StrLookup"></a>StrSubstitutor and StrLookup</h3><p>类<a href="http://logging.apache.org/log4j/2.x/log4j-core/apidocs/org/apache/logging/log4j/core/lookup/StrSubstitutor.html" title="StrSubstitutor" target="_blank" rel="external">StrSubstitutor</a>和接口<a href="http://logging.apache.org/log4j/2.x/log4j-core/apidocs/org/apache/logging/log4j/core/lookup/StrLookup.html" title="StrLookup" target="_blank" rel="external">StrLookup</a>是从<a href="https://commons.apache.org/proper/commons-lang/" title="Apache Commons Langs" target="_blank" rel="external">Apache Commons Langs</a>借过来的，然后被修改来支持日志事件。另外，<a href="http://logging.apache.org/log4j/2.x/log4j-core/apidocs/org/apache/logging/log4j/core/lookup/Interpolator.html" target="_blank" rel="external">Interpolator</a>从Apache Commons Configuration那里借鉴以允许StrSubstitutor来评估来自多个StrLookup的变量。它还被修改，以支持LogEvent的评估。这些提供的机制允许从System Properties、配置文件、 ThreadContext Map和LogEvent中结构化数据中引用变量进行配置。这些变量能够当配置被处理或每个事件被处理时来解决。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要是针对Log4j的2.x版本的文档的，&lt;a href=&quot;http://logging.apache.org/log4j/2.x/&quot; title=&quot;Log4j&quot;&gt;链接&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Architecture&quot;&gt;&lt;a href=&quot;#Architecture&quot; class=&quot;headerlink&quot; title=&quot;Architecture&quot;&gt;&lt;/a&gt;Architecture&lt;/h1&gt;&lt;h2 id=&quot;Main-Components&quot;&gt;&lt;a href=&quot;#Main-Components&quot; class=&quot;headerlink&quot; title=&quot;Main Components&quot;&gt;&lt;/a&gt;Main Components&lt;/h2&gt;&lt;p&gt;Log4j使用的类下面图表中展示。&lt;br&gt;&lt;img src=&quot;http://oaavtz33a.bkt.clouddn.com/Log4jClasses.jpg&quot; alt=&quot;Log4j Classes&quot; title=&quot;class Log4j Classes&quot;&gt;&lt;br&gt;使用Log4j的applications讲需要使用一个特定的名称向LogManager请求一个Logger。LogManager会定位到适当的LoggerContext，然后从LoggerContext中获取Logger。如果Logger必须被创建，它将与包含了如下内容的LoggerConfig进行关联：1）与Logger相同名称的LoggerConfig；b）父级package的名称的LoggerConfig；3）根级LoggerConfig。LoggerConfig对象根据配置中Logger的声明进行创建。LoggerConfig与日志事件的实际输出源联系在一起。&lt;/p&gt;
    
    </summary>
    
      <category term="Logging" scheme="http://baimoon.github.io/categories/Logging/"/>
    
    
      <category term="Log4j 结构" scheme="http://baimoon.github.io/tags/Log4j-%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Apache Kafka</title>
    <link href="http://baimoon.github.io/2016/09/28/kafka-document/"/>
    <id>http://baimoon.github.io/2016/09/28/kafka-document/</id>
    <published>2016-09-28T07:46:13.000Z</published>
    <updated>2016-12-22T07:25:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是Kafka 0.10.0文档的翻译，主要用于自学。</p>
<h1 id="1-Getting-Started"><a href="#1-Getting-Started" class="headerlink" title="1 Getting Started"></a>1 Getting Started</h1><h2 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h2><p>Kafka是一个分布式的、分区的、备份的提交日志服务。它提供了一个消息传输系统的功能，但是使用了一个独特的设计。<br>那意味着什么？<br>首先我们浏览一下基本的消息队列术语：</p>
<ul>
<li>Kafka以一种类型持续messages的提供称为topics。</li>
<li>我们称那些publish message到一个Kafka topic的进程为producers。</li>
<li>我们称那些subscribe到topics并处理被publish的message的进程为consumers。</li>
<li>kafka作为一个集群而运行，集群由一个或多个server组成，每个server成为一个broker。</li>
</ul>
<p>因此，整体来看，producers通过网络发送messages到Kafka集群，同样Kafka又为consumers服务，像这样：<br><img src="http://oaavtz33a.bkt.clouddn.com/producer_consumer.png" alt="producer_consumer" title="producer and consumer"><br>clients和servers之间的通信是通过一个简单的、高性能的、跨语言的TCP协议完成的。我们为Kafka提供了一个Java client，但是clients在很多语言中都可用。</p>
<h3 id="Topics-and-Logs"><a href="#Topics-and-Logs" class="headerlink" title="Topics and Logs"></a>Topics and Logs</h3><p>首先我们学习由Kafka提供的高级别的抽象 - topic。<br>一个topic是一种或一个提供的名称，用来publish message。对于每个topic，Kafka集群维持着一个分区日志，看起来像这样：<br><img src="http://oaavtz33a.bkt.clouddn.com/log_anatomy.png" alt="Anatomy of a Topic" title="Anatomy of a Topic"><br>每个partition是一个顺序的、不可变的连续添加的消息队列。partitions中的每个message分配一个序列id号，称为offset，用来唯一标识partition中的每条message。<br>Kafka集群保存所有publish过来的message-不管它们是否被消费，保存时长可配置。例如，如果日志保存设置为两天，那么一个message在publish后两天内是可以被消费的，但是两天之后，它将被删除以释放空间。kafka的性能对于不同数据大小是恒定有效的，因此很多的数据不是个问题。<br>实际上每个consumer仅有被保存的元数据是consumer在日志中的位置，称为offset。这个offset由consumer控制：通常一个consumer按照它读取的message，线性的推进它的offset，但是这个位置由consumer控制并且consumer能够以任意顺序消费message。例如一个consumer可以重置到一个原来的offset来重新处理。<br>这个特征的组合意味着Kafka consumer是非常廉价的 - 它们能够自由的来去，而不会影响集群或其他consumer。例如，你可以使用我们的命令行工具来tail任何topic的内容，而不需要任何已经存在的consumers改变它所消费的内容。<br>日志中的partitions有几个用途。首先，它们允许日志扩展到单个server所能容纳的日志大小之外。一个partition必须位于它所属的server上，但是一个topic可能有很多的partitions，因此它能够持有任意数量的数据。其次，它们的行为类似一个并行单元 - 汇聚更多于一点。</p>
<h3 id="Distribution"><a href="#Distribution" class="headerlink" title="Distribution"></a>Distribution</h3><p>日志的partitions在Kafka集群中跨server分布，每个server为一个共享的partition处理数据和请求。每个partition为了容灾，跨server保存多个备份，备份的数量可以配置。<br>每个partition有一个server扮演”leader”的角色，并有零个或多个servers扮演”followers”的角色。leader为partition处理所有的读和写的请求，而follower只是被动的复制leader。如果leader失效了，follower中的一个将自动成为新的leader。每个server为它自己一些的partitions扮演一个leader角色，为其他的partition扮演一个follower的角色，因此每个server在集群中的负载是很均衡的。</p>
<h3 id="Producers"><a href="#Producers" class="headerlink" title="Producers"></a>Producers</h3><p>Producers将数据publish到它们选择的topics中。Producer负责哪些message被分配到topic的哪些partition中。这可以通过简单的轮转来完成以平衡负载，或者可以一致性的partition定义函数来完成（例如基于message的某些key）。在分区上用的更多的是第二种。</p>
<h3 id="Consumers"><a href="#Consumers" class="headerlink" title="Consumers"></a>Consumers</h3><p>传统的消息传输有两种模式：queuing和publish-subscribe。在队列方式中，一个consumers池从一个server中读取，每条message只会到达某个consumer；在发布-订阅方式中，message广播给所有的consumers。Kafka提供了单个consumer抽象，它概括了上面两种方式 - consumer group。<br>consumers使用一个consumer群名称来标识它们自己，每个publish到一个topic的message被传递到每个订阅了topic的consumer组的一个consumer实例。consumer实例能够在单独的进程或单独的机器上。<br>如果所有的consumer实例拥有相同的consumer组，那么工作方式与一个传统的跨consumers负载均衡的队列类似。<br>如果所有的consumer实例都有不同的consumer组，那么工作方式与发布-订阅类似，所有的message会广播给所有的consumer。<br>更常见的，尽管我们发现那些topics有少数量的consumer组，然而每个都是一个逻辑订阅者。每个组由多个consumer实例组成，这样具有扩展性和容灾性。这也是publish-subscrib的定义，只不过subscriber是一个consumer群，而不是单个进程。<br>相对于传统消息传输系统，Kafka有更强的顺序保证。<br><img src="http://oaavtz33a.bkt.clouddn.com/consumer-groups.png" alt="consumer-groups" title="一个有两个server组成的Kafka集群有四个partitions(P0-P3)和两个consumer组。consumer组A有两个consumer实例，而组B有四个实例"><br>一个传统队列在server上按顺序保存messages，如果多个consumer从队列中消费数据，那么server以message存储的顺序拿出message。然而，虽然server按照顺序拿出message，但是message以异步方式投递给consumers，因此它们可能在不同的consumer上以不同的顺序到达。这意味着在并行消费的情况中消息的顺序丢失了。消息传输系统通过一个”exclusive consumer”的概念来解决这个问题，它只允许一个进程从队列中消费，但是这意味着没有并行处理。<br>Kafka做的更好一些。通过一个并行概念-partition-在topics中，Kafka能够在一个consumer进程池上同时提供顺序保证和负载均衡。这是通过将topic中的partitions分配给consumer组中的consumers来完成的，因此每个partition有组中确切的一个consumer来消费。通过这样，我们确保consumer值读取那一个partition，并以顺序消费数据。因为有很多partitions在很多consumer实例上是均衡负载的。注意，一个consumer组中的consumer实例不能多余partitions的数量。<br>Kafka只是在一个partition中提供了一个整体的顺序，而不是在一个topic的不同partition之间。对于大多applications，每个分区的排序联合根据key划分数据的能力是充分的。如果你要求在message上有整体的顺序，这可以通过使用一个topic只有一个partition来完成，这也意味着每个consuemr组只有一个consumer进程。</p>
<h3 id="Guarantees"><a href="#Guarantees" class="headerlink" title="Guarantees"></a>Guarantees</h3><p>在高层次上，Kafka给了如下的保证：</p>
<ul>
<li>由一个producer发送到一个特定topic partition的Messages将会以它们被发送的顺序添加。那就是，如果一个message M1与发送message M2的producer是一个，并且M1先被发送，那么M1将有一个比M2小的offset，并且要比M2更早的添加到日志中。</li>
<li>一个consumer看到messages的顺序是messages存储的顺序。</li>
<li>对于使用了复制因子为N的topic，在不丢失任何提交到log的messages丢失，我们允许最多N-1个server故障。</li>
</ul>
<p>这些保证的更多细节在文档的design章节中给出。</p>
<a id="more"></a>
<h2 id="1-2-Use-Cases"><a href="#1-2-Use-Cases" class="headerlink" title="1.2 Use Cases"></a>1.2 Use Cases</h2><p>这是一个对于一些Apache Kafka流行用例的一个描述。对于这些action的一些概述，请参考<a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying" title="What every software engineer should know about real-time data&#39;s unifying abstraction" target="_blank" rel="external">blog post</a></p>
<h3 id="Log-Aggregation"><a href="#Log-Aggregation" class="headerlink" title="Log Aggregation"></a>Log Aggregation</h3><p>很多人使用Kafka是将其作为日志聚合解决方案的替代品。日志聚合通常是收集日志文件并将日志文件日志到一个中心位置（可能是一个文件服务器或HDFS）用于处理。Kakfa从文件细节中抽象，并给出一个清晰的日志或事件数据的抽象来作为一个messages的流。这允许低延迟处理和对多个数据源及分布式数据消费的支持。相较于日志中心系统（如Scribe或Flume），Kafka提供了同样好的性能，并提供了更健壮的持久化保证用于复制，以及更低的端对端延迟。</p>
<h3 id="Stream-Processing"><a href="#Stream-Processing" class="headerlink" title="Stream Processing"></a>Stream Processing</h3><p>很多Kafka用户以处理由多个阶段组成的pipeline的方式处理数据，从Kafka topic中消费原生输入数据然后聚合、提取，或者转换到新的topic做进一步的消费或后续处理。例如，一个用于处理推荐新闻文章的pipeline可能会从RSS feeds中爬取文章内容并将其publish到一个”articles”topic中；进一步的处理可能是格式化或去重内容，并将干净的文章内容publish到一个新的topic；最后的处理阶段可能是尝试推送这些内容给用户。这样的处理流程，基于单独的topics创建实时数据流的的图表。从0.10.0.0开始，一个轻量级但是强大的流处理库（称为Kafka Streams）可以在Apache中使用，用来执行上面描述的数据处理。除了Kafka Streams，可供选择的其他开源的流处理工具包括<a href="https://storm.apache.org/" title="Storm" target="_blank" rel="external">Apache Storm</a>和<a href="http://samza.apache.org/" title="Samza" target="_blank" rel="external">Apache Samza</a>。</p>
<h3 id="Event-Sourcing"><a href="#Event-Sourcing" class="headerlink" title="Event Sourcing"></a>Event Sourcing</h3><p><a href="http://martinfowler.com/eaaDev/EventSourcing.html" title="Event sourcing" target="_blank" rel="external">Event sourcing</a>是一个apllication的设计方式，在其中，状态的变更作为一个时间顺序的记录序列被记录日志。 Kafka作为一个极好的后端，支持大量日志数据的存储，为application以这种风格惊醒构建。</p>
<h3 id="Commit-Log"><a href="#Commit-Log" class="headerlink" title="Commit Log"></a>Commit Log</h3><p>Kafka能够作为一种外部日志提交为一个分布式系统提供服务。日志用来帮助在节点之间复制数据，并且行为类似重新同步机制，用于在节点故障时重新存储它们的数据。日志压缩特性在Kafka中用来帮助这种用法。在这种用法中，Kafka与Apache BookKeeper项目类似。</p>
<h2 id="1-3-Quick-Start"><a href="#1-3-Quick-Start" class="headerlink" title="1.3 Quick Start"></a>1.3 Quick Start</h2><p>这个指南假设你刚刚开始，Kafka数据和ZooKeeper数据均为空。</p>
<h3 id="Step-1-Download-the-code"><a href="#Step-1-Download-the-code" class="headerlink" title="Step 1 : Download the code"></a>Step 1 : Download the code</h3><p><a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.0.0/kafka_2.11-0.10.0.0.tgz" title="下载Kafka 0.10.0.0版本" target="_blank" rel="external">下载</a>0.10.0.0发布版，并解压缩它。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; tar -xzf kafka_2.11-0.10.0.0.tgz</div><div class="line">&gt; <span class="built_in">cd</span> kafka_2.11-0.10.0.0</div></pre></td></tr></table></figure></p>
<h3 id="Step-2-Start-the-server"><a href="#Step-2-Start-the-server" class="headerlink" title="Step 2 : Start the server"></a>Step 2 : Start the server</h3><p>Kafka使用Zookeeper，因此如果你还没有一个ZooKeeper服务，你需要先启动一个ZooKeeper服务。你能够使用Kafka包内的方便脚本来得到一个quick-and-dirty的单节点ZooKeeper实例。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; bin/zookeeper-server-start.sh config/zookeeper.properties</div><div class="line">[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>现在，启动Kafka服务：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-server-start.sh config/server.properties</div><div class="line">[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)</div><div class="line">[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)</div><div class="line">...</div></pre></td></tr></table></figure></p>
<h3 id="Step-3-Create-a-topic"><a href="#Step-3-Create-a-topic" class="headerlink" title="Step 3 : Create a topic"></a>Step 3 : Create a topic</h3><p>创建一个名为”test”的topic，该topic只有一个partition，并且只有一个备份：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic <span class="built_in">test</span></div></pre></td></tr></table></figure></p>
<p>如果你运行列出topic的命令，你将看到上面的topic：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-topics.sh --list --zookeeper localhost:2181</div><div class="line"><span class="built_in">test</span></div></pre></td></tr></table></figure></p>
<h3 id="Step-4-Send-some-messages"><a href="#Step-4-Send-some-messages" class="headerlink" title="Step 4 : Send some messages"></a>Step 4 : Send some messages</h3><p>Kafka带有一个命令行客户端可以从一个文件或标准输入来获取输入，并将它作为messages发送给Kafka集群。默认每一行作为一个单独的message来发送。<br>运行producer并输入一些message到控制台来将它发送给server：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic <span class="built_in">test</span></div><div class="line">This is a message</div><div class="line">This is another message</div></pre></td></tr></table></figure></p>
<h3 id="Step-5-Start-a-consumer"><a href="#Step-5-Start-a-consumer" class="headerlink" title="Step 5 : Start a consumer"></a>Step 5 : Start a consumer</h3><p>Kafka还有一个命令行consumer，它将会消费message并将message转到标准输出。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic <span class="built_in">test</span> --from-beginning</div><div class="line">This is a message</div><div class="line">This is another message</div></pre></td></tr></table></figure></p>
<p>如果上面的每个命令运行在不同的terminal中，那么现在你能够在producer terminal中键入messages，然后能够看到这些message会出现在consumer terminal中。<br>所有这些命令行工具有额外的参数；以无参数方式运行这些命令将会以更加细节的方式列出命令的使用文档。</p>
<h3 id="Step-6-Setting-up-a-multi-broker-cluster"><a href="#Step-6-Setting-up-a-multi-broker-cluster" class="headerlink" title="Step 6 : Setting up a multi-broker cluster"></a>Step 6 : Setting up a multi-broker cluster</h3><p>到现在为止，我们已经针对单个broker进行运行，但是那并不好玩。对于Kafka，单个broker只一个size为1的cluster，因此除了启动稍微多一些borker实例，没有什么太多改变。只是为了感受，将我们的节点扩展到3个节点（仍然全部在本机上）。<br>首先我们为每个broker创建一个配置文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; cp config/server.properties config/server-1.properties</div><div class="line">&gt; cp config/server.properties config/server-2.properties</div></pre></td></tr></table></figure></p>
<p>现在，编辑这些新文件，并如下设置属性：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">config/server-1.properties:</div><div class="line">    broker.id=1</div><div class="line">    listeners=PLAINTEXT://:9093</div><div class="line">    log.dir=/tmp/kafka-logs-1</div><div class="line"></div><div class="line">config/server-2.properties:</div><div class="line">    broker.id=2</div><div class="line">    listeners=PLAINTEXT://:9094</div><div class="line">    log.dir=/tmp/kafka-logs-2</div></pre></td></tr></table></figure></p>
<p>这个属性<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">broker.id</div></pre></td></tr></table></figure></p>
<p>在集群中是每个节点唯一且永久的名称。我们只是重写了端口和日志目录，因为我们要在相同的机器上运行这些，为了避免它们都尝试注册相同的端口或重写了其他节点的数据。<br>我们已经有Zookeeper了，并且已经启动了一个节点，因此我们只需要启动两个新的的节点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-server-start.sh config/server-1.properties &amp;</div><div class="line">...</div><div class="line">&gt; bin/kafka-server-start.sh config/server-2.properties &amp;</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>现在，创建一个复制因子为3的新的topic：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic</div></pre></td></tr></table></figure></p>
<p>好了，现在我们已经有一个集群了，但是我们如何能够知道哪个borker在做什么呢？要想看到这些信息需要运行”describe topics”命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic</div><div class="line">Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:</div><div class="line">	Topic: my-replicated-topic	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0</div></pre></td></tr></table></figure></p>
<p>以下是一个输出说明。第一行，给出了所有partitions的一个汇总，每条额外的行给出一个partition的信息。因为这个topic只有一个partition，所以这里只有一行。</p>
<ul>
<li>“leader”是负责对给定的partition进行所有读和写的节点。partitions中的每个节点通过随机选取将可能成为leader。</li>
<li>“replicas”<br>+</li>
</ul>
<p>注意，在我的例子中，节点1是topic仅有partition的leader。<br>我们能够在原来topic上运行相同的命令，我们将看到：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic <span class="built_in">test</span></div><div class="line">Topic:<span class="built_in">test</span>	PartitionCount:1	ReplicationFactor:1	Configs:</div><div class="line">	Topic: <span class="built_in">test</span>	Partition: 0	Leader: 0	Replicas: 0	Isr: 0</div></pre></td></tr></table></figure></p>
<p>因此没有什么可惊讶的 - 原来的topic在server0上没有备份，server0是我们集群中唯一的server。<br>publish一些message到我们的新topic中：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic</div><div class="line">...</div><div class="line">my <span class="built_in">test</span> message 1</div><div class="line">my <span class="built_in">test</span> message 2</div><div class="line">^C</div></pre></td></tr></table></figure></p>
<p>我们消费这些message：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic</div><div class="line">...</div><div class="line">my <span class="built_in">test</span> message 1</div><div class="line">my <span class="built_in">test</span> message 2</div><div class="line">^C</div></pre></td></tr></table></figure></p>
<p>现在测试我们的容灾。borker 1扮演了leader的角色，因此我们kill掉它：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; ps | grep server-1.properties</div><div class="line">7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.8/Home/bin/java...</div><div class="line">&gt; <span class="built_in">kill</span> -9 7564</div></pre></td></tr></table></figure></p>
<p>leader关系已经切换到某一个slave，node1不再位于同步备份集合中：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic</div><div class="line">Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:</div><div class="line">	Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 1,2,0	Isr: 2,0</div></pre></td></tr></table></figure></p>
<p>但是messages对于消费依然是可用的，尽管原来写message的leader已经关闭：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic</div><div class="line">...</div><div class="line">my <span class="built_in">test</span> message 1</div><div class="line">my <span class="built_in">test</span> message 2</div><div class="line">^C</div></pre></td></tr></table></figure></p>
<h3 id="Step-7-Use-Kafka-Connect-to-import-export-data"><a href="#Step-7-Use-Kafka-Connect-to-import-export-data" class="headerlink" title="Step 7 : Use Kafka Connect to import/export data"></a>Step 7 : Use Kafka Connect to import/export data</h3><p>从控制台写入数据然后再将数据写出到控制台对于开始学习是很方便的，但是你很可能想要从其他数据源写入数据或从Kafka将数据到处到其他系统。对于很多系统，除了写自定义代码，你能够使用Kafka连接来导入和到处数据。Kafka Connect是包含在Kafka中的一个工具用来导入数据到Kafka或从Kafka中导出数据。它是一个可扩展的用来运行connectors的工具，它实现自定义逻辑来和一个外部系统进行交互。在快速开始中我们将看到如何使用一个简单的连接来运行Kafka，这个连接将从文件中导入数据到一个Kafka topic然后从这个topic中导出数据到一个文件中。首先，我们先创建用来测试的数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; <span class="built_in">echo</span> <span class="_">-e</span> <span class="string">"foo\nbar"</span> &gt; test.txt</div></pre></td></tr></table></figure></p>
<p>接下来，我们将以standalone模式启动了connectors，这意味着他们运行在单个、本地的进程中。我们提供了三个配置文件作为参数。第一个总是Kafka Connect进程的配置，包含常用的配置，诸如连接到的Kafka brokers、数据的序列化格式。剩下的配置文件每个指定了一个connector的创建。这些文件包含了一个唯一的connector名称，要实例化的connector类和任何被connector需要的配置。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties</div></pre></td></tr></table></figure></p>
<p>这些简单的配置文件包含在Kafka中，之前使用默认本地集群配置启动的Kafka Connect，会创建两个connectors：第一个是一个数据源connector，用来从给一个输入文件中读入行并将读到行数据产出到一个Kafka topic；第二个是一个sink connector，用来从Kafka topic读取messages并将每个message作为一行产出到一个输出文件中。在启动期间，你将会看到一定数量的日志信息，包括一些connectors已经被启动的提示。一旦Kafka Connect进程被启动，source connector将开始从下面文件中读取行数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test.txt</div></pre></td></tr></table></figure></p>
<p>并且产出这些行数据到下面topic中：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">connect-test</div></pre></td></tr></table></figure></p>
<p>，而且sink conector将会从下面topic中读取message：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">connect-test</div></pre></td></tr></table></figure></p>
<p>并将读到的message写到下面文件中：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test.sink.txt</div></pre></td></tr></table></figure></p>
<p>。通过检查输出文件的内容，我们能够检查由整个pipeline传递的数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; cat test.sink.txt</div><div class="line">foo</div><div class="line">bar</div></pre></td></tr></table></figure></p>
<p>注意，在Kafka topic中存储的数据<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">connect-test</div></pre></td></tr></table></figure></p>
<p>，因此我们还能运行一个控制台consumer来查看topic中的数据（或使用自定义consumer代码来处理它）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic connect-test --from-beginning</div><div class="line">&#123;<span class="string">"schema"</span>:&#123;<span class="string">"type"</span>:<span class="string">"string"</span>,<span class="string">"optional"</span>:<span class="literal">false</span>&#125;,<span class="string">"payload"</span>:<span class="string">"foo"</span>&#125;</div><div class="line">&#123;<span class="string">"schema"</span>:&#123;<span class="string">"type"</span>:<span class="string">"string"</span>,<span class="string">"optional"</span>:<span class="literal">false</span>&#125;,<span class="string">"payload"</span>:<span class="string">"bar"</span>&#125;</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>connectors继续处理数据，因此我们能够添加数据到文件，病看到它通过pipeline移动：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; <span class="built_in">echo</span> <span class="string">"Another line"</span> &gt;&gt; test.txt</div></pre></td></tr></table></figure></p>
<p>你应该能够看到这行数据出现在控制台consumer中和sink文件中。</p>
<h3 id="Step-8-Use-Kafka-Streams-to-process-data"><a href="#Step-8-Use-Kafka-Streams-to-process-data" class="headerlink" title="Step 8 : Use Kafka Streams to process data"></a>Step 8 : Use Kafka Streams to process data</h3><p>Kafka Streams是一个Kafka客户端库，用于实时的处理和分析存储在Kafka brokers中的数据。这个快速开始的例子将演示如何运行一个通过这个库编码的streaming application。这里是WordCountDemo例子代码的要点（）。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">KTable wordCounts = textLines</div><div class="line">    <span class="comment">// Split each text line, by whitespace, into words.</span></div><div class="line">    .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split(<span class="string">"\\W+"</span>)))</div><div class="line"></div><div class="line">    <span class="comment">// Ensure the words are available as record keys for the next aggregate operation.</span></div><div class="line">    .map((key, value) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(value, value))</div><div class="line"></div><div class="line">    <span class="comment">// Count the occurrences of each word (record key) and store the results into a table named "Counts".</span></div><div class="line">    .countByKey(<span class="string">"Counts"</span>)</div></pre></td></tr></table></figure></p>
<p>它实现WordCount算法，计算每个word在输入文本中出现的频率。然而，不像其他之前你见过的WordCount例子，那些例子的操作有限的数据上，WordCount实例application表现的略微不同因为它被设计操作一个无穷大的、无限的数据流上。与绑定变量类似，它是一个状态化算法，它跟踪并更新word的计数。然而，因为它必须假设未绑定的输入数据，在持续处理更多数据的同时它将周期性的输出它的当前状态和结果，因为它不知道何时能够处理完“所有”数据。<br>现在，我们将准备输入数据到一个Kafka topic，这些数据将随后被一个Kafka Streams application所处理。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; <span class="built_in">echo</span> <span class="_">-e</span> <span class="string">"all streams lead to kafka\nhello kafka streams\njoin kafka summit"</span> &gt; file-input.txt</div></pre></td></tr></table></figure></p>
<p>接下来，我们使用console producer将这些输入数据发送到一个名为streams-file-input的输入topic中（在实践中， 流数据像持续流入到Kafka中application启动并运行的地方）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-topics.sh --create \</div><div class="line">            --zookeeper localhost:2181 \</div><div class="line">            --replication-factor 1 \</div><div class="line">            --partitions 1 \</div><div class="line">            --topic streams-file-input</div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; cat file-input.txt | bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streams-file-input</div></pre></td></tr></table></figure>
<p>现在我们可以运行WorkCount application来处理输入数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo</div></pre></td></tr></table></figure></p>
<p>这里将不会有任何的标准输出，除了日志会作为结果持续的回写到Kafka中另一名为streams-wordcount-output的topic中。这个例子将运行一会儿并且不像通常的处理程序那样自动终止。<br>现在，我们通过application的输出topic中读取数据来检查WordCount application的输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 \</div><div class="line">            --topic streams-wordcount-output \</div><div class="line">            --from-beginning \</div><div class="line">            --formatter kafka.tools.DefaultMessageFormatter \</div><div class="line">            --property print.key=<span class="literal">true</span> \</div><div class="line">            --property print.value=<span class="literal">true</span> \</div><div class="line">            --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \</div><div class="line">            --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</div></pre></td></tr></table></figure></p>
<p>将会有如下的输出数据被打印到控制台：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">all     1</div><div class="line">streams 1</div><div class="line">lead    1</div><div class="line">to      1</div><div class="line">kafka   1</div><div class="line">hello   1</div><div class="line">kafka   2</div><div class="line">streams 2</div><div class="line">join    1</div><div class="line">kafka   3</div><div class="line">summit  1</div></pre></td></tr></table></figure></p>
<p>第一列是Kafka message key，第二列是message value，它们都使用java.lang.String格式。注意这里的输出实际上是一个持续的更新流，其中每条数据记录是单个word的更新后的数，记录的key诸如”Kafka”。对于使用相同key的多条记录，后面的记录更新前一条。<br>现在你能够写更多的输入message到streams-file-input这个topic中并且另外的messages添加到streams-wordcount-output topic中，显示被更新的word的数量（例如，像上面描述的使用console producer和console consumer）。<br>通过Ctrl-C来停止console consumer。</p>
<h2 id="1-4-Ecosystem"><a href="#1-4-Ecosystem" class="headerlink" title="1.4 Ecosystem"></a>1.4 Ecosystem</h2><p>有大量工具可以和Kafka进行集成。<a href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem" title="Ecosystem" target="_blank" rel="external">ecosystem page</a>列出了这些工具中的一些，包括流处理系统、Hadoop集成、监控和部署工具。</p>
<h2 id="Upgrading"><a href="#Upgrading" class="headerlink" title="Upgrading"></a>Upgrading</h2><p>升级暂时不考虑。</p>
<h1 id="2-API"><a href="#2-API" class="headerlink" title="2 API"></a>2 API</h1><p>Kafka包含了四个核心APIs：<br>1、 Producer API：允许applications发送数据流到Kafka集群中的topics。<br>2、 Consumer API：允许applications从Kafka集群中读取数据流。<br>3、 Streams API：允许将输入topics中的数据流转换到输出topics。<br>4、 Connect API：允许实现connectors，从一些数据源系统或application拉取数据到Kafka，或从Kafka中将数据推送到一些sink系统或applicaion。<br>Kafka在一个独立于一种语言的协议上公开了它的所有功能，在很多编程语言中都有client可用。然而只有Java客户端作为主要Kafka项目的一部分，其他的作为一个独立的开源项目可用。非Java clients的可用列表<a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients" title="Clients" target="_blank" rel="external">在这里</a>。</p>
<h2 id="2-1-Procucer-API"><a href="#2-1-Procucer-API" class="headerlink" title="2.1 Procucer API"></a>2.1 Procucer API</h2><p>Producer API允许applications发送数据流到Kafka集群中的topics。<br>在<a href="http://kafka.apache.org/0100/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html" title="KafkaProducer" target="_blank" rel="external">javadocs</a>中给出的producer如何使用的例子。<br>要使用producer，你可以使用下面的maven依赖：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;0.10.0.0&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>
<h2 id="2-2-Consumer-API"><a href="#2-2-Consumer-API" class="headerlink" title="2.2 Consumer API"></a>2.2 Consumer API</h2><p>Consumer API允许applications从Kafka集群的topics中读取数据流。<br>在<a href="http://kafka.apache.org/0100/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html" title="KafkaConsumer" target="_blank" rel="external">javadocs</a>中给出了consumer如何使用的例子。<br>要使用consumer，你需要使用下面的mava依赖：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">   &lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;0.10.0.0&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>
<h2 id="2-3-Streams-API"><a href="#2-3-Streams-API" class="headerlink" title="2.3 Streams API"></a>2.3 Streams API</h2><p>Streams API允许来自输入topics的数据流转换到输出topics。<br>如何使用这个库的例子在<a href="http://kafka.apache.org/0100/javadoc/index.html?org/apache/kafka/streams/KafkaStreams.html" title="KafkaStreams" target="_blank" rel="external">javadoc</a>中进行了展示。<br>使用Streams API的其他文档可以在<a href="http://kafka.apache.org/documentation.html#streams" title="KAFKA STREAMS" target="_blank" rel="external">这里</a>找到。<br>要是用Kafka Streams，你可以使用如下的maven依赖：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;0.10.0.0&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>
<h2 id="2-4-Connect-API"><a href="#2-4-Connect-API" class="headerlink" title="2.4 Connect API"></a>2.4 Connect API</h2><p>Connect API允许来实现connectors来持续从源数据系统拉取数据到Kafka或从Kafka拉取数据到一些sink数据系统。<br>很多Connect的用户不需要直接使用这个API，他们能够使用预先构建的connectors，而不需要写任何代码。使用Connect的额外信息在<a href="http://kafka.apache.org/documentation.html#connect" title="KAFKA CONNECT" target="_blank" rel="external">这里</a>是可用的。<br>想要实现自定义connectors的用户可以看<a href="http://kafka.apache.org/0100/javadoc/index.html?org/apache/kafka/connect" title="kafka 0.10.0.1 API" target="_blank" rel="external">这里</a>。</p>
<h2 id="2-5-Legacy-APIs"><a href="#2-5-Legacy-APIs" class="headerlink" title="2.5 Legacy APIs"></a>2.5 Legacy APIs</h2><p>更多遗留的producer和consumer api也包含在Kafka中。这些老的Scala API已经被废弃了，只是为了兼容性的目的而存在。它们的信息可以在这里找到。</p>
<h1 id="3-Configuration"><a href="#3-Configuration" class="headerlink" title="3 Configuration"></a>3 Configuration</h1><p>Kafka使用Key-Value对格式的属性文件进行配置。这些值能够通过文件或代码来提供。</p>
<h2 id="3-1-Broker-Configs"><a href="#3-1-Broker-Configs" class="headerlink" title="3.1 Broker Configs"></a>3.1 Broker Configs</h2><p>必不可少的配置如下：</p>
<ul>
<li>broker.id</li>
<li>log.dirs</li>
<li>zookeeper.connect<br>Topic级别配置和默认值将在下面更加详细的讨论。</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:left">Name</th>
<th style="text-align:left">DESCRIPTION</th>
<th style="text-align:left">TYPE</th>
<th style="text-align:left">DEFAULT</th>
<th style="text-align:left">VALID VALUES</th>
<th style="text-align:left">IMPORTANCE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">zookeeper.connect</td>
<td style="text-align:left">Zookeeper主机名的字符串</td>
<td style="text-align:left">string</td>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
<td style="text-align:left">high</td>
</tr>
<tr>
<td style="text-align:left">advertised.host.name</td>
<td style="text-align:left">废弃的：只有当’advertised.listeners’或’listeners’都没有设置时使用。使用’advertised.listeners’代替。</td>
<td style="text-align:left">string</td>
<td style="text-align:left">null</td>
<td style="text-align:left"></td>
<td style="text-align:left">high</td>
</tr>
</tbody>
</table>
<p>暂停翻译</p>
<h2 id="Producer-Configs"><a href="#Producer-Configs" class="headerlink" title="Producer Configs"></a>Producer Configs</h2><h2 id="Consumer-Configs"><a href="#Consumer-Configs" class="headerlink" title="Consumer Configs"></a>Consumer Configs</h2><h3 id="Old-Consumer-Configs"><a href="#Old-Consumer-Configs" class="headerlink" title="Old Consumer Configs"></a>Old Consumer Configs</h3><h3 id="New-Consumer-Configs"><a href="#New-Consumer-Configs" class="headerlink" title="New Consumer Configs"></a>New Consumer Configs</h3><h2 id="Kafka-Connect-Configs"><a href="#Kafka-Connect-Configs" class="headerlink" title="Kafka Connect Configs"></a>Kafka Connect Configs</h2><h2 id="Kafka-Streams-Configs"><a href="#Kafka-Streams-Configs" class="headerlink" title="Kafka Streams Configs"></a>Kafka Streams Configs</h2><h1 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><h2 id="Persistence"><a href="#Persistence" class="headerlink" title="Persistence"></a>Persistence</h2><h2 id="Efficiency"><a href="#Efficiency" class="headerlink" title="Efficiency"></a>Efficiency</h2><h2 id="The-Procucer"><a href="#The-Procucer" class="headerlink" title="The Procucer"></a>The Procucer</h2><h2 id="The-Consumer"><a href="#The-Consumer" class="headerlink" title="The Consumer"></a>The Consumer</h2><h2 id="Message-Delivery-Semantics"><a href="#Message-Delivery-Semantics" class="headerlink" title="Message Delivery Semantics"></a>Message Delivery Semantics</h2><h2 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h2><h2 id="Log-Compaction"><a href="#Log-Compaction" class="headerlink" title="Log Compaction"></a>Log Compaction</h2><h2 id="Quotas"><a href="#Quotas" class="headerlink" title="Quotas"></a>Quotas</h2><h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><h2 id="API-Design"><a href="#API-Design" class="headerlink" title="API Design"></a>API Design</h2><h2 id="Network-Layer"><a href="#Network-Layer" class="headerlink" title="Network Layer"></a>Network Layer</h2><h2 id="Messages"><a href="#Messages" class="headerlink" title="Messages"></a>Messages</h2><h2 id="Message-format"><a href="#Message-format" class="headerlink" title="Message format"></a>Message format</h2><h2 id="Log"><a href="#Log" class="headerlink" title="Log"></a>Log</h2><h2 id="Distribution-1"><a href="#Distribution-1" class="headerlink" title="Distribution"></a>Distribution</h2><h1 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h1><h2 id="Basic-Kafka-Operations"><a href="#Basic-Kafka-Operations" class="headerlink" title="Basic Kafka Operations"></a>Basic Kafka Operations</h2><h3 id="Adding-and-removing-topics"><a href="#Adding-and-removing-topics" class="headerlink" title="Adding and removing topics"></a>Adding and removing topics</h3><h3 id="Modifying-topics"><a href="#Modifying-topics" class="headerlink" title="Modifying topics"></a>Modifying topics</h3><h3 id="Graceful-shutdown"><a href="#Graceful-shutdown" class="headerlink" title="Graceful shutdown"></a>Graceful shutdown</h3><h3 id="Balancing-leadership"><a href="#Balancing-leadership" class="headerlink" title="Balancing leadership"></a>Balancing leadership</h3><h3 id="Checking-consumer-position"><a href="#Checking-consumer-position" class="headerlink" title="Checking consumer position"></a>Checking consumer position</h3><h3 id="Mirroring-data-between-clusters"><a href="#Mirroring-data-between-clusters" class="headerlink" title="Mirroring data between clusters"></a>Mirroring data between clusters</h3><h3 id="Expanding-your-cluster"><a href="#Expanding-your-cluster" class="headerlink" title="Expanding your cluster"></a>Expanding your cluster</h3><h3 id="Decommissioning-brokers"><a href="#Decommissioning-brokers" class="headerlink" title="Decommissioning brokers"></a>Decommissioning brokers</h3><h3 id="Decommissioning-brokers-1"><a href="#Decommissioning-brokers-1" class="headerlink" title="Decommissioning brokers"></a>Decommissioning brokers</h3><h3 id="Increasing-replication-factor"><a href="#Increasing-replication-factor" class="headerlink" title="Increasing replication factor"></a>Increasing replication factor</h3><h2 id="Datacenters"><a href="#Datacenters" class="headerlink" title="Datacenters"></a>Datacenters</h2><h2 id="Important-Configs"><a href="#Important-Configs" class="headerlink" title="Important Configs"></a>Important Configs</h2><h3 id="Important-Server-Configs"><a href="#Important-Server-Configs" class="headerlink" title="Important Server Configs"></a>Important Server Configs</h3><h3 id="Important-Client-Configs"><a href="#Important-Client-Configs" class="headerlink" title="Important Client Configs"></a>Important Client Configs</h3><h3 id="A-Production-Server-Configs"><a href="#A-Production-Server-Configs" class="headerlink" title="A Production Server Configs"></a>A Production Server Configs</h3><h2 id="Java-Version"><a href="#Java-Version" class="headerlink" title="Java Version"></a>Java Version</h2><h2 id="Hardware-and-OS"><a href="#Hardware-and-OS" class="headerlink" title="Hardware and OS"></a>Hardware and OS</h2><h3 id="OS"><a href="#OS" class="headerlink" title="OS"></a>OS</h3><h3 id="Disks-and-Filesystems"><a href="#Disks-and-Filesystems" class="headerlink" title="Disks and Filesystems"></a>Disks and Filesystems</h3><h3 id="Application-vs-OS-Flush-Management"><a href="#Application-vs-OS-Flush-Management" class="headerlink" title="Application vs OS Flush Management"></a>Application vs OS Flush Management</h3><h3 id="Linux-Flush-Behavior"><a href="#Linux-Flush-Behavior" class="headerlink" title="Linux Flush Behavior"></a>Linux Flush Behavior</h3><h3 id="Ext4-Notes"><a href="#Ext4-Notes" class="headerlink" title="Ext4 Notes"></a>Ext4 Notes</h3><h2 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h2><h2 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h2><h3 id="Stable-Version"><a href="#Stable-Version" class="headerlink" title="Stable Version"></a>Stable Version</h3><h3 id="Operationalization"><a href="#Operationalization" class="headerlink" title="Operationalization"></a>Operationalization</h3><h1 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h1><h2 id="Security-Overview"><a href="#Security-Overview" class="headerlink" title="Security Overview"></a>Security Overview</h2><h2 id="Encryption-and-Authentication-using-SSL"><a href="#Encryption-and-Authentication-using-SSL" class="headerlink" title="Encryption and Authentication using SSL"></a>Encryption and Authentication using SSL</h2><h2 id="Authentication-using-SASL"><a href="#Authentication-using-SASL" class="headerlink" title="Authentication using SASL"></a>Authentication using SASL</h2><h2 id="Authorization-and-ACLs"><a href="#Authorization-and-ACLs" class="headerlink" title="Authorization and ACLs"></a>Authorization and ACLs</h2><h2 id="Incorporating-Security-Features-in-a-Running-Cluster"><a href="#Incorporating-Security-Features-in-a-Running-Cluster" class="headerlink" title="Incorporating Security Features in a Running Cluster"></a>Incorporating Security Features in a Running Cluster</h2><h2 id="ZooKeeper-Authentication"><a href="#ZooKeeper-Authentication" class="headerlink" title="ZooKeeper Authentication"></a>ZooKeeper Authentication</h2><h3 id="New-Clusters"><a href="#New-Clusters" class="headerlink" title="New Clusters"></a>New Clusters</h3><h3 id="Migrating-Clusters"><a href="#Migrating-Clusters" class="headerlink" title="Migrating Clusters"></a>Migrating Clusters</h3><h3 id="Migrating-the-ZooKeeper-Ensemble"><a href="#Migrating-the-ZooKeeper-Ensemble" class="headerlink" title="Migrating the ZooKeeper Ensemble"></a>Migrating the ZooKeeper Ensemble</h3><h1 id="Kafka-Connect"><a href="#Kafka-Connect" class="headerlink" title="Kafka Connect"></a>Kafka Connect</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><h2 id="User-Guide"><a href="#User-Guide" class="headerlink" title="User Guide"></a>User Guide</h2><h2 id="Connector-Development-Guide"><a href="#Connector-Development-Guide" class="headerlink" title="Connector Development Guide"></a>Connector Development Guide</h2><h1 id="Kafka-Streams"><a href="#Kafka-Streams" class="headerlink" title="Kafka Streams"></a>Kafka Streams</h1><h2 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h2><h2 id="Developer-Guide"><a href="#Developer-Guide" class="headerlink" title="Developer Guide"></a>Developer Guide</h2><h3 id="Core-Concepts"><a href="#Core-Concepts" class="headerlink" title="Core Concepts"></a>Core Concepts</h3><h3 id="Low-Level-Processor-API"><a href="#Low-Level-Processor-API" class="headerlink" title="Low-Level Processor API"></a>Low-Level Processor API</h3><h3 id="High-Level-Streams-DSL"><a href="#High-Level-Streams-DSL" class="headerlink" title="High-Level Streams DSL"></a>High-Level Streams DSL</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是Kafka 0.10.0文档的翻译，主要用于自学。&lt;/p&gt;
&lt;h1 id=&quot;1-Getting-Started&quot;&gt;&lt;a href=&quot;#1-Getting-Started&quot; class=&quot;headerlink&quot; title=&quot;1 Getting Started&quot;&gt;&lt;/a&gt;1 Getting Started&lt;/h1&gt;&lt;h2 id=&quot;1-1-Introduction&quot;&gt;&lt;a href=&quot;#1-1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1.1 Introduction&quot;&gt;&lt;/a&gt;1.1 Introduction&lt;/h2&gt;&lt;p&gt;Kafka是一个分布式的、分区的、备份的提交日志服务。它提供了一个消息传输系统的功能，但是使用了一个独特的设计。&lt;br&gt;那意味着什么？&lt;br&gt;首先我们浏览一下基本的消息队列术语：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kafka以一种类型持续messages的提供称为topics。&lt;/li&gt;
&lt;li&gt;我们称那些publish message到一个Kafka topic的进程为producers。&lt;/li&gt;
&lt;li&gt;我们称那些subscribe到topics并处理被publish的message的进程为consumers。&lt;/li&gt;
&lt;li&gt;kafka作为一个集群而运行，集群由一个或多个server组成，每个server成为一个broker。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，整体来看，producers通过网络发送messages到Kafka集群，同样Kafka又为consumers服务，像这样：&lt;br&gt;&lt;img src=&quot;http://oaavtz33a.bkt.clouddn.com/producer_consumer.png&quot; alt=&quot;producer_consumer&quot; title=&quot;producer and consumer&quot;&gt;&lt;br&gt;clients和servers之间的通信是通过一个简单的、高性能的、跨语言的TCP协议完成的。我们为Kafka提供了一个Java client，但是clients在很多语言中都可用。&lt;/p&gt;
&lt;h3 id=&quot;Topics-and-Logs&quot;&gt;&lt;a href=&quot;#Topics-and-Logs&quot; class=&quot;headerlink&quot; title=&quot;Topics and Logs&quot;&gt;&lt;/a&gt;Topics and Logs&lt;/h3&gt;&lt;p&gt;首先我们学习由Kafka提供的高级别的抽象 - topic。&lt;br&gt;一个topic是一种或一个提供的名称，用来publish message。对于每个topic，Kafka集群维持着一个分区日志，看起来像这样：&lt;br&gt;&lt;img src=&quot;http://oaavtz33a.bkt.clouddn.com/log_anatomy.png&quot; alt=&quot;Anatomy of a Topic&quot; title=&quot;Anatomy of a Topic&quot;&gt;&lt;br&gt;每个partition是一个顺序的、不可变的连续添加的消息队列。partitions中的每个message分配一个序列id号，称为offset，用来唯一标识partition中的每条message。&lt;br&gt;Kafka集群保存所有publish过来的message-不管它们是否被消费，保存时长可配置。例如，如果日志保存设置为两天，那么一个message在publish后两天内是可以被消费的，但是两天之后，它将被删除以释放空间。kafka的性能对于不同数据大小是恒定有效的，因此很多的数据不是个问题。&lt;br&gt;实际上每个consumer仅有被保存的元数据是consumer在日志中的位置，称为offset。这个offset由consumer控制：通常一个consumer按照它读取的message，线性的推进它的offset，但是这个位置由consumer控制并且consumer能够以任意顺序消费message。例如一个consumer可以重置到一个原来的offset来重新处理。&lt;br&gt;这个特征的组合意味着Kafka consumer是非常廉价的 - 它们能够自由的来去，而不会影响集群或其他consumer。例如，你可以使用我们的命令行工具来tail任何topic的内容，而不需要任何已经存在的consumers改变它所消费的内容。&lt;br&gt;日志中的partitions有几个用途。首先，它们允许日志扩展到单个server所能容纳的日志大小之外。一个partition必须位于它所属的server上，但是一个topic可能有很多的partitions，因此它能够持有任意数量的数据。其次，它们的行为类似一个并行单元 - 汇聚更多于一点。&lt;/p&gt;
&lt;h3 id=&quot;Distribution&quot;&gt;&lt;a href=&quot;#Distribution&quot; class=&quot;headerlink&quot; title=&quot;Distribution&quot;&gt;&lt;/a&gt;Distribution&lt;/h3&gt;&lt;p&gt;日志的partitions在Kafka集群中跨server分布，每个server为一个共享的partition处理数据和请求。每个partition为了容灾，跨server保存多个备份，备份的数量可以配置。&lt;br&gt;每个partition有一个server扮演”leader”的角色，并有零个或多个servers扮演”followers”的角色。leader为partition处理所有的读和写的请求，而follower只是被动的复制leader。如果leader失效了，follower中的一个将自动成为新的leader。每个server为它自己一些的partitions扮演一个leader角色，为其他的partition扮演一个follower的角色，因此每个server在集群中的负载是很均衡的。&lt;/p&gt;
&lt;h3 id=&quot;Producers&quot;&gt;&lt;a href=&quot;#Producers&quot; class=&quot;headerlink&quot; title=&quot;Producers&quot;&gt;&lt;/a&gt;Producers&lt;/h3&gt;&lt;p&gt;Producers将数据publish到它们选择的topics中。Producer负责哪些message被分配到topic的哪些partition中。这可以通过简单的轮转来完成以平衡负载，或者可以一致性的partition定义函数来完成（例如基于message的某些key）。在分区上用的更多的是第二种。&lt;/p&gt;
&lt;h3 id=&quot;Consumers&quot;&gt;&lt;a href=&quot;#Consumers&quot; class=&quot;headerlink&quot; title=&quot;Consumers&quot;&gt;&lt;/a&gt;Consumers&lt;/h3&gt;&lt;p&gt;传统的消息传输有两种模式：queuing和publish-subscribe。在队列方式中，一个consumers池从一个server中读取，每条message只会到达某个consumer；在发布-订阅方式中，message广播给所有的consumers。Kafka提供了单个consumer抽象，它概括了上面两种方式 - consumer group。&lt;br&gt;consumers使用一个consumer群名称来标识它们自己，每个publish到一个topic的message被传递到每个订阅了topic的consumer组的一个consumer实例。consumer实例能够在单独的进程或单独的机器上。&lt;br&gt;如果所有的consumer实例拥有相同的consumer组，那么工作方式与一个传统的跨consumers负载均衡的队列类似。&lt;br&gt;如果所有的consumer实例都有不同的consumer组，那么工作方式与发布-订阅类似，所有的message会广播给所有的consumer。&lt;br&gt;更常见的，尽管我们发现那些topics有少数量的consumer组，然而每个都是一个逻辑订阅者。每个组由多个consumer实例组成，这样具有扩展性和容灾性。这也是publish-subscrib的定义，只不过subscriber是一个consumer群，而不是单个进程。&lt;br&gt;相对于传统消息传输系统，Kafka有更强的顺序保证。&lt;br&gt;&lt;img src=&quot;http://oaavtz33a.bkt.clouddn.com/consumer-groups.png&quot; alt=&quot;consumer-groups&quot; title=&quot;一个有两个server组成的Kafka集群有四个partitions(P0-P3)和两个consumer组。consumer组A有两个consumer实例，而组B有四个实例&quot;&gt;&lt;br&gt;一个传统队列在server上按顺序保存messages，如果多个consumer从队列中消费数据，那么server以message存储的顺序拿出message。然而，虽然server按照顺序拿出message，但是message以异步方式投递给consumers，因此它们可能在不同的consumer上以不同的顺序到达。这意味着在并行消费的情况中消息的顺序丢失了。消息传输系统通过一个”exclusive consumer”的概念来解决这个问题，它只允许一个进程从队列中消费，但是这意味着没有并行处理。&lt;br&gt;Kafka做的更好一些。通过一个并行概念-partition-在topics中，Kafka能够在一个consumer进程池上同时提供顺序保证和负载均衡。这是通过将topic中的partitions分配给consumer组中的consumers来完成的，因此每个partition有组中确切的一个consumer来消费。通过这样，我们确保consumer值读取那一个partition，并以顺序消费数据。因为有很多partitions在很多consumer实例上是均衡负载的。注意，一个consumer组中的consumer实例不能多余partitions的数量。&lt;br&gt;Kafka只是在一个partition中提供了一个整体的顺序，而不是在一个topic的不同partition之间。对于大多applications，每个分区的排序联合根据key划分数据的能力是充分的。如果你要求在message上有整体的顺序，这可以通过使用一个topic只有一个partition来完成，这也意味着每个consuemr组只有一个consumer进程。&lt;/p&gt;
&lt;h3 id=&quot;Guarantees&quot;&gt;&lt;a href=&quot;#Guarantees&quot; class=&quot;headerlink&quot; title=&quot;Guarantees&quot;&gt;&lt;/a&gt;Guarantees&lt;/h3&gt;&lt;p&gt;在高层次上，Kafka给了如下的保证：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由一个producer发送到一个特定topic partition的Messages将会以它们被发送的顺序添加。那就是，如果一个message M1与发送message M2的producer是一个，并且M1先被发送，那么M1将有一个比M2小的offset，并且要比M2更早的添加到日志中。&lt;/li&gt;
&lt;li&gt;一个consumer看到messages的顺序是messages存储的顺序。&lt;/li&gt;
&lt;li&gt;对于使用了复制因子为N的topic，在不丢失任何提交到log的messages丢失，我们允许最多N-1个server故障。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些保证的更多细节在文档的design章节中给出。&lt;/p&gt;
    
    </summary>
    
      <category term="Kafka" scheme="http://baimoon.github.io/categories/Kafka/"/>
    
    
      <category term="kafka document" scheme="http://baimoon.github.io/tags/kafka-document/"/>
    
  </entry>
  
  <entry>
    <title>Flume Install</title>
    <link href="http://baimoon.github.io/2016/09/21/flume-install/"/>
    <id>http://baimoon.github.io/2016/09/21/flume-install/</id>
    <published>2016-09-21T13:01:12.000Z</published>
    <updated>2016-09-24T08:04:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍自己在生产中使用flume的实际配置，以便以后查询。如果能够为他人提供参考，荣幸之至。<br>在Flume中分为三个部分source、channel和sink。source主要用于接收数据，sink用于写出数据，channel作为source和sink的连接、保存和转发使用。其中非常好用的是，channel可以使用Kafka，从而使得Flume具有了超强的存储能力，如果在加上可靠的source和sink，完全可以保证数据零丢失。<br>本文使用的例子中采用的是内存channel，这种channel的缺点是存储长度有限，重启数据丢失，有点就是速度快，低延迟。至于source，使用的是avro。最后是sink，因为我得目的是将数据写入到HDFS中，以后Hadoop集群或Spark集群进行计算，因此使用的hdfs类型的sink。</p>
<a id="more"></a>
<p>配置文件如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">apiAgent.sources = <span class="built_in">source</span>1 <span class="built_in">source</span>2</div><div class="line">apiAgent.channels = channel1 channel2</div><div class="line">apiAgent.sinks = sink1 sink2 sink3 sink4</div><div class="line"></div><div class="line">apiAgent.channels.channel1.type = memory</div><div class="line">apiAgent.channels.channel1.capacity = 1000000</div><div class="line">apiAgent.channels.channel1.transactionCapacity = 5000</div><div class="line"></div><div class="line">apiAgent.channels.channel2.type = memory</div><div class="line">apiAgent.channels.channel2.capacity = 1000000</div><div class="line">apiAgent.channels.channel2.transactionCapacity = 5000</div><div class="line"></div><div class="line">apiAgent.sources.source1.type = avro</div><div class="line">apiAgent.sources.source1.port = 50000</div><div class="line">apiAgent.sources.source1.channels = channel1</div><div class="line">apiAgent.sources.source1.bind = 0.0.0.0</div><div class="line"></div><div class="line">apiAgent.sources.source2.type = avro</div><div class="line">apiAgent.sources.source2.port = 50001</div><div class="line">apiAgent.sources.source2.channels = channel2</div><div class="line">apiAgent.sources.source2.bind = 0.0.0.0</div><div class="line"></div><div class="line">apiAgent.sinks.sink1.type = hdfs</div><div class="line">apiAgent.sinks.sink1.channel = channel1</div><div class="line">apiAgent.sinks.sink1.hdfs.path = hdfs://user/flume/events/api/%Y/%m/%d/%H/</div><div class="line">apiAgent.sinks.sink1.hdfs.filePrefix = api_sink1_server1</div><div class="line">apiAgent.sinks.sink1.hdfs.rollInterval = 3600</div><div class="line">apiAgent.sinks.sink1.hdfs.rollSize = 0</div><div class="line">apiAgent.sinks.sink1.hdfs.rollCount = 0</div><div class="line">apiAgent.sinks.sink1.hdfs.batchSize = 1000</div><div class="line">apiAgent.sinks.sink1.hdfs.fileType = DataStream</div><div class="line">apiAgent.sinks.sink1.hdfs.writeFormat = Text</div><div class="line"></div><div class="line">apiAgent.sinks.sink2.type = hdfs</div><div class="line">apiAgent.sinks.sink2.channel = channel1</div><div class="line">apiAgent.sinks.sink2.hdfs.path = hdfs:/user/flume/events/api/%Y/%m/%d/%H/</div><div class="line">apiAgent.sinks.sink2.hdfs.filePrefix = api_sink2_server1</div><div class="line">apiAgent.sinks.sink2.hdfs.rollInterval = 3600</div><div class="line">apiAgent.sinks.sink2.hdfs.rollSize = 0</div><div class="line">apiAgent.sinks.sink2.hdfs.rollCount = 0</div><div class="line">apiAgent.sinks.sink2.hdfs.batchSize = 1000</div><div class="line">apiAgent.sinks.sink2.hdfs.fileType = DataStream</div><div class="line">apiAgent.sinks.sink2.hdfs.writeFormat = Text</div><div class="line"></div><div class="line">apiAgent.sinks.sink3.type = hdfs</div><div class="line">apiAgent.sinks.sink3.channel = channel2</div><div class="line">apiAgent.sinks.sink3.hdfs.path = hdfs://user/flume/events/api/%Y/%m/%d/%H/</div><div class="line">apiAgent.sinks.sink3.hdfs.filePrefix = api_sink3_server1</div><div class="line">apiAgent.sinks.sink3.hdfs.rollInterval = 3600</div><div class="line">apiAgent.sinks.sink3.hdfs.rollSize = 0</div><div class="line">apiAgent.sinks.sink3.hdfs.rollCount = 0</div><div class="line">apiAgent.sinks.sink3.hdfs.batchSize = 1000</div><div class="line">apiAgent.sinks.sink3.hdfs.fileType = DataStream</div><div class="line">apiAgent.sinks.sink3.hdfs.writeFormat = Text</div><div class="line"></div><div class="line">apiAgent.sinks.sink4.type = hdfs</div><div class="line">apiAgent.sinks.sink4.channel = channel2</div><div class="line">apiAgent.sinks.sink4.hdfs.path = hdfs://user/flume/events/api/%Y/%m/%d/%H/</div><div class="line">apiAgent.sinks.sink4.hdfs.filePrefix = api_sink4_server1</div><div class="line">apiAgent.sinks.sink4.hdfs.rollInterval = 3600</div><div class="line">apiAgent.sinks.sink4.hdfs.rollSize = 0</div><div class="line">apiAgent.sinks.sink4.hdfs.rollCount = 0</div><div class="line">apiAgent.sinks.sink4.hdfs.batchSize = 1000</div><div class="line">apiAgent.sinks.sink4.hdfs.fileType = DataStream</div><div class="line">apiAgent.sinks.sink4.hdfs.writeFormat = Text</div></pre></td></tr></table></figure></p>
<p>补充说明<br>Flume的集群的节点个数为4，也就是说有4个进程，每台机器上一个。通过source和channel的关系可以看出，有两个source进行接收数据，每个source连接两个channel，每个channel一个sink。filePrefix配置为这样，是怕生成文件的时候有冲突，而且这样定义也方便查找问题，根据文件名可以知道对应的Flume实例。数据的存储是按小时区分的，方便计算数据的时候使用。<br>因为使用的hdfs类型的sink，因此flume的环境中还要包含Hadoop的环境配置，这样sink才能写输入到HDFS中。<br>配置的详细解释，请参考Flume配置相关的信息。</p>
<p>写入source使用的依赖如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">   &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;</div><div class="line">   &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt;</div><div class="line">   &lt;version&gt;1.5.2&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>
<p>代码参考如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">Properties props = <span class="keyword">new</span> Properties();</div><div class="line">props.put(<span class="string">"client.type"</span>, <span class="string">"default_loadbalance"</span>);</div><div class="line">props.put(<span class="string">"hosts"</span>, <span class="string">"h1 h2 h3 h4"</span>);</div><div class="line"></div><div class="line">props.put(<span class="string">"hosts.h1"</span>, <span class="string">"flume-001.yz:11000"</span>);</div><div class="line">props.put(<span class="string">"hosts.h2"</span>, <span class="string">"flume-002.yz:11000"</span>);</div><div class="line">props.put(<span class="string">"hosts.h3"</span>, <span class="string">"flume-003.yz:11000"</span>);</div><div class="line">props.put(<span class="string">"hosts.h4"</span>, <span class="string">"flume-004.yz:11000"</span>);</div><div class="line"></div><div class="line">props.put(<span class="string">"host-selector"</span>, <span class="string">"round_robin"</span>); <span class="comment">//Flume集群中节点的选取规则</span></div><div class="line">props.put(<span class="string">"backoff"</span>, <span class="string">"true"</span>);              <span class="comment">//Flume某个节点故障时，是否冷却（暂时不放到选取的集合中）</span></div><div class="line">props.put(<span class="string">"maxBackoff"</span>, <span class="string">"10000"</span>);          <span class="comment">//FLume某个节点故障时，最大冷却时间，冷却时间是随着次数集合增长的，但是最长这个时间</span></div><div class="line"></div><div class="line">flumeClient = RpcClientFactory.getInstance(props);</div><div class="line"></div><div class="line"></div><div class="line">flumeClient.appendBatch((List&lt;Event&gt;)obj);</div><div class="line">flumeClient.close()</div></pre></td></tr></table></figure></p>
<p>备注：这种写入Flume的方式是非可靠的，因为flumeClient.appendBatch()方法没有返回任何参数，无法确保零数据丢失。但是如果Flume节点挂了，会抛出异常，因此可以通过捕获异常，对数据进行必要的处理以免无谓的丢失。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍自己在生产中使用flume的实际配置，以便以后查询。如果能够为他人提供参考，荣幸之至。&lt;br&gt;在Flume中分为三个部分source、channel和sink。source主要用于接收数据，sink用于写出数据，channel作为source和sink的连接、保存和转发使用。其中非常好用的是，channel可以使用Kafka，从而使得Flume具有了超强的存储能力，如果在加上可靠的source和sink，完全可以保证数据零丢失。&lt;br&gt;本文使用的例子中采用的是内存channel，这种channel的缺点是存储长度有限，重启数据丢失，有点就是速度快，低延迟。至于source，使用的是avro。最后是sink，因为我得目的是将数据写入到HDFS中，以后Hadoop集群或Spark集群进行计算，因此使用的hdfs类型的sink。&lt;/p&gt;
    
    </summary>
    
      <category term="Flume" scheme="http://baimoon.github.io/categories/Flume/"/>
    
    
      <category term="install" scheme="http://baimoon.github.io/tags/install/"/>
    
  </entry>
  
  <entry>
    <title>Ganglia Install And Config</title>
    <link href="http://baimoon.github.io/2016/09/18/ganglia-installAndConfig/"/>
    <id>http://baimoon.github.io/2016/09/18/ganglia-installAndConfig/</id>
    <published>2016-09-18T09:30:07.000Z</published>
    <updated>2016-09-24T08:05:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是ganglia的安装和配置的笔记</p>
<h1 id="Ganglia的安装"><a href="#Ganglia的安装" class="headerlink" title="Ganglia的安装"></a>Ganglia的安装</h1><p>首先，ganglia由gmond、gmetad和gweb三部分组成。</p>
<h2 id="gmond"><a href="#gmond" class="headerlink" title="gmond"></a>gmond</h2><p>gmond（Ganglia Monitoring Daemon）是一种轻量级服务，安装在每台需要手机指标数据的节点主机上。它通过侦听/通告协议与集群内其他节点共享数据。<br>gmond的安装很简单，其所依赖的库，libconfuse、pkgconfig、PCRE和APR等在大多数现行的linux上都有安装。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum install ganglia-gmond</div></pre></td></tr></table></figure></p>
<h2 id="gmetad"><a href="#gmetad" class="headerlink" title="gmetad"></a>gmetad</h2><p>gmetad （Ganglia Meta Daemon）是一种从其他gmetad或gmond源收集指标数据，并将数据以RRD格式存储到磁盘的服务。gmetad为从主机组收集的特定指标信息提供了简单的查询机制，并支持分级授权，使得创建联合检测域成为可能。<br>gmetad除了需要安装gmond所需的依赖之外，还需要RDDtool库。它用来存储和显示从其他gmetad和gmond源收集的时间序列数据。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum install ganglia-gmetad</div></pre></td></tr></table></figure></p>
<h2 id="gweb"><a href="#gweb" class="headerlink" title="gweb"></a>gweb</h2><p>完整的Ganglia不能缺少网络接口：gweb（Ganglia Web）。gweb是一种利用浏览器显示gmetad所存储数据的PHP前端。<br>Ganglia 3.4的Web接口是一个独立的发布包，其源代码也是独立的。gweb 3.4支持gmond/gmetad 3.4.x及以上版本；gweb未来版本可能需要与gmond/gmetad未来版本相匹配。建议安装或更新gweb的时候查看安装文档，以获取更多信息。<br>安装gweb需要如下需求：</p>
<ul>
<li>Apache Web Server</li>
<li>PHP 5.2级更新版本</li>
<li>PHP JSON扩展的安装和启用</li>
</ul>
<p>首先安装Apache和PHP<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum install httpd php</div></pre></td></tr></table></figure></p>
<p>用户还需要启用PHP的JSON扩展，通过检查/etc/php.d/json.ini文件来检查JSON的扩展状态，如果已经启用扩展，文件中应该包含下面的语句：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">extension=json.ini</div></pre></td></tr></table></figure></p>
<p>下载最新的gweb(<a href="https://sourceforge.net/projects/ganglia/files/gweb/)，然后编译Makefile来安装gweb2" target="_blank" rel="external">https://sourceforge.net/projects/ganglia/files/gweb/)，然后编译Makefile来安装gweb2</a>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tar -xvzf ganglia-web-major.minor.release.tar.gz</div><div class="line"><span class="built_in">cd</span> ganglia-web-major.minor.release</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>编译Makfile并设置变量DESTDIR和APACHE_USER:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">DESTDIR = /var/www/html/ganglia2</div><div class="line">APACHE_USER = apache</div></pre></td></tr></table></figure></p>
<p>最后运行下面的命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">make install</div></pre></td></tr></table></figure></p>
<h1 id="Ganglia的配置"><a href="#Ganglia的配置" class="headerlink" title="Ganglia的配置"></a>Ganglia的配置</h1><h2 id="gmond-1"><a href="#gmond-1" class="headerlink" title="gmond"></a>gmond</h2><p>gmond通过与主机操作系统交互以获取指标数据，并与同一集群内的其他主机共享数据。集群内的每个gmond以及同一个集群内其他主机所收集的数据，并默认向任意连接gmond端口的客户端提供整个集群状态的XML格式的dump文件。</p>
<h3 id="拓扑结构"><a href="#拓扑结构" class="headerlink" title="拓扑结构"></a>拓扑结构</h3><p>gmond默认拓扑为多播模式，也就是说集群内所有节点都收发指标数据，每个节点维护一个内存数据库–以哈希表的形式存储集群内所有节点的指标。gmond内部的发送和接收两部分互不相连。Gmond自身并不交互信息，而只是和网络进行交互。指标模块收集的本地数据通过发送部分直接传送到网络，而接收部分的内部数据库只存储网络中收集的指标数据。<br>单发节点的使用消除了大型集群的运行开销。单发和单收参数的出现使得一些gmond节点可以充当其他gmond节点的专用汇聚器和中继器。单收指的是那些不传送数据的节点，它甚至不收集本节点的指标数据，但是缺汇聚集群内其他gmond的指标数据。单发指的是那些不接收网络中任何指标数据，也不侦听多播成员状态信息的节点。</p>
<p>在多播不适用的情况下，这种单发/单收拓扑可以使用UDB单播来实现。</p>
<p>更进一步，将单发/单收拓扑和默认拓扑混合，创建一种更适合特定情形的系统结构。<br>1、至少存在一个收集集群内所有节点指标数据的gmond。<br>2、gmetad必须周期性轮询以保存整个集群状态gmond。</p>
<h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>运行下面的命令可以生成gmond默认配置：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gmond -t</div></pre></td></tr></table></figure></p>
<p>配置文件由大括号括起来的截个section组成。这些section可以大致分为两类：处理主机和集群的配置；处理指标数据收集和调度的特定问题。所有的section名和属性是不区分大小写的，有些section是可选的，有些是必选的；有的可以出现多次，有的只能出现一次；有的section还可以包含section。<br>在需要大型复杂配置的情况下，include指令可以将gmond.conf文件划分为多个文件。如下列命令标识gmond可以加载/etc/ganglia/conf.d/中的所有的.conf文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">include(<span class="string">'/etc/ganglia/conf.d/*.conf'</span>)</div></pre></td></tr></table></figure></p>
<p>有8个section处理主机自身配置。</p>
<h4 id="section-globals"><a href="#section-globals" class="headerlink" title="section:globals"></a>section:globals</h4><p>globals配置守护进程本身的通用特征，只出现一次。如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">globals &#123;</div><div class="line">    daemonize = yes</div><div class="line">    setuid = yes</div><div class="line">    user = nobody</div><div class="line">    debug_level = 0</div><div class="line">    max_udp_msg_len = 1472</div><div class="line">    mute = no</div><div class="line">    deaf = no</div><div class="line">    allow_extra_data = yes</div><div class="line">    host_dmax = 86400</div><div class="line">    host_tmax = 20</div><div class="line">    cleanup_threshold = 300</div><div class="line">    gexec = no</div><div class="line">    send_metadata_interval = 0</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<table>
<thead>
<tr>
<th style="text-align:left">属性名</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">daemonize</td>
<td style="text-align:left">布尔</td>
<td style="text-align:left">当值为true时，gmond将后台分散运行。</td>
</tr>
<tr>
<td style="text-align:left">setuid</td>
<td style="text-align:left">布尔</td>
<td style="text-align:left">当值为true时，gmond将user属性指定的特定用户的UID作为有效UID；当值为false时，gmond将不会改变其有效用户。</td>
</tr>
<tr>
<td style="text-align:left">debug_level</td>
<td style="text-align:left">整数</td>
<td style="text-align:left">当为0时，gmond将正常运行。当值大于0时，gmond将在前台运行并输出调试信息。值越大，输出的信息越详细。</td>
</tr>
<tr>
<td style="text-align:left">max_udp_msg_len</td>
<td style="text-align:left">整数</td>
<td style="text-align:left">gmond发送包的最大长度。一般使用默认值即可。</td>
</tr>
<tr>
<td style="text-align:left">mute</td>
<td style="text-align:left">布尔</td>
<td style="text-align:left">当值为true时，不管其他配置指令如何，gmond将不能发送数据。单收gmond只是不向其他gmond进程发送数据，但仍然会响应诸如gmetad的外部轮询。</td>
</tr>
<tr>
<td style="text-align:left">deaf</td>
<td style="text-align:left">布尔</td>
<td style="text-align:left">当值为true时，不管其他配置指令如何，gmond将不能接收数据。在一些情况下，某些特定的节点会被预设为为单收，此时这些节点的性能指标将不会被测量，因为这些节点的任务是汇集，所以他们的性能数据会“污染”集群内其他的功能部分</td>
</tr>
<tr>
<td style="text-align:left">allow_extra_data</td>
<td style="text-align:left">布尔</td>
<td style="text-align:left">当值为false时，gmond将不会发送XML的EXTRA_ELEMENT和EXTRA_DATA部分。该值主要应用于用户使用自己的前端并希望节省带宽时。</td>
</tr>
<tr>
<td style="text-align:left">host_dmax</td>
<td style="text-align:left">整型（秒）</td>
<td style="text-align:left">dmax是delete max的缩写。当值为0时，即使远程主机停止报告，gmond也不会从列表中将该主机删除。如果为正值，当gmond在该值指定的时间内如果没有得到某台主机的数据，gmond会认为对应的主机已经崩溃。</td>
</tr>
<tr>
<td style="text-align:left">host_tmax</td>
<td style="text-align:left">整型（秒）</td>
<td style="text-align:left">tmax是timeout max的缩写。代表gmond等待一台主机更新的最长时间。因为消息可能会在网络中丢失，所以如果在4倍的host_tmax时间接收不到某台机器的任何消息，则认为对应主机crash。</td>
</tr>
<tr>
<td style="text-align:left">cleanup_threshold</td>
<td style="text-align:left">整数（秒）</td>
<td style="text-align:left">gmond清理过期数据的最小时间间隔。</td>
</tr>
<tr>
<td style="text-align:left">gexec</td>
<td style="text-align:left">布尔</td>
<td style="text-align:left">当值为true时，gmond允许主机运行gexec任务。郑重方式需要运行gexecd并安装合适的验证码。</td>
</tr>
<tr>
<td style="text-align:left">send_metadata_interval</td>
<td style="text-align:left">整数（秒）</td>
<td style="text-align:left">gmond两次发送元数据包的时间间隔。元数据包是用来描述所有激活指标的数据包。默认为0，表示gmond只有在出事启动和收到其他远程运行的gmond的节点的请求是才会发送元数据包。如果向集群中添加一台运行gmond的主机，则该主机节点需要向其他节点公布自身信息，并告知目前支持的指标标准。在多播模式下，由于任何一个节点都可以向集群内的其他节点请求发送元数据，因此该问题并不存在。然而在单播模式下，必须设重发间隔。间隔值是两次重发之间的最少秒数。</td>
</tr>
<tr>
<td style="text-align:left">module_dir</td>
<td style="text-align:left">路径（可选）</td>
<td style="text-align:left">该指令用来标识已找到的指标收集模块的路径。如果省略，则认为编译时选项值：–with-moduledir。该选值为libganglia安装目录下名为ganglia的子目录。为了在特定gmond中找到该指令的默认值，可以通过下面的命令生成一个简单的配置文件：gmond -t。</td>
</tr>
</tbody>
</table>
<h4 id="section-cluster"><a href="#section-cluster" class="headerlink" title="section:cluster"></a>section:cluster</h4><p>每个gmond守护进程会使用在cluster中定义的属性来报告它所属集群的信息。默认为”unspecified”。使用默认值即可以正常工作。该配置在文件中只能出现一次，下面为默认值：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">cluster &#123;</div><div class="line">    name = <span class="string">"unspecified"</span></div><div class="line">    owner = <span class="string">"unspecified"</span></div><div class="line">    latlong = <span class="string">"unspedified"</span></div><div class="line">    url = <span class="string">"unspedified"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<table>
<thead>
<tr>
<th style="text-align:left">属性名</th>
<th style="text-align:left">类型</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">name</td>
<td style="text-align:left">文本</td>
<td>指定集群名称。当轮询节点的集群状态的XML聚合时，把该名称插入CLUSTER元素内。轮询该节点的gmetad会使用该值来命名存储集群数据的rrd文件。该指令将取代gmetad.conf配置文件中指定的集群名称。</td>
</tr>
<tr>
<td style="text-align:left">owner</td>
<td style="text-align:left">文本</td>
<td>指定集群管理员。</td>
</tr>
<tr>
<td style="text-align:left">latlong</td>
<td style="text-align:left">文本</td>
<td>指定该集群在地区上的GPS坐标。</td>
</tr>
<tr>
<td style="text-align:left">url</td>
<td style="text-align:left">文本</td>
<td>指定携带集群特定信息（如用途和使用细节）的URL。</td>
</tr>
</tbody>
</table>
<h4 id="section-host"><a href="#section-host" class="headerlink" title="section:host"></a>section:host</h4><p>host提供运行gmond主机相关信息。目前只支持地址字符串属性。默认为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">host &#123;</div><div class="line">    location = <span class="string">"unspecified"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<table>
<thead>
<tr>
<th style="text-align:left">属性名</th>
<th style="text-align:left">类型</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">location</td>
<td style="text-align:left">文本</td>
<td>用来标识主机位置。</td>
</tr>
</tbody>
</table>
<h4 id="section-UDP-channels"><a href="#section-UDP-channels" class="headerlink" title="section:UDP channels"></a>section:UDP channels</h4><p>UDP发送通道和接收通道确定gmond节点间的交互方式。集群是由UDP通信通道所定义的，也就是说，集群只不过是共享同样发送和接收通道的一些gmond节点。<br>gmond集群内每个节点默认通过UDP将自身指标数据多播至其他节点，同时侦听其他节点的UDP多播。这种方式易于设置和维护：集群内每个节点共享多播地址，而且每个心节点可自动被发现。然而，有时需要通过单播地址指定某些节点。因此，任意数量的gmond发送和接收通道可以独立配置以满足特定需求。每个发送通道都可以定义gmond发布自己的指标数据的方式，而且每个接收通道都可以定义gmond接收其他节点指标数据的方式，可能单播，也可能多播。<br>gmond节点不能配置为向多个Ganglia集群发送指标数据。<br>UDP通道通过udp_(send|receive)_channel创建。</p>
<h5 id="udp-send-channel"><a href="#udp-send-channel" class="headerlink" title="udp_send_channel"></a>udp_send_channel</h5><p>默认的UDP发送通道如下定义：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">udp_send_channel &#123;</div><div class="line">    <span class="comment">#bind_hostname = yes</span></div><div class="line">    mcast_join = 239.2.11.1</div><div class="line">    port = 8649</div><div class="line">    ttl = 1</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<table>
<thead>
<tr>
<th style="text-align:left">属性名</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">bind_hostname</td>
<td style="text-align:left">布尔类型（可选，多播或单播）</td>
<td style="text-align:left">通知gmond使用源地hi解析主机名。</td>
</tr>
<tr>
<td style="text-align:left">mcast_join</td>
<td style="text-align:left">IP（可选，仅多播）</td>
<td style="text-align:left">当指定该选项时，gmond将创建UDP套接字，并加入由IP地址指定的多播租。该选项创建一个多播通道，并与host互斥。</td>
</tr>
<tr>
<td style="text-align:left">mcast_if</td>
<td style="text-align:left">文本（可选，仅多播）</td>
<td style="text-align:left">当指定该选项时，gmond将发送来自指定接口（如eth0）的数据。</td>
</tr>
<tr>
<td style="text-align:left">host</td>
<td style="text-align:left">文本格式或IP（可选，仅单播）</td>
<td style="text-align:left">当指定该选项时，gmond将向已命名主机发送数据。该选项将创建一个单播通道，并与mcast_join互斥。</td>
</tr>
<tr>
<td style="text-align:left">port</td>
<td style="text-align:left">数值（可选，单播或多播）</td>
<td style="text-align:left">指定gmond发送数据的端口号。如果未指定，则使用默认值8649。</td>
</tr>
<tr>
<td style="text-align:left">ttl</td>
<td style="text-align:left">数值（可选，单播或多播）</td>
<td style="text-align:left">time-to-live的缩写。该设置在多播环境中尤其重要，因为该值限制了指标数据所允许传播的越点数。当该值设置得比实际所需的更大时，指标数据将能够通过WAN连接传输到多个站点，甚至跳出WAN进入全局Internet。</td>
</tr>
</tbody>
</table>
<h5 id="udp-recv-channel"><a href="#udp-recv-channel" class="headerlink" title="udp_recv_channel"></a>udp_recv_channel</h5><p>默认的UDP接收通道<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">udp_recv_channel &#123;</div><div class="line">    mcast_join = 239.2.11.71</div><div class="line">    port = 8649</div><div class="line">    <span class="built_in">bind</span> = 239.2.11.71</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<table>
<thead>
<tr>
<th style="text-align:left">属性名</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">mcast_join</td>
<td style="text-align:left">IP（可选，仅多播）</td>
<td style="text-align:left">指定该选项时，gmond将侦听指定IP的多播组所发送的多播数据包。如果未指定多播属性，gmond将在指定端口创建单播UDP服务。</td>
</tr>
<tr>
<td style="text-align:left">mcast_if</td>
<td style="text-align:left">文本（可选，仅多播）</td>
<td style="text-align:left">当指定该选项时，gmond将侦听指定接口（如eth0）上的数据。</td>
</tr>
<tr>
<td style="text-align:left">bind</td>
<td style="text-align:left">IP（可选，多播或单播）</td>
<td style="text-align:left">当指定该选项时，gmond将绑定到指定的本地地址。</td>
</tr>
<tr>
<td style="text-align:left">port</td>
<td style="text-align:left">数字（可选，多播或单播）</td>
<td style="text-align:left">指定gmond接收数据的端口。如果为指定，则使用默认值8649。</td>
</tr>
<tr>
<td style="text-align:left">family</td>
<td style="text-align:left">inet4或inet6（可选，多播或单播）</td>
<td style="text-align:left">默认IP版本为inet4.Ganglia不允许 IPv6 =&gt; IPv4的映射。如果用户想要同时对一个端口进行inet4和inet6，请为该端口定义两个分离的接收通道。</td>
</tr>
<tr>
<td style="text-align:left">acl</td>
<td style="text-align:left">Ac定义（可选，多播或单播）</td>
<td style="text-align:left">通过指定接入的控制列表，可以对接收通道进行精确的接入空中。</td>
</tr>
</tbody>
</table>
<h4 id="section-TCP-Accept-Channel"><a href="#section-TCP-Accept-Channel" class="headerlink" title="section: TCP Accept Channel"></a>section: TCP Accept Channel</h4><p>TCP接收通道是gmond节点创建向gmetad或其他外部轮询器汇报集群状态的通道。用户可以配置任意选项。默认TCP接收通道为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tcp_accept_channel &#123;</div><div class="line">    port = 8649</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<table>
<thead>
<tr>
<th style="text-align:left">属性名</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">bind</td>
<td style="text-align:left">IP（可选）</td>
<td style="text-align:left">当指定该选项时，gmond将绑定到指定的本地地址。</td>
</tr>
<tr>
<td style="text-align:left">port</td>
<td style="text-align:left">数字</td>
<td style="text-align:left">gmond接受连接的端口号。</td>
</tr>
<tr>
<td style="text-align:left">family</td>
<td style="text-align:left">inet4或inet6（可选，多播或单播）</td>
<td style="text-align:left">默认IP版本为inet4。如果想要将端口绑定到inet6，设置inet6即可。Ganglia不允许IPv6 =&gt; IPv4的映射。如果用户想要同时对一个端口进行inet4和inet6，请为该端口定义两个分离的接收通道。</td>
</tr>
<tr>
<td style="text-align:left">interface</td>
<td style="text-align:left">文本（可选）</td>
<td style="text-align:left">当指定该选项时，gmond将侦听指定接口（例如eth0）数据。</td>
</tr>
<tr>
<td style="text-align:left">acl</td>
<td style="text-align:left">ACL（可选，多播或单播）</td>
<td style="text-align:left">通过指定接入的访问控制列表可以对接收通道进行精细的控制。</td>
</tr>
</tbody>
</table>
<h5 id="Access-control"><a href="#Access-control" class="headerlink" title="Access control"></a>Access control</h5><p>udp_recv_channel指令和tcp_accept_channel指令可以包含一个接入控制列表。该列表允许用户指定gmond接收或拒绝连接的地址和地址范围。示例如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">acl &#123;</div><div class="line">    default = <span class="string">"deny"</span></div><div class="line">    access &#123;</div><div class="line">        ip = 192.168.0.0</div><div class="line">        mask = 24</div><div class="line">        action = <span class="string">"allow"</span></div><div class="line">    &#125;</div><div class="line">    access &#123;</div><div class="line">        ip = ::ff:1.2.3.0</div><div class="line">        mask = 120</div><div class="line">        action = <span class="string">"deny"</span></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>default属性为整个ACL定义了默认方式。任意的access模块可以指定主机或ip地址，以及这些地址是allow还是deny。mask属性是以CIDR记法定义的子网掩码，允许用户指定地址范围而非一个一个的具体地址。当ACL冲突时，以第一个匹配为准。</p>
<h4 id="section-sFlow"><a href="#section-sFlow" class="headerlink" title="section:sFlow"></a>section:sFlow</h4><p>sFlow适用于监测高速路由网络的工业标准技术，现在服务于通用操作系统和诸如Tomcat、memcached和Apache Web Server等流行应用。gmond可以通过配置来充当网络中sFlow代理聚合器，收集sFlow代理的数据并实现对gmetad的透明传输。下面是sFlow的默认配置：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#sflow &#123;</span></div><div class="line"><span class="comment">#    udp_port = 6343</span></div><div class="line"><span class="comment">#    accept_vm_metrics = yes</span></div><div class="line"><span class="comment">#    accept_jvm_metrics = yes</span></div><div class="line"><span class="comment">#    multiple_jvm_instances = no</span></div><div class="line"><span class="comment">#    accept_http_metrics = yes</span></div><div class="line"><span class="comment">#    multiple_http_instances = no</span></div><div class="line"><span class="comment">#    accept_memcache_metrics = yes</span></div><div class="line"><span class="comment">#    multiple_memcache_instances = no</span></div><div class="line"><span class="comment">#&#125;</span></div></pre></td></tr></table></figure></p>
<table>
<thead>
<tr>
<th style="text-align:left">属性名</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">udp_port</td>
<td style="text-align:left">数字（可选）</td>
<td style="text-align:left">gmond接收sFlow数据的端口。</td>
</tr>
</tbody>
</table>
<h4 id="section-modules"><a href="#section-modules" class="headerlink" title="section:modules"></a>section:modules</h4><p>该模块包含了加载指标模块的必要参数。指标模块是动态可加载的共享目标文件，用于扩展gmond可收集的指标。<br>每个modules必须至少包含一个module subsection。module subsection由5个属性组成。默认配置包含了默认安装中所有可用模块（module），如果不添加新模块则无需更改该内容。如下是配置示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">modules &#123;</div><div class="line">    module &#123;</div><div class="line">        name= <span class="string">"example_module"</span></div><div class="line">        language = <span class="string">"C/C++"</span></div><div class="line">        <span class="built_in">enable</span> = yes</div><div class="line">        path = <span class="string">"modexample.so"</span></div><div class="line">        params = <span class="string">"An extra raw parameter"</span></div><div class="line">        param RandomMax &#123;</div><div class="line">            value = 75</div><div class="line">        &#125;</div><div class="line">        param ConstantValue &#123;</div><div class="line">            value = 25</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<table>
<thead>
<tr>
<th style="text-align:left">属性名</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">name</td>
<td style="text-align:left">文本</td>
<td style="text-align:left">如果模块由C/C++开发，则模块名由模块结构所决定。如果模块由诸如Python解释型语言开发完成，则模块名与源文件名相同。</td>
</tr>
<tr>
<td style="text-align:left">language</td>
<td style="text-align:left">文本（可选）</td>
<td style="text-align:left">如果未指定，则默认为C/C++。目前只支持C、C++和Python。</td>
</tr>
<tr>
<td style="text-align:left">enabled</td>
<td style="text-align:left">布尔（可选）</td>
<td style="text-align:left">通过配置文件设置模块是否可用。默认为yes。</td>
</tr>
<tr>
<td style="text-align:left">path</td>
<td style="text-align:left">文本</td>
<td style="text-align:left">指示gmond预设的加载路径。如果path不是绝对路径，则该值将附加到globals section的module_path上。</td>
</tr>
<tr>
<td style="text-align:left">param</td>
<td style="text-align:left">文本格式（可选）</td>
<td style="text-align:left">用来将字符串参数传送到模块初始化函数。通过包含多个param，可以将多个参数传递给模块初始化函数。每个param section必须命名，并包含一条value指令。</td>
</tr>
</tbody>
</table>
<h4 id="section-collection-group"><a href="#section-collection-group" class="headerlink" title="section: collection_group"></a>section: collection_group</h4><p>collection_group指定了gmond包含的指标以及gmond手机和广播这些指标的周期。用户可以定义任意多的收集组，每个收集组必须包含至少一种metric section。<br>这些逻辑指标分组基于相同的收集间隔。这些在gmond.conf中定义的分组并不影响用于Web接口中的分组，也不能用这种方式来指定Web接口分组名称。部分默认配置如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">collection_group &#123;</div><div class="line">    collect_once = yes</div><div class="line">    time_threshold = 1200</div><div class="line">    metric &#123;</div><div class="line">        name = <span class="string">"cpu_num"</span></div><div class="line">        title = <span class="string">"CPU Count"</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">collection_group &#123;</div><div class="line">    collect_every = 20</div><div class="line">    time_threshold = 90</div><div class="line">    metric &#123;</div><div class="line">        name = <span class="string">"cpu_user"</span></div><div class="line">        value_threshold = <span class="string">"1.0"</span></div><div class="line">        title = <span class="string">"CPU User"</span></div><div class="line">    &#125;</div><div class="line">    metric &#123;</div><div class="line">        name = <span class="string">"cpu_system"</span></div><div class="line">        value_threshold = <span class="string">"1.0"</span></div><div class="line">        title = <span class="string">"CPU Sustem"</span></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<table>
<thead>
<tr>
<th style="text-align:left">属性名</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">collect_once</td>
<td style="text-align:left">布尔</td>
<td style="text-align:left">有些指标不变，也就是说重启之间不存在变化。这些指标包括OS类型和系统CPU数量等，只在初始启动时收集一次，并将其collect_once属性设置为yes。该属性与collect_every相互排斥。</td>
</tr>
<tr>
<td style="text-align:left">collect_every</td>
<td style="text-align:left">整数（秒）</td>
<td style="text-align:left">指定收集组轮询的间隔。</td>
</tr>
<tr>
<td style="text-align:left">time_threshold</td>
<td style="text-align:left">整数（秒）</td>
<td style="text-align:left">gmond发送collection_group所指定的指标数据到所有已配置的udp_send_channels的最大时间。</td>
</tr>
<tr>
<td style="text-align:left">name</td>
<td style="text-align:left">文本</td>
<td style="text-align:left">指标收集模块定义的单个指标标准名称。每个加载模块一般定义好几种单独的指标。name可以由name_match参数替换。</td>
</tr>
<tr>
<td style="text-align:left">value_threshold</td>
<td style="text-align:left">数字</td>
<td style="text-align:left">每次收集到指标数据时，会将新值与上一次的数值进行比较。当二者差别大于value_threshold时，整个收集组被发送至已定义的udp_send_channels。在不同的指标模块中，该值表示不同的指标单位。</td>
</tr>
<tr>
<td style="text-align:left">title</td>
<td style="text-align:left">文本</td>
<td style="text-align:left">一种用户化的用于Web前端的指标名称。</td>
</tr>
</tbody>
</table>
<h3 id="gmetad-1"><a href="#gmetad-1" class="headerlink" title="gmetad"></a>gmetad</h3><p>gmetad（Ganglia Meta Daemon）是一种安装在主机上用来收集和汇聚gmond所收集的指标数据的守护进程。gmetad默认使用RRD文件收集和汇聚指标数据，然而也可以通过配置gmetad将指标数据传送到助于Graphite的外部系统。<br>gmetad通过tcp端口8651侦听远程gmetad连接，并向授权主机提供XML格式的网络状态。gmetad也通过tcp端口8652对交互式请求做出应答。这种交互功能不仅可以看到网格状态XML树的总体结构，也可以看到简单的局部内容。gweb使用这种交互式查询方式来标识那些不适合RRD文件的信息。</p>
<h4 id="gmetad拓扑"><a href="#gmetad拓扑" class="headerlink" title="gmetad拓扑"></a>gmetad拓扑</h4><p>最简单的gmetad拓扑结构为一个gmetad进程轮询一个或多个gmond。gmetad并不局限于轮询gmond，gmetad也可以通过轮询另外的gmetad来创建层次化的gmetad结构。</p>
<h4 id="gmetad-conf配置文件"><a href="#gmetad-conf配置文件" class="headerlink" title="gmetad.conf配置文件"></a>gmetad.conf配置文件</h4><p>gmetad.conf配置文件由单行属性及相应值组成。属性名不区分大小写，但是值区分。有些属性是可选的，有些属性是必须的；有些属性可有多个，有些属性只能有一个。</p>
<h5 id="data-source"><a href="#data-source" class="headerlink" title="data_source"></a>data_source</h5><p>data_source属性是gmetad配置的核心。每一行data_source描述一个gmetad收集信息的gmond集群或gmetad网格。gmetad能够自动区别集群和网格，所以二者的data_source语法相同。当gmetad检测到data_source引用一个集群时，gmetad将为data_source保留一整套轮询数据库。当gmetad检测到data_source引用一个网格时，gmetad只会保留概要性的RRD。<br>默认配置文件中合法data_source示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">data_<span class="built_in">source</span> <span class="string">"my cluster"</span> 10 localhost my.machine.edu:8649 1.2.3.5:8655</div></pre></td></tr></table></figure></p>
<p>每个data_source由3个字段组成。第一个字段是唯一标识data_source的字符串。第二个字段是指定轮询间隔（单位秒）的时长。第三个字段以空格分隔的所要轮询数据的主机列表，这些地址以IP地址或DNS主机名的形式指定，并可以添加端口号后缀，如果为指明端口号，则使用默认值8649。</p>
<h5 id="gmetad守护进程行为"><a href="#gmetad守护进程行为" class="headerlink" title="gmetad守护进程行为"></a>gmetad守护进程行为</h5><table>
<thead>
<tr>
<th style="text-align:left">属性</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">表示</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">gridname</td>
<td style="text-align:left">文本</td>
<td style="text-align:left">能唯一标识网格的字符串。该字符串应该与gmond中所设置的标识不同。在gmond.conf的CLUSTER标识中，用来标识gmond实例能收集到所有的主机，而gridname属性则标识由GRID标识符指定的所有数据源，可以理解为在data_source中定义一个集群集合。</td>
</tr>
<tr>
<td style="text-align:left">authority</td>
<td style="text-align:left">URL</td>
<td style="text-align:left">网格的授权URL，被其他gmetad用来查找当前gmetad数据源的图表位置。默认值为”<a href="http://hostname/ganglia&quot;。" target="_blank" rel="external">http://hostname/ganglia&quot;。</a></td>
</tr>
<tr>
<td style="text-align:left">trusted_hosts</td>
<td style="text-align:left">文本</td>
<td style="text-align:left">当前gmetad允许数据共享的主机列表，以空格作为分隔符。Localhost总是可信的。</td>
</tr>
<tr>
<td style="text-align:left">all_trusted</td>
<td style="text-align:left">on或off</td>
<td style="text-align:left">当值设置为on时，将重写trusted_host属性，允许数据与任意主机共享。</td>
</tr>
<tr>
<td style="text-align:left">setuid_username</td>
<td style="text-align:left">UID</td>
<td style="text-align:left">gmetad设置UID的用户名。默认为nobody。</td>
</tr>
<tr>
<td style="text-align:left">setuid</td>
<td style="text-align:left">on或off</td>
<td style="text-align:left">当该值设置为off时，将不能设置UID。</td>
</tr>
<tr>
<td style="text-align:left">xml_port</td>
<td style="text-align:left">数字</td>
<td style="text-align:left">gmetad侦听端口，默认为8651。</td>
</tr>
<tr>
<td style="text-align:left">interactive_port</td>
<td style="text-align:left">数字</td>
<td style="text-align:left">gmetad交互式侦听端口。默认为8652。</td>
</tr>
<tr>
<td style="text-align:left">server_threads</td>
<td style="text-align:left">数字</td>
<td style="text-align:left">允许同时连接到侦听端口的连接数，默认为4。</td>
</tr>
<tr>
<td style="text-align:left">case_sensitive_hostnames</td>
<td style="text-align:left">1或0</td>
<td style="text-align:left">在gmetad之前版本中，RRD文件区分主机名大小写，但是现在已经有所改变。如果希望使用Ganglia 3.2之前版本创建RRD，设置为1即可。</td>
</tr>
</tbody>
</table>
<h5 id="RRDtool属性"><a href="#RRDtool属性" class="headerlink" title="RRDtool属性"></a>RRDtool属性</h5><table>
<thead>
<tr>
<th style="text-align:left">属性</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">表示</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">RRA</td>
<td style="text-align:left">文本格式</td>
<td style="text-align:left">代表自定义的罗宾环存档值。默认为（15秒步进）”RRD:AVERAGE:0.5:1:5856” “RRA:AVERAGE:0.5:4:20160” “RRA:AVERAGE:0.5:40:52704”。</td>
</tr>
<tr>
<td style="text-align:left">umask</td>
<td style="text-align:left">数字</td>
<td style="text-align:left">指定已创建RRD文件及其目录的umask。默认为022。</td>
</tr>
<tr>
<td style="text-align:left">rrd_rootdir</td>
<td style="text-align:left">路径</td>
<td style="text-align:left">指定RRD文件在本地文件系统存储的基本目录。</td>
</tr>
</tbody>
</table>
<h5 id="Graphite支持"><a href="#Graphite支持" class="headerlink" title="Graphite支持"></a>Graphite支持</h5><p>通过如下的设置，可以将gmetad收集到的指标数据输出到Graphite。</p>
<table>
<thead>
<tr>
<th style="text-align:left">属性</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">表示</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">carbon_server</td>
<td style="text-align:left">地址</td>
<td style="text-align:left">远程carbon守护进程的主机名或IP。</td>
</tr>
<tr>
<td style="text-align:left">carbon_port</td>
<td style="text-align:left">数字（可选）</td>
<td style="text-align:left">carbon端口号，默认为2003。</td>
</tr>
<tr>
<td style="text-align:left">graphite_prefix</td>
<td style="text-align:left">文本</td>
<td style="text-align:left">Graphite使用点分隔的路径来管理和查阅指标数据，所以可以在指标数据前面加上诸如datacenter1.gmetad的描述内容，以便Graphite更好的管理这些指标数据。</td>
</tr>
<tr>
<td style="text-align:left">carbon_timeout</td>
<td style="text-align:left">数字（毫秒）</td>
<td style="text-align:left">gmetad等待Graphite服务器响应的毫秒数。这个设置很重要，因为gmetad的carbon发送器不是线程的，需要收到来自下游carbon守护进程的响应后才能进行后续发送。默认为500。</td>
</tr>
</tbody>
</table>
<h5 id="gmetad交互式端口查询语法"><a href="#gmetad交互式端口查询语法" class="headerlink" title="gmetad交互式端口查询语法"></a>gmetad交互式端口查询语法</h5><p>gmetad通过TCP端口8652侦听交互式查询。这种交互式查询功能使得客户端程序可以用XML方式只查询某一部分的网格状态。<br>交互式查询通过一种文本协议（类似于SMTP或HTTP）来完成，是一种以斜线开始的层次话查询方式。例如，下面的查询将返回整个网格状态的XML数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/</div></pre></td></tr></table></figure></p>
<p>为了缩减查询结果，可以指定集群名：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/cluster1</div></pre></td></tr></table></figure></p>
<p>为了进一步缩减查询结果，还可以指定集群内的主机名：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/cluster1/host1</div></pre></td></tr></table></figure></p>
<p>还可以在查询词后面添加过滤器来修改返回的指标数据类型（目前过滤器只有summary）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/cluster1?filter=summary</div></pre></td></tr></table></figure></p>
<h3 id="gweb-1"><a href="#gweb-1" class="headerlink" title="gweb"></a>gweb</h3><p>无需改变gweb的任何配置，gweb就可以启动并运行功能齐全的web客户端。</p>
<h4 id="Apache虚拟主机配置"><a href="#Apache虚拟主机配置" class="headerlink" title="Apache虚拟主机配置"></a>Apache虚拟主机配置</h4><p>详情参考<a href="http://httpd.apache.org/docs/2.0/vhosts" title="Apache" target="_blank" rel="external">http://httpd.apache.org/docs/2.0/vhosts</a>。</p>
<h4 id="gweb可选项"><a href="#gweb可选项" class="headerlink" title="gweb可选项"></a>gweb可选项</h4><p>gweb是由conf.php文件进行配置的，其实该文件重写并扩展了conf_default.php中的默认配置。conf.php位于Web跟目录下。</p>
<h4 id="应用设置"><a href="#应用设置" class="headerlink" title="应用设置"></a>应用设置</h4><p>该类中的属性会影响gewb的功能参数，用户很少需要修改这些属性，但是有一些值得注意：</p>
<table>
<thead>
<tr>
<th style="text-align:left">属性</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">表示</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">templates</td>
<td style="text-align:left">路径</td>
<td style="text-align:left">指定gweb搜索模板文件的目录。</td>
</tr>
<tr>
<td style="text-align:left">graphdir</td>
<td style="text-align:left">路径</td>
<td style="text-align:left">代表用户放置JSON格式的定制图表的路径。用户可能会以JSON格式定制报告图表，并保存在该目录，而且这些报告会显示在UI上。</td>
</tr>
<tr>
<td style="text-align:left">rrds</td>
<td style="text-align:left">路径</td>
<td style="text-align:left">指定RRD文件目录。</td>
</tr>
</tbody>
</table>
<h4 id="外观和感觉"><a href="#外观和感觉" class="headerlink" title="外观和感觉"></a>外观和感觉</h4><p>gweb可以功过配置(max_graphs)来限制一次显示的图表数目，也可以指定网络和主机的显示列数。一些布尔选项也可以影响初次启动时UI的展示形式。<br>conf.php文件中定义了许多修改UI图表功能属性的设置。</p>
<h3 id="检查安装"><a href="#检查安装" class="headerlink" title="检查安装"></a>检查安装</h3><p>gmond和gmetad都是通过TCP socket侦听入站连接的。为了检查gmond是否在指定主机上工作，telnet到gmond的TCP端口：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">telnet localhost 8649</div></pre></td></tr></table></figure></p>
<p>gmond会输出以XML格式的指标数据作为应答。如果gmond是单发或单收节点，它会反水一个仅携带CLUSTER标识符的空的XML文档。姜茶gmetad可以通过如下指令telnet：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">telnet localhost 8651</div></pre></td></tr></table></figure></p>
<p>正常运行的gmetad会输出XML格式的指标数据。</p>
<h1 id="安装实例"><a href="#安装实例" class="headerlink" title="安装实例"></a>安装实例</h1><p>安装使用的是安装包，而非自己编译。</p>
<h2 id="安装gmetad节点"><a href="#安装gmetad节点" class="headerlink" title="安装gmetad节点"></a>安装gmetad节点</h2><p>安装gmetad的节点需要安装gmetad、gmond、gweb、rrdtool、apache web server以及各自的依赖。依赖主要有：<br>gmetad节点需要同时配置gmetad.conf(/etc/ganglia/gmond.conf)、gmond.conf(/etc/ganglia/gmond.conf)和gweb的配置。gmond主要用于收集，gmetad主要用于轮询和存储。</p>
<h3 id="配置ganglia-conf"><a href="#配置ganglia-conf" class="headerlink" title="配置ganglia.conf"></a>配置ganglia.conf</h3><p>配置文件位于/etc/httpd/conf.d/ganglia.conf。主要为了修改ganglia集群的访问规则。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">cation /ganglia&gt;</div><div class="line">  <span class="comment">#Order deny,allow</span></div><div class="line">  Allow from all <span class="comment">#来自任何位置的请求都允许</span></div><div class="line">  Allow from 127.0.0.1</div><div class="line">  Allow from ::1</div><div class="line">  <span class="comment"># Allow from .example.com</span></div><div class="line">&lt;/Location&gt;</div></pre></td></tr></table></figure></p>
<h3 id="配置gmetad-conf"><a href="#配置gmetad-conf" class="headerlink" title="配置gmetad.conf"></a>配置gmetad.conf</h3><p>这里配置该文件(/etc/ganglia/gmetad.conf)，主要是为了定义数据源、gmetad轮询的间隔和轮询的主机以及其他关于rrd存储的配置。配置如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">data_<span class="built_in">source</span> <span class="string">"hadoop"</span> 10 server-001.yz  <span class="comment">#定义数据源、轮询间隔、轮询的主机</span></div><div class="line">rrd_rootdir <span class="string">"/data10/ganglia/rrds"</span>     <span class="comment">#定义rrd文件的存储位置，需要放在一个较大空间的磁盘</span></div></pre></td></tr></table></figure></p>
<h3 id="配置gmond"><a href="#配置gmond" class="headerlink" title="配置gmond"></a>配置gmond</h3><p>这里配置该文件(/etc/ganglia/gmond.conf)，主要是配置指标数据汇聚节点，用于gmetad的轮询。配置如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#集群信息</span></div><div class="line">cluster &#123;</div><div class="line">  name = <span class="string">"hadoop"</span></div><div class="line">  owner = <span class="string">"unspecified"</span></div><div class="line">  latlong = <span class="string">"unspecified"</span></div><div class="line">  url = <span class="string">"unspecified"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">#指标接收</span></div><div class="line">udp_recv_channel &#123;</div><div class="line">  port = 9649</div><div class="line">  <span class="built_in">bind</span> = server-001.yz</div><div class="line">  retry_<span class="built_in">bind</span> = <span class="literal">true</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">#用户gmetad的轮询</span></div><div class="line">tcp_accept_channel &#123;</div><div class="line">  port = 8649</div><div class="line">  gzip_output = no</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="配置gweb"><a href="#配置gweb" class="headerlink" title="配置gweb"></a>配置gweb</h3><p>这里配置该文件(/etc/ganglia/conf.php)，主要是设置web的信息，如果rrdtool的位置、rrd的位置等。该文件是/usr/share/ganglia/conf_default.php的扩展。配置信息如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$conf</span>[<span class="string">'gweb_confdir'</span>] = <span class="string">"/data10/ganglia/"</span>;  <span class="comment">#gweb的根目录</span></div><div class="line"><span class="variable">$conf</span>[<span class="string">'gmetad_root'</span>] = <span class="string">"/data10/ganglia/"</span>;   <span class="comment">#gmetad的根目录</span></div><div class="line"><span class="variable">$conf</span>[<span class="string">'rrdtool'</span>] = <span class="string">"/usr/bin/rrdtool"</span>;       <span class="comment">#rrdtool的位置</span></div></pre></td></tr></table></figure></p>
<p>这里需要注意”gmetad_root”的目录需要是ganglia用户的，且有写权限。并且目录应该包含conf和dwoo。这两个目录中包含了有关gweb的数据显示等各种文件，该文件不应该由用户自己创建，可以从/var/lib/ganglia进行拷贝。</p>
<h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">service gmond start</div><div class="line">service gmetad start</div><div class="line">service httpd start</div></pre></td></tr></table></figure>
<h2 id="安装gmond节点"><a href="#安装gmond节点" class="headerlink" title="安装gmond节点"></a>安装gmond节点</h2><p>安装gmond节点需要安装gmond以及依赖。依赖和gmetad的相同。gmond主要用于收集节点的指标数据。</p>
<h3 id="配置gmond-conf"><a href="#配置gmond-conf" class="headerlink" title="配置gmond.conf"></a>配置gmond.conf</h3><p>这里配置该文件(/etc/ganglia/gmond.conf)，主要是为了收集指标数据，并将数据以udp的方式发送给汇聚用的gmond。配置如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">cluster &#123;</div><div class="line">  name = <span class="string">"hadoop"</span></div><div class="line">  owner = <span class="string">"unspecified"</span></div><div class="line">  latlong = <span class="string">"unspecified"</span></div><div class="line">  url = <span class="string">"unspecified"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">udp_send_channel &#123;</div><div class="line">  host = server-001.yz</div><div class="line">  port = 9649</div><div class="line">  ttl = 1</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="启动服务-1"><a href="#启动服务-1" class="headerlink" title="启动服务"></a>启动服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">service gmond start</div></pre></td></tr></table></figure>
<p>访问<a href="http://server-001.yz/ganglia" target="_blank" rel="external">http://server-001.yz/ganglia</a> 就可以看到页面效果了。</p>
<h1 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h1><h2 id="图表没有数据或者数据不可读"><a href="#图表没有数据或者数据不可读" class="headerlink" title="图表没有数据或者数据不可读"></a>图表没有数据或者数据不可读</h2><p>首先要确定是真的没有数据，还是gweb没有读到数据。是否有数据可以通过telnet gmetad或在rrd目录中查看来进行核实。本例中遇到的问题是有数据，但是gweb没有读到。<br>通过各种查询发现gweb配置文件中rdd和gweb配置的根目录与存放RDD的实际目录不相符，修改后可以正常显示。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是ganglia的安装和配置的笔记&lt;/p&gt;
&lt;h1 id=&quot;Ganglia的安装&quot;&gt;&lt;a href=&quot;#Ganglia的安装&quot; class=&quot;headerlink&quot; title=&quot;Ganglia的安装&quot;&gt;&lt;/a&gt;Ganglia的安装&lt;/h1&gt;&lt;p&gt;首先，ganglia由gmond、gmetad和gweb三部分组成。&lt;/p&gt;
&lt;h2 id=&quot;gmond&quot;&gt;&lt;a href=&quot;#gmond&quot; class=&quot;headerlink&quot; title=&quot;gmond&quot;&gt;&lt;/a&gt;gmond&lt;/h2&gt;&lt;p&gt;gmond（Ganglia Monitoring Daemon）是一种轻量级服务，安装在每台需要手机指标数据的节点主机上。它通过侦听/通告协议与集群内其他节点共享数据。&lt;br&gt;gmond的安装很简单，其所依赖的库，libconfuse、pkgconfig、PCRE和APR等在大多数现行的linux上都有安装。&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;sudo yum install ganglia-gmond&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&quot;gmetad&quot;&gt;&lt;a href=&quot;#gmetad&quot; class=&quot;headerlink&quot; title=&quot;gmetad&quot;&gt;&lt;/a&gt;gmetad&lt;/h2&gt;&lt;p&gt;gmetad （Ganglia Meta Daemon）是一种从其他gmetad或gmond源收集指标数据，并将数据以RRD格式存储到磁盘的服务。gmetad为从主机组收集的特定指标信息提供了简单的查询机制，并支持分级授权，使得创建联合检测域成为可能。&lt;br&gt;gmetad除了需要安装gmond所需的依赖之外，还需要RDDtool库。它用来存储和显示从其他gmetad和gmond源收集的时间序列数据。&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;sudo yum install ganglia-gmetad&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&quot;gweb&quot;&gt;&lt;a href=&quot;#gweb&quot; class=&quot;headerlink&quot; title=&quot;gweb&quot;&gt;&lt;/a&gt;gweb&lt;/h2&gt;&lt;p&gt;完整的Ganglia不能缺少网络接口：gweb（Ganglia Web）。gweb是一种利用浏览器显示gmetad所存储数据的PHP前端。&lt;br&gt;Ganglia 3.4的Web接口是一个独立的发布包，其源代码也是独立的。gweb 3.4支持gmond/gmetad 3.4.x及以上版本；gweb未来版本可能需要与gmond/gmetad未来版本相匹配。建议安装或更新gweb的时候查看安装文档，以获取更多信息。&lt;br&gt;安装gweb需要如下需求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apache Web Server&lt;/li&gt;
&lt;li&gt;PHP 5.2级更新版本&lt;/li&gt;
&lt;li&gt;PHP JSON扩展的安装和启用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;首先安装Apache和PHP&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;yum install httpd php&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;用户还需要启用PHP的JSON扩展，通过检查/etc/php.d/json.ini文件来检查JSON的扩展状态，如果已经启用扩展，文件中应该包含下面的语句：&lt;br&gt;&lt;figure class=&quot;highlight php&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;extension=json.ini&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;下载最新的gweb(&lt;a href=&quot;https://sourceforge.net/projects/ganglia/files/gweb/)，然后编译Makefile来安装gweb2&quot;&gt;https://sourceforge.net/projects/ganglia/files/gweb/)，然后编译Makefile来安装gweb2&lt;/a&gt;:&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;tar -xvzf ganglia-web-major.minor.release.tar.gz&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; ganglia-web-major.minor.release&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="ganglia" scheme="http://baimoon.github.io/categories/ganglia/"/>
    
    
      <category term="ganglia" scheme="http://baimoon.github.io/tags/ganglia/"/>
    
  </entry>
  
  <entry>
    <title>Tuning Spark</title>
    <link href="http://baimoon.github.io/2016/09/10/spark-tuningSpark/"/>
    <id>http://baimoon.github.io/2016/09/10/spark-tuningSpark/</id>
    <published>2016-09-10T15:28:56.000Z</published>
    <updated>2016-09-27T08:00:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是Tuning Spark文档的翻译，原文档<a href="http://spark.apache.org/docs/latest/tuning.html" title="Tuning Spark" target="_blank" rel="external">请参考</a>，本文主要用于个人学习。</p>
<h1 id="Tuning-Spark"><a href="#Tuning-Spark" class="headerlink" title="Tuning Spark"></a>Tuning Spark</h1><p>因为大多数Spark计算是内存中的计算，因此集群中的任何资源都能够成为Spark程序的瓶颈：CPU、网络带宽或内存。通常，如果数据装载到内存中，瓶颈可能是网络带宽，但是有些时候，你还需要做一些调整，例如以序列化格式存储RDDs，以降低内存的使用。本指南覆盖了两个主题：数据序列化和内存调整，其中数据序列化对好的网络性能是至关重要的，并且还可以降低内存的使用。我们还会概述其他一些小的主题。</p>
<h2 id="Data-Serialization"><a href="#Data-Serialization" class="headerlink" title="Data Serialization"></a>Data Serialization</h2><p>序列化在任何分布式application的执行中扮演了很重要的角色。那些序列化缓慢的对象或消费很大数量byte的格式将极大的减慢计算。通常，这是优化一个Spark application首先要调整的东西。Spark的目标是在方便（允许你在你的操作中使用任何Java类型）和性能之间达到一个平衡。它提供了两种序列化库：</p>
<ul>
<li>Java serialization: 默认，Spark使用Java的ObjectOutputStream框架进行序列化对象操作，能够和任何你实现了java.io.Serializable接口的类型一起工作。通过继承java.io.Externalizable，你能够更加近的控制你的序列化的执行。Java序列化是灵活的，但是通常是慢的，这导致对于很多类会有很大的序列化格式。</li>
<li>Kryo serialization: Spark能够使用Kryo库（版本2）来更快的序列化对象。相对于Java序列化Kryo显然是更快且更加简单的（通常是10倍还多），但是不支持所有的可序列化类型，并且需要你在程序中将你要使用的类进行注册以便获取更好的执行。</li>
</ul>
<p>通过使用SparkConf初始化你的job，并调用conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”)来转换为使用Kryo。这个设置不止为了wroker节点混洗数据配置序列化生成器，当序列化RDDs到磁盘时也有用。Kryo不作为默认序列化生成器的唯一原因是需要自定义注册，但是我们推荐在任何网络集中型application中使用它。<br>Spark自动的很多常用的核心Scala类包含Kryo序列化生成器，涵盖在Twitter chill库的AllScalaRegistrar中。<br>要使用Kryo注册你自己的自定义类，使用registerKryoClasses方法。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</div><div class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</div><div class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div></pre></td></tr></table></figure></p>
<p><a href="https://github.com/EsotericSoftware/kryo" title="Kryo" target="_blank" rel="external">Kryo documntation</a>描述了更加高级的注册选项，如添加自定义序列化编码器。<br>如果你的对象很大，你可能需要增加spark.kryoserializer.buffer配置的值。这个值需要足够大以便保存你要序列化的最大对象。<br>最后，如果你没有注册你的自定义，Kryo将仍然能够工作，但是它将存储每个对象的全类名，这样损耗很大。</p>
<h2 id="Memory-Tuning"><a href="#Memory-Tuning" class="headerlink" title="Memory Tuning"></a>Memory Tuning</h2><p>在调整内存的用法中，有三种值得考虑：被你的对象使用的内存的总量（你可能想要将整个数据集装配到内存中）、访问这些对象的开销和垃圾回收的开销（如果你在这些对象）。<br>默认，Java对象是快速访问的，但是在它们字段的内部很容消费掉比原始数据多2-5倍的空间 。这是因为有如下原因：</p>
<ul>
<li>每个不同的Java对象有一个”object header”，它大概是是16个字节，并包含信息，诸如一个指向它的class的指针。对于一个带有非常少数据的对象（假设一个Int字段），这可能要比数据大很多。</li>
<li>Java Strings在原生的string数据上有一个大概40字节的开销（因为它们被存储到一个Chars数组中，并且保存了额外的数据，如长度），并且每个字符以两个字节存储，因为String内部使用的UTF-16进行编码。因此一个包含10个字符的字符串很容易消耗掉60个字节。</li>
<li>常见的集合类，如HashMap和LinkedList，使用linked数据结构，每个数据项是一个包装对象（如Map.Entry）。这种对象不只有header，而且还有指针（通常是8个字节）指向列表中的下一个对象。</li>
<li>原始类型常常以包装对象来存储，诸如java.lang.Integer。</li>
</ul>
<a id="more"></a>
<p>这一章我们将以Spark中内存管理的概述开始，接着讨论用户能够在他的application中使用而产生内存使用效率的具体策略。另外，我们将描述如何确定你的对象的内存使用，以及如何改善它 - 通过修改你的数据结构或使用一个序列化格式存储数据。我们将涵盖Spark的缓存调整和Java垃圾回收调整。</p>
<h3 id="Memory-Management-Overview"><a href="#Memory-Management-Overview" class="headerlink" title="Memory Management Overview"></a>Memory Management Overview</h3><p>Spark中内存的使用主要落在两种类型的一个上面：执行和存储。执行内存用于在shuffles、joins、sorts和aggregations中，而存储内存用于缓存和跨集群的内部数据传播。在Spark中执行和存储共享一个统一的区域（M）。当没有执行内存使用时，存储能够获取所有可用的内存，反之亦然。如果必要，执行可能驱逐存储，直到总的存储内存使用低于一个确切的值(R)。换句话说，R在M中描述了一个子区域用来缓存blocks，这个区域的blocks永远不会被驱逐。由于实现的复杂，存储可能不会驱逐执行。<br>这个设计确保了一些明智的属性。首先，那些没有使用缓存的application可以使用整个空间来进行执行操作，避免不必要的磁盘溢出。第二，那些使用了缓存的application保留了一个最小的存储空间（R），这里的block免于被驱逐。最后，这个方法为多样化的工作量提供了一个合理的out-of-the-box的执行，而不需要用户专业的去进行划分。<br>然而，还有两个相关的配置，通常用户不需要调整他们，使用默认值对大多数工作量都是合适的：</p>
<ul>
<li>spark.memory.fraction M空间的确切大小，以JVM heap空间的分数表示（默认为0.6）。剩余的空间（25%）为用户存储数据结构、Spark中的内部元数据以及在极少情况下和非常大的记录中保护OOM错误。</li>
<li>spark.memory.storageFraction R空间的确切大小，以M的分数表示(默认为0.5)。R是M中的存储空间，用来缓存blocks，避免被执行驱逐。</li>
</ul>
<p>spark.memory.fraction的值应该的符合JVM的old generation或”tenured” generation中heap空间。否则，当有太多空间用于缓存或执行时，tenured greneration将会满，从而引发JVM垃圾回收的时间明显增长。查看<a href="https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/sizing.html" title="4 Sizing the Generations" target="_blank" rel="external">Java GC sizing documentation</a>获取更多信息。<br>tenured generation大小由JVM的NewRatio参数控制，默认为2，意味着tenured generation是new generation(heap的剩余)的两倍大小。因此，默认，tenured generation使用2/3或大约0.66的heap。为spark.memory.fraction设置为0.6来保存old generation中存储和执行有空闲的空间。如果spark.memory.fraction增加，假设0.8，那么NewRatio可能要增加到6或更多。<br>NewRatio作为一个JVM标识对executors进行设置，这意味着添加spark.executor.extraJavaOptions=-XX:NewRatio=x到Spark job的配置中。</p>
<h3 id="Determining-Memory-Consumption"><a href="#Determining-Memory-Consumption" class="headerlink" title="Determining Memory Consumption"></a>Determining Memory Consumption</h3><p>测量创建一个RDD，一个数据需要请求的计算消耗总数的最好方式是，将它放到缓存中，并查看Web UI中”Storage”页。这个页面将会告诉你这个RDD使用的内存。<br>要计算一个特定对象的内存消耗，使用SizeEstimator的estimate方法。这对于尝试以不同数据层来缩减内存的使用是有用的，类似于确定一个广播变量在每个executor heap上占用量。</p>
<h3 id="Tuning-Data-Structures"><a href="#Tuning-Data-Structures" class="headerlink" title="Tuning Data Structures"></a>Tuning Data Structures</h3><p>降低内存消耗的首选方法是避免Java特征（Java特征会增加负载），诸如基于指针的数据结构和包装对象。有一些方法来完成这个：<br>1、设计你的数据结构引用数组对象，并且是原始类型，而不是标准的Java或Scala集合类（例如HashMap）。<a href="http://fastutil.di.unimi.it" title="fastutil: Fast &amp; compact type-specific collections for Java" target="_blank" rel="external">fastutil</a>库为原始类型提供了方便的集合类，这些集合类与Java标准库是兼容的。<br>2、如果可能，避免很多小对象的嵌套结构。<br>3、对于keys，考虑使用数值类型的ID或枚举对象取代string类型。<br>4、如果你的RAM小于32GB，设置JVM标识-XX:+UseCompressedOops使指针使用4个字节而不是8个。你能够添加这些选项到spark-env.sh中。</p>
<h3 id="Serialized-RDD-Storage"><a href="#Serialized-RDD-Storage" class="headerlink" title="Serialized RDD Storage"></a>Serialized RDD Storage</h3><p>当你的对象过于庞大，即使调优也无法有效的存储时，一个更加简单的方法是以序列化格式来存储它们以降低内存的使用，使用<a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence" title="Spark Programming Guide" target="_blank" rel="external">RDD persistence API</a>中序列化的存储级别，例如MEMORY_ONLY_SER。那么Spark将以一个大的字节数组来存储一个RDD partition。唯一的缺点是以序列化格式存储的下游，访问时间会更慢，因为获取数据时需要对每个对象进行解序列化操作。如果你想要以序列化格式缓存数据，我们高度推荐使用<a href="http://spark.apache.org/docs/latest/tuning.html#data-serialization" title="Data Serialization" target="_blank" rel="external">Kryo</a>，因为它比Java序列化的size更小（确切的是比原生的Java对象）。</p>
<h3 id="Garbage-Collection-Tuning"><a href="#Garbage-Collection-Tuning" class="headerlink" title="Garbage Collection Tuning"></a>Garbage Collection Tuning</h3><p>当你有大量”搅拌”的RDDs的数据项通过你的程序存储时，JVM垃圾回收会是一个问题。（在你的程序中读取一个RDD一次，并在上面执行很多操作通常不是问题）当Java为驱逐老数据为新数据提供空间时，你需要跟踪所有你的Java对象并找出没用的对象。这里需要记住的一点是垃圾收集的开销与Java对象的数量是成正比的，因此为少量对象使用数据结构（例如一个Ints的数组代替LinkedList）将很大的降低这个开销。甚至一个更好的方法是以序列化格式保存对象，正如上面描述的：每个RDD partition将只有一个对象（一个字节数组）。在尝试其他技术之前，首先要尝试的是使用<a href="http://spark.apache.org/docs/latest/tuning.html#serialized-rdd-storage" title="Serialized RDD Storage" target="_blank" rel="external">序列化缓存</a>来判断GC是否是问题。<br>因为你的task的工作内存（运行task所需的空间）和你节点上RDDs缓存之间的干扰，GC也将是一个问题。我们讨论如何控制RDD缓存空间的分配以缓和这个问题。</p>
<h4 id="Measuring-the-Impace-of-GC"><a href="#Measuring-the-Impace-of-GC" class="headerlink" title="Measuring the Impace of GC"></a>Measuring the Impace of GC</h4><p>在GC优化中的第一步是收集垃圾收集的频率和GC花费的时间长度。这可以通过添加-verbose:gc -XX:+printGCDetails -XX:+PrintGCTimeStamps到Java选项来实现。（查看配置指南来获取传递给Spark jobs的java选项的信息）你的Spark job下一次运行时，你将会在worker的日志中打印每一次垃圾回收的message。注意的是，这些日志将在你集群的worker节点上（这它们工作目录的stdout文件中），而不是driver程序的节点上。</p>
<h4 id="Advanced-GC-Tuning"><a href="#Advanced-GC-Tuning" class="headerlink" title="Advanced GC Tuning"></a>Advanced GC Tuning</h4><p>要近一步优化垃圾回收，我们首先理解一些关于在JVM中内存管理的基本概念：</p>
<ul>
<li>Java Heap空间分为两个区域Young和Old。Young代表示保存存活时间较短的对象，而Old代用于较长生命时长的对象。</li>
<li>Young代近一步分为三个区域Eden、Survivor1和Survivor2。</li>
<li>对于垃圾回收程序的一个简单描述是：当Eden满了时，一个轻量级的GC在Eden上运行，那些存活在Eden和Survivor1中的对象会拷贝到Survivor2中。Survivor区域是交换用的。当一个对应足够老或Survivor2满了，它被移动到Old代中。最终当Old也接近满的时候，一个完整的GC被执行。</li>
</ul>
<p>在Spark中，GC优化的目标是确保长时间存在的RDDs值被存储在Old代中，而Young代有足够的空间来存储短时间存在的对象。这将能够帮助避免完整GC收集在task执行期间产生的临时对象。一些有用的步骤如下：</p>
<ul>
<li>通过收集GC状态来确认是否有太多的垃圾收集。如果在一个task完成之前，全面的GC被执行多次，这意味着没有足够可用的内存来执行tasks。</li>
<li>在被打印的GC状态中，如果Old代接近满了，通过降低spark.memory.storageFraction的值来减少用于缓存的内存。缓存较少的对象要比减慢task的执行更好。</li>
<li>如果有很多轻量级的收集，但是没有很多的重要的GCs，为Eden分配更多的内存会有所帮助。你可以根据每个task所需的内存的预估来设置Eden。如果Eden的大小确定为E，那么你可以使用-Xmn=4/3*E来设置Young代的大小。（4/3的比例说明由Survivor区域使用的空间）</li>
<li>作为一个例子，如果你的task从HDFS读取数据，由task使用的内存量可以使用从HDFS读取数据块的size来估计。注意，一个解压缩后的数据块的大小通常是那个数据块的2到3倍。因此如果我们希望有3到4个任务量的工作空间，并且HDFS数据块的大小为64MB，那么我们能够确定的Eden的大小为4<em>3</em>64MB。</li>
<li>监控使用新的设置后垃圾收集的频率和时间。<br>我们的经验建议是，GC优化的效果依赖于你的application和可用的内存总数。这里有很多在线描述的<a href="http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html" title="gc-tuning" target="_blank" rel="external">优化选项</a>，但是在高层次上，管理完整GC发生的频率能够帮助降低开销。</li>
</ul>
<h2 id="Other-Considerations"><a href="#Other-Considerations" class="headerlink" title="Other Considerations"></a>Other Considerations</h2><h3 id="Level-of-Parallelism"><a href="#Level-of-Parallelism" class="headerlink" title="Level of Parallelism"></a>Level of Parallelism</h3><p>集群不会被充分利用，除非你为每个操作设置了足够高的并发级别。Spark依照每个文件的大小自动设置在文件上运行”map”任务的数量（虽然你可以通过SparkContext.textFile的可选参数来控制它），并且分发”reduce”操作，诸如groupByKey和reduceByKey，它使用partitions的最大父级RDD的数量。你可以传递并发姐别作为第二个参数（查看<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" title="org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="external">spark.PairRDDFunctions</a>文档），或者设置配置属性spark.default.parallelism来更改默认值。通常，我们推荐在你的集群中每个CPU 2-3个tasks。</p>
<h2 id="Memory-Usage-of-Reduce-Tasks"><a href="#Memory-Usage-of-Reduce-Tasks" class="headerlink" title="Memory Usage of Reduce Tasks"></a>Memory Usage of Reduce Tasks</h2><p>有些时候，你会得到一个OutOfMemoryError错误，这个错误并不是因为你的RDDs装不到内存中，而是因为你的tasks中的一个task的工作集合，诸如groupByKey中的reduce任务中的一个太大了。Spark的shuffle操作（sortByKey, groupByKey, reduceByKey, join等）在每个task中构建一个哈希表来执行分组，这个哈希表通常很大。最简单的解决方法是增加并行的级别，因此每个task的输入集会更小。Spark能够有效的支持最短200毫秒的tasks，因为它跨很多tasks重利用executor JVM，并且它有很低的task启动开销。因此你能够安全的增加并行级别到比你集群的cores的数量还要多。</p>
<h2 id="Broadcasting-Large-Variables"><a href="#Broadcasting-Large-Variables" class="headerlink" title="Broadcasting Large Variables"></a>Broadcasting Large Variables</h2><p>在SparkContext中使用<a href="http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables" title="Broadcast Variables" target="_blank" rel="external">广播变量功能</a>能够极大的降低每个序列化task的工作量，以及降低集群上一个job的启动开销。如果driver程序中任何task中使用了大对象，考虑将它转换为一个广播变量。Spark在master打印每个task的序列化大小，因此你可以查看并判断你的tasks是否太大；通常一个task大于20KB，就值得优化。</p>
<h2 id="Data-Locality"><a href="#Data-Locality" class="headerlink" title="Data Locality"></a>Data Locality</h2><p>数据本地化优势在Spark job的执行上有很大的影响。如果数据和操作数据的代码都在一个节点上，那么计算将会很快。如果数据和代码是分离的，那么必须将它们移动到一起。通常将代码从一个地方拷贝到另一个地方要比拷贝数据更快，因为代码的大小要比数据块晓得多。Spark围绕着数据本地化优势这个普遍原则来构建它的调度。<br>数据本地化优势是处理数据的代码和数据有多近。基于数据的当前位置，有一些级别的本地化优势。按照从近到远的顺序：</p>
<ul>
<li>PROCESS_LOCAL 数据和运行它的代码在相同的JVM中。这是最好的本地化优势。</li>
<li>NODE_LOCAL 数据和代码在相同的节点上。例如可能在相同节点的HDFS中，或者在相同节点的另一个executor中。这要比PROCESS_LOCAL慢一点，因为数据需要在进程间传输。</li>
<li>NO_PREF 等同于从任何地方访问数据，没有本地优势。</li>
<li>RACK_LOCAL 数据在服务器的相同机架上。数据在相同机架的不同服务器上，因此需要跨网咯传输，通常只跨一个交换机。</li>
<li>ANY 数据在不同相同机架上的其他网络上。<br>Spark喜欢以最好的本地化优势级别来调度所有的tasks，但这不总是可能的。在那些没有未处理的数据的空间executor上，Spark转换为较低级别的本地化优势。有两个选项：a)在数据所在的server上等待，直到有一个忙的CPU释放，然后启动一个task。b)立即在一个较远位置启动新的task，这需要请求远程数据。<br>Spark通常会等待一会儿希望能有忙碌的CPU被释放。一旦时间过期，它将从远处移动数据到空闲的CPU。每个级别的回退等待时间可以单独配置或者一起使用一个配置；查看spark.locality参数获取更多信息。如果你的task长并且缺少本地化优势，你应该增加这个配置值，但是默认值通常很好了。</li>
</ul>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>这里已经有一个简短的指南来指出关于你在优化一个Spark application时的主要关注点 - 最主要的，数据序列化和内存优化。对于大多数程序，转换为Kryo序列化并以序列化格式保存数据将解决大多数常见的性能问题。关于其他的优化实践，可以在<a href="https://spark.apache.org/community.html" title="Apache Spark Community" target="_blank" rel="external">Spark mailing list</a>上自由发问。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是Tuning Spark文档的翻译，原文档&lt;a href=&quot;http://spark.apache.org/docs/latest/tuning.html&quot; title=&quot;Tuning Spark&quot;&gt;请参考&lt;/a&gt;，本文主要用于个人学习。&lt;/p&gt;
&lt;h1 id=&quot;Tuning-Spark&quot;&gt;&lt;a href=&quot;#Tuning-Spark&quot; class=&quot;headerlink&quot; title=&quot;Tuning Spark&quot;&gt;&lt;/a&gt;Tuning Spark&lt;/h1&gt;&lt;p&gt;因为大多数Spark计算是内存中的计算，因此集群中的任何资源都能够成为Spark程序的瓶颈：CPU、网络带宽或内存。通常，如果数据装载到内存中，瓶颈可能是网络带宽，但是有些时候，你还需要做一些调整，例如以序列化格式存储RDDs，以降低内存的使用。本指南覆盖了两个主题：数据序列化和内存调整，其中数据序列化对好的网络性能是至关重要的，并且还可以降低内存的使用。我们还会概述其他一些小的主题。&lt;/p&gt;
&lt;h2 id=&quot;Data-Serialization&quot;&gt;&lt;a href=&quot;#Data-Serialization&quot; class=&quot;headerlink&quot; title=&quot;Data Serialization&quot;&gt;&lt;/a&gt;Data Serialization&lt;/h2&gt;&lt;p&gt;序列化在任何分布式application的执行中扮演了很重要的角色。那些序列化缓慢的对象或消费很大数量byte的格式将极大的减慢计算。通常，这是优化一个Spark application首先要调整的东西。Spark的目标是在方便（允许你在你的操作中使用任何Java类型）和性能之间达到一个平衡。它提供了两种序列化库：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Java serialization: 默认，Spark使用Java的ObjectOutputStream框架进行序列化对象操作，能够和任何你实现了java.io.Serializable接口的类型一起工作。通过继承java.io.Externalizable，你能够更加近的控制你的序列化的执行。Java序列化是灵活的，但是通常是慢的，这导致对于很多类会有很大的序列化格式。&lt;/li&gt;
&lt;li&gt;Kryo serialization: Spark能够使用Kryo库（版本2）来更快的序列化对象。相对于Java序列化Kryo显然是更快且更加简单的（通常是10倍还多），但是不支持所有的可序列化类型，并且需要你在程序中将你要使用的类进行注册以便获取更好的执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过使用SparkConf初始化你的job，并调用conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”)来转换为使用Kryo。这个设置不止为了wroker节点混洗数据配置序列化生成器，当序列化RDDs到磁盘时也有用。Kryo不作为默认序列化生成器的唯一原因是需要自定义注册，但是我们推荐在任何网络集中型application中使用它。&lt;br&gt;Spark自动的很多常用的核心Scala类包含Kryo序列化生成器，涵盖在Twitter chill库的AllScalaRegistrar中。&lt;br&gt;要使用Kryo注册你自己的自定义类，使用registerKryoClasses方法。&lt;br&gt;&lt;figure class=&quot;highlight scala&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;val&lt;/span&gt; conf = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;type&quot;&gt;SparkConf&lt;/span&gt;().setMaster(...).setAppName(...)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;conf.registerKryoClasses(&lt;span class=&quot;type&quot;&gt;Array&lt;/span&gt;(classOf[&lt;span class=&quot;type&quot;&gt;MyClass1&lt;/span&gt;], classOf[&lt;span class=&quot;type&quot;&gt;MyClass2&lt;/span&gt;]))&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;val&lt;/span&gt; sc = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;type&quot;&gt;SparkContext&lt;/span&gt;(conf)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/EsotericSoftware/kryo&quot; title=&quot;Kryo&quot;&gt;Kryo documntation&lt;/a&gt;描述了更加高级的注册选项，如添加自定义序列化编码器。&lt;br&gt;如果你的对象很大，你可能需要增加spark.kryoserializer.buffer配置的值。这个值需要足够大以便保存你要序列化的最大对象。&lt;br&gt;最后，如果你没有注册你的自定义，Kryo将仍然能够工作，但是它将存储每个对象的全类名，这样损耗很大。&lt;/p&gt;
&lt;h2 id=&quot;Memory-Tuning&quot;&gt;&lt;a href=&quot;#Memory-Tuning&quot; class=&quot;headerlink&quot; title=&quot;Memory Tuning&quot;&gt;&lt;/a&gt;Memory Tuning&lt;/h2&gt;&lt;p&gt;在调整内存的用法中，有三种值得考虑：被你的对象使用的内存的总量（你可能想要将整个数据集装配到内存中）、访问这些对象的开销和垃圾回收的开销（如果你在这些对象）。&lt;br&gt;默认，Java对象是快速访问的，但是在它们字段的内部很容消费掉比原始数据多2-5倍的空间 。这是因为有如下原因：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个不同的Java对象有一个”object header”，它大概是是16个字节，并包含信息，诸如一个指向它的class的指针。对于一个带有非常少数据的对象（假设一个Int字段），这可能要比数据大很多。&lt;/li&gt;
&lt;li&gt;Java Strings在原生的string数据上有一个大概40字节的开销（因为它们被存储到一个Chars数组中，并且保存了额外的数据，如长度），并且每个字符以两个字节存储，因为String内部使用的UTF-16进行编码。因此一个包含10个字符的字符串很容易消耗掉60个字节。&lt;/li&gt;
&lt;li&gt;常见的集合类，如HashMap和LinkedList，使用linked数据结构，每个数据项是一个包装对象（如Map.Entry）。这种对象不只有header，而且还有指针（通常是8个字节）指向列表中的下一个对象。&lt;/li&gt;
&lt;li&gt;原始类型常常以包装对象来存储，诸如java.lang.Integer。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="spark" scheme="http://baimoon.github.io/categories/spark/"/>
    
    
      <category term="spark tuning" scheme="http://baimoon.github.io/tags/spark-tuning/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming + Kafka Integration Guide</title>
    <link href="http://baimoon.github.io/2016/09/08/spark-streaming-kafka/"/>
    <id>http://baimoon.github.io/2016/09/08/spark-streaming-kafka/</id>
    <published>2016-09-08T08:45:09.000Z</published>
    <updated>2016-09-09T11:01:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是Spark Streaming + Kafka Integration Guide文档的翻译，原文请<a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html" title="Spark Streaming + Kafka Integration Guide" target="_blank" rel="external">参考</a>。另外，本文主要用于个人学习使用。</p>
<h1 id="Spark-Streaming-Kafka-Integration-Guide"><a href="#Spark-Streaming-Kafka-Integration-Guide" class="headerlink" title="Spark Streaming + Kafka Integration Guide"></a>Spark Streaming + Kafka Integration Guide</h1><p><a href="http://kafka.apache.org/" title="Apache Kafka" target="_blank" rel="external">Apache Kafka</a>是一个发布-订阅的消息队列，作为一个分布式的、分片的、副本提交的日志服务。这里解释如何配置Spark Streaming来从Kafka接收数据。有两种方法来达到这个目的 - 老的方法是使用Receiver和Kafka的高级API，还有一个新的实验性解决方法（在Spark 1.3版本中引入），不需要使用Receiver。它们有不同的编程模式、执行特征和语义保证，因此请仔细阅读。</p>
<h2 id="Approach-1-Receiver-based-Approach"><a href="#Approach-1-Receiver-based-Approach" class="headerlink" title="Approach 1: Receiver-based Approach"></a>Approach 1: Receiver-based Approach</h2><p>这个方法使用一个receiver来接收数据。这个Receiver使用Kafka高级别consumer API来实现。和所有receivers一样，通过一个Receiver从Kafka接收到的数据被存储到Spark executors中，然后Spark Streaming启动jobs来处理数据。<br>然而，根据默认配置，这个解决方法会因为故障而丢失数据（查看<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#receiver-reliability" title="Receiver Reliability" target="_blank" rel="external">receiver reliabillity</a>。要确保零数据丢失，你需要额外的在Spark Streaming中启用Write Ahead Logs(从Spark 1.2版本中引入)。这将同步的将从Kafka接收到的数据到以写ahead日志的方式波存到分布式文件系统中（如HDFS），因此所有的数据能够从故障中恢复。查看Streaming programming guide中<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#deploying-applications" title="Deploying Applications" target="_blank" rel="external">Deploying section</a>获取关于Write Ahead Logs的详细信息）。<br>接下来，我们讨论如何在你的streaming application中使用这个方法。</p>
<p>1、<strong>Linking:</strong> 对于使用SBT或Maven项目描述的Scala或Java application，使用如下的坐标链接你的streaming application（查看programming guide中的[Linking section]获取更多信息）。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">groupId = org.apache.spark</div><div class="line">artifactId = spark-streaming-kafka<span class="number">-0</span><span class="number">-8</span>_2<span class="number">.11</span></div><div class="line">version = <span class="number">2.0</span><span class="number">.0</span></div></pre></td></tr></table></figure></p>
<p>对于Python application，你需要在部署你的application时添加上面的库以及它的依赖。查看Deploying分项的内容。<br>2、<strong>Programming:</strong> 在streaming application代码中，导入KafkaUtils并创建一个输入DStream，如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka._</div><div class="line"></div><div class="line"><span class="keyword">val</span> kafkaStream = <span class="type">KafkaUtils</span>.createStream(streamingContext, </div><div class="line">    [<span class="type">ZK</span> quorum], [consumer group id], [per-topic number of <span class="type">Kafka</span> partitions to consume])</div></pre></td></tr></table></figure></p>
<p>你还可以指定key和value的类以及它们使用createStream的变体的相关解码类。查看<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.kafka.KafkaUtils$" title="KafkaUtils" target="_blank" rel="external">API docs</a>和<a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/KafkaWordCount.scala" title="KafkaWordCount" target="_blank" rel="external">example</a>。</p>
<h4 id="需要注意的几点："><a href="#需要注意的几点：" class="headerlink" title="需要注意的几点："></a>需要注意的几点：</h4><ul>
<li>Kafka中topic的partitions与Spark Streaming中RDDs生成的partitions没有关联。因此在KafkaUtils.createStream()中增加特定于topic的partition的数量只是增加使用的线程的数量，使用这些线程在单个receiver中对topic进行消费。它不会增加Spark数据的并发处理。参考主要文档获取它的更多信息。</li>
<li>多个Kafka输入DStream能够使用不同的group和topics来创建，以便使用多个receiver来并行接收数据。</li>
<li>如果你使用一个可靠的文件系统（像HDFS）启用了Write Ahead logs，接收到的数据已经被复制到日志中。因此，对于输入流存储的存储级别为StorageLevel.MEMORY_AND_DISK_SER（那就是KafkaUtils.createStream(…, StorageLevel.MEMORY_AND_DISK_SER)）。</li>
</ul>
<p>3、<strong>Deploying:</strong> 对于任何Spark application，spark-submit被使用来启动你的application。然而，对于Scala/Java application和Python application之间有轻微的不同。<br>对于Scala和Java application，如果你使用SBT或Maven作为项目管理，那么打包spark-streaming-kafka-0-8_2.11和它的依赖到application的JAR中。确保spark-core_2.11和spark-streaming_2.11作为依赖进行标记，因为他们已经安装到Spark中了。然后，使用spark-submit来启动你的application（查看主要编程指南中的<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#deploying-applications" title="Deploying Applications" target="_blank" rel="external">Deploying section</a>）。<br>对于Python application，它缺少SBT/Maven项目管理，spark-streaming-kafka-0-8_2.11和它的依赖可以使用–packages直接添加到spark-submit（查看<a href="http://spark.apache.org/docs/latest/submitting-applications.html" title="Submitting Applications" target="_blank" rel="external">Application Submission Guide</a>）。这样：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0 ...</div></pre></td></tr></table></figure></p>
<p>或者，你能够从<a href="http://search.maven.org/#search|ga|1|a%3A%22spark-streaming-kafka-0-8-assembly_2.11%22%20AND%20v%3A%222.0.0%22" target="_blank" rel="external">Maven repository</a>下载Maven坐标的JAR并使用–jars将其添加到spark-submit。</p>
<h2 id="Approach-2-Direct-Approach-No-Receivers"><a href="#Approach-2-Direct-Approach-No-Receivers" class="headerlink" title="Approach 2: Direct Approach (No Receivers)"></a>Approach 2: Direct Approach (No Receivers)</h2><p>这个新的五receiver的直接解决方法已经在Spark 1.3版本中引入，来确保健壮的端对端的保证。代替receivers来接收数据，这个方法周期性的查询Kafka来获取每个topic + partition中最后的offset，然后相应的定义每个batch中处理offset的范围。当处理数据的job启动时，使用Kafka的简单API从Kafka中读取定义的offset范围（类似从文件系统中读取文件）。注意这是一个在Spark 1.3中引入的实验性特征，用于Scala和Java API，在Spark 1.4中才有了Python的API。<br>这个解决方案在基于receiver的解决方案上有如下优势：</p>
<ul>
<li><em>Simplified Parallelism:</em> 不需要创建多个输入Kafka streams以及联合他们。使用directStream，Spark Streaming将会创建和要消费的Kafka partitions数量相同的RDD partitions，这样将病习惯你的从Kafka读取数据。因此在Kafka和RDD partitions之间有一个一对一的映射，这样更加容易理解和调整。</li>
<li><em>Efficiency:</em> 在第一个解决方案中要实现零数据丢失需要将数据存储到Write Ahead Log中，这种方式是进一步的复制数据。这实际上是效率低的，因为数据实际上复制了两次-一次被Kafka，另一次被Write Ahead Log。第二种解决方案消除了这个问题，因为没有了receiver，因此不需要Write Ahead Log。只要你有足够的Kafka保留时间，message能够从Kafka中恢复。</li>
<li><em>Exactly-once semantics:</em> 第一种解决方案使用Kafka的高级API将消费掉的offset存储到Zookeeper中。这是从Kafka消费数据的传统方式。而这种解决方案（和write ahead log混合）能够保证零数据丢失（例如至少一次的语义），在一些故障下，有很小的可能性是一些数据会消费两次。这种存在是因为由Spark Streaming可靠的接收的数据和由Zookeeper跟踪的offsets之间的矛盾造成的。因此，在第二解决方案中，我们使用了简单的Kafka API，简单的API没有使用Zookeeper。通过Spark Streaming中它的checkpoints来跟踪offset。这消除了Spark Streaming和Zookeeper/Kafka之间的差异，因此尽管有故障，但是被Spark Streaming接收到的记录实际上只有一次。为了达到输出你的结果只有一次的语义，你的输出操作将数据保存到外部存储中必须是幂等或原子事务的，这样来保存结果和offset（查看主编程指南中<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#semantics-of-output-operations" title="Semantics of output operations" target="_blank" rel="external">Semantics of output operations</a>获取更多信息）。</li>
</ul>
<p>注意，这种解决方案中一个不利条件是不会在Zookeeper中更新offset，因此那些基于Zookeeper的监控工具将不会更新进度。然而，你能够在这种解决方案中访问每个batch中处理过的offsets并自己更新到Zookeeper（参考下面）。<br>接下来，我们讨论如何在你的application中使用这种解决方案。</p>
<ul>
<li><p><strong>Linking:</strong> 这种解决方案在Scala和Java application中支持。使用如下的坐标来连接你的SBT/Maven项目（查看主要编程指南中<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#linking" title="Linking" target="_blank" rel="external">Linking section</a>获取更多信息）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">groupId = org.apache.spark</div><div class="line">artifactId = spark-streaming-kafka<span class="number">-0</span><span class="number">-8</span>_2<span class="number">.11</span></div><div class="line">version = <span class="number">2.0</span><span class="number">.0</span></div></pre></td></tr></table></figure>
</li>
<li><p><strong>Programming:</strong> 在Streaming application代码中，引入KafkaUtils并如下创建一个输入DStream。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka._</div><div class="line"></div><div class="line"><span class="keyword">val</span> directKafkaStream = <span class="type">KafkaUtils</span>.createDirectStream[</div><div class="line">    [key <span class="class"><span class="keyword">class</span>], [value class], [key decoder class], [value decoder class] ](<span class="params"></span></span></div><div class="line">    streamingContext, [map of <span class="type">Kafka</span> parameters], [set of topics to consume])</div></pre></td></tr></table></figure>
</li>
</ul>
<p>你还可以传递一个<em>messageHandler</em>到<em>createDirectStream</em>来访问<em>MessageAndMetadata</em>，MessageAndMetadata包含了关于当前message的元数据和要将它转换成的目标类型。查看<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.kafka.KafkaUtils$" title="KafkaUtils" target="_blank" rel="external">API docs</a>和<a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala" title="DirectKafkaWordCount" target="_blank" rel="external">examples</a>。<br>在Kafka的参数中，你必须指定metadata.broker.list或bootstrap.servers。默认，它将从每个Kafka partition的最后的offset处开始消费。如果你在Kafka参数中设置auto.offset.reset为smallest，那么它将从最小的offset处开始消费。<br>使用KafkaUtils.createDirectStream的其他变量，你还能够从任意offset处开始消费。此外，如果你想要访问每个batch中已经消费的Kafka offset，你可以如下这么做。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Hold a reference to the current offset ranges, so it can be used downstream</span></div><div class="line"><span class="keyword">var</span> offsetRanges = <span class="type">Array</span>[<span class="type">OffsetRange</span>]()</div><div class="line"></div><div class="line">directKafkaStream.transform &#123; rdd =&gt;</div><div class="line">  offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</div><div class="line">  rdd</div><div class="line">&#125;.map &#123;</div><div class="line">          ...</div><div class="line">&#125;.foreachRDD &#123; rdd =&gt;</div><div class="line">  <span class="keyword">for</span> (o &lt;- offsetRanges) &#123;</div><div class="line">    println(<span class="string">s"<span class="subst">$&#123;o.topic&#125;</span> <span class="subst">$&#123;o.partition&#125;</span> <span class="subst">$&#123;o.fromOffset&#125;</span> <span class="subst">$&#123;o.untilOffset&#125;</span>"</span>)</div><div class="line">  &#125;</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>如果你想要基于Zookeeper的Kafka监控工具来展示streaming application的进度，你还可以使用这个来更新Zookeeper。<br>注意到HasOffsetRanges的类型转换，将只有它在第一个方法在directKafkaStream上调用完成后才会成功，不会晚于一个方法链（不知道这句是否正确）。你能够使用transform()方法来代替foreachRDD()方法来作为你调用的第一个方法以便访问offsets，然后调用进一步的Spark方法。然而，需要注意的是在任何shuffle或repartition方法之后，RDD partition和Kafka partition之间的一对一映射将不再维持，例如reduceByKey()方法或window()方法。<br>另一个需要注意的是，因为这个解决方案没有使用receiver，标准的receiver-related（spark.streaming.receiver.<em>格式的配置）将不会应用到由这种解决方案生成的输入DStreams上（但是会应用到其他输入DStream上）。相反，会是用spark.streaming.kafka.</em>的配置。一个重要的东西是spark.streaming.kafka.maxRatePerPartition，它将限制该API的从每个Kafka partition读取数据的速度（每秒message的条数）。</p>
<ul>
<li><strong>Deploying:</strong> 这跟第一种解决方案中的相同。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是Spark Streaming + Kafka Integration Guide文档的翻译，原文请&lt;a href=&quot;http://spark.apache.org/docs/latest/streaming-kafka-integration.html&quot; title
    
    </summary>
    
      <category term="spark" scheme="http://baimoon.github.io/categories/spark/"/>
    
    
      <category term="spark streaming" scheme="http://baimoon.github.io/tags/spark-streaming/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming Programming Guide</title>
    <link href="http://baimoon.github.io/2016/08/17/spark-streaming/"/>
    <id>http://baimoon.github.io/2016/08/17/spark-streaming/</id>
    <published>2016-08-17T08:40:48.000Z</published>
    <updated>2016-09-12T09:30:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是Spark Streaming手册的翻译文档，会随着自己的实现进行更新，官方文档请<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" title="Spark Streming Programming Guide" target="_blank" rel="external">参考</a>。</p>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>Spark Streaming是核心Spark API的一个延伸，它对实时数据流进行可扩展的、高吞吐量的、容灾的进行处理。数据可以从很多源（如Kafka、Flume、Kinesis或TCP socket）进行提取，然后被复杂的算法组合处理，这些复杂的算法可以使用高级别的函数，如map、reduce、join和window。最后，被处理过的数据可以推出到外部文件系统、数据库和实时图表中。实际上你可以在数据流上应用Spark的<a href="http://spark.apache.org/docs/latest/ml-guide.html" title="Machine Learning Library (MLlib) Guide" target="_blank" rel="external">机器学习</a>和<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html" title="GraphX Programming Guide" target="_blank" rel="external">图处理</a>。<br><img src="http://oaavtz33a.bkt.clouddn.com/streaming-arch.png" alt="spark streaming architecture" title="spark streaming architecture"><br>在内部，它如下工作。Spark Streaming接收实时的输入数据流，并将数据划分到批次中，然后在批次中数据被Spark引擎处理并生成最终的结果流。<br><img src="http://oaavtz33a.bkt.clouddn.com/streaming-flow.png" alt="Spark Streaming data flow" title="Spark Streaming data flow"><br>Spark Streaming提供了一个高级别的抽象，叫做discretized stream或DStream，它代表了一个连续的数据流。DStream可以从来自数据源（如Kafka、Flume和Kinesis）的输入数据流创建，也可以通过在其他DStream上应用高级别的操作来创建，一个DStream以一个<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD" title="RDD" target="_blank" rel="external">RDDs</a>序列来表示。<br>本指南展示了如何开始使用DStreams来编写Spark Streaming程序。你可以使用Scala、Java或Python（从Spark1.2中引入）来编写Spark Streaming程序，这些语言的代码都会在本指南中提供。你会发现tabs在本指南中随处可见，是你可以在不同语言的代码片段之间任意选择。<br><strong>注意：</strong>在Python中有少量的APIs是不同或不可用的。贯穿整个指南，你会发现<em>Python API</em>标签高亮了这些不同。</p>
<a id="more"></a>
<p><a name="quick_example"></a></p>
<h1 id="A-Quick-Example"><a href="#A-Quick-Example" class="headerlink" title="A Quick Example"></a>A Quick Example</h1><p>在进入如何编写你的Spark Streaming程序的详细信息之前，我们快速的看一个简单的Spark Streaming程序是什么样的。假设我们想要计算来自TCP socket服务的文本数据中每个字出现的次数。所有你需要的如下：<br>首先，我们导入Spark Streaming类的名称和一些从StreamingContext到我们环境的隐式转换，以便将有用的方法添加到我们需要的其他类中。StreamingContext是全部streaming功能的主要点。我们创建一个带有两个执行线程的本地StreamingContext，并且它的批次间隔为1秒。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark._</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming._</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span>._ <span class="comment">// not necessary since Spark 1.3</span></div><div class="line"></div><div class="line"><span class="comment">// Create a local StreamingContext with two working thread and batch interval of 1 second.</span></div><div class="line"><span class="comment">// The master requires 2 cores to prevent from a starvation scenario.</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</div><div class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</div></pre></td></tr></table></figure></p>
<p>使用这个context，我们能够创建一个DStream，这个DStream代表了来自TCP数据源的流数据，TCP数据源通过主机名（如localhost）和端口号（如9999）来指定。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></div><div class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</div></pre></td></tr></table></figure></p>
<p>名为lines的DStream代表了从数据服务器接收到的数据流。DStream的每条记录是一行文本。接下来，我们根据空白字符将行拆分为字。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Split each line into words</span></div><div class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</div></pre></td></tr></table></figure></p>
<p>flatMap是一个一到多的DStream操作，会生成一个新的DStream，生成的方式是根据源DStream中的每条记录生成多条新的记录。在这个例子中，每行将被拆分成多个字，字的流以名为words的DStream来表现。接下来，我们想要对这些字进行计数。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span>._ <span class="comment">// not necessary since Spark 1.3</span></div><div class="line"><span class="comment">// Count each word in each batch</span></div><div class="line"><span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</div><div class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</div><div class="line"></div><div class="line"><span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></div><div class="line">wordCounts.print()</div></pre></td></tr></table></figure></p>
<p>名为words的DStream之后会映射（一到一的转换）到一个(word, 1)元组的DStream，新的DStream之后会进行reduce计算来计算数据的每个batch中字的频率。最终wordCounts.print()将会打印出每秒生成的计数。<br>注意，当这些行被执行时，Spark Streaming只会建立计算，只有当它启动时才会执行，在没有启动之前是不会有真正的执行的。要在所有的转换构建完毕后启动进程，最终我们调用：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ssc.start()             <span class="comment">// Start the computation</span></div><div class="line">ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></div></pre></td></tr></table></figure></p>
<p>完整的代码能够在Spark Streaming例子<a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala" title="NetworkWordCount" target="_blank" rel="external">NetworkWordCount</a>中找到。<br>如果你现在并构建了Spark，你可以如下运行这个例子。首先你需要运行netcat（一个小单元，可以在大多数类Unix系统中找到）作为一个数据服务，启动如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ nc -lk 9999</div></pre></td></tr></table></figure></p>
<p>然后，在另一个终端中使用如下命令进行启动：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/run-example streaming.<span class="type">NetworkWordCount</span> localhost <span class="number">9999</span></div></pre></td></tr></table></figure></p>
<p>然后，在运行netcat server的终端上输入的任何行将会被计数并以每秒的频率打印在屏幕上。你将会看到一些东西，这些东西看起来如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># TERMINAL 1:</span></div><div class="line"><span class="comment"># Running Netcat</span></div><div class="line"></div><div class="line">$ nc -lk 9999</div><div class="line"></div><div class="line">hello world</div></pre></td></tr></table></figure></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># <span class="type">TERMINAL</span> <span class="number">2</span>: <span class="type">RUNNING</span> <span class="type">NetworkWordCount</span></div><div class="line"></div><div class="line">$ ./bin/run-example streaming.<span class="type">NetworkWordCount</span> localhost <span class="number">9999</span></div><div class="line">...</div><div class="line">-------------------------------------------</div><div class="line"><span class="type">Time</span>: <span class="number">1357008430000</span> ms</div><div class="line">-------------------------------------------</div><div class="line">(hello,<span class="number">1</span>)</div><div class="line">(world,<span class="number">1</span>)</div><div class="line">...</div></pre></td></tr></table></figure>
<h1 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h1><p>接下来，我们越过这个简单的例子，详细描述一下Spark Streaming的基本概念。</p>
<p><a name="linking"></a></p>
<h2 id="Linking"><a href="#Linking" class="headerlink" title="Linking"></a>Linking</h2><p>与Spark类似，Spark Streaming可以通过Maven中心得到。要编写你自己的Spark Steaming程序，你需要将下面的依赖添加到你的sbt项目或Maven项目中。</p>
<ul>
<li><p>Maven</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;spark-streaming_2<span class="number">.11</span>&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;<span class="number">2.0</span><span class="number">.0</span>&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure>
</li>
<li><p>sbt</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">libraryDependencies += <span class="string">"org.apache.spark"</span> % <span class="string">"spark-streaming_2.11"</span> % <span class="string">"2.0.0"</span></div><div class="line"><span class="comment">//有的时候可能需要这样写，因为会在打包的报错：[error] (*:assembly) deduplicate: different file contents found in the following:</span></div><div class="line"><span class="comment">//libraryDependencies += "org.apache.spark" % "spark-streaming_2.10" % "1.6.1"  % "provided"</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p>要从数据源（像Kafka、Flume、Kinesis）摄取数据，这些数据源没有包含在Spark Streaming的核心API中。你需要添加对应的坐标spark-streaming-xyz_2.11到依赖中。例如，下面是一些常用的：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Source</th>
<th style="text-align:left">Artifact</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Kafka</td>
<td style="text-align:left">spark-streaming-kafka-0-8_2.11</td>
</tr>
<tr>
<td style="text-align:left">Flume</td>
<td style="text-align:left">spark-streaming-flume_2.11</td>
</tr>
<tr>
<td style="text-align:left">Kinesis</td>
<td style="text-align:left">spark-streaming-kinesis-asl_2.11 [Amazon Software License]</td>
</tr>
</tbody>
</table>
<p>最新的列表，请参考<a href="http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.spark%22%20AND%20v%3A%222.0.0%22" title="Maven repository" target="_blank" rel="external">Maven repository</a>来获取所支持的数据源的坐标的完整列表。</p>
<h2 id="Initialized-StreamingContext"><a href="#Initialized-StreamingContext" class="headerlink" title="Initialized StreamingContext"></a>Initialized StreamingContext</h2><p>要初始化一个Spark Streaming程序，必须要创建一个StreamingContext对象，它是Spark Streaming功能的主要进入点。<br>一个StreamingContext可以根据一个SparkConf对象来创建。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark._</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming._</div><div class="line"></div><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(appName).setMaster(master)</div><div class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</div></pre></td></tr></table></figure></p>
<p>参数appName是的application要在cluster UI上显示的名称。master是一个<a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" title="Master URLs" target="_blank" rel="external">Spark、Mesos或YARN集群的RUI</a>，或者是在以本地模式运行时的一个特殊字符串”local[<em>]”。在实践中，当你在一个集群上运行时，你可能不想在程序中硬编码master参数，而是想要在使用<a href="http://spark.apache.org/docs/latest/submitting-applications.html" title="Submitting Applications" target="_blank" rel="external">spark-submit启动application</a>并在这里作为参数进行接收。然而对于本地测试和单元测试，你可以传递”local[</em>]”，以进程内的方式运行Spark Streaming（在本地系统中发现core的数量）。需要注意的是，在内部会创建一个<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext" title="SparkContext" target="_blank" rel="external">SparkContext</a>（所有Spark功能的启动点），可以通过ssc.sparkContext来访问。<br>batch间隔必须要基于延迟要求和可用的系统资源来设置。查看<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#setting-the-right-batch-interval" title="Setting the Right Batch Interval" target="_blank" rel="external">性能优化</a>章节，获取更多信息。<br>也可以根据一个已经存在的SparkContext对象来创建StreamingContext对象。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.streaming._</div><div class="line"></div><div class="line"><span class="keyword">val</span> sc = ...                <span class="comment">// existing SparkContext</span></div><div class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">1</span>))</div></pre></td></tr></table></figure></p>
<p>在一个context被定义之后，你还需要做如下的事情：</p>
<ul>
<li>通过创建input DStreams来定义输入数据源。</li>
<li>通过对DStreams应用transformation和输出操作来定义流的计算。</li>
<li>使用streamingContext.start()来启动数据接收和数据处理。</li>
<li>使用streamingContext.awaitTermination()来等待进程的停止（手动或发生错误）。</li>
<li>使用streamingContext.stop()能够人为的停止进程。<br><strong>需要记住的几点：</strong></li>
<li>一旦一个context被启动，则没有新的streaming计算可以被启动或添加到context中。</li>
<li>一旦一个context被停止，它不能被重新启动。</li>
<li>在同一时刻在一个JVM中只有一个StreamingContext可以是活跃的。</li>
<li>在StreamingCotnext的stop()也会停止SparkContext。想要只是停止StreamingContext，设置stop()的可选参数为false，从而不调用stopSparkContext。</li>
<li>一个SparkContext可以重复使用来创建多个StreamingContexts，只要前面的StreamingContext在下一个StreamingContext创建之前被关闭（不关闭SparkContext）即可。</li>
</ul>
<h2 id="Discretized-Streams-DStream"><a href="#Discretized-Streams-DStream" class="headerlink" title="Discretized Streams(DStream)"></a>Discretized Streams(DStream)</h2><p>Discretized Stream或DStream是由Spark Streaming提供的基本抽象。它代表了一个连续的数据流，从数据源接收到的输入数据流或通过对输入流进行转换而生成的处理过的数据流。内部，一个DStream以一个连续的RDDs系列来表示，它是Spark的一个不可变的、分布式数据集（查看<a href="https://baimoon.github.io/blog/2016/07/25/spark-programmingGuide#resilient-distributed-datasets-rdds" title="Resilient Distributed Datasets RDDs">Spark Programming Guide</a>获取更多细节）的抽象。<br><img src="http://oaavtz33a.bkt.clouddn.com/streaming-dstream.png" alt="Streaming DStream" title="Streaming DStream"><br>任何应用于DStream上的操作都会翻译成为底层RDDs上的操作。例如，之前例子中转换行为字，在名为lines的DStream中的每个RDD上应用flatMap操作来生成名为words的DStream的RDDs。在下图图中展示。<br><img src="http://oaavtz33a.bkt.clouddn.com/streaming-dstream-ops.png" alt="Streaming DStream Operation" title="DStream Operation"><br>这些下层的RDD转换由Spark引擎转换完成。DStream操作隐藏了大部分的细节，并且为了方便，提供开发者一个高级别的API。这些操作在之后的章节中会详细讨论。</p>
<h2 id="Input-DStreams-and-Receivers"><a href="#Input-DStreams-and-Receivers" class="headerlink" title="Input DStreams and Receivers"></a>Input DStreams and Receivers</h2><p>输入DStreams是表示那些从streaming源接收输入数据流的DStream。在<a href="#quick_example" title="Quick Example">quick example</a>中，lines是一个输入DStream，它代表了从netcat服务接收数据的流。每个输入DStream（除了文件流，将在本章的后面讨论）与一个<strong>Receiver</strong>（<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.receiver.Receiver" title="Scala doc" target="_blank" rel="external">Scala文档</a>，<a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/receiver/Receiver.html" title="Java doc" target="_blank" rel="external">Java文档</a>）对象关联，Receiver从一个数据源接收数据并将数据存储到Spark的内存中以便处理。<br>Spark Streaming提供了两种内建的streaming数据源。</p>
<ul>
<li>基本数据源：在StreamingContext API中直接可用的数据源。例如：文件系统和socket连接。</li>
<li>高级数据源：像Kafka、Flume、Kinesis等这样的数据源，通过额外的实用工具类可用。正如在<a href="#linking" title="Linking">linking</a>章节中讨论的，这需要对额外的依赖进行linking。</li>
</ul>
<p>在本章的稍后，我们将讨论每中类型的数据源中的一些。<br>如果你想在你的streaming application中并行的接收多个数据流，你可以创建多个输入DStream（在<a href="#level-of-parallelism-in-data-receiving" title="Level of Parallelism in Data Receiving">Performance Tuning</a>章节中进一步讨论）。这将创建多个receiver，这些receiver将同时接收多个数据流。注意，一个Spark worker/executor是一个长时间运行的任务，因此它占用分配给Spark Streaming application的一个cores。所以，记住一个Spark Streaming application需要分配足够的cores来处理接收到的数据以及运行receiver(s)（或线程，如果是以本地模式运行）是很重要的。</p>
<p><strong>需要记住的点</strong></p>
<ul>
<li>当本地运行一个Spark Streaming程序时，不要使用”local”或”local[1]”作为master的URI。这表示只有一个线程被用于运行本地任务。如果你使用了一个基于receiver的输入DStream（如sockets、Kafka、Flume等），那么单线程会被用于运行receiver，就没有线程来处理接收到的数据。因此，当以本地模式运行时，总是使用”local[n]”作为master URI，其中n大于运行receiver的数量（查看<a href="http://spark.apache.org/docs/latest/configuration.html#spark-properties" title="Spark Properties" target="_blank" rel="external">Spark Properties</a>获取如何设置master的信息）。</li>
<li>扩展到在一个集群上运行的逻辑，分配给Spark Streaming application的cores的数量必须多于receivers的数量。否则系统将会接收数据，但是不能够处理它。</li>
</ul>
<h3 id="Basic-Sources"><a href="#Basic-Sources" class="headerlink" title="Basic Sources"></a>Basic Sources</h3><p>在<a href="#quick_example" title="A Quick Example">quick example</a>中我们已经看到了ssc.socketTextStream(…)，它根据一个接收文本数据的TCP socket连接创建一个DStream。除了sockets，StreamingContext API还提供了根据文件作为数据源创建DStream的方法。</p>
<ul>
<li><strong>File Stream：</strong>用于从任何与HDFS API符合的文件系统（HDFS、S3、NFS等）上的文件读取数据，可以这样创建一个DStream：<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">streamingContext.fileStream[<span class="type">KeyClass</span>, <span class="type">ValueClass</span>, <span class="type">InputFormatClass</span>](dataDirectory)</div></pre></td></tr></table></figure>
</li>
</ul>
<p>Spark Streaming将会监控目录dataDirectory，并处理在这个文件中创建的任何文件（不支持目录中文件的嵌套）。注意</p>
<ul>
<li>文件必须有相同的数据格式。</li>
<li>文件必须是通过原子级别移动或重命名到数据目录的方式在dataDirectory中创建的。</li>
<li>一旦被移动，这些文件不能再被更改。如果这些文件继续被追加数据，那么新的数据将不会被读取到。<br>对于简单的文本文件，有一个更加简单的方法streamingContext.textFileStream(dataDirectory)。文件流不要求运行一个receiver，因此不需要分配core。<br><em>Python API</em> fileStream在Python API中不可用，只有textFileStream是可用的。</li>
</ul>
<ul>
<li><strong>Streams based on Custom Receiver：</strong>能够使用通过自定义receivers接收的数据流来创建DStream。查看<a href="http://spark.apache.org/docs/latest/streaming-custom-receivers.html" title="Spark Streaming Custom Receivers" target="_blank" rel="external">Custom Receiver Guide</a>和<a href="https://github.com/spark-packages/dstream-akka" title="dstream-akka" target="_blank" rel="external">DStream Akka</a>获取更多细节。</li>
<li><strong>Queue of RDDs as a Stream：</strong>要使用测试数据测试一个Spark Streaming application，还可以基于一个RDDs队列来创建一个DStream，需要使用streamingContext.queueStream(queueOfRDDs)。每个被推送到队列中的RDD将被按照DStream中的一个数据批次来对待，并且像流一样进行处理。<br>对于来自sockets和files的streams的更多细节，请参考相关功能的API文档，对于Scala请<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext" title="StreamingContext" target="_blank" rel="external">参考StreamingContext</a>，对于Java请<a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html" title="JavaStreamingContext" target="_blank" rel="external">参考JavaStreamingContext</a>，对于Python请<a href="http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext" title="StreamingContext" target="_blank" rel="external">参考StreamingContext</a>。</li>
</ul>
<h3 id="Advanced-Sources"><a href="#Advanced-Sources" class="headerlink" title="Advanced Sources"></a>Advanced Sources</h3><p><em>Python API</em> 从Spark 2.0.0起，Kafka、Kinesis和Flume在Python的API中都是可用的。<br>这类数据源要求与外部非Spark库进行接口通信，它们其中一些带有复杂的依赖（如Kafka和Flume）。因此，要减少依赖版本冲突的相关问题，根据这些数据源创建DStream的功能被移动到单独的包中，在需要的时候可以进行明确的<a href="#linking" title="Linking">link</a>。<br>注意这些高级的数据源在Spark shell中是不支持的，因此基于这些数据源的application不能在shell中测试。如果你真的想要在Spark shell中使用它们，你需要下载对应的Maven依赖JAR，并将其添加到classpath中。<br>一些高级的数据源如下：</p>
<ul>
<li><em>Kafka：</em>Spark Streaming 2.0.0与Kafka 0.8.2.1匹配。查看<a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html" title="Spark Streaming + Kafka Integration Guide" target="_blank" rel="external">Kafka Integration Guide</a>获取更多细节。</li>
<li><em>Flume：</em>Spark Streaming 2.0.0与Flume 1.6.0匹配。查看<a href="http://spark.apache.org/docs/latest/streaming-flume-integration.html" title="Spark Streaming + Flume Integration Guide" target="_blank" rel="external">Flume Integragtion Guide</a>获取更多细节。</li>
<li><em>Kinesis：</em>Spark Streaming 2.0.0与Kinesis Client Library 1.2.1匹配。查看<a href="http://spark.apache.org/docs/latest/streaming-kinesis-integration.html" title="Spark Streaming + Kinesis Integration" target="_blank" rel="external">Kinesis Integration Guide</a>获取更多细节。</li>
</ul>
<h3 id="Custom-Sources"><a href="#Custom-Sources" class="headerlink" title="Custom Sources"></a>Custom Sources</h3><p><em>Python API</em> 在Python中还不支持。<br>还是可以使用自定义数据源来创建输入DStream。所有你需要做的是实现用户定义的receiver（查看下一节来了解它是什么），它能够从自定义数据源接收数据并推送到Spark中。查看<a href="http://spark.apache.org/docs/latest/streaming-custom-receivers.html" title="Spark Streaming Custom Receivers" target="_blank" rel="external">Custom Receiver Guide</a>获取更多细节。</p>
<h3 id="Receiver-Reliability"><a href="#Receiver-Reliability" class="headerlink" title="Receiver Reliability"></a>Receiver Reliability</h3><p>基于数据源的可靠性，有两种类型的数据源。数据源（像Kafka和Flume）允许转入的数据被确认。如果系统从可靠数据源接收数据，它们能够正确的对接收到的数据进行确认，它能确保在任何失败情况下不会有数据的丢失。这导致了两种类型的receivers：</p>
<ul>
<li>Reliable Receiver - 一个可靠的receiver当数据被接收并以备份的方式存储到Spark中时，会正确的发送确认信息给可靠的数据源。</li>
<li>Unreliable Receiver - 一个不可靠的receiver不会给数据源发送确认信息。这可以用于那些不支持确认的数据源，或不想进入复杂确认逻辑的可靠数据源。</li>
</ul>
<p>如何编写一个可靠的receiver的细节我们将在<a href="http://spark.apache.org/docs/latest/streaming-custom-receivers.html" title="Spark Streaming Custom Receivers" target="_blank" rel="external">Custom Receiver Guide</a>中讨论。</p>
<h2 id="Transformations-on-DStream"><a href="#Transformations-on-DStream" class="headerlink" title="Transformations on DStream"></a>Transformations on DStream</h2><p>与RDDs类似，transformation允许来自输入DStream的数据被修改。DStream支持很多在一般的Spark的RDD上可用的transformation。一些常用的如下：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Transformation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">map(func)</td>
<td style="text-align:left">通过将源DStream中的每个数据项传递给一个函数func，而返回一个新的DStream</td>
</tr>
<tr>
<td style="text-align:left">flatMap(func)</td>
<td style="text-align:left">与map类似，但是每个输入项可以映射到0个或多个输出项</td>
</tr>
<tr>
<td style="text-align:left">filter(func)</td>
<td style="text-align:left">使用函数func对源DStream中的记录进行筛选，保留返回true的选项，最终返回一个新的DStream</td>
</tr>
<tr>
<td style="text-align:left">repartition(numPartitions</td>
<td style="text-align:left">通过创建更少或更多的paritions，在这个DStream中修改并行的级别</td>
</tr>
<tr>
<td style="text-align:left">union(otherStream)</td>
<td style="text-align:left">返回一个新的DStream，新的DStream包含源DStream和otherDStream联合的所有数据项</td>
</tr>
<tr>
<td style="text-align:left">count()</td>
<td style="text-align:left">通过对源DStream的每个RDD中数据项计数，返回一个单数据项RDDs的新的DStream</td>
</tr>
<tr>
<td style="text-align:left">reduce(func)</td>
<td style="text-align:left">使用一个函数func（它有两个参数，但是只有一个返回值）对源DStream的每个RDD中的数据项进行聚合，返回一个不重复数据项的RDDs的新的DStream</td>
</tr>
<tr>
<td style="text-align:left">countByValue()</td>
<td style="text-align:left">当在数据项类型为K的DStream上调用时，返回一个新的(K, Long)元组类型的DStream，其中value是key在源DStream的每个RDD中的频率</td>
</tr>
<tr>
<td style="text-align:left">reduceByKey(func, [numTasks]</td>
<td style="text-align:left">当在一个(K, V)类型的DStream上调用时，返回一个新的(K, V)类型的DStream，其中的value是使用函数func对每个key进行聚合的值。<strong>注意</strong>：默认情况，这将使用Spark的默认并行任务数（本地模式为2，在集群模式中，这个数量通过spark.default.parallelism来决定）进行分组。你可以传递一个可选参数numTasks来设置一个不同的任务数</td>
</tr>
<tr>
<td style="text-align:left">join(otherStream, [numTasks]</td>
<td style="text-align:left">当在(K, V)类型DStream和(K, W)类型DStream上调用时，返回一个(K, (V, W))类型的新的DStream。value是每个key所有的数据项的组合</td>
</tr>
<tr>
<td style="text-align:left">cogroup(otherStream, [numTasks]</td>
<td style="text-align:left">当在(K, V)类型DStream和(K, W)类型DStream上调用时，返回一个(K, Seq[V], Seq[W])元组类型的DStream</td>
</tr>
<tr>
<td style="text-align:left">transform(func)</td>
<td style="text-align:left">对源DStream的每个RDD应用RDD-to-RDD的函数来返回一个新的DStream。这可以用来在DStream上做任意的RDD操作</td>
</tr>
<tr>
<td style="text-align:left">updateStateByKey(func)</td>
<td style="text-align:left">返回一个新的”state”DStream，其中的state通过给定函数在每个key在之前state上进行更新得到的新值。这可以用于为每个key保存任意的状态数据</td>
</tr>
</tbody>
</table>
<p>这些transformation中的一些是值得更加详细的讨论的。</p>
<h3 id="UpdateStateByKey-Operation"><a href="#UpdateStateByKey-Operation" class="headerlink" title="UpdateStateByKey Operation"></a>UpdateStateByKey Operation</h3><p>updateStateBykey操作允许你在使用新的信息连续更新它时保存任意的状态信息。要使用这个，你有两个步骤来做：</p>
<ul>
<li>Define the state - 状态可以是任意的数据类型</li>
<li>Define the state update function - 指定一个函数来说明如何使用之前的状态和来自输入DStream的值来更新状态信息。</li>
</ul>
<p>在每个batch中，Spark将为所有存在的key应用状态更新函数，不管这些keys在batch中是否有新的数据。如果更新函数返回None，那么key-value对将会被消除。<br>我们使用一个例子进行阐明。假设你想要维持一个在文本数据流中遇到的字的统计。这里，状态是计数，是Integer类型。我这样定义更新函数：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span></span>(newValues: <span class="type">Seq</span>[<span class="type">Int</span>], runningCount: <span class="type">Option</span>[<span class="type">Int</span>]): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</div><div class="line">    <span class="keyword">val</span> newCount = ...  <span class="comment">// add the new values with the previous running count to get the new count</span></div><div class="line">    <span class="type">Some</span>(newCount)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这在一个包含字的DStream上应用（假设在之前例子中，pairs是包含(words, 1)对的DStream）。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> runningCounts = pairs.updateStateByKey[<span class="type">Int</span>](updateFunction _)</div></pre></td></tr></table></figure></p>
<p>更新函数将会为每个字进行调用，newValues是一个元素个数为1的序列(来自(word, 1)对)，runningCount是之前的数量。<br>注意，使用updateStateByKey，要求checkpoint目录已经配置，checkpoint目录我们会在checkpointing章节详细讨论。</p>
<h3 id="Tansform-Operation"><a href="#Tansform-Operation" class="headerlink" title="Tansform Operation"></a>Tansform Operation</h3><p>transform操作（它的变体transformWith也一样）允许任意的RDD-to-RDD函数被应用到一个DStream上。它能够被用于执行那些没有在DStream API中出现的任何RDD操作。例如，将一个数据流中的每个batch与另一个数据集进行join操作的功能没有直接在DStream API中出现。然而你可以使用transform轻松的实现它。它非常强大。例如，通过将预先计算的数据与输入数据流进行join来实时清理数据并基于数据进行过滤。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> spamInfoRDD = ssc.sparkContext.newAPIHadoopRDD(...) <span class="comment">// RDD containing spam information</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> cleanedDStream = wordCounts.transform(rdd =&gt; &#123;</div><div class="line">  rdd.join(spamInfoRDD).filter(...) <span class="comment">// join data stream with spam information to do data cleaning</span></div><div class="line">  ...</div><div class="line">&#125;)</div></pre></td></tr></table></figure></p>
<p>注意，这个功能将每个batch间隔调用一次。这允许你做根据时间变化的RDD操作，那就是，RDD操作、partitions的数量、广播变量等，能够在batches之间变动。</p>
<h3 id="Window-Operations"><a href="#Window-Operations" class="headerlink" title="Window Operations"></a>Window Operations</h3><p>Spark Streaming还提供了窗口化操作，这允许你在一个数据滑动窗口上应用transformations。下面的图阐述了滑动窗口。<br><img src="http://oaavtz33a.bkt.clouddn.com/streaming-dstream-window.png" alt="Streaming DStream window" title="Streaming DStream window"><br>正如在图中显示的，每个时刻窗口滑动一个源DStream，落入窗口的源RDDs被组合和操作生成窗口化DStream的RDDs。在这个特殊的例子中，这个操作被应用到最后三个时间单位的数据上，并且滑动了两个时间单位。这表示，任何窗口操作都需要指定两个参数。</p>
<ul>
<li>window length - 窗口的期间（图中为3）。</li>
<li>sliding interval - 窗口操作执行的间隔（图中为2）。<br>这两个参数必须是源DStream的batch间隔的倍数（图中为1）。<br>我们使用一个例子来阐明窗口操作。假设，你想要扩展之前的例子，每十秒钟生成最后30秒数据的字数统计。要达到这个目的，我们需要在最后30秒数据的(word, 1)类型DStream上应用reduceByKey操作。这通过使用操作reduceByKeyAndWindow来完成。<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Reduce last 30 seconds of data, every 10 seconds</span></div><div class="line"><span class="keyword">val</span> windowedWordCounts = pairs.reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; (a + b), <span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</div></pre></td></tr></table></figure>
</li>
</ul>
<p>一些常用的窗口操作如下。所有的这些操作都需要两个参数- windowLength和slideInterval。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Transformation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">window(windowLength, slideInterval)</td>
<td style="text-align:left">返回一个新的DStream，新的DStream是基于窗口化的源DStream的batches计算的</td>
</tr>
<tr>
<td style="text-align:left">countByWindow(windowLength, slideInterval)</td>
<td style="text-align:left">返回一个滑动窗口，记录流中数据项的个数</td>
</tr>
<tr>
<td style="text-align:left">reduceByWindow(fun, windowLength, slidenInterval)</td>
<td style="text-align:left">返回一个不重复数据项stream，通过在一个滑动间隔上使用func函数来聚合stream中的数据项进行创建。这个函数应该被组合或交换，因此能够正确的并行计算</td>
</tr>
<tr>
<td style="text-align:left">reduceByKeyAndWindow(func, windowLength, <br>slideInterval, [numTasks])</td>
<td style="text-align:left">当在一个(K, V)类型的DStream上调用时，返回一个新的(K, V)类型的DStream，其中value是为每个key使用给定的reduce函数func在一个滑动窗口中batch上聚合的值。<strong>注意</strong>默认情况下，这将使用Spark的默认并行任务数据（对于本地模式为2，对于集群模式这个数量由配置属性spark.default.parallelism来决定）来进行分组。你可以传递一个可选参数numTasks来设置一个不同数量的任务数</td>
</tr>
<tr>
<td style="text-align:left">reduceByKeyAndWindow(func, invFunc, windowLength, <br>slideInterval,[numTasks])</td>
<td style="text-align:left">一个比上面reduceByKeyAndWindow()更加有效的版本，其中每个窗口的reduce值使用前一个窗口的reduce值进行递增计算而得到。这是通过对进入滑动窗口的新的数据进行reducing完成的，并且对来开窗口的旧数据进行反转reducing。一个例子可能是对窗口划过的key的数量进行加法和减法。然而它只适用于“反转reduce函数”，就是这些函数要有一个“反转reduce”函数（如参数invFunc）。和reduceKeyAndWindow，reduce任务的数量可以通过一个可选参数来配置。注意，要使用这个操作，<a href="#checkpointing" title="checkpointing">checkpointing</a>必须启用。</td>
</tr>
<tr>
<td style="text-align:left">countByValueAndWindow(windowLength, slideInterval, <br>[numTasks])</td>
<td style="text-align:left">当在一个(K, V)类型的DStream上调用时，返回一个新的(K, Long)类型的DStream，其中value是每个key在滑动窗口内出现的频率。类似reeduceByKeyAndWindow，reduce任务的数量通过可选参数可以配置</td>
</tr>
</tbody>
</table>
<h3 id="Join-Operations"><a href="#Join-Operations" class="headerlink" title="Join Operations"></a>Join Operations</h3><p>最后，值得对如何在Spark Streaming中轻松执行不同类型的joins操作进行重点介绍。</p>
<h4 id="Stream-stream-joins"><a href="#Stream-stream-joins" class="headerlink" title="Stream-stream joins"></a>Stream-stream joins</h4><p>Streams能够很容易的和其他streams进行join。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> stream1: <span class="type">DStream</span>[<span class="type">String</span>, <span class="type">String</span>] = ...</div><div class="line"><span class="keyword">val</span> stream2: <span class="type">DStream</span>[<span class="type">String</span>, <span class="type">String</span>] = ...</div><div class="line"><span class="keyword">val</span> joinedStream = stream1.join(stream2)</div></pre></td></tr></table></figure></p>
<p>这里，在每个batch间隔中，由stream1生成的RDD将会与由stream2生成的RDD进行join。你还可以做leftOuterJoin、rightOuterJoin、fullOuterJoin。此外，在streams的窗口上做joins常常也是非常有用的。它相当容易，如下。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> windowedStream1 = stream1.window(<span class="type">Seconds</span>(<span class="number">20</span>))</div><div class="line"><span class="keyword">val</span> windowedStream2 = stream2.window(<span class="type">Minutes</span>(<span class="number">1</span>))</div><div class="line"><span class="keyword">val</span> joinedStream = windowedStream1.join(windowedStream2)</div></pre></td></tr></table></figure></p>
<h4 id="Stream-dataset-joins"><a href="#Stream-dataset-joins" class="headerlink" title="Stream-dataset joins"></a>Stream-dataset joins</h4><p>在解释DStream.transform操作的时候已经对此介绍过了。这里还有另外一个例子，这个例子使用一个dataset与一个窗口化的stream进行join。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> dataset: <span class="type">RDD</span>[<span class="type">String</span>, <span class="type">String</span>] = ...</div><div class="line"><span class="keyword">val</span> windowedStream = stream.window(<span class="type">Seconds</span>(<span class="number">20</span>))...</div><div class="line"><span class="keyword">val</span> joinedStream = windowedStream.transform &#123; rdd =&gt; rdd.join(dataset) &#125;</div></pre></td></tr></table></figure></p>
<p>实际上，你能够动态的变更你想要join的数据集。这个功能供transform来评估每个batch间隔，因此它将使用当前dataset引用的数据集。<br>DStream的整个transformations列表在API文档中是可用的。对于Scala API，参考<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream" title="DStream" target="_blank" rel="external">DStream</a>和<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.PairDStreamFunctions" title="PairDStreamFunctions" target="_blank" rel="external">PairDStreamFunctions</a>。对于Java API，查看<a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/streaming/api/java/JavaDStream.html" title="JavaDStream" target="_blank" rel="external">JavaDStream</a>和<a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/streaming/api/java/JavaPairDStream.html" title="JavaPairDStream" target="_blank" rel="external">JavaPairDStream</a>。对于Python API，查看<a href="http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.DStream" title="DStream" target="_blank" rel="external">DStream</a>。</p>
<h2 id="Output-Operations-on-DStream"><a href="#Output-Operations-on-DStream" class="headerlink" title="Output Operations on DStream"></a>Output Operations on DStream</h2><p>输出操作允许DStream的数据被推送到外部系统，比如一个数据库或一个文件系统。因为输出操作真正的允许被转换的数据被外部系统所消费，它们触发所有DStream transformations（和RDDs的actions类似）真正执行。当前，定义了如下输出操作：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Output Operation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">print()</td>
<td style="text-align:left">打印一个DStream中每个batch的前10个数据项到运行streaming application的driver节点。这对于部署和调试是非常有用的。<em>Python API</em> 在Python API需要调用pprint()</td>
</tr>
<tr>
<td style="text-align:left">saveAsTextFiles(prefix, [suffix])</td>
<td style="text-align:left">以文本文件保存这个DStream的内容。每个batch间隔的文件的名称基于prefix和suffix生成：prefix-TIME_IN_MS[.suffix]</td>
</tr>
<tr>
<td style="text-align:left">saveAsObjectFiles(refix, [fuffix])</td>
<td style="text-align:left">以序列化的Java对象的序列化文件保存DStream的内容。每个batch间隔文件的名称基于prefix和suffix生成：prefix-TIME_IN_MS[.suffix]。<em>Python API</em> 在Python API中不可用</td>
</tr>
<tr>
<td style="text-align:left">saveAsHadoopFiles(prefix, [suffix])</td>
<td style="text-align:left">以Hadoop文件保存这个DStream的内容。每个batch间隔的文件名称基于prefix和suffix生成：prefix-TIME_IN_MS[.suffix]。<em>Python API</em> 在Python API中不可用</td>
</tr>
<tr>
<td style="text-align:left">foreachRDD(func)</td>
<td style="text-align:left">最通用的输出操作器，它在由stream生成的每个RDD上应用一个函数func。这个函数应该将每个RDD的数据推送到外部系统，例如将RDD保存为文件，或通过网络将RDD写到数据库。注意，函数func在运行streaming application的driver进程中被执行，并且通常会有RDD actions在driver进程中来触发streaming RDDs的计算</td>
</tr>
</tbody>
</table>
<h3 id="Design-Patterns-for-using-foreachRDD"><a href="#Design-Patterns-for-using-foreachRDD" class="headerlink" title="Design Patterns for using foreachRDD"></a>Design Patterns for using foreachRDD</h3><p>dstream.foreachRDD是一个强大的方法的，它允许数据被发送到外部系统。然而，例如如何正确的、有效的使用这个方法是很重要的。一些常见可避免的错误如下。<br>通常写数据到外部系统需要创建一个连接对象（例如，TCP连接到远程服务器）并使用连接对象发送数据到远程系统。为了这个目的，开发者可能不经意的尝试在Spark driver上创建一个连接对象，然后尝试在Spark worker中使用它来保存RDDs中的记录。例如（使用Scala语言），<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">dstream.foreachRDD &#123; rdd =&gt;</div><div class="line">  <span class="keyword">val</span> connection = createNewConnection()  <span class="comment">// executed at the driver</span></div><div class="line">  rdd.foreach &#123; record =&gt;</div><div class="line">    connection.send(record) <span class="comment">// executed at the worker</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这是不正确的，因为这要求连接对象被序列化，并从driver发送给worker。而这样的连接对象很难跨机器传输。这个错误可能表现为序列化错误（连接对象不能序列化）、初始化错误（连接对象需要在workers上被初始化），等。正确的解决方法是在worker上创建连接对象。</p>
<p>然而，这回导致另一个常见错误 - 为每一条记录创建一个连接。例如：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">dstream.foreachRDD &#123; rdd =&gt;</div><div class="line">  rdd.foreach &#123; record =&gt;</div><div class="line">    <span class="keyword">val</span> connection = createNewConnection()</div><div class="line">    connection.send(record)</div><div class="line">    connection.close()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>通常，创建一个连接对象需要时间和资源的开销。因此，为每条记录创建和销毁一个连接对象会引发不必要的过高负载并大大的降低系统整体的吞吐量。一个比较好的解决方法是使用rdd.foreachPartition - 创建单个连接，并使用这个连接发送一个RDD partition中的所有记录。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">dstream.foreachRDD &#123; rdd =&gt;</div><div class="line">  rdd.foreachPartition &#123; partitionOfRecords =&gt;</div><div class="line">    <span class="keyword">val</span> connection = createNewConnection()</div><div class="line">    partitionOfRecords.foreach(record =&gt; connection.send(record))</div><div class="line">    connection.close()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这将在很多记录上平摊连接创建的开销。<br>最后，通过跨多个RDDs/batches重用连接，能够进一步优化。一种方法是维持一个静态连接池，可以被多个要被推送到外部系统的batches的RDDs使用，因此能偶进一步降低负载。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">dstream.foreachRDD &#123; rdd =&gt;</div><div class="line">  rdd.foreachPartition &#123; partitionOfRecords =&gt;</div><div class="line">    <span class="comment">// ConnectionPool is a static, lazily initialized pool of connections</span></div><div class="line">    <span class="keyword">val</span> connection = <span class="type">ConnectionPool</span>.getConnection()</div><div class="line">    partitionOfRecords.foreach(record =&gt; connection.send(record))</div><div class="line">    <span class="type">ConnectionPool</span>.returnConnection(connection)  <span class="comment">// return to the pool for future reuse</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>注意，在连接池中的连接应该是后台进程延迟创建的，并且如果一段时间不用，应该可以超时。这实现了更加有效的发送数据到外部系统。<br><strong>其它需要记住的：</strong></p>
<ul>
<li>DStream通过输出操作是懒执行的，像RDDs被RDD actions懒执行一样。明确的说，DStream输出操作中的RDD action触发了被接收到的数据的处理。因此，如果你的application没有任何输出操作，或者有dstream.foreachRDD()这样的输出操作，但是其中没有任何RDD action，也是不会有任何东西执行的。系统将会简单接收数据然后丢掉。</li>
<li>默认情况下，输出操作同一时刻只执行一个。它们以它们在application中定义的顺序被执行。</li>
</ul>
<h2 id="Accumulators-and-Broadcast-Variables"><a href="#Accumulators-and-Broadcast-Variables" class="headerlink" title="Accumulators and Broadcast Variables"></a>Accumulators and Broadcast Variables</h2><p><a href="http://spark.apache.org/docs/latest/programming-guide.html#accumulators" title="Accumulators" target="_blank" rel="external">Accumulators</a>和<a href="http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables" title="Broadcast variables" target="_blank" rel="external">Broadcast variables</a>在Spark Streaming不能够从checkpint中恢复。如果你启用了checkpoint，同时还使用了Accumulators或Broadcast variables，你将必须为Accumulators和Broadcast variables创建一个懒实例化的单实例，那么它们能够在driver因失败而重启后重新被实例化。下面的例子展示了这个功能。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordBlacklist</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> instance: <span class="type">Broadcast</span>[<span class="type">Seq</span>[<span class="type">String</span>]] = <span class="literal">null</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getInstance</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">Broadcast</span>[<span class="type">Seq</span>[<span class="type">String</span>]] = &#123;</div><div class="line">    <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</div><div class="line">      synchronized &#123;</div><div class="line">        <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</div><div class="line">          <span class="keyword">val</span> wordBlacklist = <span class="type">Seq</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>)</div><div class="line">          instance = sc.broadcast(wordBlacklist)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    instance</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">DroppedWordsCounter</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> instance: <span class="type">LongAccumulator</span> = <span class="literal">null</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getInstance</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">LongAccumulator</span> = &#123;</div><div class="line">    <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</div><div class="line">      synchronized &#123;</div><div class="line">        <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</div><div class="line">          instance = sc.longAccumulator(<span class="string">"WordsInBlacklistCounter"</span>)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    instance</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">wordCounts.foreachRDD &#123; (rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)], time: <span class="type">Time</span>) =&gt;</div><div class="line">  <span class="comment">// Get or register the blacklist Broadcast</span></div><div class="line">  <span class="keyword">val</span> blacklist = <span class="type">WordBlacklist</span>.getInstance(rdd.sparkContext)</div><div class="line">  <span class="comment">// Get or register the droppedWordsCounter Accumulator</span></div><div class="line">  <span class="keyword">val</span> droppedWordsCounter = <span class="type">DroppedWordsCounter</span>.getInstance(rdd.sparkContext)</div><div class="line">  <span class="comment">// Use blacklist to drop words and use droppedWordsCounter to count them</span></div><div class="line">  <span class="keyword">val</span> counts = rdd.filter &#123; <span class="keyword">case</span> (word, count) =&gt;</div><div class="line">    <span class="keyword">if</span> (blacklist.value.contains(word)) &#123;</div><div class="line">      droppedWordsCounter.add(count)</div><div class="line">      <span class="literal">false</span></div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">  &#125;.collect().mkString(<span class="string">"["</span>, <span class="string">", "</span>, <span class="string">"]"</span>)</div><div class="line">  <span class="keyword">val</span> output = <span class="string">"Counts at time "</span> + time + <span class="string">" "</span> + counts</div><div class="line">&#125;)</div></pre></td></tr></table></figure></p>
<p>查看完整的<a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala" title="RecoverableNetworkWordCount.scala" target="_blank" rel="external">源代码</a>。</p>
<h2 id="DataFrame-and-SQL-Operations"><a href="#DataFrame-and-SQL-Operations" class="headerlink" title="DataFrame and SQL Operations"></a>DataFrame and SQL Operations</h2><p>你能够在streaming data上轻松的使用<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" title="Spark SQL, DataFrames and Datasets Guide" target="_blank" rel="external">DataFrames and SQL</a>。你需要创建使用SparkContext创建一个SparkSession，SparkContext是StreamingContext使用的那个。而且这是必须要做的，因此在driver失败后能够重启。这是通过创建一个懒实例化的SparkSession单实例来完成。这在下面的例子中展示了。它修改了之前word count example，使用DataFrames and SQL来生成字的数量。每个RDD被转变为一个DataFrame，作为一个临时表进行注册，然后使用SQL查询。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** DataFrame operations inside your streaming program */</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> words: <span class="type">DStream</span>[<span class="type">String</span>] = ...</div><div class="line"></div><div class="line">words.foreachRDD &#123; rdd =&gt;</div><div class="line"></div><div class="line">  <span class="comment">// Get the singleton instance of SparkSession</span></div><div class="line">  <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder.config(rdd.sparkContext.getConf).getOrCreate()</div><div class="line">  <span class="keyword">import</span> spark.implicits._</div><div class="line"></div><div class="line">  <span class="comment">// Convert RDD[String] to DataFrame</span></div><div class="line">  <span class="keyword">val</span> wordsDataFrame = rdd.toDF(<span class="string">"word"</span>)</div><div class="line"></div><div class="line">  <span class="comment">// Create a temporary view</span></div><div class="line">  wordsDataFrame.createOrReplaceTempView(<span class="string">"words"</span>)</div><div class="line"></div><div class="line">  <span class="comment">// Do word count on DataFrame using SQL and print it</span></div><div class="line">  <span class="keyword">val</span> wordCountsDataFrame = </div><div class="line">    spark.sql(<span class="string">"select word, count(*) as total from words group by word"</span>)</div><div class="line">  wordCountsDataFrame.show()</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>查看完整<a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala" title="SqlNetworkWordCount" target="_blank" rel="external">源代码</a><br>你还能够从不同的线程中（异步运行StreamingContext）对定义在streaming数据上的tables执行SQL查询。只要确保设置StreamingContext能够记住足够的streaming数据，以便查询执行。否则StreamingContext无法察觉异步SQL查询，将会在查询完成之前将旧的streaming数据删除。例如，如果你想查询最后的batch，但是你的查询需要花费5分钟，那么调用streamingContext.remember(Minutes(5))（在Scala，其他语言也一样）。<br>查看<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" title="Spark SQL, DataFrames and Datasets Guide" target="_blank" rel="external">DataFrames and SQL</a>指南，学习关于DataFrames的更多信息。</p>
<h2 id="MLlib-Operations"><a href="#MLlib-Operations" class="headerlink" title="MLlib Operations"></a>MLlib Operations</h2><p>你还可以轻松的使用由<a href="http://spark.apache.org/docs/latest/ml-guide.html" title="Machine Learning Library (MLlib) Guide" target="_blank" rel="external">MLlib</a>提供的机器学习算法。首先，这些streaming机器学习算法（如 <a href="http://spark.apache.org/docs/latest/mllib-linear-methods.html#streaming-linear-regression" title="Streaming linear regression" target="_blank" rel="external">Streaming Linear Regression</a>, <a href="http://spark.apache.org/docs/latest/mllib-clustering.html#streaming-k-means" title="Streaming k-means" target="_blank" rel="external">Streaming KMeans</a>等）能够从streaming数据学习的同时在这些streaming数据上应用。除了这些，对于一些较大的机器学习算法类，你可以离线训练学习模型（使用历史数据）然后对在线的数据进行应用。查看<a href="http://spark.apache.org/docs/latest/ml-guide.html" title="Machine Learning Library (MLlib) Guide" target="_blank" rel="external">MLlib</a>指南获取更多细节。</p>
<h2 id="Caching-Persistence"><a href="#Caching-Persistence" class="headerlink" title="Caching / Persistence"></a>Caching / Persistence</h2><p>与RDDs相似，DStream允许开发者将stream的数据保存到内存中。在一个DStream上使用persist()方法会自动的将对应DStream的每个RDD保存到内存中。如果DStream中的数据会被计算多次（在相同数据上计算多次），这将是非常有用的。对于基于窗口的操作（像reduceByWindow和reduceByKeyAndWindow）和基于状态的操作（像updateStateByKey），隐含是自动保存的。因此，通过基于窗口的操作生成的DStream将自动保存到内存中，而不需要开发者调用persist()方法。<br>对于通过网络（如Kafka、Flume、socket等）接收数据的输入stream，为了容灾，将设置默认存储级别，将数据复制到两个节点。<br>注意，和RDDs不同，DStream的默认存储级别会将数据序列化到内存中。在<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#memory-tuning" title="Memory Tuning" target="_blank" rel="external">Performance Tuning</a>章节中会进一步讨论。不同存储级别的更多信息可以在<a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence" title="RDD Persistence" target="_blank" rel="external">Spark Programming Guide</a>中找到。</p>
<h2 id="Checkpointing"><a href="#Checkpointing" class="headerlink" title="Checkpointing"></a>Checkpointing</h2><p>一个streaming application必须是7 * 24小时的运行，因此必须有能力面那些对与application逻辑无关（如系统故障、JVM crash等）的故障的能力。为了达到这个目的，Spark Streaming需要对一个容灾存储系统checkpoint足够的信息，这样它才能从故障中恢复。有两种类型的数据需要被checkpoint。</p>
<ul>
<li>Metadata checkpointing - 保存定义streaming计算的信息到容才存储中（例如hDFS）。这用于从运行streaming application driver的节点的故障中恢复（稍后详细讨论）。元数据包括：<ul>
<li>Configuration - 用于创建streaming application的配置。</li>
<li>DStream operations - 定义streaming application的DStream操作集合。</li>
<li>Incomplete batches - 放到队列但还没有完成的job所属Batches。</li>
</ul>
</li>
<li>Data checkpointing - 保存生成的RDDs到可靠存储。在一些跨多个batches组合数据的状态化transformations中，这是必须的。在这样的transformations中，生成的RDDs依赖于RDDs之前的batches，这会引发依赖链的长度随着时间而增长。要避免在恢复时刻无尽的增长（与时间成正比），状态化的transformations的中间RDDs周期性的被checkpoint到可靠存储中（如HDFS）来截断依赖链。</li>
</ul>
<p>作为总结，元数据checkpointing对于从driver故障中恢复是主要需要的，然而数据或RDD checkpoing对于基本功能是必须的，如果状态化transformations被使用。</p>
<h3 id="When-to-enable-Checkpointing"><a href="#When-to-enable-Checkpointing" class="headerlink" title="When to enable Checkpointing"></a>When to enable Checkpointing</h3><p>有任何如下需求的application，都需要启用checkpointing:</p>
<ul>
<li>Useage of stateful transformation - 如果updateStateByKey或reduceByKeyAndWindow在application中被使用，那么checkpoint目录必须被设置来允许周期性的RDD checkpint。</li>
<li>Recovering from failures of driver running the application - 元数据被用于恢复进程信息。<br>注意，那些没有没有使用之前提到的状态化transformation的简单的streaming application能够在没有启用checkpointing情况下运行。在这个例子中从driver故障中恢复也是局部的（一些已经接收到但未处理的数据会丢失）。这通常是可以接收的，并且很多Spark Streaming application也是以这种方式运行的。对于非Hadoop环境的支持，期望未来能够支持。<h3 id="How-to-configure-Checkpointing"><a href="#How-to-configure-Checkpointing" class="headerlink" title="How to configure Checkpointing"></a>How to configure Checkpointing</h3>通过在一个容灾的、可靠的文件系统中（如HDFS、S3等）设置一个目录来启用Checkpinting，这个目录将用于checkpoint信息的保存。这是通过使用streamingContext.checkpoint(checkpointDirectory)来完成的。这将允许你使用之前说的状态化transformation。另外，如果你想要从driver故障中恢复application信息，你应该重写你的streaming application，使其拥有如下行为：</li>
<li>当你的程序是第一次启动时，它会创建一个新的StreamingContext，装配所有的streams，然后调用start()方法。</li>
<li>当你的程序是在driver故障后被重启，它应该根据checkpoint目录中的checkpoint数据重新创建一个StreamingContext。</li>
</ul>
<p>这些行为可以通过使用StreamingContext.getOrCreate来轻松完成。如下使用：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Function to create and setup a new StreamingContext</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">functionToCreateContext</span></span>(): <span class="type">StreamingContext</span> = &#123;</div><div class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(...)   <span class="comment">// new context</span></div><div class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(...) <span class="comment">// create DStreams</span></div><div class="line">    ...</div><div class="line">    ssc.checkpoint(checkpointDirectory)   <span class="comment">// set checkpoint directory</span></div><div class="line">    ssc</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Get StreamingContext from checkpoint data or create a new one</span></div><div class="line"><span class="keyword">val</span> context = <span class="type">StreamingContext</span>.getOrCreate(checkpointDirectory, functionToCreateContext _)</div><div class="line"></div><div class="line"><span class="comment">// Do additional setup on context that needs to be done,</span></div><div class="line"><span class="comment">// irrespective of whether it is being started or restarted</span></div><div class="line">context. ...</div><div class="line"></div><div class="line"><span class="comment">// Start the context</span></div><div class="line">context.start()</div><div class="line">context.awaitTermination()</div></pre></td></tr></table></figure></p>
<p>如果checkpointDirectory目录存在，那么将根据checkpoint数据重新创建context。如果目录不存在（例如第一次运行），那么将会调用functionToCreateContext函数来创建一个新的context并建立DStream。查看Scala的例子<a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala" title="RecoverableNetworkWordCount" target="_blank" rel="external">RecoverableNetworkWordCount</a>。这个例子将网络数据的字数统计添加到一个文件中。<br>另外，要使用getOrCreate，还需要确保driver进程在遇到故障后自动重启。这可以通过运行application的部署架构来完成。这在<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#deploying-applications" title="deploying application" target="_blank" rel="external">Deployment</a>章节中进一步讨论。</p>
<h2 id="Deploying-Applications"><a href="#Deploying-Applications" class="headerlink" title="Deploying Applications"></a>Deploying Applications</h2><p>这一章我们讨论部署一个Spark Streaming application的步骤。</p>
<h3 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h3><p>要运行一个Spark Streaming application，你有如下事情要做。</p>
<ul>
<li>cluster with cluster manager - 这是任何Spark application的普遍要求。并且已经在<a href="http://spark.apache.org/docs/latest/cluster-overview.html" title="Cluster Mode Overview" target="_blank" rel="external">deployment guide</a>中讨论了。</li>
<li>package the application JAR - 你需要将你的streaming application编译到一个JAR中。如果你使用spark-submit来启动application，那么你不需要在JAR中提供Spark和Spark Streaming。然而，如果你的application使用了<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#advanced-sources" title="Advanced Sources" target="_blank" rel="external">advanced sources</a>（例如Kafka，Flume），那么你需要将链接到的额外坐标进行打包，将application的依赖打入用于部署application的JAR。例如，一个application使用了KafkaUtils，将需要包含spark-streaming-kafka-0_8.2.11以及所有它涉及的依赖到这个application的JAR中。</li>
<li>configuring sufficient memory for the executors - 因为接收到的数据必须保存到内存中，因此executors必须配置足够的内存来保存接收到的数据。注意，如果你要做10分钟的窗口操作，那么系统必须至少保存最后10分钟的数据在内存中。因此，application需要的内存依赖于application中使用的操作。</li>
<li>configuring checkpointing - 如果stream application需要它，那么在符合Hadoop API的容灾存储中（HDFS、S3等）必须配置一个目录作为checkpoint目录，streaming application以一种方式写入checkpoint信息，这些信息可以用于恢复故障。查看<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing" title="Checkpointing" target="_blank" rel="external">Checkpointing</a>章节获取更多信息。</li>
<li>configuring automatic restart of the application driver - 要自动的从一个driver故障中恢复，那么用来运行streaming application的部署基础架构必须进行监控并在它故障的时候重启动。不同的<a href="http://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types" title="cluster manager types" target="_blank" rel="external">cluster managers</a>有不同的工具来完成。<ul>
<li>Spark Standalone - 一个Spark application driver能够被提交并运行在Spark Standalone cluster中（查看<a href="http://spark.apache.org/docs/latest/spark-standalone.html#launching-spark-applications" title="launching Spark Applications" target="_blank" rel="external">cluster deploy mode</a>），那就是application driver自己运行在worker节点中的一个上。此外，Standalone集群管理器被引入来监督driver，如果driver以非0代码退出而失败时或在运行driver的节点故障时重启driver。在<a href="http://spark.apache.org/docs/latest/spark-standalone.html" title="Spark Standalone Mode" target="_blank" rel="external">Spark Standalone guide</a>中查看集群模式和集群监督来获取更多信息。</li>
<li>YARN - Yarn对自动重启一个application的支持方式类似。请参考YARN文档获取更多信息。</li>
<li>Mesos - <a href="https://github.com/mesosphere/marathon" title="marathon" target="_blank" rel="external">Marathon</a>被用来在Mesos中解决自动重启。</li>
</ul>
</li>
<li>configuring write ahead logs - 从Spark1.2开始，为了实现强壮的容灾保证，我们引入了write ahead logs。如果启用，所有从receiver接收到的数据写入到ahead日志中，ahead日志位于配置的checkpoint目录中。这防止在driver恢复时数据丢失，因此能够确保零数据丢失（将在<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-semantics" title="Fault-tolerance Semantics" target="_blank" rel="external">Fault-tolerance Semantics</a>章节中讨论）。通过设置<a href="http://spark.apache.org/docs/latest/configuration.html#spark-streaming" title="Spark Streaming" target="_blank" rel="external">配置参数</a>spark.streaming.receiver.writeAheadLog.enable为true来启用该功能。然而这个强壮的语义可能会带来个别receivers接收吞吐量的代价。通过执行<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#level-of-parallelism-in-data-receiving" title="Level of Parallelism in Data Receiving" target="_blank" rel="external">more receivers in parallel</a>来增加平均吞吐量来得到修正。另外，推荐当写ahead日志被启用时，禁用Spark中接收到数据的响应，因为日志已经存储到了可靠的存储系统中。这可以通过设置输入stream的存储级别为StorageLevel.MEMORY_AND_DISK_SER来完成。然而当为ahead日志使用S3（或任何不支持flush的文件系统）时，请记住启用spark.streaming.dirver.writeAheadLog.closeFileAfterWrite和spark.streaming.receiver.writeAheadLog.closeFileAfterWrite。查看<a href="http://spark.apache.org/docs/latest/configuration.html#spark-streaming" title="Spark Streaming" target="_blank" rel="external">Spark Streaming Configuration</a>获取更多信息。</li>
<li>setting the max receiving rate - 如果集群数据源不足够大，以便让streaming application的数据处理速度跟数据被接收的速度一样快，这些receiver能够通过设置一个最大速度来进行限制，限制每秒的记录数。查看<a href="http://spark.apache.org/docs/latest/configuration.html#spark-streaming" title="Spark Streaming" target="_blank" rel="external">configuration parameters</a>spark.streaming.receiver.maxRate（为receivers）和spark.streaming.kafka.maxRatePerPartition（为直接使用Kafka的方法）。在Spark 1.5中，我们还引入了一个特征叫做backpressure，它淘汰了设置速度限制的需要，它能够自动指出速度限制值并如果处理的条件变化了能够自动调整。通过配置参数spark.streaming.backpressure.enabled为true来启用backpressure。</li>
</ul>
<h3 id="Upgrading-Application-Code"><a href="#Upgrading-Application-Code" class="headerlink" title="Upgrading Application Code"></a>Upgrading Application Code</h3><p>如果一个运行中的Spark Streaming application需要使用新的代码来更新，有两种可行的机制。</p>
<ul>
<li>更新的Spark Streaming application被启动并且和为更新的application并行运行。一旦新的application（和旧的application一样接收相同的数据）已经热启动，就已经准备就绪了，那么旧的application就可以关闭了。注意，这能够用于那些数据源支持将数据发送到两个目的地（之前的application和更新后的application）的数据源，而且是依赖于这种数据源来完成的。</li>
<li>将正在运行的application优雅的关闭（查看<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext" title="StreamingContext" target="_blank" rel="external">StreamingContext.stop(…)</a>或<a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html" title="JavaStreamingContext" target="_blank" rel="external">JavaStreamingContext.stop(…)</a>获取优雅关闭选项），这样能够确保在关闭之前所有接收到的数据都得到处理。然后可以启动更新的application，这样能够在之前application中止的位置开始处理。注意，这种处理方式只能对输入数据源支持数据源端支持缓存（如Kafka和Flume）的有效，因为数据需要在之前application关闭且更新的application还未启动期间进行缓存。根据之前未更新代码的checkpoint信息重启不能够成功。checkpoint信息本质上包含了序列化的Scala/Java/Python对象，尝试使用新的修改后的类来反序列化可能会导致错误。在这种情况中，使用一个不同的checkpoint目录来启动新的application，或将之前的checkpoint目录删除掉。</li>
</ul>
<h2 id="Monitoring-Applications"><a href="#Monitoring-Applications" class="headerlink" title="Monitoring Applications"></a>Monitoring Applications</h2><p>除了Spark的<a href="http://spark.apache.org/docs/latest/monitoring.html" title="Monitoring and Instrumentation" target="_blank" rel="external">monitoring capabilities</a>，针对Spark Streaming还有一些附加的能力。当使用一个StreamingContext时，<a href="http://spark.apache.org/docs/latest/monitoring.html#web-interfaces" title="Web Interfaces" target="_blank" rel="external">Spark web UI</a>额外展示一个streaming的tab页，用来展示正在运行的receivers（receivers是否活跃，接收到的记录数，receiver的错误等）和完成的batches（batch处理时间，队列延迟等）的统计信息。这能够用于监控streaming application的进程。<br>web UI中的如下两个metrics是相当重要的：</p>
<ul>
<li>Processing Time - 处理每个数据batch的时间。</li>
<li>Scheduling Delay - 一个batch在队列中等待前面batches处理完成所需时间。<br>如果batch处理时间一直多于batch间隔，并且队列排队延迟保持增长，那么它表示系统对batch的处理速度跟不上batch的生成速度并且会落后。在这种情况中，需要考虑<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#reducing-the-batch-processing-times" title="Reducing the Batch Processing Times" target="_blank" rel="external">降低</a>batch的处理时间。<br>使用<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.scheduler.StreamingListener" title="StreamingListener" target="_blank" rel="external">StreamingListener</a>接口，一个Spark Streaming程序的进程也能够被监控，这个接口使你能够得到receiver状态和处理时间。注意，这是一个开发者API，之后很有可能会在此基础上进行优化（报告更多的信息）。</li>
</ul>
<h1 id="Performance-Tuning"><a href="#Performance-Tuning" class="headerlink" title="Performance Tuning"></a>Performance Tuning</h1><p>获得一个Spark Streaming application在集群上的最好性能需要一些调整。这一章会解释一定数量的参数和配置，用来调整以提高你的application的性能。在高级别，你需要考虑两个事情：<br>1、通过有效的使用集群资源来降低数据每个batch的处理时间。<br>2、设置正确的batch大小，数据的batch设置的大小以它们的处理速度和它们的接收速度一样快（数据的处理跟得上数据的接收）。</p>
<h2 id="Reducing-the-Batch-Processing-Times"><a href="#Reducing-the-Batch-Processing-Times" class="headerlink" title="Reducing the Batch Processing Times"></a>Reducing the Batch Processing Times</h2><p>这里有一些优化可以在Spark中完成以减少每个batch的处理时间。这些已经在<a href="http://spark.apache.org/docs/latest/tuning.html" title="Tuning Spark" target="_blank" rel="external">Tuning Guide</a>中讨论过了。这一章我们聚焦一些更加重要的。</p>
<h3 id="Level-of-Parallelism-in-Data-Receiving"><a href="#Level-of-Parallelism-in-Data-Receiving" class="headerlink" title="Level of Parallelism in Data Receiving"></a>Level of Parallelism in Data Receiving</h3><p>跨网络接收数据（如Kafka、Flume、socket等）要求数据被解序列化并存储在Spark中。如果数据接收成为系统的瓶颈，那么需要考虑并行接收数据。注意，每个输入DStream创建单个receiver（运行在一个worker机器上），它接收单个数据流。通过创建多个输入流并配置它们接收来自数据源的数据流的不同partitions来实现接收多个数据流。例如，单个Kafka输入DStream接收数据的两个topic，能够拆分到两个Kafka输入流，每个输入流接收一个topic。这需要运行两个receivers，允许数据并行接收，因此增加了整体的吞吐量。这多个DStreams能够联合在一起来创建单个DStream。那么用于单个输入DStream的transformation能够被应用到统一的stream上。这如下完成。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> numStreams = <span class="number">5</span></div><div class="line"><span class="keyword">val</span> kafkaStreams = (<span class="number">1</span> to numStreams).map &#123; i =&gt; <span class="type">KafkaUtils</span>.createStream(...) &#125;</div><div class="line"><span class="keyword">val</span> unifiedStream = streamingContext.union(kafkaStreams)</div><div class="line">unifiedStream.print()</div></pre></td></tr></table></figure></p>
<p>另外一个需要考虑的参数是receiver的阻塞间隔，由配置参数spark.streaming.blockInterval所决定。对于大多数的receiver，接收到的数据在存储到Spark的内存中之前会合并到数据块中。每个batch中的数据块的数量由tasks的数量决定，这些tasks用于处理（类似map这样的transformation）接收到的数据。每个batch每个receiver的任务数是近似的（batch interval / block interval）。例如，200毫秒的block间隔，每个2秒的batches将创建10个tasks。如果tasks的数量太低（假如，比每个机器的core的数量少），那么它将是低效的，因为所有可用的core没有全部用于处理数据。要为给定的batch间隔增加tasks的数量，可以降低block间隔。然而，推荐的最小block间隔值为50毫秒，低于这个值，task的启动负载将是一个问题。<br>以多输入流/recievers来接收数据的另一个选择是复制一定的输入数据流（使用inputStream.repartition(<number of="" partitions="">)）。这会在进一步处理数据之前在集群中跨指定数量的机器上分发接收到的数据batches。</number></p>
<h3 id="Level-of-Parallelism-in-Data-Processing"><a href="#Level-of-Parallelism-in-Data-Processing" class="headerlink" title="Level of Parallelism in Data Processing"></a>Level of Parallelism in Data Processing</h3><p>在计算的任何阶段，如果并行的tasks的数量不是足够高的，那么集群的资源利用率很低。例如，对于分布式的reduce操作（像reduceByKey和reduceByKeyAndWindow），默认的并行任务数有spark.default.parallelism<a href="http://spark.apache.org/docs/latest/configuration.html#spark-properties" title="Spark Properties" target="_blank" rel="external">配置属性</a>控制。你能够传递一个并行级别作为参数（查看<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.PairDStreamFunctions" title="PairDStreamFunctions" target="_blank" rel="external">PairDStreamFunctions</a>文档）或设置spark.default.parallelism配置参数来修改默认值。</p>
<h3 id="Data-Serialization"><a href="#Data-Serialization" class="headerlink" title="Data Serialization"></a>Data Serialization</h3><p>通过调整序列化格式能够降低数据序列化的负载。在streaming的情况中，有两种类型的数据被序列化。</p>
<ul>
<li>input data：默认，通过Receivers接收到的输入数据以<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.storage.StorageLevel$" title="StorageLevel" target="_blank" rel="external">StorageLevel.MEMORY_AND_DISK_SER_2</a>存储到executor的内存中。那样，数据被序列化到字节中以降低GC开销，并且为了对executor进行容灾而备份存储。同样的，数据首先保存在内存中，只有当内存没有足够的空间为streaming计算保存所有的输入数据时才会将数据溢出到磁盘。序列化显然会有负载 - receiver必须对接收到的数据进行解序列化，然后使用Spark的序列化格式重新序列化。</li>
<li>Persisted RDDs generated by Streaming Operations：通过streaming计算生成的RDDs可能会保存到内存中。例如，窗口操作保存数据到内存中，因为这些数据将被处理多次。然而，和Spark Core的默认级别StorageLevel.MEMORY_ONLY不同，有streaming计算生成的RDDs默认以StorageLevel.MEMORY_ONLY_SER进行保存，以减少GC负载。</li>
</ul>
<p>在上面两种情况中，使用Kryo序列化能够降低CPU和内存的开销。查看<a href="http://spark.apache.org/docs/latest/tuning.html#data-serialization" title="Data Serialization" target="_blank" rel="external">Spark Tuning Guide</a>获取更多详情。对于Kryo，考虑到注册自定义类禁用对象引用跟踪（在<a href="http://spark.apache.org/docs/latest/configuration.html#compression-and-serialization" title="Compression and Serialization" target="_blank" rel="external">Configuration Guide</a>中查看Kryo-related配置）。<br>在那些需要为streaming application保存的数量不大的特殊情况中，以非序列化对象来保存数据可能可以，而且不会导致过多的GC负载。例如，如果你是用了几秒的batch间隔并且没有窗口操作，那么你能够通过明确的设置相应的存储级别来禁用序列化保存数据。这将会减少在序列化期间的cpu开销，没有太多的GC开销则潜在的提高了执行性能。</p>
<h3 id="Task-Launching-Overheads"><a href="#Task-Launching-Overheads" class="headerlink" title="Task Launching Overheads"></a>Task Launching Overheads</h3><p>如果每秒启动的tasks数很高（假设每秒50个或更多），那么发送tasks给slave的负载可能是很显著的，并且使它很难实现秒内的延迟。这个负载能够通过如下的变更来降低：</p>
<ul>
<li>Execution mode：以Standalone或coarse-grained Mesos模式运行Spark会比fine-grained Mesos模式的任务启动时间更好。请参考【Running on Mesos guide](<a href="http://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/running-on-mesos.html</a> “Running Spark on Mesos”)获取更多详情。</li>
</ul>
<p>这些变更可能会降低batch处理时间（毫秒的百倍），因此允许秒内的batch大小。</p>
<h2 id="Setting-the-Right-Batch-Interval"><a href="#Setting-the-Right-Batch-Interval" class="headerlink" title="Setting the Right Batch Interval"></a>Setting the Right Batch Interval</h2><p>为了让运行在集群上的Streaming application稳定，系统处理数据的速度应该和接收数据的速度一样快。换句话说，数据batches的处理速度应该和它们的生成速度一样快。对于一个application是否是这样，可以在streaming web UI中通过<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#monitoring-applications" title="Monitoring Applications" target="_blank" rel="external">monitoring</a>处理时间来找到，其中batch处理时间应该小于batch间隔时间。<br>依赖于streaming计算的本质，使用的batch间隔可能在数据速率上有重大的影响，它能够被application在一组固定的集群数据源上所维持。例如，我们考虑一下之前WordCountNetwork的例子。对于一个特定的数据速率，系统可能能够维持每两秒报告一次字数的统计（batch间隔为2秒），而每500毫秒却不行。因此batch间隔需要被设置符合期望的数据生成速率以便持续。<br>一个为你的application指出正确batch的size的好方法是使用一个保守的batch间隔（假设5-10秒）和一个低的数据速率来测试。要核实系统是否能够跟得上数据的速度，你能够通过每个被处理的batch（在Spark driver log4j的日志中查找”total delay”或使用<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.scheduler.StreamingListener" title="StreamingListener" target="_blank" rel="external">StreamingListener</a>接口）检查端对端延迟的值。如果这个延迟能够比得上batch的size，那么这个系统是稳定的。否则，如果这个延迟连续增长，这意味着系统不能够跟得上，因此是不稳定的。一旦你有了一个稳定的配置，你能够尝试增加数据速度或降低batch的size。注意临时数据速率增长期间的延迟短暂增长是没有问题的，只要延迟能够降低回一个低的值（小于batch的size）。</p>
<h2 id="Memory-Tuning"><a href="#Memory-Tuning" class="headerlink" title="Memory Tuning"></a>Memory Tuning</h2><p>调整Spark内存的使用和Spark GC行为已经在<a href="http://spark.apache.org/docs/latest/tuning.html#memory-tuning" title="Memory Tuning" target="_blank" rel="external">Tuning Guide</a>中已经讨论的非常详细了。强烈推荐你读它。在本章中，我们讨论一些针对Spark Streaming applications的context中的参数。<br>一个Spark Streaming application需要的集群内存总数依赖于使用的transformation。例如，如果你想要在最后十分中的数据上使用一个窗口操作，那么你的集群应该有足够的内存来保存10分中的数据。或者，如果你想要对很大数量的key使用updateStateByKey，那么需要的内存会很高。相反的，如果你想要做一个简单的map-filter-store操作，那么需要的内存将非常低。<br>通常，因为通过receivers接收到的数据以StorageLevel.MEMORY_AND_DISK_SER_2级别进行存储，那些无法装配到内存中的数据将被溢出到磁盘。这可能会降低streaming application的性能，因此建议提供足够的内存供你的streaming application使用。最好尝试并查看一个小Scala代码的内存使用。<br>另一方面的内存调整是垃圾收集。对于一个streaming application，要求是低延迟，有JVM垃圾回收引起的较大暂停是不好的。<br>有一些参数能够帮助你调整内存的使用和GC负载：</p>
<ul>
<li><em>Persistence Level of DStreams:</em> 正如之前在<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#data-serialization" title="Data Serialization" target="_blank" rel="external">Data Serialization</a>，输入数据和RDDs默认以序列化字节保存。相比非序列化保存，这降低了内存的使用和GC的开销。启用Kryo序列化将进一步降低序列化的大小和内存的使用。进一步降低内存的使用可以通过压缩来完成（查看spark配置spark.rdd.compress），但是会耗费CPU时间。</li>
<li><em>Clearing old data:</em> 默认，所有的输入数据和通过DStream transformations生成的被保存的RDDs会被自动清理。Spark决定合适清理这些数据是基于使用的transformation的。例如，如果你使用10分钟的窗口操作，那么Spark Streaming将要保存最后10分钟的数据，并且积极的丢弃较老的数据。通过设置streamingContext.remember，数据能够被保留一个较长的区间（例如查询较老的数据）。</li>
<li><em>CMS Garbage Collector:</em> 强烈推荐使用并发mark-and-sweep GC来保持GC处于短暂停状态。通过并发GC来降低系统的整体处理吞吐量，推荐使用它还是为了得到始终如一的batch处理时间。确保在driver（在spark-submit中使用–driver-java-options）上和executor（使用Spark配置spark.executor.extraJavaOptions）上设置了CMS GC。</li>
<li><em>Other tips:</em> 要进一步降低GC开销，还有一些其他东西值得尝试。<ul>
<li>使用OFF_HEAP存储级别存储RDDs。在<a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence" title="RDD Persistence" target="_blank" rel="external">Spark Programming Guide</a>中查看详细信息。</li>
<li>使用更多带有更小heap大小的executors。这样能够降低每个JVM heap的CG暂停。</li>
</ul>
</li>
</ul>
<p><em>需要记住的几点：</em></p>
<ul>
<li>一个DStream与单个receiver关联。为了实现并行读取并行的多个receiver，需要创建多个DStream。一个receiver在一个executor内运行。它占用一个core。确保在receiver订阅一个core之后有足够的的core来进行处理。spark.cores.max会将receiver的订阅算到账户中。receivers以轮转的模式分配给executors。</li>
<li>当数据从一个stream数据源被接收，receiver创建数据的blocks。每个block间隔毫秒创建一个新的数据block。batch间隔期间创建N个数据block，其中N =  batchInterval/blockInterval。这些block通过当前executor的BlockManager分布到其他executors的block managers。之后，运行在driver上的网络输入跟踪器被通知得到这些block的位置，用于做进一步的处理。</li>
<li>在driver上为batch间隔期间创建的blocks创建一个RDD。batch间隔期间生成的blocks是这个RDD的partitions。在Spark每个partition是一个任务。blockInterval==batchinterval意味着是单个partition并且可能是本地处理的。</li>
<li>blocks上的map任务在executors（一个是接收block的那个，另一个位于block的副本处）中被处理，executor有的blocks和block间隔无关，除非是非本地调度。拥有更大block间隔意味着更大的blocks。一个高的spark.locality.wait值增加了本地节点上一个block的处理机会。需要找出这两个参数之间的一个平衡以确保较大blocks能够本地被处理。</li>
<li>除了依靠batchInterval和blockInterval，通过调用inputDstream.repartitions(n)你还能够定义partitions的数量。这随机的混洗RDD中的数据来创建n个partitions。是的，更好的并发。虽然带来了混洗的开销。一个RDD的处理通过driver的job调度器作为一个job来调度。在给定的时间点，只有一个job活跃。因此，如果一个job正在执行，那么其他的job将会在队列中排队。</li>
<li>如果你有两个dstream，那么将会有形成两个RDDs，并且将会创建两个jobs，这两个job会一个接一个进行调度。要避免这样，你能够联合两个dstreams。这将确保单个联合的RDD是由两个dstreams的RDDs组成的。联合的RDD将被作为单个job来考虑。然而这个RDD的partition是没有影响的。</li>
<li>如果batch处理时间比batchinterval要多，那么很明显receiver的内存将开始堆积最后会抛出异常（很有可能是BlockNotFoundException）。当前没有办法暂停receiver。使用SparkConf配置spark.streaming.receiver.maxRate，可以限制receiver的速度。</li>
</ul>
<h1 id="Fault-tolerance-Semantics"><a href="#Fault-tolerance-Semantics" class="headerlink" title="Fault-tolerance Semantics"></a>Fault-tolerance Semantics</h1><p>在本章，我们将讨论Spark Streaming applications在故障事件中的行为。</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>要理解由Spark Streaming提供的概念，让我们记住Spark的RDDs的容灾基础。<br>1、一个RDD是一个不可变的、可重新计算的分布式数据集。每个RDD记住确定的操作体系，这个操作体系被用于容灾的输入数据集上来创建RDD。<br>2、如果一个RDD的任何partition在worker节点故障期间丢失，那么那个partition将会根据原始的容灾数据集使用操作体系重新计算出来。<br>3、假设所有的RDD transformation都是确定的，那么在最终转换的RDD中的数据也将总是与Spark集群中的故障无关的。</p>
<p>Spark在容灾文件系统（例如HDFS或S3）中的数据上操作，因此所有从容灾数据上生成的RDDs也是容灾的。然而这不适合Spark Streaming，因为在多数情况下数据是通过网络接收到的（除了使用fileStream时）。要为所有生成的RDDs达到相同的容灾属性，被接收到的数据被复制到集群中的worker节点中多个executor中（默认的重复因子为2）。这使得在故障时系统中的两种数据需要被恢复：<br>1、Data received and replicated - 这种数据能够从单个worker节点故障中幸存，因为这个数据的一个拷贝在另外一个节点上。<br>2、Data received but buffered for replication - 因为这个数据没有重复存放，恢复这个数据的唯一方式是从数据源获取。<br>此外，有两种类型故障我们应该关心：<br>1、Failure of a Worker Node - 任何运行executors的worker节点都可能发生故障，并且所有在内存中的数据会丢失。如果有运行在故障节点的receivers，那么它们缓存的数据将会丢失。<br>2、Failure of the Driver Node - 如果运行Spark Streaming application的driver节点故障，显然SparkContext也被丢失，并且所有executors和他们内存中的数据也会丢失。</p>
<p>有了这些基础知识，我们开始理解Spark Streaming的容灾概念。</p>
<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p>常常听到的一个streaming系统的语义是每条记录能够被系统处理多少次。有三种类型的保证，由底层可能的操作条件所提供.<br>1、最多一次：每条记录被处理一次或不被处理。<br>2、至少一次：每条记录会被处理一次或多次。这中要比最多一次的健壮，因为它保证数据不会丢失。但是可能会重复。<br>3、只有一次：每条记录明确只处理一次 - 没有数据丢失也没有数据会被处理多次。很明显这是三种保证中最健壮的。</p>
<h2 id="Basic-Semantics"><a href="#Basic-Semantics" class="headerlink" title="Basic Semantics"></a>Basic Semantics</h2><p>在任何stream处理系统中，在数据处理中大概有三个步骤。<br>1、Receiving the data：使用Receiver或其他的从数据源接收数据。<br>2、Transforming the data：接收到的数据使用DStream transformation和RDD transformations被转换。<br>3、pushing out the data：将转换的最终数据推送到外部系统，像文件系统、数据库、dashboards等。<br>如果一个streaming application需要完成端对端的只有一次的保证，那么每个步骤需要提供一个只有一次的保证。那就是每条记录必须只被接收一次、转换一次并且推送到下游系统一次。我们理解以下在Spark Streaming的context中的这些语义。<br>1、Receiving the data: 不同的输入数据源提供了不同的保证。这将在下一章中详细讨论。<br>2、Transforming the data: 所有已经被接收到的数据将只被处理一次，这个保证由RDDs提供。如果有失败，只要接收到的输入数据可以访问，最终转换成的RDDs将总是包含相同的内容。<br>3、Pushing out the data: 默认输出操作确保为至少一次，因为它依赖于输出操作的类型（是否幂等）和下游系统的语义（是否支持事务）。但是用户能够定义他们自己事务机制来达到只有一次的语义。这将在本章的之后详细讨论。</p>
<h2 id="Semantics-of-Receivecd-Data"><a href="#Semantics-of-Receivecd-Data" class="headerlink" title="Semantics of Receivecd Data"></a>Semantics of Receivecd Data</h2><p>不同的输入数据源提供了不同的保证，涵盖了从至少一次到只有一次的所有情况。更加详细的阅读。</p>
<h3 id="With-Files"><a href="#With-Files" class="headerlink" title="With Files"></a>With Files</h3><p>如果所有输入数据都已经位于一个容灾的文件系统中，如HDFS，Spark Streaming能够总是从任何故障中恢复并处理所有的数据。这给了一个只有一次的语义，意味着所有的数据不管有什么失败，只会被处理一次。</p>
<h3 id="With-Receiver-based-Sources"><a href="#With-Receiver-based-Sources" class="headerlink" title="With Receiver-based Sources"></a>With Receiver-based Sources</h3><p>对于基于receiver的输入数据源，容错依赖于故障情节和redeiver的类型。正如我们之前讨论的，有两种类型的receiver：<br>1、Reliable Receiver - 这些receiver之后在将数据重复存储后才会给可靠数据源发送确认信息。如果一个这样的receiver失败，这个数据源将不会收到缓存数据的确认。因此，如果receiver被重启，数据源将会重新发送数据，因此不会有数据在故障期间丢失。<br>2、Unreliable Receiver - 这类receiver不会发送确认，因此当时worker故障或driver故障期间会丢失数据。<br>根据使用的什么类型的receivers，我们达到如下语义。如果一个worker节点失败，那么使用可靠receiver不会有数据丢失；使用不可靠receiver，也接受但还未复制保存的数据会丢失。如果是driver节点失败，那么除了这些会丢失，所有接收和复制保存的数据也会丢失。这将会影响状态化装换的结果。<br>要避免已经接收到的数据的丢失，Spark 1.2引入了write ahead log，它保存接收到的数据到容灾存储中。启用write ahead logs配合可靠receiver，可以零数据丢失。在这些语义中，提供至少一次的保障。<br>下面的表格汇总了故障的含义：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Deployment Scenario</th>
<th style="text-align:left">Worker Failure</th>
<th style="text-align:left">Driver Failure</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark 1.1或更早，或没有开启写ahead日志Spark 1.2或更高版本</td>
<td style="text-align:left">使用不可靠receiver将会丢失缓存的数据。使用可靠receiver将零数据丢失，至少一次的语义</td>
<td style="text-align:left">使用不可靠receivers缓存的数据丢失，任何类型的receiver，已经传递的数据会丢失，未定义的语义</td>
</tr>
<tr>
<td style="text-align:left">开启了写ahead日志的Spark 1.2或更高版本</td>
<td style="text-align:left">使用可靠receiver将零数据丢失，至少一次的语义</td>
<td style="text-align:left">使用可靠receivers和文件将零数据丢失，至少一次的语义</td>
</tr>
</tbody>
</table>
<h3 id="With-Kafka-Direct-API"><a href="#With-Kafka-Direct-API" class="headerlink" title="With Kafka Direct API"></a>With Kafka Direct API</h3><p>在Spark 1.3版本中，我们引入了一个新的Kafka直接API，它能够确保所有由Spark Streaming接收的Kafka数据只会接收一次。如果你实现了只有一次的输出操作，你可以达到端对端只有一次的保证。这个方法（）将在<a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html" title="Spark Streaming + Kafka Integration Guide" target="_blank" rel="external">Kafka Integration Guide</a>中进一步讨论。</p>
<h2 id="Semantics-of-output-operations"><a href="#Semantics-of-output-operations" class="headerlink" title="Semantics of output operations"></a>Semantics of output operations</h2><p>输出操作（像foreachRDD）有至少一次的语义，那就是被转换的数据在worker故障事件中可能会写出多次。对于保存到文件（使用saveAs<em>*</em>Files）是可以接受的（因为文件对于相同的数据会简单的覆盖），另外可能必须要达到只有一次的语义。有两种方法。</p>
<ul>
<li>Idempotent updates: 尝试多次写相同的数据。例如，saveAs<em>*</em>Files总是写习惯你同的数据来生成文件。</li>
<li>Transactional updates: 所有的更新都会产生事务，因此更新只会确切的做一次。唯一这样做的方式如下。<ul>
<li>使用batch时间（在foreachRDD中可用）和RDD的partition索引来创建一个唯一标识。这个标识唯一表示streaming application的一条数据。</li>
<li>使用这个唯一标识以事务方式（确切一次，自动的）更新外部系统。如果这个标识还没有提交，提交这个partition的数据和这个标识。否则，如果已经提交，跳过这个更新。<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">dstream.foreachRDD &#123; (rdd, time) =&gt;</div><div class="line">  rdd.foreachPartition &#123; partitionIterator =&gt;</div><div class="line">    <span class="keyword">val</span> partitionId = <span class="type">TaskContext</span>.get.partitionId()</div><div class="line">    <span class="keyword">val</span> uniqueId = generateUniqueId(time.milliseconds, partitionId)</div><div class="line">    <span class="comment">// use this uniqueId to transactionally commit the data in partitionIterator</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h1 id="Migration-Guide-from-0-9-1-or-below-to-1-x"><a href="#Migration-Guide-from-0-9-1-or-below-to-1-x" class="headerlink" title="Migration Guide from 0.9.1 or below to 1.x"></a>Migration Guide from 0.9.1 or below to 1.x</h1><p>在Spark 0.9.1和Spark 1.0之间有一些API变更，以确保之后API稳定性。这一章详细的介绍将已存在的代码合并到1.0的步骤。<br><em>Input DStreams:</em>所有在一个输入流中创建的操作（例如，StreamingContext.socketStream、FlumeUtils.createStream等），对于Scala，则返回InputDStream或ReceiverInputDStream（代替了DStream），对于Java，则返回JavaInputDStream、JavaPairInputDStream、JavaReceiverInputDStream或JavaPairReceiverInputDStream（代替JavaDStream）。这确保那些特定于输入流的功能在未来能够被添加到这些类，而不需要打破两者的兼容性。注意，你的已存在的Spark Streaming application应该不需要任何改变（因为这些新类是DStream或JavaDStream的子类）处理可能需要使用Spark 1.0进行编译。<br><em>Custom Network Receivers:</em>因为要发布Spark Streaming，使用类NetworkReciever，可以在Scala中定义自定义网络receivers。然而这个API在错误处理和reporting中受到限制，并且不能在Java中使用。从Spark 1.0开始，这个类被<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.receiver.Receiver" title="Receiver" target="_blank" rel="external">Receiver</a>所替换，这个Receiver有如下优势。</p>
<ul>
<li>添加了像stop和restart方法，以便更好的控制一个receiver的生命周期。查看<a href="http://spark.apache.org/docs/latest/streaming-custom-receivers.html" title="Spark Streaming Custom Receivers" target="_blank" rel="external">custom receiver guide</a>获取更多的细节。</li>
<li>自定义receiver能够使用Scala和Java来实现。</li>
</ul>
<p>要将你的自定义receiver从之前的NetworkReceiver合并到最新的Receiver，你需要做如下的事情。</p>
<ul>
<li>确保你的自定义receiver类继承了<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.receiver.Receiver" title="Receiver" target="_blank" rel="external">org.apache.spark.streaming.receiver.Receiver</a>而不是org.apache.spark.streaming.dstream.NetworkReceiver。</li>
<li>之前，一个BlockGenerator对象需要通过自定义receiver来创建，接收到的数据会被添加到BlockGenerator来存储到Spark中。它需要明确的在onStart()和onStop()方法中启动和停止。新的Receiver类使这成为非必须的，因为它增加了一组名为store(<data>)的方法，可以调用这些方法来将数据存储到Spark中。因此，要合并你的网络receiver，移除任何的BlockGenerator对象（在Spark 1.0中不再存在）并在接收到的数据上使用store(…)方法。</data></li>
</ul>
<p><em>Actor-based Receivers:</em>基于actor的Receiver APIs已经被移动到<a href="https://github.com/spark-packages/dstream-akka" title="DStream akka" target="_blank" rel="external">DStream Akka</a>。请参考这个项目获取更多细节。</p>
<h1 id="Where-to-Go-from-Here"><a href="#Where-to-Go-from-Here" class="headerlink" title="Where to Go from Here"></a>Where to Go from Here</h1><ul>
<li>Additional guides<ul>
<li><a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html" title="Spark Streaming + Kafka Integration Guide" target="_blank" rel="external">Kafka Integration Guide</a></li>
<li><a href="http://spark.apache.org/docs/latest/streaming-kinesis-integration.html" title="Spark Streaming + Kinesis Integration" target="_blank" rel="external">Kinesis Integration Guide</a></li>
<li><a href="http://spark.apache.org/docs/latest/streaming-custom-receivers.html" title="Spark Streaming Custom Receivers" target="_blank" rel="external">Custom Receiver Guide</a></li>
</ul>
</li>
<li>External DStream data sources:<ul>
<li><a href="https://github.com/spark-packages/dstream-mqtt" title="DStream MQTT" target="_blank" rel="external">DStream MQTT</a></li>
<li><a href="https://github.com/spark-packages/dstream-twitter" title="DStream Twitter" target="_blank" rel="external">DStream Twitter</a></li>
<li><a href="https://github.com/spark-packages/dstream-akka" title="DStream Akka" target="_blank" rel="external">DStream Akka</a></li>
<li><a href="https://github.com/spark-packages/dstream-zeromq" title="DStream ZeroMQ" target="_blank" rel="external">DStream ZeroMQ</a></li>
</ul>
</li>
<li>API documention<ul>
<li>Scala docs<ul>
<li><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext" title="StreamingContext" target="_blank" rel="external">streamingContext</a> and <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream" title="DStream" target="_blank" rel="external">DStream</a></li>
<li><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.kafka.KafkaUtils$" title="KafkaUtils" target="_blank" rel="external">KafkaUtils</a>、<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.flume.FlumeUtils$" title="FlumeUtils" target="_blank" rel="external">FlumeUtils</a>、<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.kinesis.KinesisUtils$" title="KinesisUtils" target="_blank" rel="external">KinesisUtils</a></li>
</ul>
</li>
<li>Java docs<ul>
<li><a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html" title="JavaStreamingContext" target="_blank" rel="external">JavaStreamingContext</a>、<a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/streaming/api/java/JavaDStream.html" title="JavaDStream" target="_blank" rel="external">JavaDStream</a>and<a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/streaming/api/java/JavaPairDStream.html" title="JavaPairDStream" target="_blank" rel="external">JavaPairDStream</a></li>
<li><a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/streaming/kafka/KafkaUtils.html" title="KafkaUtils" target="_blank" rel="external">KafkaUtils</a>、<a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/streaming/flume/FlumeUtils.html" title="FlumeUtils" target="_blank" rel="external">FlumeUtils</a>、<a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/streaming/kinesis/KinesisUtils.html" title="KinesisUtils" target="_blank" rel="external">KinesisUtils</a></li>
</ul>
</li>
<li>Python docs<ul>
<li><a href="http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext" title="StreamingContext" target="_blank" rel="external">StreamingContext</a> and <a href="http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.DStream" title="DStream" target="_blank" rel="external">DStream</a></li>
<li><a href="http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.kafka.KafkaUtils" title="KafkaUtils" target="_blank" rel="external">KafkaUtils</a></li>
</ul>
</li>
</ul>
</li>
<li>More examples in <a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/streaming" title="Scala examples" target="_blank" rel="external">Scala</a> and <a href="https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples/streaming" title="Java examples" target="_blank" rel="external">Java</a> and <a href="https://github.com/apache/spark/tree/master/examples/src/main/python/streaming" title="Python examples" target="_blank" rel="external">Python</a></li>
<li><a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-259.pdf" title="Paper" target="_blank" rel="external">Paper</a> and <a href="http://youtu.be/g171ndOHgJ0" title="video" target="_blank" rel="external">video</a> describing Spark Streaming</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是Spark Streaming手册的翻译文档，会随着自己的实现进行更新，官方文档请&lt;a href=&quot;http://spark.apache.org/docs/latest/streaming-programming-guide.html&quot; title=&quot;Spark Streming Programming Guide&quot;&gt;参考&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h1&gt;&lt;p&gt;Spark Streaming是核心Spark API的一个延伸，它对实时数据流进行可扩展的、高吞吐量的、容灾的进行处理。数据可以从很多源（如Kafka、Flume、Kinesis或TCP socket）进行提取，然后被复杂的算法组合处理，这些复杂的算法可以使用高级别的函数，如map、reduce、join和window。最后，被处理过的数据可以推出到外部文件系统、数据库和实时图表中。实际上你可以在数据流上应用Spark的&lt;a href=&quot;http://spark.apache.org/docs/latest/ml-guide.html&quot; title=&quot;Machine Learning Library (MLlib) Guide&quot;&gt;机器学习&lt;/a&gt;和&lt;a href=&quot;http://spark.apache.org/docs/latest/graphx-programming-guide.html&quot; title=&quot;GraphX Programming Guide&quot;&gt;图处理&lt;/a&gt;。&lt;br&gt;&lt;img src=&quot;http://oaavtz33a.bkt.clouddn.com/streaming-arch.png&quot; alt=&quot;spark streaming architecture&quot; title=&quot;spark streaming architecture&quot;&gt;&lt;br&gt;在内部，它如下工作。Spark Streaming接收实时的输入数据流，并将数据划分到批次中，然后在批次中数据被Spark引擎处理并生成最终的结果流。&lt;br&gt;&lt;img src=&quot;http://oaavtz33a.bkt.clouddn.com/streaming-flow.png&quot; alt=&quot;Spark Streaming data flow&quot; title=&quot;Spark Streaming data flow&quot;&gt;&lt;br&gt;Spark Streaming提供了一个高级别的抽象，叫做discretized stream或DStream，它代表了一个连续的数据流。DStream可以从来自数据源（如Kafka、Flume和Kinesis）的输入数据流创建，也可以通过在其他DStream上应用高级别的操作来创建，一个DStream以一个&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD&quot; title=&quot;RDD&quot;&gt;RDDs&lt;/a&gt;序列来表示。&lt;br&gt;本指南展示了如何开始使用DStreams来编写Spark Streaming程序。你可以使用Scala、Java或Python（从Spark1.2中引入）来编写Spark Streaming程序，这些语言的代码都会在本指南中提供。你会发现tabs在本指南中随处可见，是你可以在不同语言的代码片段之间任意选择。&lt;br&gt;&lt;strong&gt;注意：&lt;/strong&gt;在Python中有少量的APIs是不同或不可用的。贯穿整个指南，你会发现&lt;em&gt;Python API&lt;/em&gt;标签高亮了这些不同。&lt;/p&gt;
    
    </summary>
    
      <category term="spark" scheme="http://baimoon.github.io/categories/spark/"/>
    
    
      <category term="spark streaming" scheme="http://baimoon.github.io/tags/spark-streaming/"/>
    
  </entry>
  
  <entry>
    <title>Configuration</title>
    <link href="http://baimoon.github.io/2016/08/09/spark-configuration/"/>
    <id>http://baimoon.github.io/2016/08/09/spark-configuration/</id>
    <published>2016-08-09T02:28:26.000Z</published>
    <updated>2016-08-17T15:06:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是对Spark配置的翻译，主要用于本人学习使用，原文<a href="http://spark.apache.org/docs/latest/configuration.html" title="Spark Configuration" target="_blank" rel="external">请参考</a></p>
<p>Spark提供了三个用于对系统配置的位置：</p>
<ul>
<li>Spark properties控制大多数application参数，可以通过使用SparkConf对象设置或通过Java系统属性设置。</li>
<li>Environment variables可以设置每台机器的设置，如IP地址，通过每个节点上的conf/spark-env.sh脚本。</li>
<li>Logging可以通过log4j.properties来配置。</li>
</ul>
<h1 id="Spark-Properties"><a href="#Spark-Properties" class="headerlink" title="Spark Properties"></a>Spark Properties</h1><p>Spark属性控制大多数application设置，并且对每个application进行独立配置。这些属性可以直接在SparkConf上设置，SparkConf会传递给你的SparkContext.SparkConf，来允许你控制一些常用属性（如master的URI和application的名称等），通过set()方法达到和key-value对一样。例如，我们可以使用两个线程来初始化一个application，如下：<br>注意我们使用local[2]运行，意味着两个线程-表示最低的并行，这样能够帮助我们发现那些只有在分布式context上运行才会出现的bug。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</div><div class="line">             .setMaster(<span class="string">"local[2]"</span>)</div><div class="line">             .setAppName(<span class="string">"CountingSheep"</span>)</div><div class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div></pre></td></tr></table></figure></p>
<p>注意在本地模式中我们可以使用多个线程，但是在像Spark Streaming中，我们实际上要求使用多个线程，来避免任何饥饿情况的发生。<br>指定时间属性时需要配置时间单位，下面的格式是可以接受的：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">25ms (milliseconds)</div><div class="line">5s (seconds)</div><div class="line">10m or 10min (minutes)</div><div class="line">3h (hours)</div><div class="line">5d (days)</div><div class="line">1y (years)</div></pre></td></tr></table></figure></p>
<p>指定字节大小的属性应该配置一个大小单位，下面的格式是可以接受的：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">1b (bytes)</div><div class="line">1k or 1kb (kibibytes = 1024 bytes)</div><div class="line">1m or 1mb (mebibytes = 1024 kibibytes)</div><div class="line">1g or 1gb (gibibytes = 1024 mebibytes)</div><div class="line">1t or 1tb (tebibytes = 1024 gibibytes)</div><div class="line">1p or 1pb (pebibytes = 1024 tebibytes)</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<h2 id="Dynamically-Loading-Spark-Properties"><a href="#Dynamically-Loading-Spark-Properties" class="headerlink" title="Dynamically Loading Spark Properties"></a>Dynamically Loading Spark Properties</h2><p>在一些情况中，我们可能想要避免在SparkConf中硬编码某一配置。例如，如果你想要以不同的master或不同数量的内存来运行相同的application。Spark允许你简单的创建一个空的配置对象：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="keyword">new</span> <span class="type">SparkConf</span>())</div></pre></td></tr></table></figure></p>
<p>然后，你可以在运行时提供配置值：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit --name <span class="string">"My app"</span> --master <span class="built_in">local</span>[4] --conf spark.eventLog.enabled=<span class="literal">false</span> --conf <span class="string">"spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps"</span> myApp.jar</div></pre></td></tr></table></figure></p>
<p>Spark shell和spark-submit工具支持两种方式来动态加载配置信息。第一种是命令行选项，诸如上面展示的–master。spark-submit能够接收任何使用–conf标识指定的Spark属性，但是对于作为Spark application启动一部分的属性使用特殊标识。运行./bin/spark-submit –help将展示选项的全部列表。<br>bin/spark-submit还将从conf/spark-defaults.conf中读取配置选项，在这个文件中，每行包含使用空格符分隔的一个key和一个value。例如：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">spark.master            spark:<span class="comment">//5.6.7.8:7077</span></div><div class="line">spark.executor.memory   <span class="number">4</span>g</div><div class="line">spark.eventLog.enabled  <span class="literal">true</span></div><div class="line">spark.serializer        org.apache.spark.serializer.<span class="type">KryoSerializer</span></div></pre></td></tr></table></figure></p>
<p>任何通过标识或子属性文件中指定的值将会传递到application并通过SparkConf进行合并。直接在SparkConf中设置的属性拥有最高优先级，然后是通过spark-submit或spark-shell传递的标识，最后是spark-defaults.conf。少部分的keys已经从早期的版本中被重命名了；在这种情况中，旧的key的名字仍然可以接受，但是比任何新key的实例的优先级都低。</p>
<h2 id="Viewing-Spark-Properties"><a href="#Viewing-Spark-Properties" class="headerlink" title="Viewing Spark Properties"></a>Viewing Spark Properties</h2><p>Spark的web UI（地址为http://<driver>:4040）中的”Environment”tab页中累出了Spark的属性。这个地方用来检查你的属性是否已经被正确设置是很有用的。注意，只有通过spark-default.conf、SparkConf或命令行指定的值才会出现在这里。对于其他的值，你可以假设它们使用的是默认值。</driver></p>
<h2 id="Available-Properties"><a href="#Available-Properties" class="headerlink" title="Available Properties"></a>Available Properties</h2><p>大多数控制内部设置的属性都设置了合理的默认值。一些常用的选项是如下设置：</p>
<h3 id="Application-Properties"><a href="#Application-Properties" class="headerlink" title="Application Properties"></a>Application Properties</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.app.name</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">你的application的名称。这将会出现在UI和日志数据中</td>
</tr>
<tr>
<td style="text-align:left">spark.driver.cores</td>
<td style="text-align:left">1</td>
<td style="text-align:left">用于driver进程的核的数量，只在集群模式中有用</td>
</tr>
<tr>
<td style="text-align:left">spark.driver.maxResultSize</td>
<td style="text-align:left">1g</td>
<td style="text-align:left">为每个Spark action（如collect）限制全部parition的序列化结果的总大小。至少应该为1M或者设置为0表示不限制。如果Job的Result的总大小超过这个限制，Job将会被中断。设置一个高的限制可能会引发driver的内存溢出（依赖spark.driver.memory和JVM中对象的内存开销）。设置一个合适的值可以保护driver远离内存溢出错误</td>
</tr>
<tr>
<td style="text-align:left">spark.driver.memory</td>
<td style="text-align:left">1g</td>
<td style="text-align:left">用于driver进程的总内存数，例如SparkContext在多大内存中初始化（如：1g、2g）。注意：在client模式中，这个配置不能在你的application中通过SparkConf直接设置，因为在这个时候driver JVM已经启动了。应该通过–driver-memory命令行选项或在你的默认属性文件中配置来设置。</td>
</tr>
<tr>
<td style="text-align:left">spark.executor.memory</td>
<td style="text-align:left">1g</td>
<td style="text-align:left">每个executor进程使用的内存总数</td>
</tr>
<tr>
<td style="text-align:left">spark.extraListeners</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">一个以逗号分隔的实现了SparkListener的类的列表；当初始化SparkContext时，这些类使用Spark的listener bus被创建并和注册。如果类有只接收SparkConf对象作为参数的构造函数，则会调用这个构造函数，否则调用无参的构造函数。如果没有找到有效的构造函数，SparkContext将创建失败并抛出异常</td>
</tr>
<tr>
<td style="text-align:left">spark.local.dir</td>
<td style="text-align:left">/tmp</td>
<td style="text-align:left">在Spark中用于”打草稿”的空间，包括mpa输出文件和在磁盘上进行排序的RDD。这应该是你系统中的快盘。它也可以是一个以逗号分隔的多个不同磁盘上的不同目录的列表。注意：在Spark1.0和后续版本中，这将被集群管理器设置的SPARK_LOCAL_DIRS（standalone, Mesos）或LOCAL_DIRS（YARN）环境变量重写。</td>
</tr>
<tr>
<td style="text-align:left">spark.logConf</td>
<td style="text-align:left">false</td>
<td style="text-align:left">当一个SparkContext被启动时，以INFO级别对起作用的SparkConf记录日志。</td>
</tr>
<tr>
<td style="text-align:left">spark.master</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">指定要连接的集群管理器。查看<a href="https://baimoon.github.io/blog/2016/07/25/spark-submittingApplications#master-urls" title="Master URLs">允许的masterURL</a>列表</td>
</tr>
<tr>
<td style="text-align:left">spark.submit.deployMode</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">Spark driver程序的部署模式，”client”或”cluster”，它们表示在集群中的一个节点上本地(“client”)还是远程(“cluster”)启动driver程序</td>
</tr>
</tbody>
</table>
<p>除了这些，下面的属性也是可用的，并且可能在一些问题中有用：</p>
<h3 id="Runtime-Environment"><a href="#Runtime-Environment" class="headerlink" title="Runtime Environment"></a>Runtime Environment</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaing</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.driver.extraClassPath</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">额外的classpath项，用来追加到driver的classpath之前。<strong>注意</strong>：在client模式中，这个配置必须不能通过在你的application中的SparkConf直接设置，因为driver JVM在这个时候已经启动了。应该通过–driver-class-path命令行选项类设置或在你的默认属性文件中设置</td>
</tr>
<tr>
<td style="text-align:left">spark.driver.extraJavaOptions</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">传给driver的额外的JVM选项字符串。例如GC设置或其他日志记录。需要注意的是使用这个选项来设置heap的的最大值（-Xmx）是非法的。heap最大值，在集群模式中可以使用spark.driver.memory设置，在client模式中可以通过–driver-memory命令行选项来设置。<strong>注意</strong>：在client模式中，这个配置必须不能通过在你的application使用SparkConf直接设置，因为在这个时刻，driver JVM已经启动。应该通过–driver-java-options命令行选项来设置，或者在你的默认属性文件中设置</td>
</tr>
<tr>
<td style="text-align:left">spark.driver.extraLibraryPath</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">当启动driver JVM时，设置一个特殊的library路径提供使用。注意，在client模式中，这个配置必须不能通过你的applicaiton中的SparkConf直接设置，因为这个时候driver JVM一就能够启动，应该使用–driver-java-options命令行选项来设置，或在你的默认属性文件中设置</td>
</tr>
<tr>
<td style="text-align:left">spark.driver.userClassPathFirst</td>
<td style="text-align:left">false</td>
<td style="text-align:left">（实验性的）在driver中加载类时，是否给用户添加的jars比Spark自身的jars更高的优先级。这个功能可以用来减缓Spark依赖和用于依赖之间的冲突。它当前还是实验性的，并且只能用于集群模式中</td>
</tr>
<tr>
<td style="text-align:left">spark.executor.extraClassPath</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">额外的classpath记录预先添加到executors的classpath。它的存在是为了向后兼容老版本的Spark。用户通常不需要设置这个选项</td>
</tr>
<tr>
<td style="text-align:left">spark.executor.extraJavaOptions</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">传递给executors的额外的JVM选项的字符串。例如，GC设置或其他日志记录。注意，使用这个选项来设置Spark的属性或设置heap最大值是违法的。Spark的属性应该使用SparkConf对象设置或使用park-submit脚本时使用spark-defaults.conf来设置。heap最大值应该通过spark.executor.memory来设置</td>
</tr>
<tr>
<td style="text-align:left">spark.executor.extraLibraryPath</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">在启动executor JVM时设置一个特殊的library路径</td>
</tr>
<tr>
<td style="text-align:left">spark.executor.logs.rolling.maxRetainedFiles</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">设置由系统保留的日志文件的个数。较老的日志文件将被删除。默认不可以</td>
</tr>
<tr>
<td style="text-align:left">spark.executor.logs.rolling.maxSize</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">设置executor日志滚动的最大字节大小，超过这个大小的日志将被滚动。滚动默认是被禁用的。自动清理老的日志请查看spark.executor.logs.rolling.maxRetainedFiles</td>
</tr>
<tr>
<td style="text-align:left">spark.executor.logs.rolling.strategy</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">设置executor日志滚动策略。默认它是禁用的。它可以设置为”time”（按时间滚动）或”size”（按大小滚动）。对于”time”，使用spark.executor.logs.rolling.time.interval来设置滚动间隔；对于”size”，使用spark.executor.logs.rolling.maxSize来设置滚动的最大文件size</td>
</tr>
<tr>
<td style="text-align:left">spark.executor.logs.rolling.time.interval</td>
<td style="text-align:left">daily</td>
<td style="text-align:left">设置executor日志滚动的时间间隔。默认滚动是禁用的。有效的值有daily、hourly、minutely或任何以秒为单位的间隔数。老日志的自动清理请参考spark.executor.logs.rolling.maxRetainedFiles</td>
</tr>
<tr>
<td style="text-align:left">spark.executor.userClassPathFirst</td>
<td style="text-align:left">false</td>
<td style="text-align:left">（实验性的）与spark.driver.userClassPathFirst功能相同，只是应用在executor实例上</td>
</tr>
<tr>
<td style="text-align:left">spark.executorEnv.[EnvironmentVariableName]</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">添加由EnvironmentVariableName指定的环境变量到executor进程。用户可以指定多个这样的变量来设置多个环境变量</td>
</tr>
<tr>
<td style="text-align:left">spark.python.profile</td>
<td style="text-align:left">false</td>
<td style="text-align:left">在Python worker中启用性能分析，描述结果将通过sc.show_profiles()显示出来，或在driver退出之前显示。描述结果也可以通过sc.dump_profiles(path)方法dump到磁盘。如果一些描述结果被人为禁用了，在driver退出之前它们不会被显示。默认使用pyspark.profiler.BasicProfiler，但是可以通过传递一个profiler类给SparkContext构造器来重写这个profiler</td>
</tr>
<tr>
<td style="text-align:left">spark.python.profile.dump</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">在driver退出之前，描述结果dump到的目录。描述结果会将每个RDD作为单独的文件进行dump。它们可以通过ptats.Stats()进行加载。如果这个值被指定了，那么描述结果将不会自动被显示</td>
</tr>
<tr>
<td style="text-align:left">spark.python.worker.memory</td>
<td style="text-align:left">512m</td>
<td style="text-align:left">聚合期间，每个python worker进程使用的内存总数，使用和JVM内存字符串相同的格式（如 512m，2g）。如果在聚合期间内存的使用超过这个值，将会溢出数据到磁盘</td>
</tr>
<tr>
<td style="text-align:left">spark.python.worker.reuse</td>
<td style="text-align:left">true</td>
<td style="text-align:left">是否重用Python worker。如果设置为true，将使用固定数量的Python worker，不需要为每个task来fork()一个Python进程。如果有一个很大的广播，这将是非常有用的，那么这个广播就不需要为每个task将JVM装换为Python worker</td>
</tr>
<tr>
<td style="text-align:left">spark.files</td>
<td style="text-align:left"></td>
<td style="text-align:left">以逗号分隔的文件列表，用于放置每个executor的工作目录</td>
</tr>
<tr>
<td style="text-align:left">spark.submit.pyFiles</td>
<td style="text-align:left"></td>
<td style="text-align:left">以逗号分隔的.zip、.egg或.py文件的列表。用于为Python apps设置PYTHONPATH</td>
</tr>
<tr>
<td style="text-align:left">spark.jars</td>
<td style="text-align:left"></td>
<td style="text-align:left">以逗号分隔的本地jars的列表，以便将这些jars包含到driver和executor classpath中</td>
</tr>
<tr>
<td style="text-align:left">spark.jars.packages</td>
<td style="text-align:left"></td>
<td style="text-align:left">以逗号分隔的jars的Maven坐标，以便将这些jars包含到driver和executor classpath中。查询的顺序是本地库、mavne中最后是通过spark.jars.ivy指定的远程库。坐标的格式为groupId:artifactId:version</td>
</tr>
<tr>
<td style="text-align:left">spark.jars.excludes</td>
<td style="text-align:left"></td>
<td style="text-align:left">以逗号分隔的groupId:artifactId列表，用于在解析由spark.jars.packages提供的依赖时进行排除，以避免依赖冲突</td>
</tr>
<tr>
<td style="text-align:left">spark.jars.ivy</td>
<td style="text-align:left"></td>
<td style="text-align:left">以逗号分隔的附加远程库的列表，用来搜索spark.jars.packages中提供的坐标的jars</td>
</tr>
</tbody>
</table>
<h3 id="Shuffle-Behavior"><a href="#Shuffle-Behavior" class="headerlink" title="Shuffle Behavior"></a>Shuffle Behavior</h3><table>
<thead>
<tr>
<th style="text-align:left">property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.reducer.maxSizeInFlight</td>
<td style="text-align:left">48m</td>
<td style="text-align:left">从每个reduce任务同时拉取map输出的最大size。因为每个输出要求我们创建一个buffer来接收它，这表示每个reduce任务一个固定内存的开销，因此保持它小，除非你有很大的内存</td>
</tr>
<tr>
<td style="text-align:left">spark.reducer.maxReqsInFlight</td>
<td style="text-align:left">Int.MaxValue</td>
<td style="text-align:left">这个配置限制了在任何点获取blocks的远程请求的数量。随着集群主机的数量的增长，可能会导致到一个或多个节点的拨入连接数量很大，引发workers负载下的失败。通过限制获取请求的数量，可以环节这种情况</td>
</tr>
<tr>
<td style="text-align:left">spark.shuffle.compress</td>
<td style="text-align:left">true</td>
<td style="text-align:left">是否加密map的输出文件。通常是一个好主意。压缩将事宜哦那个spark.io.compression.codec</td>
</tr>
<tr>
<td style="text-align:left">spark.shuffle.file.buffer</td>
<td style="text-align:left">32k</td>
<td style="text-align:left">每个shuffle文件输出流的内存buffer的大小。这些buffer在创建中间shuffle文件过程中降低了磁盘寻址的次数和系统调用</td>
</tr>
<tr>
<td style="text-align:left">spark.shuffle.io.maxRetries</td>
<td style="text-align:left">3</td>
<td style="text-align:left">如果这个值设置为非零值，那么由于IO相关异常而失败的获取将会自动重试。这个重试逻辑帮助稳定shuffle面对长s时间的GC暂停或短暂的网络连通的问题</td>
</tr>
<tr>
<td style="text-align:left">spark.shuffle.io.numConnectionsPerPeer</td>
<td style="text-align:left">1</td>
<td style="text-align:left">对于大集群，为了减少连接的建立，主机之间的连接会被重用。对于有用很多磁盘，但有很少主机的集群，这可能会导致到所有磁盘的并发不足，因此用户可能会考虑增加这个值</td>
</tr>
<tr>
<td style="text-align:left">spark.shuffle.io.preferDirectBufs</td>
<td style="text-align:left">true</td>
<td style="text-align:left">shuffle和cache block transfer期间用于垃圾回收的off-heap huffer。对于off-heap内存被紧紧限制的环境，用户可能希望将其关闭来触发来自Nettry的分配到on-heap中</td>
</tr>
<tr>
<td style="text-align:left">spark.shuffle.io.retryWait</td>
<td style="text-align:left">5s</td>
<td style="text-align:left">fetch的重试之间等待的时长。默认由重试引发的最大延迟为15秒，是通过maxRetries * retryWait计算出来的</td>
</tr>
<tr>
<td style="text-align:left">spark.shuffle.service.enabled</td>
<td style="text-align:left">false</td>
<td style="text-align:left">启用外部的shuffle服务。这个服务保存了由executors写的shuffle文件，因此executors能够被安全移除。如果spark.dynamicAllocation.enabled设置为”true”，那么该功能必须启用。为了启用它，外部的shuffle服务必须被启动。查看<a href="http://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup" title="dynamic allocation configuration and setup documentation" target="_blank" rel="external">dynamic allocation configuration and setup documentation</a>获取更多信息</td>
</tr>
<tr>
<td style="text-align:left">spark.shuffle.service.prot</td>
<td style="text-align:left">7337</td>
<td style="text-align:left">外部shuffle服务运行的端口号</td>
</tr>
<tr>
<td style="text-align:left">spark.shuffle.sort.bypassMergeThreshold</td>
<td style="text-align:left">200</td>
<td style="text-align:left">（高级的）在基于排序的shuffle管理器中，如果没有map端的聚合，避免合并排序数据，并且最多有这么多的reduce partition</td>
</tr>
<tr>
<td style="text-align:left">spark.shuffle.splill.compress</td>
<td style="text-align:left">true</td>
<td style="text-align:left">在shuffle期间，是否对溢出的数据进行压缩。压缩将使用spark.io.compression.codec</td>
</tr>
</tbody>
</table>
<h3 id="Spark-UI"><a href="#Spark-UI" class="headerlink" title="Spark UI"></a>Spark UI</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.eventLog.compress</td>
<td style="text-align:left">false</td>
<td style="text-align:left">如果spark.eventLog.enabled为true，是否对记录的event进行加密</td>
</tr>
<tr>
<td style="text-align:left">spark.eventLog.dir</td>
<td style="text-align:left">file:///tmp/spark-events</td>
<td style="text-align:left">如果spark.eventLog.enabled为true，Spark events被记录的基本目录。在这个基本目录中，Spark为每个application创建一个子目录，并记录application的events到这个目录中。用户可能想要设置这个值为一个统一的位置，例如一个HDFS目录，因此历史文件可以被历史服务器读取</td>
</tr>
<tr>
<td style="text-align:left">spark.eventLog.enable</td>
<td style="text-align:left">false</td>
<td style="text-align:left">是否记录Spark events，用于在application完成之后重新构建Web UI</td>
</tr>
<tr>
<td style="text-align:left">spark.ui.killEnabled</td>
<td style="text-align:left">true</td>
<td style="text-align:left">允许计划和对应的jobs可以通过web ui被kill掉</td>
</tr>
<tr>
<td style="text-align:left">spark.ui.port</td>
<td style="text-align:left">4040</td>
<td style="text-align:left">application的报表图的端口，用来显示内存和负载数据</td>
</tr>
<tr>
<td style="text-align:left">spark.ui.retainedJobs</td>
<td style="text-align:left">1000</td>
<td style="text-align:left">在垃圾回收之前，被Spark UI和status APIs记住的job的数量</td>
</tr>
<tr>
<td style="text-align:left">spark.ui.retainedStages</td>
<td style="text-align:left">1000</td>
<td style="text-align:left">在垃圾回收之前，被Spark UI和status APIs记住的计划的数量</td>
</tr>
<tr>
<td style="text-align:left">spark.worker.ui.retainedExecutors</td>
<td style="text-align:left">1000</td>
<td style="text-align:left">在垃圾回收之前，被Spark UI和status APIs记住的已完成的executors的数量</td>
</tr>
<tr>
<td style="text-align:left">spark.worker.ui.retainedDrivers</td>
<td style="text-align:left">1000</td>
<td style="text-align:left">在垃圾回收之前，被Spark UI和status APIs记住的已完成的drivers的数量</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.ui.retainedExecutors</td>
<td style="text-align:left">1000</td>
<td style="text-align:left">在垃圾回收之前，被Spark UI和status APIs记住的已完成的executions的数量</td>
</tr>
<tr>
<td style="text-align:left">spark.streaming.ui.retainedBatches</td>
<td style="text-align:left">1000</td>
<td style="text-align:left">在垃圾回收之前，被Spark UI和status APIs记住的已完成的batches的数量</td>
</tr>
<tr>
<td style="text-align:left">spark.ui.retainedDeadExecutors</td>
<td style="text-align:left">100</td>
<td style="text-align:left">在垃圾回收之前，被Spark UI和status APIs记住的死掉的executors的数量</td>
</tr>
</tbody>
</table>
<h3 id="Compression-and-Seriallzation"><a href="#Compression-and-Seriallzation" class="headerlink" title="Compression and Seriallzation"></a>Compression and Seriallzation</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.broadcast.compress</td>
<td style="text-align:left">true</td>
<td style="text-align:left">在发送广播变量之前是否对其进行压缩。通常是个好主意</td>
</tr>
<tr>
<td style="text-align:left">spark.io.compression.codec</td>
<td style="text-align:left">lz4</td>
<td style="text-align:left">用户对中间数据（诸如RDD partition、广播变量以及shuffle输出）进行压缩的编码器。默认，Spark提供了3中编码器：lz4、lzf和snappy。你也可以使用完整的类名来指定编码器，如：org.apache.spark.io.LZ4CompressionCodeC、org.apache.spark.io.LZFCompressionCodec和org.apache.spark.io.SnappyCompressionCodec</td>
</tr>
<tr>
<td style="text-align:left">spark.io.compression.lz4.blockSize</td>
<td style="text-align:left">32k</td>
<td style="text-align:left">当LZ4压缩编码器使用时，LZ4压缩使用的block的大小。当LZ4被使用时，降低这个block的大小也会降低shuffle内存的使用</td>
</tr>
<tr>
<td style="text-align:left">spark.io.compression.snappy.blockSize</td>
<td style="text-align:left">32k</td>
<td style="text-align:left">当使用Snappy压缩编码器时，Snappy压缩使用的block的大小。当Snappy被使用时，降低这个block的大小也会降低shuffle内存的使用</td>
</tr>
<tr>
<td style="text-align:left">spark.kryo.classesToRegister</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">如果你使用了Kryo序列化，给出一个以逗号分隔的自定义类名的列表，这些类要使用Kryo注册。查看<a href="http://spark.apache.org/docs/latest/tuning.html#data-serialization" title="tuning guide" target="_blank" rel="external">tuning guide</a>获取更多细节</td>
</tr>
<tr>
<td style="text-align:left">spark.kryo.referenceTracking</td>
<td style="text-align:left">true(当使用Spark SQL Thrift Server时为false)</td>
<td style="text-align:left">当使用Kryo序列化数据时，是否跟踪相同对象的引用，如果你的对象图有环那么这是必须的，并且如果相同对象包含多个拷贝，这对性能也是有用的。如果你知道这不是问题，可以禁用它来提高性能</td>
</tr>
<tr>
<td style="text-align:left">spark.kryo.registrator</td>
<td style="text-align:left">(none)</td>
<td style="text-align:left">如果你是用Kryo序列化，给出一个以逗号分隔的类的列表，这些类是用来使用Kryo进行注册的自定义类。如果你需要以自定义的方式注册你的类，这是非常有用的，例如，指定一个自定义字段序列化器。否则，<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.serializer.KryoRegistrator" title="classToRegistor" target="_blank" rel="external">spark.kryo.classToRegistor</a>是简单的。查看<a href="http://spark.apache.org/docs/latest/tuning.html#data-serialization" title="Tuning Guide" target="_blank" rel="external">tuning guide</a>获取更多细节</td>
</tr>
<tr>
<td style="text-align:left">spark.kryoserializer.buffer.max</td>
<td style="text-align:left">64m</td>
<td style="text-align:left">Kryo序列化huffer的允许的最大大小。这个必须要任何你试图序列化的对象大。如果你在Kryo中得到一个”buffer limit exceeded”，则增加这个值</td>
</tr>
<tr>
<td style="text-align:left">spark.kryoserializer.buffer</td>
<td style="text-align:left">64k</td>
<td style="text-align:left">Kryo序列化buffer的初始大小。注意在每个worker上一个core一个buffer。如果需要，这个值会增长到spark.kryoserializer.buffer.max</td>
</tr>
<tr>
<td style="text-align:left">spark.rdd.compress</td>
<td style="text-align:left">false</td>
<td style="text-align:left">是否压缩序列化的RDD（例如，在Java或Scala中的StorageLevel.MEMORY_ONLY_SER或Python中的StorageLevel.MEMORY_ONLY）。耗费额外的CPU来节省大量的空间</td>
</tr>
<tr>
<td style="text-align:left">spark.serializer</td>
<td style="text-align:left">org.apache.spark.serializer.JavaSerializer (当使用Spark SQL Thrift Server时是org.apache.spark.serializer.KryoSerializer)</td>
<td style="text-align:left">用于对那些需要通过网络发送的对象或需要以序列化格式缓存的对象进行序列化的类。默认使用Java序列化生成器但是它太慢了，当你要求速度的时候，我们<a href="http://spark.apache.org/docs/latest/tuning.html" title="Tuning Spark" target="_blank" rel="external">推荐使用org.apache.spark.serializer.KryoSerializer并配置Kryo序列化</a>。该值可以是任何<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.serializer.Serializer" title="org.apache.spark.Serializer" target="_blank" rel="external">org.apache.spark.Serializer</a>的子类</td>
</tr>
<tr>
<td style="text-align:left">spark.serializer.objectStreamReset</td>
<td style="text-align:left">100</td>
<td style="text-align:left">当使用org.apache.spark.serializer.JavaSerializer进行序列化时，序列化生成器会缓存对象来防止写冗余数据，然而这会停止这些对象的垃圾回收。通过调用reset你可以从序列化生成器中刷新那些信息，从而允许旧的对象被收集。要关掉这个周期操作，可以将其设置为-1。默认情况下，将会没100个对象reset一次序列化生成器</td>
</tr>
</tbody>
</table>
<h3 id="Memory-Management"><a href="#Memory-Management" class="headerlink" title="Memory Management"></a>Memory Management</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.memory.fraction</td>
<td style="text-align:left">0.6</td>
<td>用于执行和存储的分数（300MB的heap空间）。这个值越低，溢出和数据缓存回收发生的就越频繁。这个配置的目的是为内部元数据、用户数据结构和很少情况中不精确size的估算、异常大的记录设置一个预留内存。推荐使用默认值。更多细节，包括关于当增加这个值时正确调节JVM垃圾回收的重要信息，查看这个<a href="http://spark.apache.org/docs/latest/tuning.html#memory-management-overview" title="memory management overview" target="_blank" rel="external">描述</a></td>
</tr>
<tr>
<td style="text-align:left">spark.memory.storageFraction</td>
<td style="text-align:left">0.5</td>
<td>免于回收的存储内存的总数，以spark.memory.fraction设置的预留区域大小的小数来表示。这个值越高，用于执行的工作内存也少，并且tasks溢出到磁盘可能越频繁。推荐使用默认值。更多细节，请参考<a href="http://spark.apache.org/docs/latest/tuning.html#memory-management-overview" title="memory management overview" target="_blank" rel="external">这个描述</a></td>
</tr>
<tr>
<td style="text-align:left">spark.memory.offHeap.enabled</td>
<td style="text-align:left">false</td>
<td>如果为true，Spark将会试图为某些操作使用off-heap内存。如果off-heap内存被启用，那么spark.memory.offHeap.size必须确定</td>
</tr>
<tr>
<td style="text-align:left">spark.memory.offHeap.size</td>
<td style="text-align:left">0</td>
<td>可以被用于off-heap分配的内存的绝对值，以字节为单位。这个设置对heap内存的使用没有影响，因此如果你的executor的总内存的消耗必须适合一些硬件的限制，然后确保相应的缩小你的JVM heap大小</td>
</tr>
<tr>
<td style="text-align:left">spark.memory.useLegacyMode</td>
<td style="text-align:left">false</td>
<td>是否启用遗留的内存管理模式，用于Spark1.5及之前版本。这个遗留模式，严格的划分heap空间到固定大小区域，如果applicaiton没有优化，可能会导致过多的溢出。除非该设置启用，否则不需要了解下面的废弃的内存配置：spark.shuffle.memoryFraction spark.storage.memoryFraction spark.storage.unrollFraction</td>
</tr>
<tr>
<td style="text-align:left">spark.shuffle.memoryFraction</td>
<td style="text-align:left">0.2</td>
<td>废弃</td>
</tr>
<tr>
<td style="text-align:left">spark.storage.memoryFraction</td>
<td style="text-align:left">0.6</td>
<td>废弃</td>
</tr>
<tr>
<td style="text-align:left">spark.storage.unrollFraction</td>
<td style="text-align:left">0.2</td>
<td>废弃</td>
</tr>
</tbody>
</table>
<h3 id="Execution-Behavior"><a href="#Execution-Behavior" class="headerlink" title="Execution Behavior"></a>Execution Behavior</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.broadcast.blockSize</td>
<td style="text-align:left">4m</td>
<td style="text-align:left">一个TorrentBroadcastFactory block的每个碎片的大小。这个值太大，在广播期间会降低并发性（使它变慢）。然而，如果太小，BlockManager可能会有性能问题</td>
</tr>
<tr>
<td style="text-align:left">spark.executor.cores</td>
<td style="text-align:left">在YARN模式中为1，在standalone和Mesos coarsegrained模式中为worker上所有可用的core</td>
<td style="text-align:left">每个executor使用的core的数量。在standalong和Mesos coarse-grained模式中，设置这个参数可以允许一个application在用一个worker上运行多个core，假如worker上有足够的core。否则，在每个worker上每个application只能运行一个executor</td>
</tr>
<tr>
<td style="text-align:left">spark.default.parallelism</td>
<td style="text-align:left">对于分布式shuffle操作（像reduceByKey和join），一个父级RDD的最大partitions的数量。对于像parallelize这样没有父级RDD的操作，依赖于集群管理器：+ 本地模式：本机上core的数量 + Mesos fine grained模式：8 + 其他：所有executor节点上的core的总和或2，去较大值</td>
<td style="text-align:left">在RDDs中由transformations（如join、reduceByKey和parallelize）返回的默认partitions的数量，如果用户没有设置这个数量的话。</td>
</tr>
<tr>
<td style="text-align:left">spark.executor.heartbeatInterval</td>
<td style="text-align:left">10s</td>
<td style="text-align:left">每个executor到driver的心跳间隔。心跳使driver知道executor仍然活着，并使用metrics为进程内的tasks更新driver</td>
</tr>
<tr>
<td style="text-align:left">spark.files.fetchTimeout</td>
<td style="text-align:left">60s</td>
<td style="text-align:left">当从driver通过SparkContext.addFile()获取文件时使用的通信超时时间</td>
</tr>
<tr>
<td style="text-align:left">spark.files.useFetchCache</td>
<td style="text-align:left">true</td>
<td style="text-align:left">如果设置为true（默认），文件的拉取将使用一个本地缓存，这个本地缓存被属于相同application的executors共享，当在同一个主机上运行多个executors时，能够增进task的启动性能。如果设置为false，这些缓存的优化将被禁用，所有的executors将会获取它们自己的文件拷贝。这个优化被禁用可能是为了使用Spark本地目录，这些目录位于NFS文件系统（查看<a href="https://issues.apache.org/jira/browse/SPARK-6313" title="spark-6313" target="_blank" rel="external">SPARK-6313</a>获取更多细节）上</td>
</tr>
<tr>
<td style="text-align:left">spark.files.overwrite</td>
<td style="text-align:left">false</td>
<td style="text-align:left">通过SparkContext.addFile()添加文件时，如果目标文件已经存在并且它的内容和源文件不匹配，是否对文件重写</td>
</tr>
<tr>
<td style="text-align:left">spark.hadoop.cloneConf</td>
<td style="text-align:left">false</td>
<td style="text-align:left">如果为true，为每个task复制一个新的Hadoop Configuration对象。该选项应该启用来解决对Configuration有线程安全的问题（查看<a href="https://issues.apache.org/jira/browse/SPARK-2546" title="SPARK-2546" target="_blank" rel="external">SPARK-2546</a>获取更多细节）。该功能默认是禁用，为了避免不会受那些问题影响的job会有异常的性能下降</td>
</tr>
<tr>
<td style="text-align:left">spark.hadoop.validateOutputSpecs</td>
<td style="text-align:left">true</td>
<td style="text-align:left">如果设置为true，验证输出规范（如，是否检查输出目录已经存在），该验证用于saveAsHadoopFile和其他变体形式中。该功能可以被禁用，在输出目录预先存在时不抛出异常。我们建议用户不要禁用此功能，除非是尝试与之前版本的Spark进行兼容。简单的通过手动使用Hadoop的文件系统API来删除输出目录。这个设置会被由Spark Streaming的StreamingContext生成的Job忽略，因为在checkpoint恢复期间，数据可能需要重新写入到已存在的输出目录中</td>
</tr>
<tr>
<td style="text-align:left">spark.storage.memoryMapThreshold</td>
<td style="text-align:left">2m</td>
<td style="text-align:left">当从磁盘读取一个block时，Spark将block映射到内存，该值为映射内存的大小。这可以放置Spark内存映射到非常小的block。通常内存映射需要很高的负载来关闭blocks或降低操作系统的page size</td>
</tr>
</tbody>
</table>
<h3 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.rpc.message.maxSize</td>
<td style="text-align:left">128</td>
<td style="text-align:left">在“控制层”通讯中所允许的最大message的大小；通常只应用在map在executors和driver之间输出信息的大小。如果你以上千个map和reduce任务来运行jobs，可以增加这个值并查看关于RPC message大小的信息</td>
</tr>
<tr>
<td style="text-align:left">spark.blockManager.port</td>
<td style="text-align:left">(random)</td>
<td style="text-align:left">所有block manager的监听端口。它们存在于driver和executors上</td>
</tr>
<tr>
<td style="text-align:left">spark.driver.host</td>
<td style="text-align:left">(local hostname)</td>
<td style="text-align:left">driver监听的IP地址或主机名。这用于和executors和standalone Master的通信</td>
</tr>
<tr>
<td style="text-align:left">spark.driver.port</td>
<td style="text-align:left">(random)</td>
<td style="text-align:left">driver监听的端口号。用于和executors和standalone Master的通信</td>
</tr>
<tr>
<td style="text-align:left">spark.network.timeout</td>
<td style="text-align:left">120s</td>
<td style="text-align:left">所有网络交互的默认超时时间。这个配置将被放置在spark.core.connection.ack.wait.timeout、spark.storage.blockManagerSlaveTimeoutMs、spark.shuffle.io.connectionTimeout、spark.rpc.askTimeout或spark.rpc.lookupTimeout中，如果这些值没有配置</td>
</tr>
<tr>
<td style="text-align:left">spark.port.maxRetries</td>
<td style="text-align:left">16</td>
<td style="text-align:left">在绑定一个端口时，在放弃之前的最大尝试次数。当端口被指定为特定值（非0）时，每个后续的尝试操作在重试之前会在之前尝试的端口号上加一。本质上，这允许尝试的端口号的范围为指定端口号到指定的端口号+ maxRetries</td>
</tr>
<tr>
<td style="text-align:left">spark.rpc.numRetries</td>
<td style="text-align:left">3</td>
<td style="text-align:left">在一个RPC任务放弃之前，重试的次数。一个RPC任务最多运行这些次</td>
</tr>
<tr>
<td style="text-align:left">spark.rpc.retry.wait</td>
<td style="text-align:left">3s</td>
<td style="text-align:left">一个RPC ask操作在重试之前等待的时长</td>
</tr>
<tr>
<td style="text-align:left">spark.rpc.askTimeout</td>
<td style="text-align:left">120s</td>
<td style="text-align:left">一个RPC ask操作在超时之前等待的时长</td>
</tr>
<tr>
<td style="text-align:left">spark.rpc.lookupTimeout</td>
<td style="text-align:left">120s</td>
<td style="text-align:left">一个RPC远程端口查找操作在超时之前等待的时长</td>
</tr>
</tbody>
</table>
<h3 id="Scheduling"><a href="#Scheduling" class="headerlink" title="Scheduling"></a>Scheduling</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaing</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.cores.max</td>
<td style="text-align:left">(not set)</td>
<td style="text-align:left">当在standalone部署集群或“coarse-grained”共享模式的Mesos集群上运行时，跨集群（不是从每台机器）为applicaiton请求CPU core的最大数量。如果不设置，则在Spark的standalone集群管理器上使用spark.deploy.defaultCores配置的值，对于Mesos集群管理器则不限制（所有可用的core）</td>
</tr>
<tr>
<td style="text-align:left">spark.locality.wait</td>
<td style="text-align:left">3s</td>
<td style="text-align:left">在放弃之前启动本地优势数据的任务的等待时长，然后在一个非本地的节点上启动。这个等待时间会用于其他几个位置级别（process-local、node-local、rack-local和其他）。通过设置spark.locality.wait.node等配置，你也可以为每个级别设置自定义的等待时长。如果你的task很长并且有很少的本地性任务，那么你应该增加这个值，但是通常默认值工作的很好</td>
</tr>
<tr>
<td style="text-align:left">spark.locality.wait.node</td>
<td style="text-align:left">spark.locality.wait</td>
<td style="text-align:left">为node本地性设置自定义的本地性等待。例如，你可以设置这个值为0来跳过node本地性并立即为rack本地性进行查找（如果你的集群有rack信息）</td>
</tr>
<tr>
<td style="text-align:left">spark.locality.wait.process</td>
<td style="text-align:left">spark.locality.wait</td>
<td style="text-align:left">为本地性进程自定义本地性等待。这个配置影响了那些在并行executor进程中试图访问缓存数据的tasks</td>
</tr>
<tr>
<td style="text-align:left">spark.locality.wait.rack</td>
<td style="text-align:left">spark.locality.wait</td>
<td style="text-align:left">为本地性rack自定义本地性等待</td>
</tr>
<tr>
<td style="text-align:left">spark.scheduler.<br>maxRegisteredResourcesWaitingTime</td>
<td style="text-align:left">30s</td>
<td style="text-align:left">在调度开始之前，等待资源注册的最长时间</td>
</tr>
<tr>
<td style="text-align:left">spark.shceduler.<br>minRegisteredResourcesRatio</td>
<td style="text-align:left">对于YARN是0.8；对于standalone和Mesos coarse-grained模式是0.0</td>
<td style="text-align:left">在开始调度之前等待已注册资源的比例（已注册的资源 / 总的期望资源）（在yarn模式中子域按时executors，在standalone模式和Mesos coarsed-grained模式中是CPU cores[对于Mesos coarse-grained模式，期望的资源的总值为’spark.cores.max’的值]）。值应该是0.0到1.0之间的double类型值。不管最小比例的资源是否已经到达，只会等待通过配置spark.scheduler.<br>maxRegisteredResourcesWaitingTime的时间时长</td>
</tr>
<tr>
<td style="text-align:left">spark.scheduler.mode</td>
<td style="text-align:left">FIFO</td>
<td style="text-align:left">提交到相同SparkContext的jobs之间<a href="http://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application" title="scheduling mode" target="_blank" rel="external">调度模式</a>。可以设置为FAIR，来使用公平模式以代替队列模式。主要用于多用户的服务中</td>
</tr>
<tr>
<td style="text-align:left">spark.scheduler.revive.interval</td>
<td style="text-align:left">1s</td>
<td style="text-align:left">调度器恢复worker资源供tasks运行的间隔长度</td>
</tr>
<tr>
<td style="text-align:left">spark.speculation</td>
<td style="text-align:left">false</td>
<td style="text-align:left">如果设置为true，执行tasks的推测执行。这表示如果一个或多个tasks在一个阶段运行缓慢，他们将被重新启动</td>
</tr>
<tr>
<td style="text-align:left">spark.speculation.interval</td>
<td style="text-align:left">100ms</td>
<td style="text-align:left">Spark为tasks进行推测的检测频率</td>
</tr>
<tr>
<td style="text-align:left">spark.speculation.multiplier</td>
<td style="text-align:left">1.5</td>
<td style="text-align:left">一个task比中值慢多长时间则可以考虑推测</td>
</tr>
<tr>
<td style="text-align:left">spark.speculation.quantile</td>
<td style="text-align:left">0.75</td>
<td style="text-align:left">某个特定阶段在推测启动之前tasks必须完成的百分比</td>
</tr>
<tr>
<td style="text-align:left">spark.task.cpus</td>
<td style="text-align:left">1</td>
<td style="text-align:left">每个task分配的core的数量</td>
</tr>
<tr>
<td style="text-align:left">spark.task.maxFailures</td>
<td style="text-align:left">4</td>
<td style="text-align:left">job在放弃之前能够失败的task的数量（超过这个数量就放弃）。应该大于等于1.允许重试的数量为该值 - 1</td>
</tr>
</tbody>
</table>
<h3 id="Dynamic-Allocation"><a href="#Dynamic-Allocation" class="headerlink" title="Dynamic Allocation"></a>Dynamic Allocation</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.dynamicAllocation.enabled</td>
<td style="text-align:left">false</td>
<td style="text-align:left">是否使用动态资源分配，它会基于工作负载对application注册或注销executor。更多的细节，查看<a href="http://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation" title="dynamic resource allocation" target="_blank" rel="external">这里</a>的描述。这要求spark.shuffle.service.enabled被设置。如下是相关的配置：spark.dynamicAllocation.minExecutors、spark.dynamicAllocation.maxExecutors和spark.dynamicAllocation.initialExecutors</td>
</tr>
<tr>
<td style="text-align:left">spark.dynamicAllocation.executorIdleTimeout</td>
<td style="text-align:left">60s</td>
<td style="text-align:left">如果动态分配被启用且一个executors已经空闲的时间超过该值，这个executor将被移除。更多细节，请参考这个<a href="http://spark.apache.org/docs/latest/job-scheduling.html#resource-allocation-policy" title="resource allocation policy" target="_blank" rel="external">描述</a></td>
</tr>
<tr>
<td style="text-align:left">spark.dynamicAllocation.<br>cachedExecutorIdleTimeout</td>
<td style="text-align:left">infinity</td>
<td style="text-align:left">如果动态分配被启用并且某个缓存了数据块的executor的空闲时间超过这个值，这个executor将会被移除。更多细节，请参考<a href="http://spark.apache.org/docs/latest/job-scheduling.html#resource-allocation-policy" title="resource allocation policy" target="_blank" rel="external">描述</a></td>
</tr>
<tr>
<td style="text-align:left">spark.dynamicAllocation.initialExecutors</td>
<td style="text-align:left">spark.dynamicAllocation.minExecutors</td>
<td style="text-align:left">如果动态分配启用，初始运行的executor的数量</td>
</tr>
<tr>
<td style="text-align:left">spark.dyanmicAllocation.maxExecutors</td>
<td style="text-align:left">infinity</td>
<td style="text-align:left">如果动态分配被启用，executors的上限</td>
</tr>
<tr>
<td style="text-align:left">spark.dynamicAllocation.minExecutors</td>
<td style="text-align:left">0</td>
<td style="text-align:left">如果动态分配被启用，executors的下限</td>
</tr>
<tr>
<td style="text-align:left">spark.dyanmicAllocation.schedulerBacklogTimeout</td>
<td style="text-align:left">1s</td>
<td style="text-align:left">如果动态分配被启用，并且有为启动的tasks积压超过这个时长，将会请求新的executor。更多的细节，请参考<a href="http://spark.apache.org/docs/latest/job-scheduling.html#resource-allocation-policy" title="resource allocation policy" target="_blank" rel="external">描述</a></td>
</tr>
<tr>
<td style="text-align:left">spark.dynamicAllocation.<br>sustainedSchedulerBacklogTimeout</td>
<td style="text-align:left">schedulerBacklogTimeout</td>
<td style="text-align:left">与spark.dyanmicAllocation.<br>schedulerBacklogTimeout相同，但是只对后续的executor请求有用。更多细节请参考<a href="http://spark.apache.org/docs/latest/job-scheduling.html#resource-allocation-policy" title="resource allocation policy" target="_blank" rel="external">描述</a></td>
</tr>
</tbody>
</table>
<h3 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.acls.enable</td>
<td style="text-align:left">false</td>
<td style="text-align:left">是否启用Spark的访问控制列表。如果启用，这将检查用户是否有权限浏览或修改job。注意，这要求用户需要知道，如果用户以null访问，那么验证则不能完成。过滤器可以和UI配合使用进行用户的认证和设置</td>
</tr>
<tr>
<td style="text-align:left">spark.admin.acls</td>
<td style="text-align:left">Empty</td>
<td style="text-align:left">以逗号分隔的用户或管理员的列表，这些用户或管理员能够浏览和修改所有的Spark job。这可以被用来运行一个共享集群，并且有一组管理员或开发人员，他们能够在不正常工作的时候帮助调试。设置”*”在列表中则表示任何用户都有管理员特权</td>
</tr>
<tr>
<td style="text-align:left">spark.admin.acls.groups</td>
<td style="text-align:left">Empty</td>
<td style="text-align:left">以逗号分隔的组列表，这些组能够浏览和修改所有的Spark jobs。如果你有一组管理员或开发人员来帮助维护和调试底层架构，这样是非常有用的。在列表中放置”*”表示在任何组中的用户都有管理员的特权。用户组从通过spark.user.groups.mapping指定的组映射提供器实例获取组信息。检查spark.user.group.mapping获取更多信息</td>
</tr>
<tr>
<td style="text-align:left">spark.user.groups.mapping</td>
<td style="text-align:left">org.apache.spark.security.<br>ShellBasedGroupsMappingProvider</td>
<td style="text-align:left">群组列表，用户通过由接口org.apache.spark.security.<br>GroupMappingServiceProvider定义的群组映射服务获取，接口的实现通过该属性配置。默认是一个基于unix shell的实现，通过org.apache.spark.security.<br>ShellBasedGroupMappingProvider提供，它决定了用户所属的群组列表。注意，这个实现只对基于Unix/Linux的环境提供支持。Windows环境当前不支持。然而一个新的平台/协议通过实现org.apache.spark.security.<br>GroupMappingServiceProvider接口而被支持</td>
</tr>
<tr>
<td style="text-align:left">spark.authenticate</td>
<td style="text-align:left">false</td>
<td style="text-align:left">Spark是否要对内部连接进行认证。如果不是运行在YARN上，查看spark.authenticate.secret</td>
</tr>
<tr>
<td style="text-align:left">spark.authenticate.secret</td>
<td style="text-align:left">None</td>
<td style="text-align:left">设置用于Spark组件之间认证的密钥</td>
</tr>
<tr>
<td style="text-align:left">spark.authenticate.enableSaslEncryption</td>
<td style="text-align:left">false</td>
<td style="text-align:left">当启用认证时，启用加密通信。如果不是运行在YARN上且认证被启用，那么这需要被设置</td>
</tr>
<tr>
<td style="text-align:left">spark.network.sasl.serverAlwaysEncrypt</td>
<td style="text-align:left">false</td>
<td style="text-align:left">对于那些支持SASL认证的服务，禁用非加密的连接</td>
</tr>
<tr>
<td style="text-align:left">spark.core.connection.ack.wait.timeout</td>
<td style="text-align:left">60s</td>
<td style="text-align:left">在超时并放弃之前连接等待确认的时长。要避免由GC引发的长时间暂停导致的非意愿性的超时，你可以设置一个较大的值</td>
</tr>
<tr>
<td style="text-align:left">spark.core.connection.auth.wait.timeout</td>
<td style="text-align:left">30s</td>
<td style="text-align:left">在超时并放弃之前，连接等待认证的时长</td>
</tr>
<tr>
<td style="text-align:left">spark.modify.acls</td>
<td style="text-align:left">Empty</td>
<td style="text-align:left">以逗号分隔的用户列表，这些用户将有修改Spark job的权限。默认只有启动Spark job的用户能够修改job（如kill掉job）。在列表中放置一个“*”表示所有用户都可以修改job</td>
</tr>
<tr>
<td style="text-align:left">spark.modify.acls.groups</td>
<td style="text-align:left">Empty</td>
<td style="text-align:left">逗号分隔的组列表，这些组拥有修改Spark job的权限。假设你有一组来自相同team的管理员或开发人员，这些人想要控制job的权限，那么该配置很有用。将“*”放置在列表中则表示任何人都有修改Spark job的权限。用户的groups可以从由spark.user.groups.mapping指定的映射提供器来获得。查看spark.user.groups.mapping项获取更多细节</td>
</tr>
<tr>
<td style="text-align:left">spark.ui.filters</td>
<td style="text-align:left">None</td>
<td style="text-align:left">被应用到Spark web UI的过滤器类的名字的列表，以逗号分隔。过滤器应该是一个标准的<a href="http://docs.oracle.com/javaee/6/api/javax/servlet/Filter.html" title="javax.servlet.Filter" target="_blank" rel="external">javax.servlet.Filter</a>。每个filter的参数可以通过设置一个java系统属性来指定：spark.<class name="" of="" filter="">.params=’param1=value1,param2=value2’，例如：-Dspark.ui.filters=com.test.filter1 -Dspark.com.test.filter1.params=’param1=foo,<br>param2=testing’</class></td>
</tr>
<tr>
<td style="text-align:left">spark.ui.view.acls</td>
<td style="text-align:left">Empty</td>
<td style="text-align:left">以逗号分隔的用户列表，这些用户有浏览Spark web UI的权限。默认只有启动Spark job的用户有浏览权限。将“*”放到列表中，则表示任何用户都可以浏览Spark job</td>
</tr>
<tr>
<td style="text-align:left">spark.ui.view.acls.groups</td>
<td style="text-align:left">Empty</td>
<td style="text-align:left">以逗号分隔的用户组列表，这些组可以访问Spark web UI来浏览Spark Job的详细信息。如果你有一组管理员或开发人员或用户能够监控spark job的提交，可以使用组功能。在列表中放置“*”，意味着任何组中的用户东iu可以在Spark web ui中浏览Job的明细。用户分组从有spark.ui.groups.mapping指定的组映射提供器的实例中中得到。查看spark.user.groups.mapping项获取更多信息</td>
</tr>
</tbody>
</table>
<h3 id="Encryption"><a href="#Encryption" class="headerlink" title="Encryption"></a>Encryption</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.ssl.enabled</td>
<td style="text-align:left">false</td>
<td style="text-align:left">是否在所有已支持的协议上启用SSL连接。所有的SSL设置（例如spark.ssl.xxx），其中xxx是一个特殊的配置属性，表示所有已支持的协议的全局配置。为了给某个特殊协议重写全局配置，这些属性必须在明确的协议命名空间中重写。<br>使用spark.ssl.YYY.XXX来为由YYY表示的特殊协议重写全局配置。当前YYY只能是文件服务fs</td>
</tr>
<tr>
<td style="text-align:left">spark.ssl.enabledAlgorithms</td>
<td style="text-align:left">Empty</td>
<td style="text-align:left">逗号分隔的算法列表。指定的算法必须被JVM支持。引用的协议列表可以在<a href="https://blogs.oracle.com/java-platform-group/entry/diagnosing_tls_ssl_and_https" title="Diagnosing TLS, SSL, and HTTPS" target="_blank" rel="external">这个页面</a>找到。注意：如果不设置，它将使用适合JVM的默认算法</td>
</tr>
<tr>
<td style="text-align:left">spark.ssl.keyPassword</td>
<td style="text-align:left">None</td>
<td style="text-align:left">密钥存储中私钥的密码</td>
</tr>
<tr>
<td style="text-align:left">spark.ssl.keyStore</td>
<td style="text-align:left">None</td>
<td style="text-align:left">密钥存储文件的路径。该路径可以是绝对路径，也可以是组件启动目录的相对路径</td>
</tr>
<tr>
<td style="text-align:left">spark.ssl.keyStorePassword</td>
<td style="text-align:left">None</td>
<td style="text-align:left">密钥存储的密码</td>
</tr>
<tr>
<td style="text-align:left">spark.ssl.keyStroeType</td>
<td style="text-align:left">JKS</td>
<td style="text-align:left">密钥存储的类型</td>
</tr>
<tr>
<td style="text-align:left">spark.ssl.protocol</td>
<td style="text-align:left">None</td>
<td style="text-align:left">协议名称。该协议必须被JVM支持。协议列表的引用可以在<a href="https://blogs.oracle.com/java-platform-group/entry/diagnosing_tls_ssl_and_https" title="Diagnosing TLS, SSL, and HTTPS" target="_blank" rel="external">这个页面</a>找到</td>
</tr>
<tr>
<td style="text-align:left">spark.ssl.needClientAuth</td>
<td style="text-align:left">false</td>
<td style="text-align:left">如果SSL需要客户端认证，则设置为true</td>
</tr>
<tr>
<td style="text-align:left">spark.ssl.trustStore</td>
<td style="text-align:left">None</td>
<td style="text-align:left">到信任存储文件的路径。该路径可以是绝对路径，也可以是组件启动目录的相对路径</td>
</tr>
<tr>
<td style="text-align:left">spark.ssl.trustStorePassword</td>
<td style="text-align:left">None</td>
<td style="text-align:left">信任存储的密码</td>
</tr>
<tr>
<td style="text-align:left">spark.ssl.trustStoreType</td>
<td style="text-align:left">JKS</td>
<td style="text-align:left">信任存储的类型</td>
</tr>
</tbody>
</table>
<h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h3><p>执行SET -v命令将会展示SQL配置列表。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// spark is an existing SparkSession</span></div><div class="line">spark.sql(<span class="string">"SET -v"</span>).show(numRows = <span class="number">200</span>, truncate = <span class="literal">false</span>)</div></pre></td></tr></table></figure></p>
<h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.streaming.backpressure.enabled</td>
<td style="text-align:left">false</td>
<td style="text-align:left">启用或禁用Spark Streaming的内部反压力机制（从1.5开始）。这将启用Spark Streaming基于当前批次的调度延迟和处理时间来控制接收比率，因此系统的接收只能和系统的处理速度一样快。这将动态设置receivers的最大接收比率。这个值的上限为spark.streaming.receiver.maxRate和spark.streaming.kafka.maxRatePerPartition，如果这些属性被设置</td>
</tr>
<tr>
<td style="text-align:left">spark.streaming.backpressure.initialRate</td>
<td style="text-align:left">not set</td>
<td style="text-align:left">初始最大接收速率，当反压机制启用时，每个接收器将以这个速率接收第一批数据</td>
</tr>
<tr>
<td style="text-align:left">spark.streaming.blockInterval</td>
<td style="text-align:left">200ms</td>
<td style="text-align:left">被Spark Streaming receivers接收的数据，在将数据存储到Spark中之前，拆分这些数据到数据块的间隔。最小的推荐值为50毫秒。查看Spark Streaming编程指南中<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#level-of-parallelism-in-data-receiving" title="Level of Parallelism in Data Receiving" target="_blank" rel="external">performance tuning</a>章节获取更多信息</td>
</tr>
<tr>
<td style="text-align:left">spark.streaming.receiver.maxRate</td>
<td style="text-align:left">not set</td>
<td style="text-align:left">每个receiver接收数据的最大速率（每秒的记录数）。实际上，每个stream每秒最多消费这些条的记录。将该配置设置为0或负数将不会限制这个速率。查看Spark编程指南中的<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#deploying-applications" title="Deploying Applications" target="_blank" rel="external">部署指南</a>获取更多细节</td>
</tr>
<tr>
<td style="text-align:left">spark.streaming.receiver.<br>writeAheadLog.enable</td>
<td style="text-align:left">false</td>
<td style="text-align:left">为receivers启用写ahead日志。所有通过接收器接收到的输入数据将被写到ahead日志一般保存，这样将在driver失败之后进行恢复。查看Spark Streaming编程指南中的<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#deploying-applications" title="deploying applications" target="_blank" rel="external">部署指南</a>获取更多信息</td>
</tr>
<tr>
<td style="text-align:left">spark.streaming.unpersist</td>
<td style="text-align:left">true</td>
<td style="text-align:left">促使由Spark Streaming生成和保存的RDDs从Spark’s内存中自动取消保存。由Spark Streaming接收的原生数据也会被自动清除。设置这个值为false，将允许原生数据和保存的RDDs能够在streaming application外部访问，因为它们不会被自动清理。但是它带来更高的内存使用的开销</td>
</tr>
<tr>
<td style="text-align:left">spark.streaming.stopGracefullyOnShutdown</td>
<td style="text-align:left">false</td>
<td style="text-align:left">如果为true，在JVM关闭时会优雅的将streamingContext关闭，而不是立即关闭StreamingContext</td>
</tr>
<tr>
<td style="text-align:left">spark.streaming.kafka.maxRatePerPartition</td>
<td style="text-align:left">not set</td>
<td style="text-align:left">当使用新的Kafka stream API时，从每个Kafka partition读取数据的最大速率（每秒记录的数量）。查看<a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html" title="Spark Streaming + Kafka Integration Guide" target="_blank" rel="external">Kafka Integration guide</a>获取更多细节</td>
</tr>
<tr>
<td style="text-align:left">spark.streaming.kafka.maxRetries</td>
<td style="text-align:left">1</td>
<td style="text-align:left">driver为了在每个partition的leader上找到最后的offset（默认值为1，表示driver最多尝试两次），连续尝试的最大数。只能应用于新的Kafka stream API</td>
</tr>
<tr>
<td style="text-align:left">spark.streaming.ui.retainedBatches</td>
<td style="text-align:left">1000</td>
<td style="text-align:left">Spark Streaming UI和status APIs能够记住的batches的数量，超过这个数量的将被回收</td>
</tr>
<tr>
<td style="text-align:left">spark.streaming.driver.writeAheadLog.<br>closeFileAfterWrite</td>
<td style="text-align:left">false</td>
<td style="text-align:left">在driver上写完一条ahead日志记录后，是否要关闭对应的文件。当你想要使用S3时（或其他不支持flush的文件系统）设置这个选项为true</td>
</tr>
<tr>
<td style="text-align:left">spark.streaming.receiver.writeAheadLog.<br>closeFileAfterWrite</td>
<td style="text-align:left">false</td>
<td style="text-align:left">在receiver上写完一条ahead日志记录后，是否要关闭对应的文件。当你想要使用S3时（或任何其他不支持flush的文件系统），设置这个选项为true</td>
</tr>
</tbody>
</table>
<h3 id="SparkR"><a href="#SparkR" class="headerlink" title="SparkR"></a>SparkR</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.r.numRBackendThreads</td>
<td style="text-align:left">2</td>
<td style="text-align:left">被RBackend用来处理来自SparkR包的RPC访问的线程数</td>
</tr>
<tr>
<td style="text-align:left">spark.r.command</td>
<td style="text-align:left">Rscript</td>
<td style="text-align:left">用于在集群模式中为driver和workers执行R脚本的执行器</td>
</tr>
<tr>
<td style="text-align:left">spark.r.driver.command</td>
<td style="text-align:left">spark.r.command</td>
<td style="text-align:left">用户在客户端模式中为driver执行R脚本的执行器。在集群模式中忽略</td>
</tr>
</tbody>
</table>
<h3 id="Deploy"><a href="#Deploy" class="headerlink" title="Deploy"></a>Deploy</h3><table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.deploy.recoveryMode</td>
<td style="text-align:left">NONE</td>
<td style="text-align:left">设置恢复模式，当以集群模式提交的jobs失败并重启时jobs如何恢复。这只能应用于以Standalone或Mesos方式管理的集群模式的job</td>
</tr>
<tr>
<td style="text-align:left">spark.deploy.zookeeper.url</td>
<td style="text-align:left">None</td>
<td style="text-align:left">当’spark.deploy.recoveryMode’被设置为ZOOKEEPER时，这个配置用于设置要连接的Zookeeper URL</td>
</tr>
<tr>
<td style="text-align:left">spark.deploy.zookeeper.dir</td>
<td style="text-align:left">None</td>
<td style="text-align:left">当’spark.deploy.recoveryMode’被设置为ZOOKEEPER时，这个配置用于设置保存恢复状态的Zookeeper目录</td>
</tr>
</tbody>
</table>
<h3 id="Cluster-Managers"><a href="#Cluster-Managers" class="headerlink" title="Cluster Managers"></a>Cluster Managers</h3><p>Spark中的每个cluser管理器都有额外的配置选项。对于每个模式的配置，可以在这些页面中找到：</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/running-on-yarn.html#configuration" title="Running Spark on YARN" target="_blank" rel="external"><strong>YARN</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/running-on-mesos.html#configuration" title="Running Spark on Mesos" target="_blank" rel="external"><strong>Mesos</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts" title="Spark Standalone Mode" target="_blank" rel="external"><strong>Standalone Mode</strong></a></li>
</ul>
<h1 id="Environment-Variables"><a href="#Environment-Variables" class="headerlink" title="Environment Variables"></a>Environment Variables</h1><p>某些Spark的配置可以通过环境变量来设置，这些环境变量会从Spark的安装目录中的conf/spark-env.sh脚本中读取（对于windows，会从conf/spark-env.cmd中读取）。在Standalone和Mesos模式中，这些文件可以给出机器的特定信息（如主机名）。当运行本地Spark application或提交脚本时，还会source（unix命令）这个文件。<br>注意，当Spark被安装时，默认是不存在conf/spark-env.sh文件的。然而，你可以拷贝conf/spark-env.sh.template来创建它。确保拷贝后的文件可以被执行。<br>下面的变量可以在spark-env.sh中设置：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Environment Variable</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">JAVA_HOME</td>
<td style="text-align:left">Java的安装位置（如果没有在你的默认路径中）</td>
</tr>
<tr>
<td style="text-align:left">PYSPARK_PYTHON</td>
<td style="text-align:left">在driver和worker中，用于PySpark执行Python库</td>
</tr>
<tr>
<td style="text-align:left">PYSPARK_DRIVER_PYTHON</td>
<td style="text-align:left">只在driver中，用于PySpark执行的Python库</td>
</tr>
<tr>
<td style="text-align:left">SPARK_DRIVER_R</td>
<td style="text-align:left">用于SparkR shell执行的R库（默认是R）</td>
</tr>
<tr>
<td style="text-align:left">SPARK_LOCAL_IP</td>
<td style="text-align:left">机器要绑定的IP地址</td>
</tr>
<tr>
<td style="text-align:left">SPARK_PUBLIC_DNS</td>
<td style="text-align:left">你的Spark程序要通知给其他机器的主机名</td>
</tr>
</tbody>
</table>
<p>除了上面的这些，还有一些选项用于Spark <a href="http://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts" title="Cluster Launch Scripts" target="_blank" rel="external">standalone cluster scripts</a>，例如每个机器使用的cores的数量和最大内存。<br>因为spark-env.sh是一个shell脚本，环境变量需要在你的conf/spark-defaults.conf中使用soaprk.yarn.appMasterEnv.[EnvironmentVariableName]属性设置。在集群模式中，在spark-env.sh中设置的环境变量将不能在YARN Application Master进程中引用到。查看<a href="http://spark.apache.org/docs/latest/running-on-yarn.html#spark-properties" title="Running Spark on YARN" target="_blank" rel="external">YARN-related Spark Properties</a>获取更多信息。</p>
<h1 id="Configuring-Logging"><a href="#Configuring-Logging" class="headerlink" title="Configuring Logging"></a>Configuring Logging</h1><p>Spark使用<a href="http://logging.apache.org/log4j/2.x/" title="Log4J" target="_blank" rel="external">log4j</a>记录日志。你可以通过在conf目录中添加一个log4j.properties文件来配置它。一种方法是拷贝已经存在的log4j.properties.template文件。</p>
<h1 id="Overriding-configuration-directory"><a href="#Overriding-configuration-directory" class="headerlink" title="Overriding configuration directory"></a>Overriding configuration directory</h1><p>要指定一个不同与默认”SPARK_HOME/conf”的配置目录，你可以设置<strong>SPARK_CONF_DIR</strong>。Spark将使用来自这个目录的配置文件（spark-defaults.conf、spark-env.sh、log4j.properties）。</p>
<h1 id="Inheritiong-Hadoop-Cluster-Configuration"><a href="#Inheritiong-Hadoop-Cluster-Configuration" class="headerlink" title="Inheritiong Hadoop Cluster Configuration"></a>Inheritiong Hadoop Cluster Configuration</h1><p>如果你想要使用Spark读写HDFS，有两个Hadoop配置文件应该包含在Spark的classpath中：</p>
<ul>
<li>hdfs-site.xml，它为HDFS客户端提供了默认行为。</li>
<li>core-site.xml，它设置了默认文件系统名称。<br>这些配置文件的位置根据CDH和HDP的版本而变化，但是常用的一个位置是在/etc/hadoop/conf中。一些工具，诸如Cloudera Manager，以on-the-fly算法创建，但是提供一种机制来下载这些配置的副本。<br>要是这些文件对于Spark可见，在$SPARK_HOME/spark-env.sh中设置HADOOP_CONF_DIR为包含这些配置文件的位置。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是对Spark配置的翻译，主要用于本人学习使用，原文&lt;a href=&quot;http://spark.apache.org/docs/latest/configuration.html&quot; title=&quot;Spark Configuration&quot;&gt;请参考&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Spark提供了三个用于对系统配置的位置：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spark properties控制大多数application参数，可以通过使用SparkConf对象设置或通过Java系统属性设置。&lt;/li&gt;
&lt;li&gt;Environment variables可以设置每台机器的设置，如IP地址，通过每个节点上的conf/spark-env.sh脚本。&lt;/li&gt;
&lt;li&gt;Logging可以通过log4j.properties来配置。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Spark-Properties&quot;&gt;&lt;a href=&quot;#Spark-Properties&quot; class=&quot;headerlink&quot; title=&quot;Spark Properties&quot;&gt;&lt;/a&gt;Spark Properties&lt;/h1&gt;&lt;p&gt;Spark属性控制大多数application设置，并且对每个application进行独立配置。这些属性可以直接在SparkConf上设置，SparkConf会传递给你的SparkContext.SparkConf，来允许你控制一些常用属性（如master的URI和application的名称等），通过set()方法达到和key-value对一样。例如，我们可以使用两个线程来初始化一个application，如下：&lt;br&gt;注意我们使用local[2]运行，意味着两个线程-表示最低的并行，这样能够帮助我们发现那些只有在分布式context上运行才会出现的bug。&lt;br&gt;&lt;figure class=&quot;highlight scala&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;val&lt;/span&gt; conf = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;type&quot;&gt;SparkConf&lt;/span&gt;()&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;             .setMaster(&lt;span class=&quot;string&quot;&gt;&quot;local[2]&quot;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;             .setAppName(&lt;span class=&quot;string&quot;&gt;&quot;CountingSheep&quot;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;val&lt;/span&gt; sc = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;type&quot;&gt;SparkContext&lt;/span&gt;(conf)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;注意在本地模式中我们可以使用多个线程，但是在像Spark Streaming中，我们实际上要求使用多个线程，来避免任何饥饿情况的发生。&lt;br&gt;指定时间属性时需要配置时间单位，下面的格式是可以接受的：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;25ms (milliseconds)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5s (seconds)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10m or 10min (minutes)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3h (hours)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5d (days)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;1y (years)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;指定字节大小的属性应该配置一个大小单位，下面的格式是可以接受的：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1b (bytes)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;1k or 1kb (kibibytes = 1024 bytes)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;1m or 1mb (mebibytes = 1024 kibibytes)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;1g or 1gb (gibibytes = 1024 mebibytes)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;1t or 1tb (tebibytes = 1024 gibibytes)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;1p or 1pb (pebibytes = 1024 tebibytes)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="spark" scheme="http://baimoon.github.io/categories/spark/"/>
    
    
      <category term="spark" scheme="http://baimoon.github.io/tags/spark/"/>
    
  </entry>
  
</feed>
