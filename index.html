<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/blog/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/blog/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/blog/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico?v=5.0.1" />






<meta name="description" content="Baimoon&apos;s blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Baimoon's Note">
<meta property="og:url" content="http://baimoon.github.io/index.html">
<meta property="og:site_name" content="Baimoon's Note">
<meta property="og:description" content="Baimoon&apos;s blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Baimoon's Note">
<meta name="twitter:description" content="Baimoon&apos;s blog">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://baimoon.github.io/"/>

  <title> Baimoon's Note </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/blog/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Baimoon's Note</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/blog/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/blog/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/10/11/spark-2-11-ApplicationMaster/" itemprop="url">
                  spark 2.11 ApplicationMaster
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-10-11T19:51:13+08:00" content="2018-10-11">
              2018-10-11
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-11/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.11</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/10/11/spark-2-11-YarnRMClient/" itemprop="url">
                  spark 2.11 YarnRMClient
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-10-11T16:23:43+08:00" content="2018-10-11">
              2018-10-11
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-11/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.11</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文是对 org.apache.spark.deploy.yarn.YarnRMClient 源码进行学习的分析，spark的版本为2.11。</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>YarnRMClient主要用来处理application master向Yarn resourceManager的注册和注销。</p>
<h1 id="主要方法分析"><a href="#主要方法分析" class="headerlink" title="主要方法分析"></a>主要方法分析</h1><h2 id="register"><a href="#register" class="headerlink" title="register"></a>register</h2><p>该方法很简单，就是向YARN ResourceManager注册application master，该方法会在 ApplicationMaster的registerAM方法中调用。具体方法实现<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">register</span></span>(</div><div class="line">      driverUrl: <span class="type">String</span>,</div><div class="line">      driverRef: <span class="type">RpcEndpointRef</span>,</div><div class="line">      conf: <span class="type">YarnConfiguration</span>,</div><div class="line">      sparkConf: <span class="type">SparkConf</span>,</div><div class="line">      uiAddress: <span class="type">Option</span>[<span class="type">String</span>],</div><div class="line">      uiHistoryAddress: <span class="type">String</span>,</div><div class="line">      securityMgr: <span class="type">SecurityManager</span>,</div><div class="line">      localResources: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">LocalResource</span>]</div><div class="line">    ): <span class="type">YarnAllocator</span> = &#123;</div><div class="line"></div><div class="line">    <span class="comment">// 调用AMRMClient自身的方法来生成AMRMClient，再使用 Yarn 配置进行初始化，启动AMRMClient</span></div><div class="line">    amClient = <span class="type">AMRMClient</span>.createAMRMClient()</div><div class="line">    amClient.init(conf)</div><div class="line">    amClient.start()</div><div class="line">    <span class="keyword">this</span>.uiHistoryAddress = uiHistoryAddress</div><div class="line"></div><div class="line">    <span class="keyword">val</span> trackingUrl = uiAddress.getOrElse &#123;</div><div class="line">      <span class="keyword">if</span> (sparkConf.get(<span class="type">ALLOW_HISTORY_SERVER_TRACKING_URL</span>)) uiHistoryAddress <span class="keyword">else</span> <span class="string">""</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    logInfo(<span class="string">"Registering the ApplicationMaster"</span>)</div><div class="line">    <span class="comment">// 向 ResourceManager 注册 application master，从代码看出 application master就是本机，TODO 这个本机是啥呢？？？</span></div><div class="line">    synchronized &#123;</div><div class="line">      amClient.registerApplicationMaster(<span class="type">Utils</span>.localHostName(), <span class="number">0</span>, trackingUrl)</div><div class="line">      registered = <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 生成 YarnAllocator</span></div><div class="line">    <span class="comment">// driverUrl和driverRef需要说一下，</span></div><div class="line">    <span class="comment">// driverUrl，是driver运行的地址，会传递给Executor，应该是用于Execurot与driver进行交互</span></div><div class="line">    <span class="comment">// driverRef 在YarnAllocator中使用，用于同步executor的id，以及 发送删除executor的信息</span></div><div class="line">    <span class="keyword">new</span> <span class="type">YarnAllocator</span>(driverUrl, driverRef, conf, sparkConf, amClient, getAttemptId(), securityMgr,</div><div class="line">      localResources, <span class="keyword">new</span> <span class="type">SparkRackResolver</span>())</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>此方法逻辑很简单，一眼就看明白。生成AMRMClient（用于访问ResourceManager），向ResourceManager注册applicationMaster，生成YarnAllocator。但是需要注意生成YarnAllocator的参数。</p>
<h2 id="unregister"><a href="#unregister" class="headerlink" title="unregister"></a>unregister</h2><p>作用与register方法相反，从YARN ResourceManager中注销 application master。具体方法实现<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unregister</span></span>(status: <span class="type">FinalApplicationStatus</span>, diagnostics: <span class="type">String</span> = <span class="string">""</span>): <span class="type">Unit</span> = synchronized &#123;</div><div class="line">	<span class="keyword">if</span> (registered) &#123;</div><div class="line">	  amClient.unregisterApplicationMaster(status, diagnostics, uiHistoryAddress)</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="getMaxRegAttempts"><a href="#getMaxRegAttempts" class="headerlink" title="getMaxRegAttempts"></a>getMaxRegAttempts</h2><p>此方法就是用来定义注册application master的最大尝试次数。具体方法定义<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** 获取注册AM的最大尝试次数 分别从spark配置和yarn配置中读取 如果spark配置中设置了，则使用spark和yarn配置中最小那个值 */</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getMaxRegAttempts</span></span>(sparkConf: <span class="type">SparkConf</span>, yarnConf: <span class="type">YarnConfiguration</span>): <span class="type">Int</span> = &#123;</div><div class="line">  <span class="keyword">val</span> sparkMaxAttempts = sparkConf.get(<span class="type">MAX_APP_ATTEMPTS</span>).map(_.toInt)</div><div class="line">  <span class="keyword">val</span> yarnMaxAttempts = yarnConf.getInt(</div><div class="line">    <span class="type">YarnConfiguration</span>.<span class="type">RM_AM_MAX_ATTEMPTS</span>, <span class="type">YarnConfiguration</span>.<span class="type">DEFAULT_RM_AM_MAX_ATTEMPTS</span>)</div><div class="line">  sparkMaxAttempts <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(x) =&gt; <span class="keyword">if</span> (x &lt;= yarnMaxAttempts) x <span class="keyword">else</span> yarnMaxAttempts</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt; yarnMaxAttempts</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>此方法也很简单，分别从spark配置和yarn配置中读取 如果spark配置中设置了，则使用spark和yarn配置中最小那个值。没有在spark中配置，则使用yarn配置中的。</p>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h2 id="哪里生成YarnRMClient对象"><a href="#哪里生成YarnRMClient对象" class="headerlink" title="哪里生成YarnRMClient对象"></a>哪里生成YarnRMClient对象</h2><p>答案就是在ApplicationMaster的main方法中，代码：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">  </div><div class="line">  ...</div><div class="line">  </div><div class="line">  <span class="type">SparkHadoopUtil</span>.get.runAsSparkUser &#123; () =&gt;</div><div class="line">    master = <span class="keyword">new</span> <span class="type">ApplicationMaster</span>(amArgs, <span class="keyword">new</span> <span class="type">YarnRMClient</span>)</div><div class="line">    <span class="type">System</span>.exit(master.run())</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="哪里调用-YarnRMClient的register方法"><a href="#哪里调用-YarnRMClient的register方法" class="headerlink" title="哪里调用 YarnRMClient的register方法"></a>哪里调用 YarnRMClient的register方法</h2><p>在register方法中看到了YarnAllocator的生成，那么在哪里调用register方法呢？答案就是 org.apache.spark.deploy.yarn.ApplicationMaster中。而且ApplicationMaster含有main方法，是程序的入口。代码：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerAM</span></span>(</div><div class="line">      _sparkConf: <span class="type">SparkConf</span>,</div><div class="line">      _rpcEnv: <span class="type">RpcEnv</span>,</div><div class="line">      driverRef: <span class="type">RpcEndpointRef</span>,</div><div class="line">      uiAddress: <span class="type">Option</span>[<span class="type">String</span>],</div><div class="line">      securityMgr: <span class="type">SecurityManager</span>) = &#123;</div><div class="line">    </div><div class="line">    ...</div><div class="line"></div><div class="line">    allocator = client.register(driverUrl,</div><div class="line">      driverRef,</div><div class="line">      yarnConf,</div><div class="line">      _sparkConf,</div><div class="line">      uiAddress,</div><div class="line">      historyAddress,</div><div class="line">      securityMgr,</div><div class="line">      localResources)</div><div class="line"></div><div class="line">    allocator.allocateResources()</div><div class="line">    reporterThread = launchReporterThread()</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/10/10/spark-2-11-YarnAllocator/" itemprop="url">
                  spark 2.11 YarnAllocator
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-10-10T15:51:42+08:00" content="2018-10-10">
              2018-10-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-11/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.11</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文是对 org.apache.spark.deploy.yarn.YarnAllocator 类源码进行学习的分析，spark的版本为2.11。</p>
<h1 id="总体概述"><a href="#总体概述" class="headerlink" title="总体概述"></a>总体概述</h1><p>YarnAllocator可以理解成一个Container的筛选器。当调用了YarnAllocator.allocateResources()方法后，程序就会进行各种处理，最终调用ExecutorRunnable类来启动Executor。在YarnAllocator类中，最主要的方法有：allocateResources()、updateResourceRequests()、handleAllocatedContainers()、runAllocatedContainers()和processCompletedContainers()。而整个这些方法的调用，是通过allocateResources()来调用的。基本的流程如下图：</p>
<h1 id="主要方法的分析"><a href="#主要方法的分析" class="headerlink" title="主要方法的分析"></a>主要方法的分析</h1><h2 id="allocateResources"><a href="#allocateResources" class="headerlink" title="allocateResources"></a>allocateResources</h2><p>资源分配的入口，首先看方法的定义<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">allocateResources</span></span>(): <span class="type">Unit</span> = synchronized &#123;</div><div class="line">    updateResourceRequests()</div><div class="line"></div><div class="line">    <span class="comment">// 处理指示器</span></div><div class="line">    <span class="keyword">val</span> progressIndicator = <span class="number">0.1</span>f</div><div class="line">    <span class="comment">// Poll the ResourceManager. This doubles as a heartbeat if there are no pending container</span></div><div class="line">    <span class="comment">// requests.</span></div><div class="line">    <span class="comment">// 调用 AMRMClient 分配资源</span></div><div class="line">    <span class="keyword">val</span> allocateResponse = amClient.allocate(progressIndicator)</div><div class="line"></div><div class="line">    <span class="comment">// 得到已经分配的 container</span></div><div class="line">    <span class="keyword">val</span> allocatedContainers = allocateResponse.getAllocatedContainers()</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (allocatedContainers.size &gt; <span class="number">0</span>) &#123;</div><div class="line">      <span class="comment">// 输出日志信息，包括 分配的container数量，正在运行的以及启动的executor数量，以及可用的资源信息</span></div><div class="line">      logDebug((<span class="string">"Allocated containers: %d. Current executor count: %d. "</span> +</div><div class="line">        <span class="string">"Launching executor count: %d. Cluster resources: %s."</span>)</div><div class="line">        .format(</div><div class="line">          allocatedContainers.size,</div><div class="line">          numExecutorsRunning.get,</div><div class="line">          numExecutorsStarting.get,</div><div class="line">          allocateResponse.getAvailableResources))</div><div class="line"></div><div class="line">      <span class="comment">// 处理已经分配的container</span></div><div class="line">      handleAllocatedContainers(allocatedContainers.asScala)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 获取已经执行完成的 container</span></div><div class="line">    <span class="keyword">val</span> completedContainers = allocateResponse.getCompletedContainersStatuses()</div><div class="line">    <span class="keyword">if</span> (completedContainers.size &gt; <span class="number">0</span>) &#123;</div><div class="line">      logDebug(<span class="string">"Completed %d containers"</span>.format(completedContainers.size))</div><div class="line">      <span class="comment">//处理已经完成的container</span></div><div class="line">      processCompletedContainers(completedContainers.asScala)</div><div class="line">      logDebug(<span class="string">"Finished processing %d completed containers. Current running executor count: %d."</span></div><div class="line">        .format(completedContainers.size, numExecutorsRunning.get))</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>这个方法首先就是要更新资源的申请（调用updateResourceRequests()方法，我们稍后再看），然后就是调用AMRMClient（amClient）来分配资源，分配资源的返回值（allocateResponse）会包含三部分信息：已经分配的Container（allocatedContainers）、可用的资源和完成的Container（completedContainers）。对于已经分配的和完成的Container，会有对应的方法去处理；对于可用的资源，只是输出到日志。</p>
<h2 id="updateResourceRequests"><a href="#updateResourceRequests" class="headerlink" title="updateResourceRequests"></a>updateResourceRequests</h2><p>更新资源的请求信息，首先看方法的定义<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateResourceRequests</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="comment">// 得到正在添加的container 并 得到正在添加 container的数量</span></div><div class="line">    <span class="keyword">val</span> pendingAllocate = getPendingAllocate</div><div class="line">    <span class="keyword">val</span> numPendingAllocate = pendingAllocate.size</div><div class="line"></div><div class="line">    <span class="comment">// 计划要启动的executor数量 - 正在请求启动的executor - 已经启动的executor - 已经运行任务的executor = 缺少多少executor</span></div><div class="line">    <span class="comment">// 所以这里是在计算 比预计要启动的executor，还缺少多少个</span></div><div class="line">    <span class="keyword">val</span> missing = targetNumExecutors - numPendingAllocate -</div><div class="line">      numExecutorsStarting.get - numExecutorsRunning.get</div><div class="line">    logDebug(<span class="string">s"Updating resource requests, target: <span class="subst">$targetNumExecutors</span>, "</span> +</div><div class="line">      <span class="string">s"pending: <span class="subst">$numPendingAllocate</span>, running: <span class="subst">$&#123;numExecutorsRunning.get&#125;</span>, "</span> +</div><div class="line">      <span class="string">s"executorsStarting: <span class="subst">$&#123;numExecutorsStarting.get&#125;</span>"</span>)</div><div class="line">    <span class="comment">// &lt;1&gt;</span></div><div class="line"></div><div class="line">    <span class="comment">// missing 可以为正数 也可能为负数，负数则说明 动态分配分配多了，但是没有超过最大个数，这个数是通过计划启动个数算出来的</span></div><div class="line">    <span class="keyword">if</span> (missing &gt; <span class="number">0</span>) &#123;</div><div class="line">      logInfo(<span class="string">s"Will request <span class="subst">$missing</span> executor container(s), each with "</span> +</div><div class="line">        <span class="string">s"<span class="subst">$&#123;resource.getVirtualCores&#125;</span> core(s) and "</span> +</div><div class="line">        <span class="string">s"<span class="subst">$&#123;resource.getMemory&#125;</span> MB memory (including <span class="subst">$memoryOverhead</span> MB of overhead)"</span>)</div><div class="line"></div><div class="line">      <span class="comment">// 将要添加的container请求拆分到三个组：位置匹配列表、位置不匹配列表 和 无位置列表。</span></div><div class="line">      <span class="comment">// 对于位置匹配的 container 请求，将他们放到可用的地方，等待分配</span></div><div class="line">      <span class="comment">// 对于位置不匹配的和无位置的container请求，取消这些container的请求，因为 位置优先权已经变了，</span></div><div class="line">      <span class="keyword">val</span> (localRequests, staleRequests, anyHostRequests) = splitPendingAllocationsByLocality(</div><div class="line">        hostToLocalTaskCounts, pendingAllocate)</div><div class="line">      <span class="comment">// &lt;2&gt;</span></div><div class="line"></div><div class="line">      <span class="comment">// cancel "stale" requests for locations that are no longer needed</span></div><div class="line">      <span class="comment">// 对于位置不匹配的container，进行移除操作，并记录日志， 移除了N个container</span></div><div class="line">      staleRequests.foreach &#123; stale =&gt;</div><div class="line">        amClient.removeContainerRequest(stale)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">val</span> cancelledContainers = staleRequests.size</div><div class="line">      <span class="keyword">if</span> (cancelledContainers &gt; <span class="number">0</span>) &#123;</div><div class="line">        logInfo(<span class="string">s"Canceled <span class="subst">$cancelledContainers</span> container request(s) (locality no longer needed)"</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;3&gt;</span></div><div class="line"></div><div class="line">      <span class="comment">// 计算还可以分配的container的数量，就是 缺少的 + 取消的</span></div><div class="line">      <span class="comment">// 因为cancelledContainers的个数实际上就是从pendingAllocate 中取消的</span></div><div class="line">      <span class="keyword">val</span> availableContainers = missing + cancelledContainers</div><div class="line"></div><div class="line">      <span class="comment">// 计算潜在的container就是 可以分配的container + 上面那些 不限制位置的的contianer的数量</span></div><div class="line">      <span class="keyword">val</span> potentialContainers = availableContainers + anyHostRequests.size</div><div class="line"></div><div class="line">      <span class="comment">// TODO 这是在弄啥 ？？？ 应该是根据 潜在container的数量，生成对应个的contaner的位置引用</span></div><div class="line">      <span class="keyword">val</span> containerLocalityPreferences = <span class="keyword">if</span> (labelExpression.isEmpty) &#123;</div><div class="line">        containerPlacementStrategy.localityOfRequestedContainers(</div><div class="line">          potentialContainers, numLocalityAwareTasks, hostToLocalTaskCounts,</div><div class="line">          allocatedHostToContainersMap, localRequests)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="type">Array</span>.empty[<span class="type">ContainerLocalityPreferences</span>]</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// 根据上面的containerLocalityPreferences 创建container添加请求（注意是请求）</span></div><div class="line">      <span class="comment">// newLocalityRequest 用来记录要添加的container的请求</span></div><div class="line">      <span class="keyword">val</span> newLocalityRequests = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">ContainerRequest</span>]</div><div class="line">      containerLocalityPreferences.foreach &#123;</div><div class="line">        <span class="comment">// createContainerRequest 用来生成container的创建请求</span></div><div class="line">        <span class="keyword">case</span> <span class="type">ContainerLocalityPreferences</span>(nodes, racks) <span class="keyword">if</span> nodes != <span class="literal">null</span> =&gt;</div><div class="line">          newLocalityRequests += createContainerRequest(resource, nodes, racks)</div><div class="line">        <span class="keyword">case</span> _ =&gt;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// 需要再次判断 总的container的数量，如果availableContainers &gt; newLocalityRequests 表示，还不够</span></div><div class="line">      <span class="comment">// 为啥 availableContainers 会大于 newLocalityRequests ？ 因为 labelExpression.isEmpty 为空时，会生成一个空的Array</span></div><div class="line">      <span class="keyword">if</span> (availableContainers &gt;= newLocalityRequests.size) &#123;</div><div class="line">        <span class="comment">// more containers are available than needed for locality, fill in requests for any host</span></div><div class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until (availableContainers - newLocalityRequests.size)) &#123;</div><div class="line">          <span class="comment">// createContainerRequest 用来生成container的创建请求</span></div><div class="line">          newLocalityRequests += createContainerRequest(resource, <span class="literal">null</span>, <span class="literal">null</span>)</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line"></div><div class="line">        <span class="comment">// 这里的 newLocalityRequests 实际上对应的个是 potentialContainers = availableContainers + anyHostRequests.size</span></div><div class="line">        <span class="comment">// 因此，如果 newLocalityRequests &gt; availableContainers 则表示生成多了，且 anyHostRequests 的多了</span></div><div class="line"></div><div class="line">        <span class="keyword">val</span> numToCancel = newLocalityRequests.size - availableContainers</div><div class="line">        <span class="comment">// 因此释放到一些多余的 anyHostRequests</span></div><div class="line">        <span class="comment">// cancel some requests without locality preferences to schedule more local containers</span></div><div class="line">        anyHostRequests.slice(<span class="number">0</span>, numToCancel).foreach &#123; nonLocal =&gt;</div><div class="line">          amClient.removeContainerRequest(nonLocal)</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span> (numToCancel &gt; <span class="number">0</span>) &#123;</div><div class="line">          logInfo(<span class="string">s"Canceled <span class="subst">$numToCancel</span> unlocalized container requests to resubmit with locality"</span>)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;4&gt;</span></div><div class="line"></div><div class="line">      <span class="comment">// AMRMClient 请求添加container</span></div><div class="line">      newLocalityRequests.foreach &#123; request =&gt;</div><div class="line">        amClient.addContainerRequest(request)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;5&gt;</span></div><div class="line"></div><div class="line">      <span class="keyword">if</span> (log.isInfoEnabled()) &#123;</div><div class="line">        <span class="keyword">val</span> (localized, anyHost) = newLocalityRequests.partition(_.getNodes() != <span class="literal">null</span>)</div><div class="line">        <span class="keyword">if</span> (anyHost.nonEmpty) &#123;</div><div class="line">          logInfo(<span class="string">s"Submitted <span class="subst">$&#123;anyHost.size&#125;</span> unlocalized container requests."</span>)</div><div class="line">        &#125;</div><div class="line">        localized.foreach &#123; request =&gt;</div><div class="line">          logInfo(<span class="string">s"Submitted container request for host <span class="subst">$&#123;hostStr(request)&#125;</span>."</span>)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;6&gt;</span></div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (numPendingAllocate &gt; <span class="number">0</span> &amp;&amp; missing &lt; <span class="number">0</span>) &#123;</div><div class="line">      <span class="comment">// 如果不缺少executor，并且还有正在添加的executor</span></div><div class="line"></div><div class="line">      <span class="comment">// 计算要取消的contaner的数量，为什么是最小值，个人这样理解：-missing，其实是多出来的，但是在计算missing的时候，</span></div><div class="line">      <span class="comment">// 已经减去 numPendingAllocate 了，也就是说 numPendingAllocate 认为是已经使用的数量</span></div><div class="line">      <span class="comment">// 因此，如果取最大值，那么当 numPendingAllocate &gt; -missing 时，删除的container就太多了</span></div><div class="line">      <span class="keyword">val</span> numToCancel = math.min(numPendingAllocate, -missing)</div><div class="line">      logInfo(<span class="string">s"Canceling requests for <span class="subst">$numToCancel</span> executor container(s) to have a new desired "</span> +</div><div class="line">        <span class="string">s"total <span class="subst">$targetNumExecutors</span> executors."</span>)</div><div class="line"></div><div class="line">      <span class="comment">// 获取匹配的请求，并且有匹配的数据，则移除这些container（而且我猜测，这里的寻找匹配的请求，是在pending的队列中找的）</span></div><div class="line">      <span class="keyword">val</span> matchingRequests = amClient.getMatchingRequests(<span class="type">RM_REQUEST_PRIORITY</span>, <span class="type">ANY_HOST</span>, resource)</div><div class="line">      <span class="keyword">if</span> (!matchingRequests.isEmpty) &#123;</div><div class="line">        matchingRequests.iterator().next().asScala</div><div class="line">          .take(numToCancel).foreach(amClient.removeContainerRequest)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        logWarning(<span class="string">"Expected to find pending requests, but found none."</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;7&gt;</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p><1> 部分主要就是计算系统是否已经达到了目标个数的executor（container是executor的容器，目前一个container只包含了一个executor），并计算缺少的个数。计算的公式就是val missing = targetNumExecutors - numPendingAllocate - numExecutorsStarting.get - numExecutorsRunning.get。targetNumExecutors是配置中 spark.dynamicAllocation.minExecutors 、spark.dynamicAllocation.initialExecutors 以及 spark.executor.instances 三个配置中的最大值（开启了动态分配的话，如果没开启，则使用 spark.executor.instances 的值），并且要求 spark.dynamicAllocation.maxExecutors &gt;= spark.dynamicAllocation.initialExecutors &gt;= spark.dynamicAllocation.minExecutors（如果没有开启动态分配，则不存在这种要求）。这里涉及了4个配置项。numPendingAllocate的值调用AMRMClient的getMatchingRequests方法，获取location为*的所有请求。</1></p>
<p><2> 这一部分就是对添加中的请求（pendingAllocate）进行分类，分为位置自由的、位置匹配的和位置不匹配的。分类的依据是ContainerRequest.getNodes。如果nodes为空，则认为是位置自由的，如果nodes不空，则判断nodes是否hostToLocalTaskCounts的keyset有交集，如果有则认为是匹配的，否则不匹配。这里的匹配和不匹配，大概就是寻找本地的containerRequest（？？？？？？？？？？？？？？？？）。</2></p>
<p><3> 这一部分是对上面找出来的位置不匹配的请求，进行取消，并记录日志。筛选的规则是优先使用位置符合的，坚决不使用位置不符合的，数量不够的时候，使用位置自由的。</3></p>
<p><4> 这一部分是计算出需要创建的container个数。进行ContainerRequest的创建，首先创建位置符合的，然后创建位置自由的，并且将多余的位置自由的请求取消掉。</4></p>
<p><5> 在这里调用AMRMClient的addContainerRequest方法来增加ContainerRequest。</5></p>
<p><6> 记录日志</6></p>
<p><7> 将超出目标个数的container（位置自由的）取消。<br>所以从总体来看，这个方法其实就是把container的个数控制在目标个数范围内，如果缺少了，则增加，如果多了，则取消一些请求。</7></p>
<h2 id="handleAllocatedContainers"><a href="#handleAllocatedContainers" class="headerlink" title="handleAllocatedContainers"></a>handleAllocatedContainers</h2><p>此方法用来处理申请资源的container。方法的定义如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleAllocatedContainers</span></span>(allocatedContainers: <span class="type">Seq</span>[<span class="type">Container</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="comment">// 用来保存要使用的Containers</span></div><div class="line">    <span class="keyword">val</span> containersToUse = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>](allocatedContainers.size)</div><div class="line"></div><div class="line">    <span class="comment">// Match incoming requests by host 根据host进行匹配，匹配不成功的下面继续匹配</span></div><div class="line">    <span class="keyword">val</span> remainingAfterHostMatches = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]</div><div class="line">    <span class="keyword">for</span> (allocatedContainer &lt;- allocatedContainers) &#123;</div><div class="line">      matchContainerToRequest(allocatedContainer, allocatedContainer.getNodeId.getHost,</div><div class="line">        containersToUse, remainingAfterHostMatches)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// Match remaining by rack 对上面按照host匹配不成功的进行机架匹配</span></div><div class="line">    <span class="keyword">val</span> remainingAfterRackMatches = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]</div><div class="line">    <span class="keyword">for</span> (allocatedContainer &lt;- remainingAfterHostMatches) &#123;</div><div class="line">      <span class="keyword">val</span> rack = resolver.resolve(conf, allocatedContainer.getNodeId.getHost)</div><div class="line">      matchContainerToRequest(allocatedContainer, rack, containersToUse,</div><div class="line">        remainingAfterRackMatches)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 通过上面的两次匹配可以看到，最优先使用的是 host -&gt; rack -&gt; *</span></div><div class="line"></div><div class="line">    <span class="comment">// Assign remaining that are neither node-local nor rack-local</span></div><div class="line">    <span class="keyword">val</span> remainingAfterOffRackMatches = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]</div><div class="line">    <span class="keyword">for</span> (allocatedContainer &lt;- remainingAfterRackMatches) &#123;</div><div class="line">      matchContainerToRequest(allocatedContainer, <span class="type">ANY_HOST</span>, containersToUse,</div><div class="line">        remainingAfterOffRackMatches)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 三次匹配都没有成功的，就释放掉了</span></div><div class="line">    <span class="keyword">if</span> (!remainingAfterOffRackMatches.isEmpty) &#123;</div><div class="line">      logDebug(<span class="string">s"Releasing <span class="subst">$&#123;remainingAfterOffRackMatches.size&#125;</span> unneeded containers that were "</span> +</div><div class="line">        <span class="string">s"allocated to us"</span>)</div><div class="line">      <span class="keyword">for</span> (container &lt;- remainingAfterOffRackMatches) &#123;</div><div class="line">        internalReleaseContainer(container)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 启动所有的container，这些container上经过上面过滤后的可以使用container</span></div><div class="line">    runAllocatedContainers(containersToUse)</div><div class="line"></div><div class="line">    logInfo(<span class="string">"Received %d containers from YARN, launching executors on %d of them."</span></div><div class="line">      .format(allocatedContainers.size, containersToUse.size))</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>从代码来看，这个方法其实就是对allocatedContainers再次进行过滤，根据主机、机架 和 通配主机（*）。最后将不匹配的释放掉（不同于取消请求）。并在方法的最后启动所有分配的container启动。</p>
<h2 id="runAllocatedContainers"><a href="#runAllocatedContainers" class="headerlink" title="runAllocatedContainers"></a>runAllocatedContainers</h2><p>运行 container，方法定义如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runAllocatedContainers</span></span>(containersToUse: <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">for</span> (container &lt;- containersToUse) &#123;</div><div class="line">      <span class="comment">// executorIdConter 应该是同步的，用来生成新的executor的id</span></div><div class="line">      executorIdCounter += <span class="number">1</span></div><div class="line">      <span class="keyword">val</span> executorHostname = container.getNodeId.getHost</div><div class="line">      <span class="keyword">val</span> containerId = container.getId</div><div class="line">      <span class="keyword">val</span> executorId = executorIdCounter.toString</div><div class="line">      assert(container.getResource.getMemory &gt;= resource.getMemory)</div><div class="line">      logInfo(<span class="string">s"Launching container <span class="subst">$containerId</span> on host <span class="subst">$executorHostname</span> "</span> +</div><div class="line">        <span class="string">s"for executor with ID <span class="subst">$executorId</span>"</span>)</div><div class="line">      <span class="comment">//&lt;1&gt;</span></div><div class="line"></div><div class="line"></div><div class="line">      <span class="comment">// 主要是为了更新各种计数</span></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">updateInternalState</span></span>(): <span class="type">Unit</span> = synchronized &#123;</div><div class="line">        <span class="comment">// 正在运行的executor加一</span></div><div class="line">        numExecutorsRunning.incrementAndGet()</div><div class="line">        <span class="comment">// 启动中的executor 减一，因为已经运行了，就不能处于启动中了</span></div><div class="line">        numExecutorsStarting.decrementAndGet()</div><div class="line"></div><div class="line">        <span class="comment">//保存 executor -&gt; container的关系 以及 container -&gt; executorId的关系</span></div><div class="line">        executorIdToContainer(executorId) = container</div><div class="line">        containerIdToExecutorId(container.getId) = executorId</div><div class="line"></div><div class="line">        <span class="comment">// 记录 某个主机上都有哪些 containers</span></div><div class="line">        <span class="keyword">val</span> containerSet = allocatedHostToContainersMap.getOrElseUpdate(executorHostname,</div><div class="line">          <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">ContainerId</span>])</div><div class="line">        containerSet += containerId</div><div class="line">        <span class="comment">// 记录contain运行在那个主机上</span></div><div class="line">        allocatedContainerToHostMap.put(containerId, executorHostname)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;2&gt;</span></div><div class="line"></div><div class="line">      <span class="comment">// 如果正在运行的executor的数量小于 想要启动的executor个数</span></div><div class="line">      <span class="keyword">if</span> (numExecutorsRunning.get &lt; targetNumExecutors) &#123;</div><div class="line"></div><div class="line">        <span class="comment">// 将启动的executor的计数加一</span></div><div class="line">        <span class="comment">// 这里为啥要加一呢？？？</span></div><div class="line">        <span class="comment">// 因为 接下来要启动 container了，没有启动起来之前则认为处于启动中，因此先加一，一旦启动完成，则调用上面的updateInternalState</span></div><div class="line">        <span class="comment">// 方法，在这个方法中会对运行中的executor加一，启动中的executor减一。</span></div><div class="line">        numExecutorsStarting.incrementAndGet()</div><div class="line"></div><div class="line">        <span class="comment">// spark.yarn.launchContainers 配置的值</span></div><div class="line">        <span class="keyword">if</span> (launchContainers) &#123;</div><div class="line">          launcherPool.execute(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</div><div class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">              <span class="keyword">try</span> &#123;</div><div class="line">                <span class="comment">// 调用 ExecutorRunnalbe方法来启动container</span></div><div class="line">                <span class="keyword">new</span> <span class="type">ExecutorRunnable</span>(</div><div class="line">                  <span class="type">Some</span>(container),</div><div class="line">                  conf,</div><div class="line">                  sparkConf,</div><div class="line">                  driverUrl,</div><div class="line">                  executorId,</div><div class="line">                  executorHostname,</div><div class="line">                  executorMemory,</div><div class="line">                  executorCores,</div><div class="line">                  appAttemptId.getApplicationId.toString,</div><div class="line">                  securityMgr,</div><div class="line">                  localResources</div><div class="line">                ).run()</div><div class="line"></div><div class="line">                <span class="comment">// 跟新各种计数</span></div><div class="line">                updateInternalState()</div><div class="line">              &#125; <span class="keyword">catch</span> &#123;</div><div class="line">                <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">                  numExecutorsStarting.decrementAndGet()</div><div class="line">                  <span class="keyword">if</span> (<span class="type">NonFatal</span>(e)) &#123;</div><div class="line">                    logError(<span class="string">s"Failed to launch executor <span class="subst">$executorId</span> on container <span class="subst">$containerId</span>"</span>, e)</div><div class="line">                    <span class="comment">// Assigned container should be released immediately</span></div><div class="line">                    <span class="comment">// to avoid unnecessary resource occupation.</span></div><div class="line">                    <span class="comment">// 启动失败，则释放掉这个container</span></div><div class="line">                    amClient.releaseAssignedContainer(containerId)</div><div class="line">                  &#125; <span class="keyword">else</span> &#123;</div><div class="line">                    <span class="keyword">throw</span> e</div><div class="line">                  &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="comment">// For test only</span></div><div class="line">          updateInternalState()</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        logInfo((<span class="string">"Skip launching executorRunnable as runnning Excecutors count: %d "</span> +</div><div class="line">          <span class="string">"reached target Executors count: %d."</span>).format(</div><div class="line">          numExecutorsRunning.get, targetNumExecutors))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// &lt;3&gt;</span></div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p><1> 获取container和executor的信息，方便下面使用。</1></p>
<p><2> 定义了一个方法，这个方法就是用来更新各种数据关系，有executor运行的数量、executor启动的数量、executorId到container的对应关系、containerId到executorId的对应关系、host上运行的contianer的对应关系、container在哪个host运行的关系。</2></p>
<p><3> 异步生成ExecutorRunnable，并启动。</3></p>
<h2 id="processCompletedContainers"><a href="#processCompletedContainers" class="headerlink" title="processCompletedContainers"></a>processCompletedContainers</h2><p>此方法也在allocateResources()方法中调用，用来处理完成的container，完成的container可能是被kill掉，也可能是正常完成的。方法定义如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[yarn] <span class="function"><span class="keyword">def</span> <span class="title">processCompletedContainers</span></span>(completedContainers: <span class="type">Seq</span>[<span class="type">ContainerStatus</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="comment">// 循环完成的container</span></div><div class="line">    <span class="keyword">for</span> (completedContainer &lt;- completedContainers) &#123;</div><div class="line"></div><div class="line">      <span class="comment">// 获取containerId, 判断container是否在releasedContainers 中，获取 container所在的host</span></div><div class="line">      <span class="keyword">val</span> containerId = completedContainer.getContainerId</div><div class="line">      <span class="keyword">val</span> alreadyReleased = releasedContainers.remove(containerId)</div><div class="line">      <span class="keyword">val</span> hostOpt = allocatedContainerToHostMap.get(containerId)</div><div class="line">      <span class="keyword">val</span> onHostStr = hostOpt.map(host =&gt; <span class="string">s" on host: <span class="subst">$host</span>"</span>).getOrElse(<span class="string">""</span>)</div><div class="line">      <span class="comment">// &lt;1&gt;</span></div><div class="line"></div><div class="line">      <span class="comment">// 如果alreadyReleased为false，则表示releasedContainers 中没有这个container，或者是有但是删除失败</span></div><div class="line">      <span class="keyword">val</span> exitReason = <span class="keyword">if</span> (!alreadyReleased) &#123;</div><div class="line">        <span class="comment">// Decrement the number of executors running. The next iteration of</span></div><div class="line">        <span class="comment">// the ApplicationMaster's reporting thread will take care of allocating.</span></div><div class="line">        numExecutorsRunning.decrementAndGet()</div><div class="line">        logInfo(<span class="string">"Completed container %s%s (state: %s, exit status: %s)"</span>.format(</div><div class="line">          containerId,</div><div class="line">          onHostStr,</div><div class="line">          completedContainer.getState,</div><div class="line">          completedContainer.getExitStatus))</div><div class="line">        <span class="comment">// 获取container的退出状态</span></div><div class="line">        <span class="keyword">val</span> exitStatus = completedContainer.getExitStatus</div><div class="line"></div><div class="line">        <span class="comment">// 根据退出状态来判断是否是由于Application的原因</span></div><div class="line">        <span class="comment">// 以及退出的原因</span></div><div class="line">        <span class="comment">// ContainerExitStatus.SUCCESS 表示是因为YARN事件，不是因为运行的job发生error而导致的</span></div><div class="line">        <span class="comment">// ContainerExitStatus.PREEMPTED 表示主机上的端口被占用了</span></div><div class="line">        <span class="comment">// VMEM_EXCEEDED_EXIT_CODE 表示 虚拟内存的问题</span></div><div class="line">        <span class="comment">// PMEM_EXCEEDED_EXIT_CODE 表示 物理内存的问题</span></div><div class="line">        <span class="comment">// 否则</span></div><div class="line">        <span class="comment">// 除了第一种和第二种，其他的认为都属于 Application的原因</span></div><div class="line">        <span class="keyword">val</span> (exitCausedByApp, containerExitReason) = exitStatus <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="type">ContainerExitStatus</span>.<span class="type">SUCCESS</span> =&gt;</div><div class="line">            (<span class="literal">false</span>, <span class="string">s"Executor for container <span class="subst">$containerId</span> exited because of a YARN event (e.g., "</span> +</div><div class="line">              <span class="string">"pre-emption) and not because of an error in the running job."</span>)</div><div class="line">          <span class="keyword">case</span> <span class="type">ContainerExitStatus</span>.<span class="type">PREEMPTED</span> =&gt;</div><div class="line">            <span class="comment">// Preemption is not the fault of the running tasks, since YARN preempts containers</span></div><div class="line">            <span class="comment">// merely to do resource sharing, and tasks that fail due to preempted executors could</span></div><div class="line">            <span class="comment">// just as easily finish on any other executor. See SPARK-8167.</span></div><div class="line">            (<span class="literal">false</span>, <span class="string">s"Container <span class="subst">$&#123;containerId&#125;</span><span class="subst">$&#123;onHostStr&#125;</span> was preempted."</span>)</div><div class="line">          <span class="comment">// Should probably still count memory exceeded exit codes towards task failures</span></div><div class="line">          <span class="keyword">case</span> <span class="type">VMEM_EXCEEDED_EXIT_CODE</span> =&gt;</div><div class="line">            (<span class="literal">true</span>, memLimitExceededLogMessage(</div><div class="line">              completedContainer.getDiagnostics,</div><div class="line">              <span class="type">VMEM_EXCEEDED_PATTERN</span>))</div><div class="line">          <span class="keyword">case</span> <span class="type">PMEM_EXCEEDED_EXIT_CODE</span> =&gt;</div><div class="line">            (<span class="literal">true</span>, memLimitExceededLogMessage(</div><div class="line">              completedContainer.getDiagnostics,</div><div class="line">              <span class="type">PMEM_EXCEEDED_PATTERN</span>))</div><div class="line">          <span class="keyword">case</span> _ =&gt;</div><div class="line">            <span class="comment">// Enqueue the timestamp of failed executor</span></div><div class="line">            failedExecutorsTimeStamps.enqueue(clock.getTimeMillis())</div><div class="line">            (<span class="literal">true</span>, <span class="string">"Container marked as failed: "</span> + containerId + onHostStr +</div><div class="line">              <span class="string">". Exit status: "</span> + completedContainer.getExitStatus +</div><div class="line">              <span class="string">". Diagnostics: "</span> + completedContainer.getDiagnostics)</div><div class="line"></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// 如果是由于Application的原因退出的，则以警告记录日志，否则以info级别记录日志</span></div><div class="line">        <span class="keyword">if</span> (exitCausedByApp) &#123;</div><div class="line">          logWarning(containerExitReason)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          logInfo(containerExitReason)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// 生成一个ExecutorExited 对象并返回给变量</span></div><div class="line">        <span class="type">ExecutorExited</span>(exitStatus, exitCausedByApp, containerExitReason)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// 如果 alreadyReleased 为true，则表示 container已经被kill掉了，在 internalReleaseContainer 方法中会操作</span></div><div class="line">        <span class="type">ExecutorExited</span>(completedContainer.getExitStatus, exitCausedByApp = <span class="literal">false</span>,</div><div class="line">          <span class="string">s"Container <span class="subst">$containerId</span> exited from explicit termination request."</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;2&gt;</span></div><div class="line"></div><div class="line">      <span class="keyword">for</span> &#123;</div><div class="line">        host &lt;- hostOpt</div><div class="line">        <span class="comment">// 根据host获取上面所运行的container</span></div><div class="line">        containerSet &lt;- allocatedHostToContainersMap.get(host)</div><div class="line">      &#125; &#123;</div><div class="line">        <span class="comment">// 将container从host所包含的container集合中删除，这样host上的container集合就含有这个container了</span></div><div class="line">        containerSet.remove(containerId)</div><div class="line">        <span class="comment">// 删除完成后，如果 container 集合列表空了，则说明 host上只运行了这一个contianer，则删除host与container的对应关系，否则就更新一下</span></div><div class="line">        <span class="keyword">if</span> (containerSet.isEmpty) &#123;</div><div class="line">          allocatedHostToContainersMap.remove(host)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          allocatedHostToContainersMap.update(host, containerSet)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// 将container 到 host的映射关系也删除</span></div><div class="line">        allocatedContainerToHostMap.remove(containerId)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// 移除 container到executor的对应关系</span></div><div class="line">      containerIdToExecutorId.remove(containerId).foreach &#123; eid =&gt;</div><div class="line">        <span class="comment">// 将executor到container的对应关系也删除</span></div><div class="line">        executorIdToContainer.remove(eid)</div><div class="line"></div><div class="line">        <span class="comment">// 从 pendingLossReasonRequests 移除 executor</span></div><div class="line">        pendingLossReasonRequests.remove(eid) <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="type">Some</span>(pendingRequests) =&gt;</div><div class="line">            <span class="comment">// Notify application of executor loss reasons so it can decide whether it should abort</span></div><div class="line">            pendingRequests.foreach(_.reply(exitReason))</div><div class="line"></div><div class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">            releasedExecutorLossReasons.put(eid, exitReason)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// &lt;3&gt;</span></div><div class="line"></div><div class="line">        <span class="comment">// 如果没有被kill掉</span></div><div class="line">        <span class="keyword">if</span> (!alreadyReleased) &#123;</div><div class="line">          <span class="comment">// The executor could have gone away (like no route to host, node failure, etc)</span></div><div class="line">          <span class="comment">// Notify backend about the failure of the executor</span></div><div class="line">          <span class="comment">// container非异常释放的计数加一</span></div><div class="line">          numUnexpectedContainerRelease += <span class="number">1</span></div><div class="line">          <span class="comment">// 发送删除 execurot的请求</span></div><div class="line">          driverRef.send(<span class="type">RemoveExecutor</span>(eid, exitReason))</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// &lt;4&gt;</span></div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p><1> 获取containerId, 判断container是否在releasedContainers 中，获取 container所在的host，以便后面使用。</1></p>
<p><2> 分析contianer退出的原因，退出的类型分为 App引发的和非App引发的。具体的判断，可以看代码和注释。</2></p>
<p><3> 主要为了清理container各种关系的保存信息。</3></p>
<p><4> 使用Netty RPC发送移除Executor的信息。</4></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从上面的代码的调用过程以及实现我们可以看出，YarnAllocator实际上类似一个过滤器，它会从Resource Manager那里申请资源（通过AMRMClient获取），对获取到资源按照host -&gt; stack -&gt; any的顺序筛选container，并将合适的container启动后，反馈给调用者。<br>这个类功能很简单，但是在整个集群中又比较不太好理解，首先需要确定的是，YarnAllocator自己不管理资源（不对资源进行操作），只是筛选，虽然也会有释放、取消操作，但是这些操作都是调用Resource Manager的api来完成的。<br>另外，YarnAllocator是运行在driver上的，由AM来调用（？？？？？需要再次确认），因此，如果每个application都会有自己的YarnAllocator，它只是为自己的job的运行筛选container，而不是全局为所有的application统一筛选。</p>
<h1 id="AMRMClient的一些方法"><a href="#AMRMClient的一些方法" class="headerlink" title="AMRMClient的一些方法"></a>AMRMClient的一些方法</h1><table>
<thead>
<tr>
<th style="text-align:left">方法名</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">AllocateResponse allocate(float progressIndicator)</td>
<td style="text-align:left">请求额外的container并接收新的container的分配。</td>
</tr>
<tr>
<td style="text-align:left">List&lt;? extends Collection<t>&gt; getMatchingRequests(Priority priority, String resourceName, Resource capability)</t></td>
<td style="text-align:left">获取与给定参数匹配的未完成的请求。</td>
</tr>
<tr>
<td style="text-align:left">void removeContainerRequest(T req)</td>
<td style="text-align:left">移除之前的container请求。</td>
</tr>
<tr>
<td style="text-align:left">void addContainerRequest(T req)</td>
<td style="text-align:left">在调用allocate之前为container申请资源。</td>
</tr>
<tr>
<td style="text-align:left">void releaseAssignedContainer(ContainerId containerId)</td>
<td style="text-align:left">释放由Resource Manager分配的container。</td>
</tr>
<tr>
<td style="text-align:left">RegisterApplicationMasterResponse registerApplicationMaster(String appHostName, int appHostPort, String appTrackingUrl)</td>
<td style="text-align:left">注册application Master</td>
</tr>
</tbody>
</table>
<h1 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h1><h2 id="YarnAllocator是什么时候生成的"><a href="#YarnAllocator是什么时候生成的" class="headerlink" title="YarnAllocator是什么时候生成的"></a>YarnAllocator是什么时候生成的</h2><p>是在YarnRMClient的register方法中，向AMRMClient注册ApplicationMaster（调用AMRMClient.registerApplicationMaster方法）完成之后生成的。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">register</span></span>(</div><div class="line">      driverUrl: <span class="type">String</span>,</div><div class="line">      driverRef: <span class="type">RpcEndpointRef</span>,</div><div class="line">      conf: <span class="type">YarnConfiguration</span>,</div><div class="line">      sparkConf: <span class="type">SparkConf</span>,</div><div class="line">      uiAddress: <span class="type">Option</span>[<span class="type">String</span>],</div><div class="line">      uiHistoryAddress: <span class="type">String</span>,</div><div class="line">      securityMgr: <span class="type">SecurityManager</span>,</div><div class="line">      localResources: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">LocalResource</span>]</div><div class="line">    ): <span class="type">YarnAllocator</span> = &#123;</div><div class="line">    amClient = <span class="type">AMRMClient</span>.createAMRMClient()</div><div class="line">    amClient.init(conf)</div><div class="line">    amClient.start()</div><div class="line">    <span class="keyword">this</span>.uiHistoryAddress = uiHistoryAddress</div><div class="line"></div><div class="line">    <span class="keyword">val</span> trackingUrl = uiAddress.getOrElse &#123;</div><div class="line">      <span class="keyword">if</span> (sparkConf.get(<span class="type">ALLOW_HISTORY_SERVER_TRACKING_URL</span>)) uiHistoryAddress <span class="keyword">else</span> <span class="string">""</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    logInfo(<span class="string">"Registering the ApplicationMaster"</span>)</div><div class="line">    synchronized &#123;</div><div class="line">      amClient.registerApplicationMaster(<span class="type">Utils</span>.localHostName(), <span class="number">0</span>, trackingUrl)</div><div class="line">      registered = <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">new</span> <span class="type">YarnAllocator</span>(driverUrl, driverRef, conf, sparkConf, amClient, getAttemptId(), securityMgr,</div><div class="line">      localResources, <span class="keyword">new</span> <span class="type">SparkRackResolver</span>())</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/09/17/spark-resource-study/" itemprop="url">
                  Spark Resource Study
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-09-17T14:36:19+08:00" content="2018-09-17">
              2018-09-17
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Core"><a href="#Core" class="headerlink" title="Core"></a>Core</h1><h2 id="Partition-分区"><a href="#Partition-分区" class="headerlink" title="Partition 分区"></a>Partition 分区</h2><p>在Partition特征中，只定义了index，用来表示分区在父级RDD中索引。</p>
<h2 id="Dependency-依赖"><a href="#Dependency-依赖" class="headerlink" title="Dependency 依赖"></a>Dependency 依赖</h2><p>依赖的定义在 org.apache.spark.Dependency类中，从代码中可以看出，Dependency有2个子类，分别代表了2中类型的依赖，分别为：NarrowDependency和ShuffleDependency。其中NarrowDependency有两个子类：OneToOneDependency和RangeDependency。</p>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD是Resilient Distributed Dataset的简称，是Spark中的基本抽象。要实例化一个RDD，需要两个参数：SparkContext和Dependency列表。需要SparkContext是因为SparkContext提供了RDD的一些操作（如生成RDD的id，清理RDD的缓存、缓存RDD等），而Dependency则是因为它表示了RDD的继承关系。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RDD</span>[<span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></div><div class="line">    @transient private var _sc: <span class="type">SparkContext</span>,</div><div class="line">    @transient private var deps: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]]</div><div class="line">  ) <span class="keyword">extends</span> <span class="title">Serializable</span> <span class="keyword">with</span> <span class="title">Logging</span> &#123;</div></pre></td></tr></table></figure></p>
<p>此处可以看到RDD的基本构造方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(<span class="meta">@transient</span> oneParent: <span class="type">RDD</span>[_]) =</div><div class="line">    <span class="keyword">this</span>(oneParent.context, <span class="type">List</span>(<span class="keyword">new</span> <span class="type">OneToOneDependency</span>(oneParent)))</div></pre></td></tr></table></figure>
<p>使用已有的RDD构造新的RDD。</p>
<p>此外，RDD定义了一些抽象，需要子类进行实现：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] -- 计算给定的分区，返回一个迭代器</div><div class="line"></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] -- 返回<span class="type">RDD</span>的所有分区</div><div class="line"></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps -- 返回<span class="type">RDD</span>的到父类<span class="type">RDD</span>的所有依赖</div></pre></td></tr></table></figure></p>
<h2 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h2><h1 id="wordCount分析"><a href="#wordCount分析" class="headerlink" title="wordCount分析"></a>wordCount分析</h1><p>了解了一些代码之后，决定从wordCount的案例进行分析，以便了解Spark进行计算时的具体操作。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"master"</span>, <span class="string">"testApplication"</span>);</div><div class="line">sc.hadoopFile(<span class="string">"path"</span>, <span class="number">5</span>).map(_ =&gt; <span class="number">1</span>).count()</div></pre></td></tr></table></figure></p>
<p>因为在Spark中，transform是延迟执行的，也就是说，action之前的transform只有在遇到后面的action之后，才开始执行。因此上面的代码就要从count()开始。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum  </div><div class="line"><span class="comment">//Utils.getIteratorSize方法，在之后的代码块中展示，其实现就是根据参数的Iterator，计算一下这个迭代器中的数据个数（猜测迭代器最终是RDD分区的迭代器）</span></div><div class="line"><span class="comment">//这里的runJob的定义是</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</div><div class="line">    runJob(rdd, func, <span class="number">0</span> until rdd.partitions.length)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//然后又指向</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>, partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</div><div class="line">    <span class="keyword">val</span> cleanedFunc = clean(func)</div><div class="line">    runJob(rdd, (ctx: <span class="type">TaskContext</span>, it: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedFunc(it), partitions)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//然后继续指向</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>, partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</div><div class="line">    <span class="keyword">val</span> results = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">U</span>](partitions.size)</div><div class="line">    runJob[<span class="type">T</span>, <span class="type">U</span>](rdd, func, partitions, (index, res) =&gt; results(index) = res)</div><div class="line">    results</div><div class="line">  &#125;</div><div class="line"></div><div class="line"><span class="comment">//继续指向</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>, partitions: <span class="type">Seq</span>[<span class="type">Int</span>], resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    ...</div><div class="line">    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</div><div class="line">    progressBar.foreach(_.finishAll())</div><div class="line">    rdd.doCheckpoint()</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>最终执行Job落到了 dagScheduler 对象身上<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>, partitions: <span class="type">Seq</span>[<span class="type">Int</span>], callSite: <span class="type">CallSite</span>, resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>, properties: <span class="type">Properties</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> start = <span class="type">System</span>.nanoTime</div><div class="line">    <span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</div><div class="line">    ...</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>dagScheduler 的runJob方法中调用submitJob来提交任务</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>, partitions: <span class="type">Seq</span>[<span class="type">Int</span>], callSite: <span class="type">CallSite</span>, resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>, properties: <span class="type">Properties</span>):<span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</div><div class="line">    ...</div><div class="line">    <span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>(<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</div><div class="line">    eventProcessLoop.post(<span class="type">JobSubmitted</span>(jobId, rdd, func2, partitions.toArray, callSite, waiter, <span class="type">SerializationUtils</span>.clone(properties)))</div><div class="line">    waiter</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>最终，通过eventProcessLoop的post将任务提交到了任务执行队列。 这里需要注意的一个问题，加入任务队列的是一个 JobSubmitted对象。为什么要如此处理呢？需要从eventProcessLoop对象入手。<br>eventProcessLoop是DAGSchedulerEventProcessLoop的实例<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[scheduler] <span class="keyword">val</span> eventProcessLoop = <span class="keyword">new</span> <span class="type">DAGSchedulerEventProcessLoop</span>(<span class="keyword">this</span>)</div></pre></td></tr></table></figure></p>
<p>查看 DAGSchedulerEventProcessLoop 的定义<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[scheduler] <span class="class"><span class="keyword">class</span> <span class="title">DAGSchedulerEventProcessLoop</span>(<span class="params">dagScheduler: <span class="type">DAGScheduler</span></span>) <span class="keyword">extends</span> <span class="title">EventLoop</span>[<span class="type">DAGSchedulerEvent</span>](<span class="params">"dag-scheduler-event-loop"</span>) <span class="keyword">with</span> <span class="title">Logging</span></span></div></pre></td></tr></table></figure></p>
<p>DAGSchedulerEventProcessLoop继承于 EventLoop，EventLoop的内部有一个EventThread的线程，该线程从事件队列中循环获取数据<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> (!stopped.get) &#123;</div><div class="line">  <span class="keyword">val</span> event = eventQueue.take()</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    onReceive(event)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>DAGSchedulerEventProcessLoop的doOnReceive方法的定义如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</div><div class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>当从事件队列中获取到数据后，如果JobSubmitted对象，则调用dagScheduler的handleJobSubmitted方法。由此也知道了为什么eventProcessLoop.post()推的数据是 JobSubmitted 对象了。</p>
<p>再看handleJobSubmitted方法的定义：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</div><div class="line">  finalRDD: <span class="type">RDD</span>[_],</div><div class="line">  func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</div><div class="line">  partitions: <span class="type">Array</span>[<span class="type">Int</span>],</div><div class="line">  callSite: <span class="type">CallSite</span>,</div><div class="line">  listener: <span class="type">JobListener</span>,</div><div class="line">  properties: <span class="type">Properties</span>) &#123;</div><div class="line">	<span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></div><div class="line">	<span class="keyword">try</span> &#123;</div><div class="line">	  <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span></div><div class="line">	  <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span></div><div class="line">	  finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</div><div class="line">	&#125; <span class="keyword">catch</span> &#123;</div><div class="line">	  <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</div><div class="line">	    logWarning(<span class="string">"Creating new stage failed due to exception - job: "</span> + jobId, e)</div><div class="line">	    listener.jobFailed(e)</div><div class="line">	    <span class="keyword">return</span></div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, callSite, listener, properties)</div><div class="line">	</div><div class="line">	jobIdToActiveJob(jobId) = job</div><div class="line">	activeJobs += job</div><div class="line">	finalStage.setActiveJob(job)</div><div class="line">	<span class="keyword">val</span> stageIds = jobIdToStageIds(jobId).toArray</div><div class="line">	<span class="keyword">val</span> stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</div><div class="line">	listenerBus.post(</div><div class="line">	  <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))</div><div class="line">	submitStage(finalStage)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>由此看到了创建步骤（createResultStage(finalRDD, func, partitions, jobId, callSite)）和提交步骤（submitStage(finalStage)）的代码。</p>
<p>？？？上面的代码分析过程中，我们知道整个transform的触发点是从action（count()）开始的，而count是最后一个RDD（map生成的那个RDD）的方法。map对应的RDD是MapPartitionsRDD。在MapPartitionsRDD的compute方法中，而compute方法中使用的迭代器是从最开始的那个RDD开始的（ firstParent[T].iterator(split, context) ）。</p>
<p>创建步骤 createResultStage(finalRDD, func, partitions, jobId, callSite)</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/09/07/scal-study/" itemprop="url">
                  Scala Study
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-09-07T14:29:54+08:00" content="2018-09-07">
              2018-09-07
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文只是用来记录scala的一些语法信息</p>
<h1 id="Scala-Option"><a href="#Scala-Option" class="headerlink" title="Scala Option"></a>Scala Option</h1><p>/System/Library/Frameworks/JavaVM.framework/Versions/Current/Commands/java_home</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/09/05/optparse-python/" itemprop="url">
                  Python学习之optparse
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-09-05T11:30:34+08:00" content="2018-09-05">
              2018-09-05
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="optparse"><a href="#optparse" class="headerlink" title="optparse"></a>optparse</h1><p>在使用python进行命令开发的过程中，经常需要使用的就是命令行参数了，本章节介绍一个功能强大，易于使用的内建命令行参数处理模块optparse。</p>
<h2 id="简单的使用"><a href="#简单的使用" class="headerlink" title="简单的使用"></a>简单的使用</h2><p>首先需要创建一个OptionParser对象，该类属于optparse模块，因此使用前需要导入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> optparse <span class="keyword">import</span> OptionParser</div><div class="line">parser = OptionParser(usage=<span class="string">"usage: %prog action [options]"</span>,)</div><div class="line"></div><div class="line">或</div><div class="line"></div><div class="line"><span class="keyword">import</span> optparse</div><div class="line">parser = optparse.OptionParser(usage=<span class="string">"usage: %prog action [options]"</span>,)</div></pre></td></tr></table></figure>
<p>可以不写参数，参数用来指定帮助的显示信息，如果没有指定，则默认显示“usage: %prog [options]”。</p>
<h3 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">def __init__(self,</div><div class="line">             usage=None,</div><div class="line">             option_list=None,</div><div class="line">             option_class=Option,</div><div class="line">             version=None,</div><div class="line">             conflict_handler=&quot;error&quot;,</div><div class="line">             description=None,</div><div class="line">             formatter=None,</div><div class="line">             add_help_option=True,</div><div class="line">             prog=None,</div><div class="line">             epilog=None):</div><div class="line"></div><div class="line">作者：fuyoufang</div><div class="line">链接：https://www.jianshu.com/p/bec089061742</div><div class="line">來源：简书</div><div class="line">简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。</div></pre></td></tr></table></figure>
<h3 id="add-option函数"><a href="#add-option函数" class="headerlink" title="add_option函数"></a>add_option函数</h3><p>创建OptionParser对象之后，就可以使用add_option来定义命令行参数了<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">parsser.add_option(...)</div></pre></td></tr></table></figure></p>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p>add_option的前两参数分别为短参数名和长参数名。其中长参数名可以省略。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">parser.add_option(<span class="string">"-f"</span>, dest=<span class="string">"fileName"</span>)</div></pre></td></tr></table></figure></p>
<h4 id="dest"><a href="#dest" class="headerlink" title="dest"></a>dest</h4><p>定义程序内参数值的名字，之后通过这个名字来取参数的值。如果为空，则使用不加“-”的短参数名。</p>
<h4 id="action"><a href="#action" class="headerlink" title="action"></a>action</h4><p>action是add_option的一个参数，用来指定解析到参数后的操作。默认值为“store”，表示将参数值保存到options对象中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">parser.add_option(<span class="string">"-f"</span>, <span class="string">"--file"</span>, action=<span class="string">"store"</span>, dest=<span class="string">"filename"</span>)</div><div class="line">(options, args) = parser.parse_args([<span class="string">"-f"</span>, <span class="string">"myfile.log"</span>])</div><div class="line"><span class="keyword">print</span> options.filename</div></pre></td></tr></table></figure></p>
<p>其中action值的store还可以有其他两种类型：store_ture和store_false，用来处理不带参数值的参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">parser.add_option(<span class="string">"-g"</span>, action=<span class="string">"store_true"</span>, dest=<span class="string">"isGoodUser"</span>)</div><div class="line">parser.add_option(<span class="string">"-s"</span>, action=<span class="string">"store_false"</span>, dest=<span class="string">"isGoodUser"</span>)</div></pre></td></tr></table></figure></p>
<p>action可以使用的值出了store_true和store_false之外，还可以使用store、store_const、append、count、callback等。</p>
<h4 id="type"><a href="#type" class="headerlink" title="type"></a>type</h4><p>type是add_option方法的一个参数，用来指定参数值的类型，默认为String。还可以指定为“int”、“float”等<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">parser.add_option(<span class="string">"-n"</span>, type=<span class="string">"int"</span>, dest=<span class="string">"num"</span>)</div></pre></td></tr></table></figure></p>
<h4 id="default"><a href="#default" class="headerlink" title="default"></a>default</h4><p>通过add_option方法的default参数可以对命令行参数设置默认值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">parser.add_option(<span class="string">"-f"</span>, <span class="string">"--file"</span>, default=<span class="string">"test.log"</span>)</div></pre></td></tr></table></figure></p>
<h4 id="help"><a href="#help" class="headerlink" title="help"></a>help</h4><p>用于指定参数在帮助程序中显示的信息，详细请见下方的“生成帮助”章节。</p>
<h4 id="metavar"><a href="#metavar" class="headerlink" title="metavar"></a>metavar</h4><p>metavar配合help参数，用于帮助提醒用户该命令行所期待的参数，详细请见下方的“生成帮助”章节。</p>
<h4 id="choices"><a href="#choices" class="headerlink" title="choices"></a>choices</h4><p>当type为choices时，需要设置此值。</p>
<h4 id="const"><a href="#const" class="headerlink" title="const"></a>const</h4><p>指定一个常数，配合action为store_const和append_const时一起使用。</p>
<h3 id="parse-args函数"><a href="#parse-args函数" class="headerlink" title="parse_args函数"></a>parse_args函数</h3><p>一旦定义好了所有的命令行参数，就可以调用parse_args()来解析命令行。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(options, args) = parser.parse_args()</div></pre></td></tr></table></figure></p>
<h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h4><p>你可以传递一个命令行参数列表给parse_args方法，否则默认使用sys.argv[:1]。<br>parse_args方法有两个返回值：</p>
<blockquote>
<p>options optpars.Values对象，保存了所有命令行的值。只要知道命令行参数名，就可以得到。<br>args 一个由 positional arguments组成的列表。</p>
</blockquote>
<h3 id="set-defaults函数"><a href="#set-defaults函数" class="headerlink" title="set_defaults函数"></a>set_defaults函数</h3><p>除了在add_option方法中使用default参数设置默认值，还可以使用set_default函数，来统一设置默认值。该方法应该在所有add_option函数之前调用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">parser.set_default(filename=<span class="string">"test.log"</span>, isGoodUser=<span class="keyword">False</span>)</div></pre></td></tr></table></figure></p>
<p>set_defaults函数，可以用来设置多个默认值。</p>
<h3 id="has-option方法"><a href="#has-option方法" class="headerlink" title="has_option方法"></a>has_option方法</h3><p>用来检查是否有相应的选项<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> parser.has_option(<span class="string">"fileName"</span>)</div></pre></td></tr></table></figure></p>
<h3 id="remove-option"><a href="#remove-option" class="headerlink" title="remove_option()"></a>remove_option()</h3><p>用来删除相应的选项。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> parser.remove_option(<span class="string">"fileName"</span>)</div></pre></td></tr></table></figure></p>
<h2 id="生成帮助"><a href="#生成帮助" class="headerlink" title="生成帮助"></a>生成帮助</h2><p>optparse的另一个方便的功能便是自动生成帮助，而你需要做的事情就是在调用add_option方法时指定help参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">parser = optparse.OptionParser(usage=<span class="string">"usage: %prog action [options]"</span>,)</div><div class="line">parser.add_option(<span class="string">"-v"</span>, action=<span class="string">"store_true"</span>, metavar=<span class="string">"参数的期望值"</span>, help=<span class="string">"这是-v的参数的意义"</span>)</div></pre></td></tr></table></figure></p>
<p>当optparse解析到-h或者–help时，就会调用parser.print_help()方法来打印帮助信息。</p>
<h2 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h2><h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><p>在出现用户输入无效的、不完整的命令行参数而发生异常时，optparse可以自动检测并处理，比如参数值类型错误等。<br>用户也可以使用parser.error函数来手动抛出异常。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">parser.parse_args()</div><div class="line"><span class="keyword">if</span> parser.isGooodUser:</div><div class="line">    parse.error(<span class="string">"this is an exception"</span>)</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/09/04/ambari-resource/" itemprop="url">
                  Ambari Resource 01
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-09-04T15:03:02+08:00" content="2018-09-04">
              2018-09-04
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在使用Ambari的过程中遇到了好多问题，比如删除一个cluster（使用ambari-server reset命令）后，重启Ambari Server之后一直报错，提示找不到集群。尝试各种方法之后，无法找到满意的解决之道的情况下，只好硬着头皮读读源码，了解一下Ambari的启动机制。在此记下源码阅读的笔记和心得。</p>
<p>首先分析一下Ambari Server的启动脚本(ambari-server.py)，以便了解Ambari Server是如何启动的。</p>
<p>首先看入口函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  <span class="keyword">try</span>:</div><div class="line">    mainBody()</div><div class="line">  <span class="keyword">except</span> (KeyboardInterrupt, EOFError):</div><div class="line">    print(<span class="string">"\nAborting ... Keyboard Interrupt."</span>)</div><div class="line">    sys.exit(<span class="number">1</span>)</div></pre></td></tr></table></figure></p>
<p>没什么可说的，就是调用文件中的 mainBody() 方法。</p>
<h3 id="mainBody方法"><a href="#mainBody方法" class="headerlink" title="mainBody方法"></a>mainBody方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mainBody</span><span class="params">()</span>:</span></div><div class="line">  <span class="comment">#初始化命令行参数解析器</span></div><div class="line">  parser = optparse.OptionParser(usage=<span class="string">"usage: %prog action [options]"</span>,)</div><div class="line">  action = sys.argv[<span class="number">1</span>]</div><div class="line"></div><div class="line">  <span class="comment">#init_action_parser方法，在脚本中会有两个方法，一个是针对windows的一个是针对linux的，所以看代码的时候需要注意（方法上使用@OsFamilyFuncImpl标示），我们这里看的是linux的</span></div><div class="line">  <span class="comment">#就是对命令行解析器进行初始化，并针对行为进行进一步的初始化</span></div><div class="line">  init_action_parser(action, parser)</div><div class="line"></div><div class="line">  <span class="comment">#使用命令行参数解析器，对命令行进行解析</span></div><div class="line">  (options, args) = parser.parse_args()</div><div class="line"></div><div class="line">  <span class="comment"># check if only silent key set</span></div><div class="line">  default_options = parser.get_default_values()</div><div class="line">  silent_options = default_options</div><div class="line">  silent_options.silent = <span class="keyword">True</span></div><div class="line"></div><div class="line">  <span class="keyword">if</span> options == silent_options:</div><div class="line">    options.only_silent = <span class="keyword">True</span></div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    options.only_silent = <span class="keyword">False</span></div><div class="line"></div><div class="line">  <span class="comment"># varbose是在init_action_parser方法中设置的，对应的-v参数。是否打印状态信息</span></div><div class="line">  <span class="comment"># set_varbose方法位于 ambari_commons.logging_utils中，其实就是设置全局变量_VERBOSE的值。</span></div><div class="line">  <span class="comment"># set verbose</span></div><div class="line">  set_verbose(options.verbose)</div><div class="line"></div><div class="line">  <span class="comment">#接下来就是调用main方法。这里目测是同一个main方法，最大的区别就是有异常处理，有待之后补充TODO--------------------？？？</span></div><div class="line">  <span class="keyword">if</span> options.verbose:</div><div class="line">    main(options, args, parser)</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">      main(options, args, parser)</div><div class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">      print_error_msg(<span class="string">"Unexpected &#123;0&#125;: &#123;1&#125;"</span>.format((e).__class__.__name__, str(e)) +\</div><div class="line">      <span class="string">"\nFor more info run ambari-server with -v or --verbose option"</span>)</div><div class="line">      sys.exit(<span class="number">1</span>)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_action_parser</span><span class="params">(action, parser)</span>:</span></div><div class="line">  <span class="comment">#定义了参数可用的行为，如ambari-server start，其中start就是行为。这里定义了行为以及对应行为处理操作。</span></div><div class="line">  <span class="comment">#这些行为的处理方法其实是进一步的初始化参数解析器，会根据对应的行为对参数解析器进行不同的初始化。</span></div><div class="line">  action_parser_map = &#123;</div><div class="line">    SETUP_ACTION: init_setup_parser_options,</div><div class="line">    SETUP_JCE_ACTION: init_empty_parser_options,</div><div class="line">    START_ACTION: init_start_parser_options,</div><div class="line">    STOP_ACTION: init_empty_parser_options,</div><div class="line">    RESTART_ACTION: init_start_parser_options,</div><div class="line">    RESET_ACTION: init_reset_parser_options,</div><div class="line">    STATUS_ACTION: init_empty_parser_options,</div><div class="line">    UPGRADE_ACTION: init_empty_parser_options,</div><div class="line">    LDAP_SETUP_ACTION: init_ldap_setup_parser_options,</div><div class="line">    LDAP_SYNC_ACTION: init_ldap_sync_parser_options,</div><div class="line">    SET_CURRENT_ACTION: init_set_current_parser_options,</div><div class="line">    SETUP_SECURITY_ACTION: init_setup_security_parser_options,</div><div class="line">    REFRESH_STACK_HASH_ACTION: init_empty_parser_options,</div><div class="line">    BACKUP_ACTION: init_empty_parser_options,</div><div class="line">    RESTORE_ACTION: init_empty_parser_options,</div><div class="line">    UPDATE_HOST_NAMES_ACTION: init_empty_parser_options,</div><div class="line">    CHECK_DATABASE_ACTION: init_empty_parser_options,</div><div class="line">    ENABLE_STACK_ACTION: init_enable_stack_parser_options,</div><div class="line">    SETUP_SSO_ACTION: init_setup_sso_options,</div><div class="line">    DB_PURGE_ACTION: init_db_purge_parser_options,</div><div class="line">    INSTALL_MPACK_ACTION: init_install_mpack_parser_options,</div><div class="line">    UNINSTALL_MPACK_ACTION: init_uninstall_mpack_parser_options,</div><div class="line">    UPGRADE_MPACK_ACTION: init_upgrade_mpack_parser_options,</div><div class="line">    PAM_SETUP_ACTION: init_pam_setup_parser_options,</div><div class="line">    KERBEROS_SETUP_ACTION: init_kerberos_setup_parser_options,</div><div class="line">  &#125;</div><div class="line">  parser.add_option(<span class="string">"-v"</span>, <span class="string">"--verbose"</span>,</div><div class="line">                    action=<span class="string">"store_true"</span>, dest=<span class="string">"verbose"</span>, default=<span class="keyword">False</span>,</div><div class="line">                    help=<span class="string">"Print verbose status messages"</span>)</div><div class="line">  parser.add_option(<span class="string">"-s"</span>, <span class="string">"--silent"</span>,</div><div class="line">                    action=<span class="string">"store_true"</span>, dest=<span class="string">"silent"</span>, default=<span class="keyword">False</span>,</div><div class="line">                    help=<span class="string">"Silently accepts default prompt values. For db-cleanup command, silent mode will stop ambari server."</span>)</div><div class="line">  <span class="keyword">try</span>:</div><div class="line">    <span class="comment">#根据行为，做进一步的解析器初始化，我们以setup行为为例，那么setup行为的参数解析器对应的是init_setup_parser_options函数。</span></div><div class="line">    action_parser_map[action](parser)</div><div class="line">  <span class="keyword">except</span> KeyError:</div><div class="line">    parser.error(<span class="string">"Invalid action: "</span> + action)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_setup_parser_options</span><span class="params">(parser)</span>:</span></div><div class="line">  database_group = optparse.OptionGroup(parser, <span class="string">'Database options (command need to include all options)'</span>)</div><div class="line">  database_group.add_option(<span class="string">'--database'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Database to use embedded|oracle|mysql|mssql|postgres|sqlanywhere"</span>, dest=<span class="string">"dbms"</span>)</div><div class="line">  database_group.add_option(<span class="string">'--databasehost'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Hostname of database server"</span>, dest=<span class="string">"database_host"</span>)</div><div class="line">  database_group.add_option(<span class="string">'--databaseport'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Database port"</span>, dest=<span class="string">"database_port"</span>)</div><div class="line">  database_group.add_option(<span class="string">'--databasename'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Database/Service name or ServiceID"</span>,</div><div class="line">                            dest=<span class="string">"database_name"</span>)</div><div class="line">  database_group.add_option(<span class="string">'--databaseusername'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Database user login"</span>, dest=<span class="string">"database_username"</span>)</div><div class="line">  database_group.add_option(<span class="string">'--databasepassword'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Database user password"</span>, dest=<span class="string">"database_password"</span>)</div><div class="line">  parser.add_option_group(database_group)</div><div class="line"></div><div class="line">  jdbc_group = optparse.OptionGroup(parser, <span class="string">'JDBC options (command need to include all options)'</span>)</div><div class="line">  jdbc_group.add_option(<span class="string">'--jdbc-driver'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Specifies the path to the JDBC driver JAR file or archive "</span> \</div><div class="line">                                                            <span class="string">"with all required files(jdbc jar, libraries and etc), for the "</span> \</div><div class="line">                                                            <span class="string">"database type specified with the --jdbc-db option. "</span> \</div><div class="line">                                                            <span class="string">"Used only with --jdbc-db option. Archive is supported only for"</span> \</div><div class="line">                                                            <span class="string">" sqlanywhere database."</span> ,</div><div class="line">                        dest=<span class="string">"jdbc_driver"</span>)</div><div class="line">  jdbc_group.add_option(<span class="string">'--jdbc-db'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Specifies the database type [postgres|mysql|mssql|oracle|hsqldb|sqlanywhere] for the "</span> \</div><div class="line">                                                        <span class="string">"JDBC driver specified with the --jdbc-driver option. Used only with --jdbc-driver option."</span>,</div><div class="line">                        dest=<span class="string">"jdbc_db"</span>)</div><div class="line">  parser.add_option_group(jdbc_group)</div><div class="line"></div><div class="line">  other_group = optparse.OptionGroup(parser, <span class="string">'Other options'</span>)</div><div class="line"></div><div class="line">  other_group.add_option(<span class="string">'-j'</span>, <span class="string">'--java-home'</span>, default=<span class="keyword">None</span>,</div><div class="line">                         help=<span class="string">"Use specified java_home.  Must be valid on all hosts"</span>)</div><div class="line">  other_group.add_option(<span class="string">'--stack-java-home'</span>, dest=<span class="string">"stack_java_home"</span>, default=<span class="keyword">None</span>,</div><div class="line">                    help=<span class="string">"Use specified java_home for stack services.  Must be valid on all hosts"</span>)</div><div class="line">  other_group.add_option(<span class="string">'--skip-view-extraction'</span>, action=<span class="string">"store_true"</span>, default=<span class="keyword">False</span>, help=<span class="string">"Skip extraction of system views"</span>, dest=<span class="string">"skip_view_extraction"</span>)</div><div class="line">  other_group.add_option(<span class="string">'--postgresschema'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Postgres database schema name"</span>,</div><div class="line">                         dest=<span class="string">"postgres_schema"</span>)</div><div class="line">  other_group.add_option(<span class="string">'--sqla-server-name'</span>, default=<span class="keyword">None</span>, help=<span class="string">"SQL Anywhere server name"</span>, dest=<span class="string">"sqla_server_name"</span>)</div><div class="line">  other_group.add_option(<span class="string">'--sidorsname'</span>, default=<span class="string">"sname"</span>, help=<span class="string">"Oracle database identifier type, Service ID/Service "</span></div><div class="line">                                                               <span class="string">"Name sid|sname"</span>, dest=<span class="string">"sid_or_sname"</span>)</div><div class="line">  other_group.add_option(<span class="string">'--enable-lzo-under-gpl-license'</span>, action=<span class="string">"store_true"</span>, default=<span class="keyword">False</span>, help=<span class="string">"Automatically accepts GPL license"</span>, dest=<span class="string">"accept_gpl"</span>)</div><div class="line"></div><div class="line">  <span class="comment"># the --master-key option is needed in the event passwords in the ambari.properties file are encrypted</span></div><div class="line">  other_group.add_option(<span class="string">'--master-key'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Master key for encrypting passwords"</span>, dest=<span class="string">"master_key"</span>)</div><div class="line"></div><div class="line">  parser.add_option_group(other_group)</div></pre></td></tr></table></figure>
<p>这个方法就是为setup的行为，做了进一步的解析器设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">_VERBOSE = <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_verbose</span><span class="params">(newVal)</span>:</span></div><div class="line">  <span class="keyword">global</span> _VERBOSE</div><div class="line">  _VERBOSE = newVal</div></pre></td></tr></table></figure>
<p>该方法就是设置全局变量_VERBOSE的值。</p>
<h3 id="main"><a href="#main" class="headerlink" title="main"></a>main</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(options, args, parser)</span>:</span></div><div class="line">  init_logging()</div><div class="line"></div><div class="line">  <span class="comment"># set silent</span></div><div class="line">  set_silent(options.silent)</div><div class="line"></div><div class="line">  <span class="comment"># debug mode</span></div><div class="line">  set_debug_mode_from_options(options)</div><div class="line">  init_debug(options)</div><div class="line"></div><div class="line">  <span class="comment">#perform checks</span></div><div class="line"></div><div class="line">  options.warnings = []</div><div class="line"></div><div class="line">  <span class="keyword">if</span> len(args) == <span class="number">0</span>:</div><div class="line">    <span class="keyword">print</span> parser.print_help()</div><div class="line">    parser.error(<span class="string">"No action entered"</span>)</div><div class="line"></div><div class="line">  action_map = create_user_action_map(args, options)</div><div class="line"></div><div class="line">  action = args[<span class="number">0</span>]</div><div class="line"></div><div class="line">  <span class="keyword">try</span>:</div><div class="line">    action_obj = action_map[action]</div><div class="line">  <span class="keyword">except</span> KeyError:</div><div class="line">    parser.error(<span class="string">"Invalid action: "</span> + action)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> action == SETUP_ACTION:</div><div class="line">    <span class="keyword">if</span> are_cmd_line_db_args_blank(options):</div><div class="line">      options.must_set_database_options = <span class="keyword">True</span></div><div class="line">    <span class="keyword">elif</span> <span class="keyword">not</span> are_cmd_line_db_args_valid(options):</div><div class="line">      parser.error(<span class="string">'All database options should be set. Please see help for the options.'</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      options.must_set_database_options = <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="comment">#correct database</span></div><div class="line">    fix_database_options(options, parser)</div><div class="line"></div><div class="line">  matches = <span class="number">0</span></div><div class="line">  <span class="keyword">for</span> args_number_required <span class="keyword">in</span> action_obj.possible_args_numbers:</div><div class="line">    matches += int(len(args) == args_number_required)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> matches == <span class="number">0</span>:</div><div class="line">    <span class="keyword">print</span> parser.print_help()</div><div class="line">    possible_args = <span class="string">' or '</span>.join(str(x) <span class="keyword">for</span> x <span class="keyword">in</span> action_obj.possible_args_numbers)</div><div class="line">    parser.error(<span class="string">"Invalid number of arguments. Entered: "</span> + str(len(args)) + <span class="string">", required: "</span> + possible_args)</div><div class="line"></div><div class="line">  options.exit_message = <span class="string">"Ambari Server '%s' completed successfully."</span> % action</div><div class="line">  options.exit_code = <span class="keyword">None</span></div><div class="line"></div><div class="line">  <span class="keyword">try</span>:</div><div class="line">    <span class="keyword">if</span> action <span class="keyword">in</span> _action_option_dependence_map:</div><div class="line">      required, optional = _action_option_dependence_map[action]</div><div class="line">      <span class="keyword">for</span> opt_str, opt_dest <span class="keyword">in</span> required:</div><div class="line">        <span class="keyword">if</span> hasattr(options, opt_dest) <span class="keyword">and</span> getattr(options, opt_dest) <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">          <span class="keyword">print</span> <span class="string">"Missing option &#123;0&#125; for action &#123;1&#125;"</span>.format(opt_str, action)</div><div class="line">          print_action_arguments_help(action)</div><div class="line">          <span class="keyword">print</span> <span class="string">"Run ambari-server.py --help to see detailed description of each option"</span></div><div class="line">          <span class="keyword">raise</span> FatalException(<span class="number">1</span>, <span class="string">"Missing option"</span>)</div><div class="line">    action_obj.execute()</div><div class="line"></div><div class="line">    <span class="keyword">if</span> action_obj.need_restart:</div><div class="line">      pstatus, pid = is_server_runing()</div><div class="line">      <span class="keyword">if</span> pstatus:</div><div class="line">        <span class="keyword">print</span> <span class="string">'NOTE: Restart Ambari Server to apply changes'</span> + \</div><div class="line">              <span class="string">' ("ambari-server restart|stop+start")'</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> options.warnings:</div><div class="line">      <span class="keyword">for</span> warning <span class="keyword">in</span> options.warnings:</div><div class="line">        print_warning_msg(warning)</div><div class="line">        <span class="keyword">pass</span></div><div class="line">      options.exit_message = <span class="string">"Ambari Server '%s' completed with warnings."</span> % action</div><div class="line">      <span class="keyword">pass</span></div><div class="line">  <span class="keyword">except</span> FatalException <span class="keyword">as</span> e:</div><div class="line">    <span class="keyword">if</span> e.reason <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">      print_error_msg(<span class="string">"Exiting with exit code &#123;0&#125;. \nREASON: &#123;1&#125;"</span>.format(e.code, e.reason))</div><div class="line">      logger.exception(str(e))</div><div class="line">    sys.exit(e.code)</div><div class="line">  <span class="keyword">except</span> NonFatalException <span class="keyword">as</span> e:</div><div class="line">    options.exit_message = <span class="string">"Ambari Server '%s' completed with warnings."</span> % action</div><div class="line">    <span class="keyword">if</span> e.reason <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">      print_warning_msg(e.reason)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> options.exit_message <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">    <span class="keyword">print</span> options.exit_message</div><div class="line"></div><div class="line">  <span class="keyword">if</span> options.exit_code <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:  <span class="comment"># not all actions may return a system exit code</span></div><div class="line">    sys.exit(options.exit_code)</div></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/20/ambari-doc/" itemprop="url">
                  ambari_doc
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-20T15:57:27+08:00" content="2018-08-20">
              2018-08-20
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/Ambari/" itemprop="url" rel="index">
                    <span itemprop="name">Ambari</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Stacks-and-Services"><a href="#Stacks-and-Services" class="headerlink" title="Stacks and Services"></a>Stacks and Services</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Ambari支持Stack的概念，并且将服务组合在一个Stack定义中。通过堆栈的作用，Ambari有统一定义的安装接口，管理并监控一组服务，并且引入了Stacks+Servides来提供延伸。<br>从Ambari2.3开始，还支持Extension的概念，并将自定义服务组合在一个Extension定义中。</p>
<h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h2><table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Stack</td>
<td style="text-align:left">定义了一组服务，并且这些服务从这里获取软件包。一个Stack能够有一个或多个版本，并且每个版本可以是活跃/不活跃的。例如, Stack=”HDP-1.3.3”。</td>
</tr>
<tr>
<td style="text-align:left">Extension</td>
<td style="text-align:left">定义了一组自定义服务，这些自定义服务可以被添加到一个stack版本中。一个Extension能够有一个或多个版本。</td>
</tr>
<tr>
<td style="text-align:left">Service</td>
<td style="text-align:left">定义Components（MASTER、SLAVE、CLIENT）组成了Service。如，Service=”HDFS”。</td>
</tr>
<tr>
<td style="text-align:left">Component</td>
<td style="text-align:left">每个Component遵循确切的生命周期（start、stop、install等）。例如：Service=”HDFS”包含的组件有：”NameNode(MASTER)”、”Secondary NameNode(MASTER)”、”DataNode(SLAVE)”和”HDFS Client(CLIENT)”</td>
</tr>
</tbody>
</table>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Stack的定义可以在源码结构的/ambari-server/src/main/resources/stacks中找到。在你安装了Ambari服务之后，Stack的定义可以在/var/lib/ambari-server/resources/stacks中找到。</p>
<h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><p>Stack定义的结构如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">|_ stacks</div><div class="line">   |_ &lt;stack_name&gt;</div><div class="line">      |_ &lt;stack_version&gt;</div><div class="line">         metainfo.xml</div><div class="line">         |_ hooks</div><div class="line">         |_ repos</div><div class="line">            repoinfo.xml</div><div class="line">         |_ services</div><div class="line">            |_ &lt;service_name&gt;</div><div class="line">               metainfo.xml</div><div class="line">               metrics.json</div><div class="line">               |_ configuration</div><div class="line">                  &#123;configuration files&#125;</div><div class="line">               |_ package</div><div class="line">                  &#123;files, scripts, templates&#125;</div></pre></td></tr></table></figure></p>
<h2 id="Defining-a-Service-and-Components"><a href="#Defining-a-Service-and-Components" class="headerlink" title="Defining a Service and Components"></a>Defining a Service and Components</h2><p>Service中的metainfo.xml描述了这个service、这个service的Components以及管理执行命令的脚本。服务的一个组件只能是MASTER、SLAVE或CLIENT三种类型中的一个。组件的类型用来告诉Ambari管理和监控这个组件的默认脚本。<br>对于每个Component，<commandscript>用来指从合适执行脚本。这个Component必须支持的一组默认命令。</commandscript></p>
<p>| Component Category | Default Lifecycle Commands |<br>| MASTER             | install, start, stop, configure, status |<br>| SLAVE              | install, start, stop, configure, status |<br>| CLIENT             | install, configure, status |</p>
<p>Ambari支持用PYTHON写的不同命令。类型用来知道如何执行命令脚本。如果你的Component需要支持多于默认生命周期的命令，你也能够创建自定义命令。<br>例如，下面的metainfo.xml在YARN服务描述了ResourceManager组件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">&lt;component&gt;</div><div class="line">  &lt;name&gt;RESOURCEMANAGER&lt;/name&gt;</div><div class="line">  &lt;category&gt;MASTER&lt;/category&gt;</div><div class="line">  &lt;commandScript&gt;</div><div class="line">    &lt;script&gt;scripts/resourcemanager.py&lt;/script&gt;</div><div class="line">    &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">    &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">  &lt;/commandScript&gt;</div><div class="line">  &lt;customCommands&gt;</div><div class="line">    &lt;customCommand&gt;</div><div class="line">      &lt;name&gt;DECOMMISSION&lt;/name&gt;</div><div class="line">      &lt;commandScript&gt;</div><div class="line">        &lt;script&gt;scripts/resourcemanager.py&lt;/script&gt;</div><div class="line">        &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">        &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">      &lt;/commandScript&gt;</div><div class="line">    &lt;/customCommand&gt;</div><div class="line">  &lt;/customCommands&gt;</div><div class="line">&lt;/component&gt;</div></pre></td></tr></table></figure></p>
<p>ResourceManager是一个MASTER组件，这个命令脚本为scripts/resourcemanager.py，这个脚本可以在services/YARN/package目录中找到。这个脚本是使用PYTHON写的，并且这个以python方法的方式实现了默认的生命周期命令。下面是install方法，对应的是默认的INSTALL命令：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Resourcemanager</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    self.install_packages(env)</div><div class="line">    self.configure(env)</div></pre></td></tr></table></figure></p>
<p>你还可以看到有一个自定义命令DECOMMISSION，这意味着在python命令脚本中还有一个decommission方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">decommission</span><span class="params">(self, env)</span>:</span></div><div class="line">  <span class="keyword">import</span> params</div><div class="line"> </div><div class="line">  ...</div><div class="line"> </div><div class="line">  Execute(yarn_refresh_cmd,</div><div class="line">          user=yarn_user</div><div class="line">  )</div><div class="line">  <span class="keyword">pass</span></div></pre></td></tr></table></figure></p>
<h2 id="Using-Stack-Inheritance"><a href="#Using-Stack-Inheritance" class="headerlink" title="Using Stack Inheritance"></a>Using Stack Inheritance</h2><p>Stacks能够从其他Stack进行继承，以便共享命令脚本和配置。通过如下方式降低了代码的重复：</p>
<blockquote>
<p>为子Stack定义了repositories。<br>在子Stack中添加新的Service（不是在父Stack中）。<br>重写父级Services的命令脚本。<br>重写父级Services的配置。</p>
</blockquote>
<p>例如：HDP 2.1 Stack继承了HDP 2.0.6 Stack，因此只需要在Stack定义中对HDP 2.1 Stack中适当的修改。这个extension在metainfo.xml中对HDP 2.1 Stack进行定义。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">metainfo</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">versions</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">active</span>&gt;</span>true<span class="tag">&lt;/<span class="name">active</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">versions</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">extends</span>&gt;</span>2.0.6<span class="tag">&lt;/<span class="name">extends</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">metainfo</span>&gt;</span></div></pre></td></tr></table></figure></p>
<h2 id="Example-Implementing-a-Custom-Service"><a href="#Example-Implementing-a-Custom-Service" class="headerlink" title="Example: Implementing a Custom Service"></a>Example: Implementing a Custom Service</h2><p>在这个例子中，我们将创建一个名为”SAMPLESRV”的自定义service，并将它添加到已有的Stack定义中。这个service含有 MASTER、SLAVE和CLIENT组件。</p>
<h3 id="Create-and-Add-the-Service"><a href="#Create-and-Add-the-Service" class="headerlink" title="Create and Add the Service"></a>Create and Add the Service</h3><p>1、在Ambari server上，跳转到/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services目录。在这个例子中，我们将浏览到HDP 2.0 stack定义。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services</div></pre></td></tr></table></figure></p>
<p>2、创建一个目录：/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/SAMPLESRV。它用来包含SAMPLESEV的service的定义。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/SAMPLESRV</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/SAMPLESRV</div></pre></td></tr></table></figure></p>
<p>3、跳转到新创建的SAMPLESRV目录，创建metainfo.xml文件，这个文件用来描述新的service。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</div><div class="line">&lt;metainfo&gt;</div><div class="line">    &lt;schemaVersion&gt;2.0&lt;/schemaVersion&gt;</div><div class="line">    &lt;services&gt;</div><div class="line">        &lt;service&gt;</div><div class="line">            &lt;name&gt;SAMPLESRV&lt;/name&gt;</div><div class="line">            &lt;displayName&gt;New Sample Service&lt;/displayName&gt;</div><div class="line">            &lt;comment&gt;A New Sample Service&lt;/comment&gt;</div><div class="line">            &lt;version&gt;1.0.0&lt;/version&gt;</div><div class="line">            &lt;components&gt;</div><div class="line">                &lt;component&gt;</div><div class="line">                    &lt;name&gt;SAMPLESRV_MASTER&lt;/name&gt;</div><div class="line">                    &lt;displayName&gt;Sample Srv Master&lt;/displayName&gt;</div><div class="line">                    &lt;category&gt;MASTER&lt;/category&gt;</div><div class="line">                    &lt;cardinality&gt;1&lt;/cardinality&gt;</div><div class="line">                    &lt;commandScript&gt;</div><div class="line">                        &lt;script&gt;scripts/master.py&lt;/script&gt;</div><div class="line">                        &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">                        &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">                    &lt;/commandScript&gt;</div><div class="line">                &lt;/component&gt;</div><div class="line">                &lt;component&gt;</div><div class="line">                    &lt;name&gt;SAMPLESRV_SLAVE&lt;/name&gt;</div><div class="line">                    &lt;displayName&gt;Sample Srv Slave&lt;/displayName&gt;</div><div class="line">                    &lt;category&gt;SLAVE&lt;/category&gt;</div><div class="line">                    &lt;cardinality&gt;1+&lt;/cardinality&gt;</div><div class="line">                    &lt;commandScript&gt;</div><div class="line">                        &lt;script&gt;scripts/slave.py&lt;/script&gt;</div><div class="line">                        &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">                        &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">                    &lt;/commandScript&gt;</div><div class="line">                &lt;/component&gt;</div><div class="line">                &lt;component&gt;</div><div class="line">                    &lt;name&gt;SAMPLESRV_CLIENT&lt;/name&gt;</div><div class="line">                    &lt;displayName&gt;Sample Srv Client&lt;/displayName&gt;</div><div class="line">                    &lt;category&gt;CLIENT&lt;/category&gt;</div><div class="line">                    &lt;cardinality&gt;1+&lt;/cardinality&gt;</div><div class="line">                    &lt;commandScript&gt;</div><div class="line">                        &lt;script&gt;scripts/sample_client.py&lt;/script&gt;</div><div class="line">                        &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">                        &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">                    &lt;/commandScript&gt;</div><div class="line">                &lt;/component&gt;</div><div class="line">            &lt;/components&gt;</div><div class="line">            &lt;osSpecifics&gt;</div><div class="line">                &lt;osSpecific&gt;</div><div class="line">                    &lt;osFamily&gt;any&lt;/osFamily&gt;  &lt;!-- note: use osType rather than osFamily for Ambari 1.5.0 and 1.5.1 --&gt;</div><div class="line">                &lt;/osSpecific&gt;</div><div class="line">            &lt;/osSpecifics&gt;</div><div class="line">        &lt;/service&gt;</div><div class="line">    &lt;/services&gt;</div><div class="line">&lt;/metainfo&gt;</div></pre></td></tr></table></figure></p>
<p>4、在上面，我们的service名为“SAMPLESRV”，它包含：</p>
<blockquote>
<p>一个名为”SAMPLESRV_MASTER”的MASTER的组件。<br>一个名为“SLAVE”的SLAVE的组件。<br>一个名为“CLIENT”的CLIENT的组件。</p>
</blockquote>
<p>5、接下来，创建命令脚本。为脚本创建目录/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/SAMPLESRV/package/scripts，并在这个目录中定义service的metainfo。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir -p /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/SAMPLESRV/package/scripts</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/SAMPLESRV/package/scripts</div></pre></td></tr></table></figure></p>
<p>跳转到script目录，并创建.py命令脚本文件。<br>master.py文件：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> *</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Master</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Stop the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Start the Sample Srv Master'</span>;</div><div class="line">     </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">status</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Status of the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the Sample Srv Master'</span>;</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  Master().execute()</div></pre></td></tr></table></figure></p>
<p>slave.py文件：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> *</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Slave</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Stop the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Start the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">status</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Status of the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the Sample Srv Slave'</span>;</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  Slave().execute()</div></pre></td></tr></table></figure></p>
<p>sample_client.py文件：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> *</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SampleClient</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the Sample Srv Client'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the Sample Srv Client'</span>;</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  SampleClient().execute()</div></pre></td></tr></table></figure></p>
<p>7、现在重启Ambari Server以便新的service定义分发到集群的所有Agents。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ambari-server restart</div></pre></td></tr></table></figure></p>
<h3 id="Install-the-Service-Via-Ambari-WEb-“Add-Service”"><a href="#Install-the-Service-Via-Ambari-WEb-“Add-Service”" class="headerlink" title="Install the Service(Via Ambari WEb “Add Service”)"></a>Install the Service(Via Ambari WEb “Add Service”)</h3><p>从Ambari 1.7.0开始，可以通过Ambari Web来添加自定义服务。</p>
<blockquote>
<p>在Ambari web页面，跳转到services，并点击左边service导航部分的Action按钮。<br>点击“Add Services”。你将看到一个包含“My Sample Service”（在metainfo.xml文件中定义service的<displayname>）的选项。<br>选择“My Sample service”并点击下一步。<br>选择“Sample Srv Master”并点击下一步。<br>选择host来安装”Sample Srv Client”，并点击下一步。<br>一旦完成，”My Sample Service”将会在在service导航区可用。<br>如果你想要为所有host添加“Sample Srv Client”，你可以跳转到Host，并到航道指定的host并点击”+ Add”。</displayname></p>
</blockquote>
<h2 id="Example-Implementing-a-Custom-Client-only-Service"><a href="#Example-Implementing-a-Custom-Client-only-Service" class="headerlink" title="Example: Implementing a Custom Client-only Service"></a>Example: Implementing a Custom Client-only Service</h2><p>在这个例子中，我们将创建一个名为“TESTSRV”的自定义service，添加到已经存在的Stack定义上，并Ambari APIs来安装/配置这个service。这个service是一个CLIENT，因此它有两个命令：install和configure。</p>
<h3 id="Create-and-Add-the-Service-1"><a href="#Create-and-Add-the-Service-1" class="headerlink" title="Create and Add the Service"></a>Create and Add the Service</h3><p>1、在Ambari Service上，跳转到 /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services目录。在这个例子中，我们将跳转到HDP2.0 Stack 定义中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services</div></pre></td></tr></table></figure></p>
<p>2、创建一个名为/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTSRV的目录，它用来包含TESTSRV的service定义。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTSRV</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTSRV</div></pre></td></tr></table></figure></p>
<p>3、跳转到新创建的TESTSRV目录，创建一个名为metainfo.xml的文件用来描述新的service。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</div><div class="line">&lt;metainfo&gt;</div><div class="line">    &lt;schemaVersion&gt;2.0&lt;/schemaVersion&gt;</div><div class="line">    &lt;services&gt;</div><div class="line">        &lt;service&gt;</div><div class="line">            &lt;name&gt;TESTSRV&lt;/name&gt;</div><div class="line">            &lt;displayName&gt;New Test Service&lt;/displayName&gt;</div><div class="line">            &lt;comment&gt;A New Test Service&lt;/comment&gt;</div><div class="line">            &lt;version&gt;0.1.0&lt;/version&gt;</div><div class="line">            &lt;components&gt;</div><div class="line">                &lt;component&gt;</div><div class="line">                    &lt;name&gt;TEST_CLIENT&lt;/name&gt;</div><div class="line">                    &lt;displayName&gt;New Test Client&lt;/displayName&gt;</div><div class="line">                    &lt;category&gt;CLIENT&lt;/category&gt;</div><div class="line">                    &lt;cardinality&gt;1+&lt;/cardinality&gt;</div><div class="line">                    &lt;commandScript&gt;</div><div class="line">                        &lt;script&gt;scripts/test_client.py&lt;/script&gt;</div><div class="line">                        &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">                        &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">                    &lt;/commandScript&gt;</div><div class="line">                    &lt;customCommands&gt;</div><div class="line">                      &lt;customCommand&gt;</div><div class="line">                        &lt;name&gt;SOMETHINGCUSTOM&lt;/name&gt;</div><div class="line">                        &lt;commandScript&gt;</div><div class="line">                          &lt;script&gt;scripts/test_client.py&lt;/script&gt;</div><div class="line">                          &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">                          &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">                        &lt;/commandScript&gt;</div><div class="line">                      &lt;/customCommand&gt;</div><div class="line">                    &lt;/customCommands&gt;</div><div class="line">                &lt;/component&gt;</div><div class="line">            &lt;/components&gt;</div><div class="line">            &lt;osSpecifics&gt;</div><div class="line">                &lt;osSpecific&gt;</div><div class="line">                    &lt;osFamily&gt;any&lt;/osFamily&gt;  &lt;!-- note: use osType rather than osFamily for Ambari 1.5.0 and 1.5.1 --&gt;</div><div class="line">                &lt;/osSpecific&gt;</div><div class="line">            &lt;/osSpecifics&gt;</div><div class="line">        &lt;/service&gt;</div><div class="line">    &lt;/services&gt;</div><div class="line">&lt;/metainfo&gt;</div></pre></td></tr></table></figure></p>
<p>4、在上面，我们的service名为“TESTSRV”，并且它包含了一个名为“TEST_CLIENT”的组件，这个组件属于CLIENT类型。这个client通过命令脚本scripts/test_client.py来管理。接下来创建命令脚本。<br>5、为命令脚本创建目录/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTSRV/package/scripts。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir -p /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTSRV/package/scripts</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTSRV/package/scripts</div></pre></td></tr></table></figure></p>
<p>6、跳转到scripts目录，并创建test_client.py文件：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> *</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestClient</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the client'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the client'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">somethingcustom</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Something custom'</span>;</div><div class="line"> </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  TestClient().execute()</div></pre></td></tr></table></figure></p>
<p>7、现在，重启Ambari Server，将新的service定义分发到集群的所有Agents。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ambari-server restart</div></pre></td></tr></table></figure></p>
<h3 id="Install-the-Service-Via-the-Ambari-REST-API"><a href="#Install-the-Service-Via-the-Ambari-REST-API" class="headerlink" title="Install the Service(Via the Ambari REST API)"></a>Install the Service(Via the Ambari REST API)</h3><p>1、将Service添加到Cluster。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">POST</div><div class="line">/api/v1/clusters/MyCluster/services</div><div class="line"> </div><div class="line">&#123;</div><div class="line">&quot;ServiceInfo&quot;: &#123;</div><div class="line">  &quot;service_name&quot;:&quot;TESTSRV&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>2、添加组件到Service。这个例子中，添加TEST_CLIENT到TESTSRV。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">POST</div><div class="line">/api/v1/clusters/MyCluster/services/TESTSRV/components/TEST_CLIENT</div></pre></td></tr></table></figure></p>
<p>3、将组件添加到所有host。例如，要安装到c6402.ambari.apache.org和c6403.ambari.apache.org上，首先使用POST在这些主机上创建host_component源<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">POST</div><div class="line">/api/v1/clusters/MyCluster/hosts/c6402.ambari.apache.org/host_components/TEST_CLIENT</div><div class="line"> </div><div class="line">POST</div><div class="line">/api/v1/clusters/MyCluster/hosts/c6403.ambari.apache.org/host_components/TEST_CLIENT</div></pre></td></tr></table></figure></p>
<p>4、现在，需要在所有主机上安装组件。在这个命令中，你指导Ambari来安装与这个service有关的所有组件。调用每个主机上命令脚本的install方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">PUT</div><div class="line">/api/v1/clusters/MyCluster/services/TESTSRV</div><div class="line"> </div><div class="line">&#123;</div><div class="line">  &quot;RequestInfo&quot;: &#123;</div><div class="line">    &quot;context&quot;: &quot;Install Test Srv Client&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;Body&quot;: &#123;</div><div class="line">    &quot;ServiceInfo&quot;: &#123;</div><div class="line">      &quot;state&quot;: &quot;INSTALLED&quot;</div><div class="line">    &#125;   </div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>除了同时安装所有的组件，你还可以只在某一台机器上装组件。在这个例子中，我们在c6402.ambari.apache.org上安装TEST_CLIENT：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">PUT</div><div class="line">/api/v1/clusters/MyCluster/hosts/c6402.ambari.apache.org/host_components/TEST_CLIENT</div><div class="line"> </div><div class="line">&#123;</div><div class="line">  &quot;RequestInfo&quot;: &#123;</div><div class="line">    &quot;context&quot;:&quot;Install Test Srv Client&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;Body&quot;: &#123;</div><div class="line">    &quot;HostRoles&quot;: &#123;</div><div class="line">      &quot;state&quot;:&quot;INSTALLED&quot;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>6、使用如下信息配置主机上的client。这将最终调用命令脚本中的configure()方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">POST</div><div class="line">/api/v1/clusters/MyCluster/requests</div><div class="line">  </div><div class="line">&#123;</div><div class="line">  &quot;RequestInfo&quot; : &#123;</div><div class="line">    &quot;command&quot; : &quot;CONFIGURE&quot;,</div><div class="line">    &quot;context&quot; : &quot;Config Test Srv Client&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;Requests/resource_filters&quot;: [&#123;</div><div class="line">    &quot;service_name&quot; : &quot;TESTSRV&quot;,</div><div class="line">    &quot;component_name&quot; : &quot;TEST_CLIENT&quot;,</div><div class="line">    &quot;hosts&quot; : &quot;c6403.ambari.apache.org&quot;</div><div class="line">  &#125;]</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>7、如果你想看哪些主机完成了安装。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">GET</div><div class="line">/api/v1/clusters/MyCluster/components/TEST_CLIENT</div></pre></td></tr></table></figure></p>
<h3 id="Install-the-Service-via-Ambari-Web-“Add-Services”"><a href="#Install-the-Service-via-Ambari-Web-“Add-Services”" class="headerlink" title="Install the Service(via Ambari Web “Add Services”)"></a>Install the Service(via Ambari Web “Add Services”)</h3><blockquote>
<p>1、在Ambari Web界面，跳转到Services并点击左侧Service导航区的Actions按钮。<br>2、点击“Add Services”。你将看到一个“My Test Service”的选项（在service的metainfo.xml文件中services的<displayname>中定义）。<br>3、选择“My Test Service”并点击下一步。<br>4、选择主机来安装“New Test Client”并点击下一步。<br>5、一旦完成，“My Test Service”将在Service导航区中可用。<br>6、如果你想要在其他主机上添加“New Test Client”，你可以跳转到Hosts，并指定主机后点击“+ Add”。</displayname></p>
</blockquote>
<h2 id="Example-Implementing-a-Custom-Client-only-Service-with-Configs"><a href="#Example-Implementing-a-Custom-Client-only-Service-with-Configs" class="headerlink" title="Example: Implementing a Custom Client-only Service (with Configs)"></a>Example: Implementing a Custom Client-only Service (with Configs)</h2><p>在这个例子中，我们将创建一个名为“TESTCONFIGSRV”的自定义service，并将其添加到已有的Stack定义上。这个service是一个CLIENT类型，因此它有两个命令：install和configure。service还包含”test-confg”配置类型。</p>
<h3 id="Create-and-Add-the-Service-to-Stack"><a href="#Create-and-Add-the-Service-to-Stack" class="headerlink" title="Create and Add the Service to Stack"></a>Create and Add the Service to Stack</h3><p>1、在Ambari Server上，跳转到/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services目录。在这个例子中，我们将跳转到HDP 2.0 Stack定义。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services</div></pre></td></tr></table></figure></p>
<p>2、创建名为/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV的目录，它包含了为TESTCONFIGSRV定义的service。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV</div></pre></td></tr></table></figure></p>
<p>3、跳转到刚刚创建的TESTCONFIGSRV目录，创建一个metainfo.xml文件，这个文件描述了这个新的service。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</div><div class="line">&lt;metainfo&gt;</div><div class="line">    &lt;schemaVersion&gt;2.0&lt;/schemaVersion&gt;</div><div class="line">    &lt;services&gt;</div><div class="line">        &lt;service&gt;</div><div class="line">            &lt;name&gt;TESTCONFIGSRV&lt;/name&gt;</div><div class="line">            &lt;displayName&gt;New Test Config Service&lt;/displayName&gt;</div><div class="line">            &lt;comment&gt;A New Test Config Service&lt;/comment&gt;</div><div class="line">            &lt;version&gt;0.1.0&lt;/version&gt;</div><div class="line">            &lt;components&gt;</div><div class="line">                &lt;component&gt;</div><div class="line">                    &lt;name&gt;TESTCONFIG_CLIENT&lt;/name&gt;</div><div class="line">                    &lt;displayName&gt;New Test Config Client&lt;/displayName&gt;</div><div class="line">                    &lt;category&gt;CLIENT&lt;/category&gt;</div><div class="line">                    &lt;cardinality&gt;1+&lt;/cardinality&gt;</div><div class="line">                    &lt;commandScript&gt;</div><div class="line">                        &lt;script&gt;scripts/test_client.py&lt;/script&gt;</div><div class="line">                        &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">                        &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">                    &lt;/commandScript&gt;</div><div class="line">                &lt;/component&gt;</div><div class="line">            &lt;/components&gt;</div><div class="line">            &lt;osSpecifics&gt;</div><div class="line">                &lt;osSpecific&gt;</div><div class="line">                    &lt;osFamily&gt;any&lt;/osFamily&gt;  &lt;!-- note: use osType rather than osFamily for Ambari 1.5.0 and 1.5.1 --&gt;</div><div class="line">                &lt;/osSpecific&gt;</div><div class="line">            &lt;/osSpecifics&gt;</div><div class="line">        &lt;/service&gt;</div><div class="line">    &lt;/services&gt;</div><div class="line">&lt;/metainfo&gt;</div></pre></td></tr></table></figure></p>
<p>4、在上面，我的service的名为“TESTCONFIGSRV”，并且它包含一个名为“TESTCONFIG_CLIENT”组件，这个组件的类型为“CLINT”。这个client通过命令脚本scripts/test_client.py来管理。接下来创建这个命令脚本。<br>5、为命令脚本创建目录/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV/package/scripts，这个脚本在ervice metainfo <commandscript>中指定。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir -p /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV/package/scripts</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV/package/scripts</div></pre></td></tr></table></figure></commandscript></p>
<p>6、调转到scripts目录，并创建test_client.py文件。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import sys</div><div class="line">from resource_management import *</div><div class="line"> </div><div class="line">class TestClient(Script):</div><div class="line">  def install(self, env):</div><div class="line">    print &apos;Install the config client&apos;;</div><div class="line">  def configure(self, env):</div><div class="line">    print &apos;Configure the config client&apos;;</div><div class="line"> </div><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">  TestClient().execute()</div></pre></td></tr></table></figure></p>
<p>7、现在，为这个service定义配置类型。为配置目录/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV/configuration创建一个目录。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir -p /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV/configuration</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV/configuration</div></pre></td></tr></table></figure></p>
<p>8、跳转到配置目录，并创建test-config.xml文件。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</div><div class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</div><div class="line"> </div><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;some.test.property&lt;/name&gt;</div><div class="line">    &lt;value&gt;this.is.the.default.value&lt;/value&gt;</div><div class="line">    &lt;description&gt;This is a kool description.&lt;/description&gt;</div><div class="line"> &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
<p>9、现在，重启Ambari Server以便将service分发到集群中的所有Agent上。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ambari-server restart</div></pre></td></tr></table></figure></p>
<h1 id="How-To-Define-Stacks-and-Services"><a href="#How-To-Define-Stacks-and-Services" class="headerlink" title="How-To Define  Stacks and Services"></a>How-To Define  Stacks and Services</h1><p>Ambari管理的Services在Ambari的stacks文件夹中定义。<br>要定义自己的services和stacks并被Ambari管理，请遵循如下步骤。<br>上面的create your custom stack and service的例子也可以学习。<br>stack是services的集合。一个stack可以定义多个版本，每个版本有自己的一组service。Ambari中的Stacks在 ambari-server/src/main/resources/stacks 文件夹中定义，这个文件夹可以在安装之后的/var/lib/ambari-server/resources/stacks目录找到。<br>被stack管理的servces能够在 ambari-server/src/main/resources/common-services 或 ambari-server/src/main/resources/stacks 文件夹中定义。这些文件对应安装后的目录分别为：/var/lib/ambari-server/resources/common-services 或  /var/lib/ambari-server/resources/stacks。</p>
<blockquote>
<p>Question : 什么时候在 common-services 或 stacks 文件夹中定义service呢<br>当service可能被用于多个stacks时，我们将在common-services文件夹中定义service。例如，几乎所有的stacks都需要HDFS service，因此将它定义在common-services而不是在每个stack中定义是值得推荐的。同样，如果一个service从不会被共享，它能够被定义在stack文件夹中。基本上来说stacks文件夹中定义services是不推荐的，而推荐将service定义在common-services中。</p>
</blockquote>
<h2 id="Define-Service"><a href="#Define-Service" class="headerlink" title="Define Service"></a>Define Service</h2><p>下面展示了如何在common-services文件夹中定义一个service。在stacks文件夹中定义services时，也可以使用相同的方法，具体会在定义stack章节介绍。</p>
<p><img src="http://oaavtz33a.bkt.clouddn.com/Screen%20Shot%202016-03-01%20at%202.47.32%20PM.png"></p>
<p>Services必须提供主要的metainfo.xml文件，它提供了关于这个service的重要元数据。<br>除此之外，其他文件提供了关于server的更多信息。关于这些文件的更多信息将在下面提供。</p>
<p>一个service也可能继承自它的之前版本或common services。关于继承的更多信息，请查看<a href="https://cwiki.apache.org/confluence/display/AMBARI/Service+Inheritance" title="Service Inheritance" target="_blank" rel="external">Service Inheritance</a>。</p>
<h2 id="metainfo-xml"><a href="#metainfo-xml" class="headerlink" title="metainfo.xml"></a>metainfo.xml</h2><p>在metainfo.xml服务描述符中，首先被定义的是service和它的components。<br>完整的参考文献可以在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Writing+metainfo.xml" title="Writing metainfo.xml" target="_blank" rel="external">Writing metainfo.xml</a>中找到。<br>值得推荐的metainfo.xml实现是<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/metainfo.xml#L27" title="HDFS metainfo.xml" target="_blank" rel="external">HDFS metainfo.xml</a>。</p>
<blockquote>
<p>Question : 是否可以在同一个metainfo.xml中定义多个services？<br>可以。尽管可以，但是强烈拒绝在相同的service文件夹中定义多个services。<br>YARN和MapReduces2就被一起定义在YARN文件夹中。它的metainfo.xml同时定义了两个services。</p>
</blockquote>
<h3 id="Scripts"><a href="#Scripts" class="headerlink" title="Scripts"></a>Scripts</h3><p>对于components的定义，我们需要提供脚本来处理service的不同阶段以及组件的生命周期。<br>管理service和components的脚本在metainfo.xml(<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/metainfo.xml#L35" title="HDFS" target="_blank" rel="external">HDFS</a>)中指定。<br>每个脚本应该继承<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-common/src/main/python/resource_management/libraries/script/script.py" title="Script" target="_blank" rel="external">Script</a>类，这个父类提供了有用的方法。例如：<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py#L68" title="NameNode script" target="_blank" rel="external">NameNode script</a>。</p>
<p><img src="http://oaavtz33a.bkt.clouddn.com/Screen%20Shot%202016-03-02%20at%2012.39.49%20PM.png"><br>这些脚本应该在<service-id>/<service-version>/package/script文件夹中提供。<br><img src="http://oaavtz33a.bkt.clouddn.com/Screen%20Shot%202016-03-02%20at%2012.32.58%20PM.png"></service-version></service-id></p>
<table>
<thead>
<tr>
<th style="text-align:left">Folder</th>
<th style="text-align:left">Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">package/script</td>
<td style="text-align:left">包含了由Ambari执行的脚本。这些脚本使用正确的环境被加载到执行路径中。例如：<a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts" title="HDFS" target="_blank" rel="external">HDFS</a></td>
</tr>
<tr>
<td style="text-align:left">package/files</td>
<td style="text-align:left">包含被上面脚本使用的文件。一般是其他一些作为独立进程执行的脚本（bash、python等）。例如：checkWebUI.py在HDFS检查中运行，用来确定Journal Node是否活跃。</td>
</tr>
<tr>
<td style="text-align:left">package/tmplates</td>
<td style="text-align:left">上述脚本在管理节点上生成的临时文件。一般是service需要操作的配置文件。例如：<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/templates/exclude_hosts_list.j2" title="exclude_hosts_list.j2" target="_blank" rel="external">exclude_hosts_list.j2</a> ，被脚本使用来产生/etc/hadoop/conf/dfs.exclude。</td>
</tr>
</tbody>
</table>
<h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>Ambari默认支持python脚本来管理service和components。<br>component脚本应该继承resource_management.Script类并提供component的生命周期所需的方法。<br>参考<a href="https://cwiki.apache.org/confluence/display/AMBARI/Defining+a+Custom+Stack+and+Services" title="how to create custom stack" target="_blank" rel="external">how to create custom stack</a>页面，MASTER、SLAVE和CLIENT组件贯穿生命周期所需的方法如下：<br>master.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> Script</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Master</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Stop the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Start the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">status</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Status of the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the Sample Srv Master'</span>;</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  Master().execute()</div></pre></td></tr></table></figure></p>
<p>slave.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> Script</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Slave</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Stop the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Start the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">status</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Status of the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the Sample Srv Slave'</span>;</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  Slave().execute()</div></pre></td></tr></table></figure></p>
<p>client.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> Script</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SampleClient</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the Sample Srv Client'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the Sample Srv Client'</span>;</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  SampleClient().execute()</div></pre></td></tr></table></figure></p>
<p>Ambari提供了有用的Python库，以便在以下方面提供写servier脚本的帮助。对于这些库的完整介绍，请通过<a href="https://cwiki.apache.org/confluence/display/AMBARI/Ambari+Python+Libraries" title="Ambari Python Libraries" target="_blank" rel="external">Ambari Python Libraries</a>页面访问。</p>
<blockquote>
<p>resource_management<br>ambari_commons<br>ambari_simplejson</p>
</blockquote>
<h4 id="OS-Variant-Script"><a href="#OS-Variant-Script" class="headerlink" title="OS Variant Script"></a>OS Variant Script</h4><p>如果service支持多个操作系统，则需要根据不同的操作系统由独立的脚本，可以继承resource_management.Script类并使用不同的@OSFamilyImpl()注解。<br>这能够区分组件的不同操作系统的方法。<br>例如： <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py#L126" title="NameNode default script" target="_blank" rel="external">NameNode default script</a>， <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py#L346" title="NameNode Windows script" target="_blank" rel="external">NameNode Windows script</a></p>
<blockquote>
<p>Examples<br>NameNode <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py#L93" title="start" target="_blank" rel="external">Start</a>， <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py#L208" title="Stop" target="_blank" rel="external">Stop</a><br>DataNode <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_datanode.py#L68" title="
Start and Stop" target="_blank" rel="external">Start and Stop</a><br>HDFS <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs.py#L31" title="configurations persistence" target="_blank" rel="external">configurations persistence</a></p>
</blockquote>
<h3 id="Custom-Actions"><a href="#Custom-Actions" class="headerlink" title="Custom Actions"></a>Custom Actions</h3><p>有些时候，services需要执行一些行为，这些行为不同于Ambari提供的默认行为（install、start、stop、configure等）。<br>services能够定义一些action，并将这些action在UI中展示给用户，因此这些行为能够方便执行。<br>举例说明，如HDFS实现的Rebalance HDFS自定义行为。</p>
<h4 id="Stack-Changes"><a href="#Stack-Changes" class="headerlink" title="Stack Changes"></a>Stack Changes</h4><blockquote>
<p>1、在metainfo.xml中component的<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/metainfo.xml#L49" title="Define custom command insid the customCommands section" target="_blank" rel="external"><customcommands>部分中定义指定义命令</customcommands></a>。<br>2、在metainfo.xml相关的脚本中，用相同的名字实现方法，来作为自定义命令。<br>    a）如果自定义命令不含有操作系统变量，它可以在同一个继承resource_management.Script的类中被实现。<br>    b）如果含有操作系统变量，每个类中的不同方法可以通过@OsFamilyImpl(os_family=…)来实现。<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py#L273" title="Default rebalancehdfs" target="_blank" rel="external">Default rebalancehdfs</a>, <a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py#L354" title="Windows rebalancehdfs" target="_blank" rel="external">Windows rebalancehdfs</a>。<br>这将提供在安装了service的被管理的主机上以后端方式运行脚本的能力。</p>
</blockquote>
<h4 id="UI-Changes"><a href="#UI-Changes" class="headerlink" title="UI Changes"></a>UI Changes</h4><p>在host页面上查看自定义action不需要修改UI。<br>action将展示在主机组件的action列表中。任何master-component action将自动展示在service的action菜单上。<br>当action被点击后，将自动产生POST调用来触发上面定义的脚本。</p>
<blockquote>
<p>Question ： 如何为UI中的自定义action提供自己的标签和图标？<br>在Ambari UI中，使用自定义图标和名称，添加你的component action到App.HostComponentActionMap对象。</p>
</blockquote>
<h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h2><p>service的配置文件应当位于默认的configuration文件夹中。<br>如果使用了不同的文件夹，metainfo.xml中的<configuration-dir>，可以用来指明使用的文件夹。<br>metainfo.xml中需要考虑配置的重要部分是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</div><div class="line">&lt;metainfo&gt;</div><div class="line">  &lt;schemaVersion&gt;2.0&lt;/schemaVersion&gt;</div><div class="line">  &lt;services&gt;</div><div class="line">    &lt;service&gt;</div><div class="line">      &lt;name&gt;HDFS&lt;/name&gt;</div><div class="line">      &lt;displayName&gt;HDFS&lt;/displayName&gt;</div><div class="line">      &lt;comment&gt;Apache Hadoop Distributed File System&lt;/comment&gt;</div><div class="line">      &lt;version&gt;2.1.0.2.0&lt;/version&gt;</div><div class="line">      &lt;components&gt;</div><div class="line">        ...</div><div class="line">        &lt;component&gt;</div><div class="line">          &lt;name&gt;HDFS_CLIENT&lt;/name&gt;</div><div class="line">          ...</div><div class="line">          &lt;configFiles&gt;</div><div class="line">            &lt;configFile&gt;</div><div class="line">              &lt;type&gt;xml&lt;/type&gt;</div><div class="line">              &lt;fileName&gt;hdfs-site.xml&lt;/fileName&gt;</div><div class="line">              &lt;dictionaryName&gt;hdfs-site&lt;/dictionaryName&gt;</div><div class="line">            &lt;/configFile&gt;</div><div class="line">            &lt;configFile&gt;</div><div class="line">              &lt;type&gt;xml&lt;/type&gt;</div><div class="line">              &lt;fileName&gt;core-site.xml&lt;/fileName&gt;</div><div class="line">              &lt;dictionaryName&gt;core-site&lt;/dictionaryName&gt;</div><div class="line">            &lt;/configFile&gt;</div><div class="line">            &lt;configFile&gt;</div><div class="line">              &lt;type&gt;env&lt;/type&gt;</div><div class="line">              &lt;fileName&gt;log4j.properties&lt;/fileName&gt;</div><div class="line">              &lt;dictionaryName&gt;hdfs-log4j,yarn-log4j&lt;/dictionaryName&gt;</div><div class="line">            &lt;/configFile&gt;                         </div><div class="line">            &lt;configFile&gt;</div><div class="line">              &lt;type&gt;env&lt;/type&gt;</div><div class="line">              &lt;fileName&gt;hadoop-env.sh&lt;/fileName&gt;</div><div class="line">              &lt;dictionaryName&gt;hadoop-env&lt;/dictionaryName&gt;</div><div class="line">            &lt;/configFile&gt;</div><div class="line">          &lt;/configFiles&gt;</div><div class="line">          ...</div><div class="line">          &lt;configuration-dependencies&gt;</div><div class="line">             &lt;config-type&gt;core-site&lt;/config-type&gt;</div><div class="line">             &lt;config-type&gt;hdfs-site&lt;/config-type&gt;</div><div class="line">          &lt;/configuration-dependencies&gt;</div><div class="line">        &lt;/component&gt;</div><div class="line">          ...</div><div class="line">      &lt;/components&gt;</div><div class="line">  </div><div class="line">      &lt;configuration-dir&gt;configuration&lt;/configuration-dir&gt;</div><div class="line">      &lt;configuration-dependencies&gt;</div><div class="line">        &lt;config-type&gt;core-site&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;hdfs-site&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;hadoop-env&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;hadoop-policy&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;hdfs-log4j&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ranger-hdfs-plugin-properties&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ssl-client&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ssl-server&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ranger-hdfs-audit&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ranger-hdfs-policymgr-ssl&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ranger-hdfs-security&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ams-ssl-client&lt;/config-type&gt;</div><div class="line">      &lt;/configuration-dependencies&gt;</div><div class="line">    &lt;/service&gt;</div><div class="line">  &lt;/services&gt;</div><div class="line">&lt;/metainfo&gt;</div></pre></td></tr></table></figure></configuration-dir></p>
<blockquote>
<p>config-type - 字符串表示的一组配置。例如：core-site, hdfs-site, yarn-site等。当配置在Ambari中保存，它们被固化到一个config-type版本中，而且这个版本是不可变的。如果你更改并保存HDFS core-site配置4次，你将有4个版本的config-type core-site。同样，当一个service的配置被保存时，只有更改的config-type被更新。<br>configFiles - 列出由<component>处理的配置文件。<br>configFile - 某种类型的一个配置文件。<br>    type - 基于文件内容的不同指定文件的类型<br>        xml - Hadoop中友好的方式，XML文件。<br>        env - 通常用于将内容值作为模版的脚本。模版具有配置标签，并且它的值在运行时生成。<br>        properties - 生成属性文件，每条属性的格式为key=value。<br>    dictionaryName - 配置类型的名字。<br>configuration-dependencies - 列出component或service所依赖的config-type的列表。<br>configuration-dir - configFiles所指定的文件所处的目录。可选的，默认为configuration。</component></p>
</blockquote>
<h3 id="Adding-new-configs-in-a-config-type"><a href="#Adding-new-configs-in-a-config-type" class="headerlink" title="Adding new configs in a config-type"></a>Adding new configs in a config-type</h3><p>向config-type中添加一个配置项时有很多不同的参数可选。它们在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Configuration+support+in+Ambari" title="config-type的可选属性" target="_blank" rel="external">这里</a>被全面介绍。</p>
<h3 id="UI-Categories"><a href="#UI-Categories" class="headerlink" title="UI - Categories"></a>UI - Categories</h3><p>上面的定义的配置在service的配置页面显示。<br>要自定义分类并在UI中对配置进行排序，需要更新下面的文件。<br>Create Category - 更新 ambari-web/app/models/stack_service.js 文件，用来添加自己的service，以及你的新分类。<br>Use Category - 要将配置置于某种分类中，并指定配置的顺序，将配置添加到  ambari-web/app/data/HDP2/site_properties.js 文件中。在这个文件中，可以指定需要使用到分类，以及配置的索引。ambari-web/app/data中的stack文件夹时分层的且继承自前一个版本。片段中的配置属性在这里定义。例如 <a href="https://github.com/apache/ambari/blob/trunk/ambari-web/app/data/HDP2.2/hive_properties.js" title="Hive Categories" target="_blank" rel="external">Hive Categories</a>, <a href="https://github.com/apache/ambari/blob/trunk/ambari-web/app/data/HDP2.2/tez_properties.js" title="Tez Categories" target="_blank" rel="external">Tez Categories</a></p>
<h3 id="UI-Enhanced-Configs"><a href="#UI-Enhanced-Configs" class="headerlink" title="UI - Enhanced Configs"></a>UI - Enhanced Configs</h3><p>Enhanced Config特性使得服务提供者能够定制他们自己的service配置，并确定哪些配置主要显示给用户，而不需要修改任何UI代码。自定义包括为service提供友好的布局，更好的控制（sliders, combos, lists, toggles, spinners, etc）、更好的验证（minimum, maximum, enums）、自动的单位转换（MB, GB, seconds, milliseconds, etc.）、配置依赖以及默认值的动态推荐。<br>servier提供者能够达成上面所有的，只需要在stacks文件夹中修改它们service的定义。<br>在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Enhanced+Configs" title="Enhanced Configs" target="_blank" rel="external">Enhanced Configs</a>页面中查看更多。</p>
<h2 id="Alerts"><a href="#Alerts" class="headerlink" title="Alerts"></a>Alerts</h2><p>通过提供一个alert.js文件，每个service都能够定义Ambari应该跟踪的警报。<br>在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Alerts" title="Alerts wiki page" target="_blank" rel="external">Alerts wiki page</a>页面能够读到更多关于报警框架的信息，而alerts.json文件的格式在<a href="https://github.com/apache/ambari/blob/branch-2.1/ambari-server/docs/api/v1/alert-definitions.md" title="Alerts definition document" target="_blank" rel="external">Alerts definition document</a>中可以了解到。</p>
<h2 id="Kerberos"><a href="#Kerberos" class="headerlink" title="Kerberos"></a>Kerberos</h2><p>Ambari能够对一个集群启用或禁用Kerberos。要通知Ambari服务及其组件使用的身份和配置，每个服务需要提供一个kerberos.json文件。<br>在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Automated+Kerberizaton" title="Automated Kerberization" target="_blank" rel="external">Automated Kerberization</a>wiki页面可以读到关于Kerberos的支持的信息，还可以在<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/docs/security/kerberos/kerberos_descriptor.md" title="Kerberos Descriptor documentation" target="_blank" rel="external">Kerberos Descriptor documentation</a>中得到Kerberos的描述信息。</p>
<h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><p>对于Hadoop和Ambari管理的集群，Ambari提供了<a href="https://cwiki.apache.org/confluence/display/AMBARI/Metrics" title="Ambari Metrics System" target="_blank" rel="external">Ambari Metrics System</a>服务，用来收集、聚合系统的metrics。<br>每个service可以定义哪些metrics能够被AMS收集，通过metrics.json文件来定义。你可以在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Stack+Defined+Metrics" title="Stack Defined Metrics" target="_blank" rel="external">Stack Defined Metrics</a>页面中得到关于metrics.json格式的信息。</p>
<h2 id="Quick-Links"><a href="#Quick-Links" class="headerlink" title="Quick Links"></a>Quick Links</h2><p>一个service通过向一个文本添加metainfo来实现向Ambari web UI中添加一个快速链接的列表，添加数据的文本遵循一个预定义JSON格式。Ambari server解析quicklink JSON文件，并将它的内容展示在UI。因此，Ambari web UI能够根据这些信息计算quick link URLs，并相应的填充quicklink的下拉列表。<br>关于quick link的JSON文件的设计，可以参看<a href="https://cwiki.apache.org/confluence/display/AMBARI/Quick+Links" title="Quick Links" target="_blank" rel="external">Quick Links</a>页面。</p>
<h2 id="Widgets"><a href="#Widgets" class="headerlink" title="Widgets"></a>Widgets</h2><p>每个service都可以通过定一个widgets.json文件来定义在service的摘要页面上默认显示哪些widgets和heatmaps。<br>你可以在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Enhanced+Service+Dashboard" title="Enhanced Service Dashboard" target="_blank" rel="external">Enhanced Service Dashboard</a>页面中看到更多关于widgets描述符的信息。</p>
<h2 id="Role-Command-Order"><a href="#Role-Command-Order" class="headerlink" title="Role Command Order"></a>Role Command Order</h2><p>从Ambari 2.2开始，每个service通过在service文件夹中包含一个role_rommand_order.json文件来定义自己的role command order。这个service应当只指定它的组件到其他组件之间的关系。换句话说，如果service只包含COMP_X，那么servier应当只列出与COMP_X相关的依赖。如果COMP_X启动，它依赖于NameNode的启动，当NameNode停止时，NameNode应该要等COMP_X先停止，下面的信息将被包含在role command order中：<br>Example service role_command_order.json<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&quot;COMP_X-START&quot;: [&quot;NAMENODE-START&quot;],</div><div class="line">&quot;NAMENODE-STOP&quot;: [&quot;COMP_X-STOP&quot;]</div></pre></td></tr></table></figure></p>
<p>service的role command order条目将会与stack中定义的role command order合并。例如，因为stack已经依赖NAMENODE_STOP，在上面的例子中，COMP_X-STOP将被添加到NAMENODE-STOP的依赖，此外，COMP_X-START对NAMENODE-START的依赖将作为一个新的依赖项被添加。<br>对于role command order的更多信息，可以查看<a href="https://cwiki.apache.org/confluence/display/AMBARI/How-To+Define+Stacks+and+Services#How-ToDefineStacksandServices-RoleCommandOrder" title="Role Command Order" target="_blank" rel="external">Role Command Order</a>章节。</p>
<h2 id="Service-Advisor"><a href="#Service-Advisor" class="headerlink" title="Service Advisor"></a>Service Advisor</h2><p>从Ambari 2.4开始，每个service可以选择定义自己的service advisor，而不是在stack advisor中定义它的配置和布局的细节。这专门用于哪些没有在stack中定义的自定义service。service能够在它的service文件夹中编写一个名为service-advisor.py的Python脚本来提供Service Advisor的能力。这个文件夹可以位于定义service的stack的services目录或者用来定义可继承service的common-services目录。例如：<a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/common-services/HAWQ/2.0.0" title="common-services/HAWQ/2.0.0" target="_blank" rel="external">common-services/HAWQ/2.0.0</a>。<br>与Stack-advisor脚本不同，service-advisor脚本不会自动的继承父级service的service-advisor脚本。service-advisor脚本需要声明来继承它们父级service的service-advisor脚本。下面的代码向你展示了如何引用父级service的service-advisor.py。在这个例子中，它继承了位于resource/stacks中的顶级service-advisor.py。<br>Sample service-advisor.py file inheritance<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))</div><div class="line">STACKS_DIR = os.path.join(SCRIPT_DIR, <span class="string">'../../../stacks/'</span>)</div><div class="line">PARENT_FILE = os.path.join(STACKS_DIR, <span class="string">'service_advisor.py'</span>)</div><div class="line"> </div><div class="line"><span class="keyword">try</span>:</div><div class="line">  <span class="keyword">with</span> open(PARENT_FILE, <span class="string">'rb'</span>) <span class="keyword">as</span> fp:</div><div class="line">    service_advisor = imp.load_module(<span class="string">'service_advisor'</span>, fp, PARENT_FILE, (<span class="string">'.py'</span>, <span class="string">'rb'</span>, imp.PY_SOURCE))</div><div class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">  traceback.print_exc()</div><div class="line">  <span class="keyword">print</span> <span class="string">"Failed to load parent"</span></div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HAWQ200ServiceAdvisor</span><span class="params">(service_advisor.ServiceAdvisor)</span>:</span></div></pre></td></tr></table></figure></p>
<p>与stack advisors类似，service advisor在4个重要概念上提供了信息：</p>
<blockquote>
<p>1、推荐集群上service的布局。<br>2、推荐service配置。<br>3、验证集群上service的布局。<br>4、验证service配置。<br>通过提供的service-advisor.py文件，service能够动态控制上面的每一个。<br>对于service-advisor脚本来说主要接口是如何调用上面的每一项，以及给它们提供什么数据。</p>
</blockquote>
<p>Base service_advisor.py from resources/stacks<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ServiceAdvisor</span><span class="params">(DefaultStackAdvisor)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line">  Abstract class implemented by all service advisors.</div><div class="line">  """</div><div class="line"> </div><div class="line">  <span class="string">"""</span></div><div class="line">  If any components of the service should be colocated with other services,</div><div class="line">  this is where you should set up that layout.  Example:</div><div class="line"> </div><div class="line">    # colocate HAWQSEGMENT with DATANODE, if no hosts have been allocated for HAWQSEGMENT</div><div class="line">    hawqSegment = [component for component in serviceComponents if component["StackServiceComponents"]["component_name"] == "HAWQSEGMENT"][0]</div><div class="line">    if not self.isComponentHostsPopulated(hawqSegment):</div><div class="line">      for hostName in hostsComponentsMap.keys():</div><div class="line">        hostComponents = hostsComponentsMap[hostName]</div><div class="line">        if &#123;"name": "DATANODE"&#125; in hostComponents and &#123;"name": "HAWQSEGMENT"&#125; not in hostComponents:</div><div class="line">          hostsComponentsMap[hostName].append( &#123; "name": "HAWQSEGMENT" &#125; )</div><div class="line">        if &#123;"name": "DATANODE"&#125; not in hostComponents and &#123;"name": "HAWQSEGMENT"&#125; in hostComponents:</div><div class="line">          hostComponents.remove(&#123;"name": "HAWQSEGMENT"&#125;)</div><div class="line">  """</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">colocateService</span><span class="params">(self, hostsComponentsMap, serviceComponents)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"> </div><div class="line">  <span class="string">"""</span></div><div class="line">  Any configuration recommendations for the service should be defined in this function.</div><div class="line">  This should be similar to any of the recommendXXXXConfigurations functions in the stack_advisor.py</div><div class="line">  such as recommendYARNConfigurations().</div><div class="line">  """</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getServiceConfigurationRecommendations</span><span class="params">(self, configurations, clusterSummary, services, hosts)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"> </div><div class="line">  <span class="string">"""</span></div><div class="line">  Returns an array of Validation objects about issues with the hostnames to which components are assigned.</div><div class="line">  This should detect validation issues which are different than those the stack_advisor.py detects.</div><div class="line">  The default validations are in stack_advisor.py getComponentLayoutValidations function.</div><div class="line">  """</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getServiceComponentLayoutValidations</span><span class="params">(self, services, hosts)</span>:</span></div><div class="line">    <span class="keyword">return</span> []</div><div class="line"> </div><div class="line">  <span class="string">"""</span></div><div class="line">  Any configuration validations for the service should be defined in this function.</div><div class="line">  This should be similar to any of the validateXXXXConfigurations functions in the stack_advisor.py</div><div class="line">  such as validateHDFSConfigurations.</div><div class="line">  """</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getServiceConfigurationsValidationItems</span><span class="params">(self, configurations, recommendedDefaults, services, hosts)</span>:</span></div><div class="line">    <span class="keyword">return</span> []</div></pre></td></tr></table></figure></p>
<h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><ul><br>    <li><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/service_advisor.py#L51" title="Service Advisor interface" target="_blank" rel="external">Service Advisor interface</a></li><br>    <li><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HAWQ/2.0.0/service_advisor.py" title="HAWQ 2.0.0 Service Advisor implementation" target="_blank" rel="external">HAWQ 2.0.0 Service Advisor implementation</a></li><br>    <li><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/PXF/3.0.0/service_advisor.py" title="PXF 3.0.0 Service Advisor implementation" target="_blank" rel="external">PXF 3.0.0 Service Advisor implementation</a></li><br></ul>

<h2 id="Service-Upgrade"><a href="#Service-Upgrade" class="headerlink" title="Service Upgrade"></a>Service Upgrade</h2><p>从Ambari开始，每个service能够在它的service definition中定义它自己的更新。这对哪些不再需要修改stack的upgrade-packs的自定义service，以便它们融合到集群的更新。</p>
<p>每个service能够定义upgrade-packs，upgrade-packs是一些XML文件，它们描述了某个service的更新进程已经这个更新包如何与所有的stack更新包相关联。这些upgrade-pack XML文件在service的upgrades/文件夹中的独立的子文件夹中，这些子文件夹指明了需要扩展的stack版本。测试代码中的一些例子。</p>
<h3 id="Examples-1"><a href="#Examples-1" class="headerlink" title="Examples"></a>Examples</h3><ul><br>    <li><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/test/resources/stacks/HDP/2.0.5/services/HDFS/upgrades/" title="Upgrades folder" target="_blank" rel="external">Upgrades folder</a></li><br>    <li><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/test/resources/stacks/HDP/2.0.5/services/HDFS/upgrades/HDP/2.2.0/upgrade_test_15388.xml" title="Upgrade-pack XML" target="_blank" rel="external">Upgrade-pack XML</a></li><br></ul>

<p>service定义的每个upgrade-pack通过一个特定的stack版本，应当匹配service定义的文件名。例如，在测试代码中，HDP 2.2.0有一个名为upgrade_test_15388.xml的upgrade-pack。HDFS service定义了一个extension来扩展那个upgrade pack<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/test/resources/stacks/HDP/2.0.5/services/HDFS/upgrades/HDP/2.2.0/upgrade_test_15388.xml" title="HDP/2.0.5/services/HDFS/upgrades/HDP/2.2.0/upgrade_test_15388.xml" target="_blank" rel="external">HDP/2.0.5/services/HDFS/upgrades/HDP/2.2.0/upgrade_test_15388.xml</a>。在这个例子中，upgrade-pack定义在HDP/2.0.5的stack中。这个upgrade-pack是HDP/2.2.0的一个扩展，因为他被定义在upgrade/HDP/2.2.0目录中。最终，扩展到upgrad-pack upgrade_test_15388.xml的service的名字与HDP/2.2.0/upgrades中的upgrade-pack的名字匹配。<br>对于service的文件格式与stack的有很大的相同。target、target-stack和type属性应该和stack的upgrade-pack的信息完全对应。service能够添加自己的前提检测。</p>
<p>General Attributes and Prerequisite Checks<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">upgrade</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">target</span>&gt;</span>2.4.*<span class="tag">&lt;/<span class="name">target</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">target-stack</span>&gt;</span>HDP-2.4.0<span class="tag">&lt;/<span class="name">target-stack</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">type</span>&gt;</span>ROLLING<span class="tag">&lt;/<span class="name">type</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">prerequisite-checks</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">check</span>&gt;</span>org.apache.ambari.server.checks.FooCheck<span class="tag">&lt;/<span class="name">check</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">prerequisite-checks</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>upgrade-pack的<order>部分，由<group>标签组成，就像stack的upgrade-pack。关键的不同是如何定义这些<group>，使它们与stack的upgrade pack的<group>或其他service的upgrade pack的<group>相关联。在第一个例子中，我们引入了名为PRE_CLUSTER的<group>并为名为FOO的service新增了一个<execute-stage>。该项应该在基于<add-after-group-entry>标签的HDFS之后的<execute-stage>中添加。</execute-stage></add-after-group-entry></execute-stage></group></group></group></group></group></order></p>
<p>Order Section - Add After Group Entry<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">order</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">group</span> <span class="attr">xsi:type</span>=<span class="string">"cluster"</span> <span class="attr">name</span>=<span class="string">"PRE_CLUSTER"</span> <span class="attr">title</span>=<span class="string">"Pre &#123;&#123;direction.text.proper&#125;&#125;"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">add-after-group-entry</span>&gt;</span>HDFS<span class="tag">&lt;/<span class="name">add-after-group-entry</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">execute-stage</span> <span class="attr">service</span>=<span class="string">"FOO"</span> <span class="attr">component</span>=<span class="string">"BAR"</span> <span class="attr">title</span>=<span class="string">"Backup FOO"</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">task</span> <span class="attr">xsi:type</span>=<span class="string">"manual"</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">message</span>&gt;</span>Back FOO up.<span class="tag">&lt;/<span class="name">message</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">task</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">execute-stage</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">group</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>同样的语法也可以被用于service检查优先级和group services等。</p>
<p>Order Section - Further Add After Group Entry Examples<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">group</span> <span class="attr">name</span>=<span class="string">"SERVICE_CHECK1"</span> <span class="attr">title</span>=<span class="string">"All Service Checks"</span> <span class="attr">xsi:type</span>=<span class="string">"service-check"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">add-after-group-entry</span>&gt;</span>ZOOKEEPER<span class="tag">&lt;/<span class="name">add-after-group-entry</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">priority</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">service</span>&gt;</span>HBASE<span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">priority</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">group</span>&gt;</span></div><div class="line"> </div><div class="line"><span class="tag">&lt;<span class="name">group</span> <span class="attr">name</span>=<span class="string">"CORE_MASTER"</span> <span class="attr">title</span>=<span class="string">"Core Masters"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">add-after-group-entry</span>&gt;</span>YARN<span class="tag">&lt;/<span class="name">add-after-group-entry</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">service</span> <span class="attr">name</span>=<span class="string">"HBASE"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">component</span>&gt;</span>HBASE_MASTER<span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">group</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>还可以在stack的upgrade-pack中增加新的group，并将它们排列在其他group之后。在下面的例子中，我们在使用<add-after-group>标签的HIVE的group之后增加了一个名为FOO的group。</add-after-group></p>
<p>Order Section - Add After Group<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">group</span> <span class="attr">name</span>=<span class="string">"FOO"</span> <span class="attr">title</span>=<span class="string">"Foo"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">add-after-group</span>&gt;</span>HIVE<span class="tag">&lt;/<span class="name">add-after-group</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">skippable</span>&gt;</span>true<span class="tag">&lt;/<span class="name">skippable</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">allow-retry</span>&gt;</span>false<span class="tag">&lt;/<span class="name">allow-retry</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">service</span> <span class="attr">name</span>=<span class="string">"FOO"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">component</span>&gt;</span>BAR<span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">group</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>你还可以在同一个<group>中同时创建<add-after-group>和<add-after-groujp-entry>。这将会在指定的group不存在的情况下才会创建一个新的group，并且会将他排列在<add-after-group>指定的group之后。<add-after-group-entry>将会确定它的group的service的内部排序、优先级和执行阶段。</add-after-group-entry></add-after-group></add-after-groujp-entry></add-after-group></group></p>
<p>Order Section - Add After Group<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">group</span> <span class="attr">name</span>=<span class="string">"FOO"</span> <span class="attr">title</span>=<span class="string">"Foo"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">add-after-group</span>&gt;</span>HIVE<span class="tag">&lt;/<span class="name">add-after-group</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">add-after-group-entry</span>&gt;</span>FOO<span class="tag">&lt;/<span class="name">add-after-group-entry</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">skippable</span>&gt;</span>true<span class="tag">&lt;/<span class="name">skippable</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">allow-retry</span>&gt;</span>false<span class="tag">&lt;/<span class="name">allow-retry</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">service</span> <span class="attr">name</span>=<span class="string">"FOO2"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">component</span>&gt;</span>BAR2<span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">group</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>upgrade-pack剩余的<processing>部分，与stack的upgrade-pack的相同。</processing></p>
<p>Processing Section<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">processing</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">service</span> <span class="attr">name</span>=<span class="string">"FOO"</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">component</span> <span class="attr">name</span>=<span class="string">"BAR"</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">upgrade</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">task</span> <span class="attr">xsi:type</span>=<span class="string">"restart-task"</span> /&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">upgrade</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">component</span> <span class="attr">name</span>=<span class="string">"BAR2"</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">upgrade</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">task</span> <span class="attr">xsi:type</span>=<span class="string">"restart-task"</span> /&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">upgrade</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">processing</span>&gt;</span></div></pre></td></tr></table></figure></p>
<h1 id="Define-Stack"><a href="#Define-Stack" class="headerlink" title="Define Stack"></a>Define Stack</h1><p>一个Stack就是一个版本化的service的集合。每个stack就是一个定义在ambari-server/src/main/resource/stacks中的一个文件夹。安装ambari之后，stack的定义则位于ambari-server主机的/var/lib/ambari-server/resources/stacks中。<br>每个stack文件夹中包含该stack的每个版本的子文件夹。一些stack版本可用，一些不可用。每个stack版本包含一些service，这些service有的继承自common-services，有些在stack版本的services中定义。<br><img src="http://oaavtz33a.bkt.clouddn.com/Screen%20Shot%202016-03-08%20at%2012.46.40%20PM.png"><br>Example : <a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/stacks/HDP/2.4" title="HDP stack.HDP-2.4 stack version" target="_blank" rel="external">HDP stack.HDP-2.4 stack version</a>。</p>
<h2 id="Stack-Version-Descriptor"><a href="#Stack-Version-Descriptor" class="headerlink" title="Stack-Version Descriptor"></a>Stack-Version Descriptor</h2><p>每个Stack-version应当提供一个metainfo.xml（如：<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.3/metainfo.xml" title="HDP-2.3" target="_blank" rel="external">HDP-2.3</a>、<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.4/metainfo.xml" title="HDP-2.4" target="_blank" rel="external">HDP-2.4</a> ）文件作为描述符，它如下描述了stack-verion：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">metainfo</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">versions</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">active</span>&gt;</span>true<span class="tag">&lt;/<span class="name">active</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">versions</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">extends</span>&gt;</span>2.3<span class="tag">&lt;/<span class="name">extends</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">minJdk</span>&gt;</span>1.7<span class="tag">&lt;/<span class="name">minJdk</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">maxJdk</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maxJdk</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">metainfo</span>&gt;</span></div></pre></td></tr></table></figure></p>
<blockquote>
<p>versions/active - 当前版本的stack是否还可以用于安装。如果不可用，这个版本在安装的时候将不会在UI中显示。<br>extends - 当前stack继承的版本。进行继承的stack版本会继承service以及父stack版本的所有方面。<br>minJdk - stack版本支持的最低JDK版本。在安装向导期间如果被Ambari使用的JDK低于这个版本，用户将被警告。<br>maxJdk - stack版本支持的最高JDK版本。在安装向导期间，如果被Ambari使用的JDK版本高于这个版本，用户将被警告。</p>
</blockquote>
<h2 id="Stack-Properties"><a href="#Stack-Properties" class="headerlink" title="Stack Properties"></a>Stack Properties</h2><p>stack必须包含或继承一个属性字典，属性字典包含两个文件：<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/properties/stack_features.json" title="stack_features.json" target="_blank" rel="external">stack_features.json</a>和<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/properties/stack_tools.json" title="stack_tools.json" target="_blank" rel="external">stack_tools.json</a>。这个字典是在Ambari 2.4中新增的。<br>stack_features.json中包含了一个features的列表，这个列表指定了哪些版本的stack包含这些特性。<br>特性列表由特定的Ambari版本所确定。特定Ambari版本的详细列表能够在HDP/2.0.6/properties/stack_features.json中找到。每个feature由name、description以及特性所支持stack的最高版本和最低版本来构成。<br><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"stack_features"</span>: [</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"name"</span>: <span class="string">"snappy"</span>,</div><div class="line">      <span class="attr">"description"</span>: <span class="string">"Snappy compressor/decompressor support"</span>,</div><div class="line">      <span class="attr">"min_version"</span>: <span class="string">"2.0.0.0"</span>,</div><div class="line">      <span class="attr">"max_version"</span>: <span class="string">"2.2.0.0"</span></div><div class="line">    &#125;,</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>stack_tools.json包含了stack_selector和conf_selector这两个工具对应的名称以及安装位置。<br>任何自定义的stack必须包含这两个JSON文件。更多的信息请查看<a href="https://cwiki.apache.org/confluence/display/AMBARI/Stack+Properties" title="Stack Properties" target="_blank" rel="external">Stack Properties</a>的wiki页面。</p>
<h2 id="Services"><a href="#Services" class="headerlink" title="Services"></a>Services</h2><p>每个stack版本中都包含services，这些services要么是引用的common-services中的，要么是在stack版本中services文件夹下定义的。<br>common-services中定义的services能够被多个stack共享。如果他们不会被共享，那么他们可以定义在stack版本中。</p>
<h3 id="Reference-common-services"><a href="#Reference-common-services" class="headerlink" title="Reference common-services"></a>Reference common-services</h3><p>要引用common-services中的一个service，service描述文件需要使用<extends>项。（例如： <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/HDFS/metainfo.xml" title="DFS in HDP-2.0.6" target="_blank" rel="external">HDFS in HDP-2.0.6</a>）<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">metainfo</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">schemaVersion</span>&gt;</span>2.0<span class="tag">&lt;/<span class="name">schemaVersion</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">services</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">service</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>HDFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">extends</span>&gt;</span>common-services/HDFS/2.1.0.2.0<span class="tag">&lt;/<span class="name">extends</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">services</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">metainfo</span>&gt;</span></div></pre></td></tr></table></figure></extends></p>
<h3 id="Define-Service-1"><a href="#Define-Service-1" class="headerlink" title="Define Service"></a>Define Service</h3><p>与common-services中定义的services格式相同，可以子啊services文件夹中定义新的service。<br>Examples：</p>
<ul><br>    <li><a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/stacks/BIGTOP/0.8/services/HDFS" title="HDFS in BIGTOP-0.8" target="_blank" rel="external">HDFS in BIGTOP-0.8</a></li><br>    <li><a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/stacks/HDP/2.3.GlusterFS/services/GLUSTERFS" title="GlusterFS in HDP-2.3.GlusterFS" target="_blank" rel="external">GlusterFs in HDP-2.3.CusterFs</a></li><br></ul>

<h3 id="Extend-Service"><a href="#Extend-Service" class="headerlink" title="Extend Service"></a>Extend Service</h3><p>当一个版本继承另外一个版本时，它继承父级service的所有细节。它也可以自由的重写或删除继承的service定义的任何部分。<br>Examples：</p>
<ul><br>    <li>HDP-2.3/HDFS - <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.3/services/HDFS/metainfo.xml" title="添加NFS_GATEWAY组件，更新service版本和OS特定包" target="_blank" rel="external">添加NFS_GATEWAY组件，更新service版本和OS特定包</a></li><br>    <li>HDP-2.2/Storm - <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.2/services/STORM/metainfo.xml" title="删除了STORM_REST_API组件，更新service版本和OS特定包" target="_blank" rel="external">删除了STORM_REST_API组件，更新service版本和OS特定包</a></li><br>    <li>HDP-2.3/YARN - <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.3/services/YARN/configuration/capacity-scheduler.xml" title="从capacity-scheduler.mxl中删除YARN node-lable配置" target="_blank" rel="external">从capacity-scheduler.mxl中删除YARN node-lable配置</a></li><br>    <li>HDP-2.3/Kafka - <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.3/services/KAFKA/alerts.json" title="增加Kafka Broker进程告警" target="_blank" rel="external">增加Kafka Broker进程告警</a></li><br></ul>

<h2 id="Role-Command-Order-1"><a href="#Role-Command-Order-1" class="headerlink" title="Role Command Order"></a>Role Command Order</h2><p>Role是Component（如：NAMENODE、DATANODE、RESOURCEMANAGER、HBASE_MASTER等）的另一个名称。<br>顾名思义，它可以告诉Amberi在你stack中定义的component执行命令的顺序。<br>例如：”ZooKeeper Server 应当在启动NameNode之前启动”。“HBase Master应当在NameNode和DataNode启动之后再启动”。<br>这可以通过在stack-version文件夹中包含<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/role_command_order.json" title="role_command_order.json" target="_blank" rel="external">role_command_order.json</a>来具体说明。</p>
<h3 id="Format"><a href="#Format" class="headerlink" title="Format"></a>Format</h3><p>以JSON格式指定，这个文件包含一个JSON对象，并且顶级key是section名称或comments。如：<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/role_command_order.json" title="HDP-2.0.6" target="_blank" rel="external">HDP-2.0.6</a>。<br>在每个section对象内部，key描述了它对应的component的行为，value列出当前component-action之前应当完成的component-action。<br>Structure of role_command_order.json<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  "_comment": "Section 1 comment",</div><div class="line">  "section_name_1": &#123;</div><div class="line">    "_comment": "Section containing role command orders",</div><div class="line">    "&lt;DEPENDENT_COMPONENT_1&gt;-&lt;COMMAND&gt;": ["&lt;DEPENDS_ON_COMPONENT_1&gt;-&lt;COMMAND&gt;", "&lt;DEPENDS_ON_COMPONENT_1&gt;-&lt;COMMAND&gt;"],</div><div class="line">    "&lt;DEPENDENT_COMPONENT_2&gt;-&lt;COMMAND&gt;": ["&lt;DEPENDS_ON_COMPONENT_3&gt;-&lt;COMMAND&gt;"],</div><div class="line">    ...</div><div class="line">  &#125;,</div><div class="line">  "_comment": "Next section comment",</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="Sections"><a href="#Sections" class="headerlink" title="Sections"></a>Sections</h3><p>Ambari只使用了如下的sections：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Section Name</th>
<th style="text-align:left">When Used</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">general_deps</td>
<td style="text-align:left">适用于所有情况</td>
</tr>
<tr>
<td style="text-align:left">optional_glusterfs</td>
<td style="text-align:left">当集群有GLUSTERFS服务实例时</td>
</tr>
<tr>
<td style="text-align:left">optional_no_glusterfs</td>
<td style="text-align:left">当集群没有GLUSTERFS服务实例时</td>
</tr>
<tr>
<td style="text-align:left">namenode_optional_ha</td>
<td style="text-align:left">当安装了HDFS服务，且有JOURNALNODE组件时</td>
</tr>
<tr>
<td style="text-align:left">resourcemanager_optional_ha</td>
<td style="text-align:left">当安装了YARN服务，且存在多个RESOURCEMANAGER host-components存在时</td>
</tr>
</tbody>
</table>
<h3 id="Commands"><a href="#Commands" class="headerlink" title="Commands"></a>Commands</h3><p>Ambari当前支持的命令有：</p>
<blockquote>
<p>INSTALL<br>UNINSTALL<br>START<br>RESTART<br>STOP<br>EXECUTE<br>ABORT<br>UPGRADE<br>SERVICE_CHECK<br>CUSTOM_COMMAND<br>ACTIONEXECUTE</p>
</blockquote>
<h3 id="Examples-2"><a href="#Examples-2" class="headerlink" title="Examples"></a>Examples</h3><table>
<thead>
<tr>
<th style="text-align:left">Role Command Order</th>
<th style="text-align:left">Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">“HIVE_METASTORE-START”:[“MYSQL_SERVER-START”, “NAMENODE-START”]</td>
<td style="text-align:left">启动Hive Metastore之前先启动MySQL和NameNode。</td>
</tr>
<tr>
<td style="text-align:left">“MAPREDUCE_SERVICE_CHECK-SERVICE_CHECK”:[“NODEMANAGER-START”, “RESOURCEMANAGER-START”]</td>
<td style="text-align:left">MapReduce服务检查需要ResourceManager和NodeManager的启动。</td>
</tr>
<tr>
<td style="text-align:left">“ZOOKEEPER_SERVER-STOP”:[“HBASE_MASTER-STOP”, “HBASE_REGIONSERVER-STOP”, “METRICS_COLLECTOR-STOP”]</td>
<td style="text-align:left">在停止Zookeeper之前，应该先确保HBase Master、Hbase RegionServers和AMS Metrics收集器先停止。</td>
</tr>
</tbody>
</table>
<h2 id="Repositories"><a href="#Repositories" class="headerlink" title="Repositories"></a>Repositories</h2><p>通过提供一个repos/repoinfo.xml（如 <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/repos/repoinfo.xml" title="HDP-2.0.6" target="_blank" rel="external">HDP-2.0.6</a>），每个stack版本可以提供package的库的位置来使用。<br>repoinfo.xml文件中包含的库根据操作系统进行分组。每个os指定一个库列表，这些库列表会在stack版本安装时展示给用户。<br>这些库与<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/metainfo.xml#L161" title="packages defined in a service&#39;s metainfo.xml" target="_blank" rel="external">packages defined in a service’s metainfo.xml</a>配合使用，以便在系统上安装正确的。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">reposinfo</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">os</span> <span class="attr">family</span>=<span class="string">"redhat6"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">repo</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">baseurl</span>&gt;</span>http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.1<span class="tag">&lt;/<span class="name">baseurl</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">repoid</span>&gt;</span>HDP-2.0.6<span class="tag">&lt;/<span class="name">repoid</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">reponame</span>&gt;</span>HDP<span class="tag">&lt;/<span class="name">reponame</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">repo</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">repo</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">baseurl</span>&gt;</span>http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.17/repos/centos6<span class="tag">&lt;/<span class="name">baseurl</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">repoid</span>&gt;</span>HDP-UTILS-1.1.0.17<span class="tag">&lt;/<span class="name">repoid</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">reponame</span>&gt;</span>HDP-UTILS<span class="tag">&lt;/<span class="name">reponame</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">repo</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">os</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">reposinfo</span>&gt;</span></div></pre></td></tr></table></figure></p>
<blockquote>
<p>baseurl - RPM库的URL，可以在这里找到repoid提供的软件。<br>repoid - baseurl地址使用的repo id。<br>reponame - 需要使用的repo的展示名。</p>
</blockquote>
<h3 id="Latest-Builds"><a href="#Latest-Builds" class="headerlink" title="Latest Builds"></a>Latest Builds</h3><p>尽管repository基本URL能够对某个特定repo提供更新，但是必须在构建时定义它。当repository变更位置或更新包位于不同网站时，这就会成为一个问题。<br>对于这样的情况，stack-version能够提供一个JSON文件，来提供要使用的其他repo URL。<br>例如： <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.3/repos/repoinfo.xml" title="HDP-2.3 repoinfo.xml uses &lt;latest&gt; file" target="_blank" rel="external">HDP-2.3 repoinfo.xml uses <latest> file</latest></a>，它指出最新的构建包的repository URL。</p>
<figure class="highlight plain"><figcaption><span>json</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    ...</div><div class="line">    &quot;HDP-2.3&quot;:&#123;</div><div class="line">        &quot;latest&quot;:&#123;</div><div class="line">            &quot;centos6&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos6/2.x/BUILDS/2.3.6.0-3586/&quot;,</div><div class="line">            &quot;centos7&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/2.x/BUILDS/2.3.6.0-3586/&quot;,</div><div class="line">            &quot;debian6&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/debian6/2.x/BUILDS/2.3.6.0-3586/&quot;,</div><div class="line">            &quot;debian7&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/debian7/2.x/BUILDS/2.3.6.0-3586/&quot;,</div><div class="line">            &quot;suse11&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/suse11sp3/2.x/BUILDS/2.3.6.0-3586/&quot;,</div><div class="line">            &quot;ubuntu12&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/ubuntu12/2.x/BUILDS/2.3.6.0-3586/&quot;,</div><div class="line">            &quot;ubuntu14&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/ubuntu14/2.x/BUILDS/2.3.6.0-3586/&quot;</div><div class="line">        &#125;</div><div class="line">    &#125;,</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Hooks"><a href="#Hooks" class="headerlink" title="Hooks"></a>Hooks</h2><p>stack-version会有非常基本且通用的指令，这些指令需要在某个Ambari命令之前或之后运行。<br>避免将代码在service脚本之间复制并要求用户确认，通过将前置代码和后置代码放到hooks文件夹中，Ambari提供了Hooks的功能。（如：<a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks" title="HDP-2.0.6" target="_blank" rel="external">HDP-2.0.6</a>）<br><img src=""></p>
<h3 id="Command-Sub-Folders"><a href="#Command-Sub-Folders" class="headerlink" title="Command Sub-Folders"></a>Command Sub-Folders</h3><p>hooks子文件夹的命名模式为”<before|after>-<any|<commandname>&gt;”。<br>那意味着子文件夹中的scripts/hook.py文件是在命令之前运行还是之后运行。<br>Examples：</any|<commandname></before|after></p>
<table>
<thead>
<tr>
<th style="text-align:left">Sub-Folder</th>
<th style="text-align:left">Purpose</th>
<th style="text-align:left">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">before-START</td>
<td style="text-align:left">hook脚本，会在stack-version的任何组件启动之前被调用</td>
<td style="text-align:left"><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py#L30" title="HDP-2.0.6" target="_blank" rel="external">HDP-2.0.6</a> 1、设置hadoop的日志和pid目录。2、创建javahome的symlink。3、创建/etc/hadoop/conf/topology_script.py脚本</td>
</tr>
<tr>
<td style="text-align:left">before-INSTALL</td>
<td style="text-align:left">hook脚本，会在stack-version的任何组件安装之前被调用</td>
<td style="text-align:left"><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py#L30" title="HDP-2.0.6" target="_blank" rel="external">HDP-2.0.6</a> 1、在/etc/yum.repos.d中创建repo文件。 2、安装基本包，如curl、unzip等</td>
</tr>
</tbody>
</table>
<p>Ambari当前支持的命令，根据需要可以创建如下的子文件夹</p>
<table>
<thead>
<tr>
<th style="text-align:left">Prefix</th>
<th style="text-align:left">Command</th>
<th style="text-align:left">Details</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">before</td>
<td style="text-align:left">INSTALL</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">before</td>
<td style="text-align:left">UNINSTALL</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">before</td>
<td style="text-align:left">START</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">before</td>
<td style="text-align:left">RESTART</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">before</td>
<td style="text-align:left">STOP</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">after</td>
<td style="text-align:left">EXECUTE</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">after</td>
<td style="text-align:left">ABORT</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">after</td>
<td style="text-align:left">UPGRADE</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">after</td>
<td style="text-align:left">SERVICE_CHECK</td>
<td style="text-align:left">&nbps;</td>
</tr>
<tr>
<td style="text-align:left">after</td>
<td style="text-align:left">&lt; custom_command&gt;</td>
<td style="text-align:left">用户指定的自定义命令，如HDFS指定的DECOMMISSION或REBALANCEHDFS这两个命令。</td>
</tr>
</tbody>
</table>
<p>script/hooks.py脚本应该导入<a href="https://github.com/apache/ambari/blob/trunk/ambari-common/src/main/python/resource_management/libraries/script/hook.py" title="resource_management.libraries.script.hook" target="_blank" rel="external">resource_management.libraries.script.hook</a>模块，并继承Hook类。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> resource_management.libraries.script.hook <span class="keyword">import</span> Hook</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomHook</span><span class="params">(Hook)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hook</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="comment"># Do custom work</span></div><div class="line">     </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  CustomHook().execute()</div></pre></td></tr></table></figure></p>
<h2 id="Configurations"><a href="#Configurations" class="headerlink" title="Configurations"></a>Configurations</h2><p>尽管大多数配置是在service级别设置的，但是也可以有适用于所有servies的配置，以便指示安装了此stack的集群的状态。<br>例如，像<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/configuration/cluster-env.xml#L25" target="_blank" rel="external">is security enabled?</a>，<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/configuration/cluster-env.xml#L46" target="_blank" rel="external">what user runs smoke tests?</a> 等。<br>这样的配置可以定义在sstack的<a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/configuration" target="_blank" rel="external">configuration文件夹</a>中。它们就像service级配置一样访问。</p>
<h3 id="Stack-Advisor"><a href="#Stack-Advisor" class="headerlink" title="Stack Advisor"></a>Stack Advisor</h3><p>由于每个stack包含多个复杂的service，因此有必要动态确定services的布局以及确定某些配置的值。<br>stacks在services/目录中编写一个名为stack-advisor.py的Python脚本，使Ambari提供了Stack Advisor的能力。例如：<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py" title="HDP-2.0.6" target="_blank" rel="external">HDP-2.0.6</a>。Stack advisor脚本能够自动继承父级stack版本的stack advisor脚本。这允许较新的stack版本能够改变行为而不会影响之前的版本的行为。<br>Stack advisor在4个重要概念上提供了信息：</p>
<blockquote>
<p>Recommend layout of services on cluster。<br>Recommend service configurations。<br>Validate layout of services on cluster。<br>Validate service configurations。</p>
</blockquote>
<p>通过提供stack-advisor.py文件，能够动态的控制上面的每一项。<br>stack-advisor脚本的主要接口描述了上面每项应当如何调用，以及提供什么数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">StackAdvisor</span><span class="params">(object)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line">  Abstract class implemented by all stack advisors. Stack advisors advise on stack specific questions.</div><div class="line"> </div><div class="line"> </div><div class="line">  Currently stack advisors provide following abilities:</div><div class="line">  - Recommend where services should be installed in cluster</div><div class="line">  - Recommend configurations based on host hardware</div><div class="line">  - Validate user selection of where services are installed on cluster</div><div class="line">  - Validate user configuration values</div><div class="line"> </div><div class="line">  Each of the above methods is passed in parameters about services and hosts involved as described below.</div><div class="line"> </div><div class="line"> </div><div class="line">    @type services: dictionary</div><div class="line">    @param services: Dictionary containing all information about services selected by the user.</div><div class="line">      Example: &#123;</div><div class="line">      "services": [</div><div class="line">        &#123;</div><div class="line">          "StackServices": &#123;</div><div class="line">            "service_name" : "HDFS",</div><div class="line">            "service_version" : "2.6.0.2.2",</div><div class="line">          &#125;,</div><div class="line">          "components" : [</div><div class="line">            &#123;</div><div class="line">              "StackServiceComponents" : &#123;</div><div class="line">                "cardinality" : "1+",</div><div class="line">                "component_category" : "SLAVE",</div><div class="line">                "component_name" : "DATANODE",</div><div class="line">                "display_name" : "DataNode",</div><div class="line">                "service_name" : "HDFS",</div><div class="line">                "hostnames" : []</div><div class="line">              &#125;,</div><div class="line">              "dependencies" : []</div><div class="line">            &#125;, &#123;</div><div class="line">              "StackServiceComponents" : &#123;</div><div class="line">                "cardinality" : "1-2",</div><div class="line">                "component_category" : "MASTER",</div><div class="line">                "component_name" : "NAMENODE",</div><div class="line">                "display_name" : "NameNode",</div><div class="line">                "service_name" : "HDFS",</div><div class="line">                "hostnames" : []</div><div class="line">              &#125;,</div><div class="line">              "dependencies" : []</div><div class="line">            &#125;,</div><div class="line">            ...</div><div class="line">          ]</div><div class="line">        &#125;,</div><div class="line">        ...</div><div class="line">      ]</div><div class="line">    &#125;</div><div class="line">  @type hosts: dictionary</div><div class="line">  @param hosts: Dictionary containing all information about hosts in this cluster</div><div class="line">    Example: &#123;</div><div class="line">      "items": [</div><div class="line">        &#123;</div><div class="line">          Hosts: &#123;</div><div class="line">            "host_name": "c6401.ambari.apache.org",</div><div class="line">            "public_host_name" : "c6401.ambari.apache.org",</div><div class="line">            "ip": "192.168.1.101",</div><div class="line">            "cpu_count" : 1,</div><div class="line">            "disk_info" : [</div><div class="line">              &#123;</div><div class="line">              "available" : "4564632",</div><div class="line">              "used" : "5230344",</div><div class="line">              "percent" : "54%",</div><div class="line">              "size" : "10319160",</div><div class="line">              "type" : "ext4",</div><div class="line">              "mountpoint" : "/"</div><div class="line">              &#125;,</div><div class="line">              &#123;</div><div class="line">              "available" : "1832436",</div><div class="line">              "used" : "0",</div><div class="line">              "percent" : "0%",</div><div class="line">              "size" : "1832436",</div><div class="line">              "type" : "tmpfs",</div><div class="line">              "mountpoint" : "/dev/shm"</div><div class="line">              &#125;</div><div class="line">            ],</div><div class="line">            "host_state" : "HEALTHY",</div><div class="line">            "os_arch" : "x86_64",</div><div class="line">            "os_type" : "centos6",</div><div class="line">            "total_mem" : 3664872</div><div class="line">          &#125;</div><div class="line">        &#125;,</div><div class="line">        ...</div><div class="line">      ]</div><div class="line">    &#125;</div><div class="line"> </div><div class="line"> </div><div class="line">    Each of the methods can either return recommendations or validations.</div><div class="line"> </div><div class="line">    Recommendations are made in a Ambari Blueprints friendly format.</div><div class="line">    Validations are an array of validation objects.</div><div class="line">  """</div><div class="line"> </div><div class="line"> </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">recommendComponentLayout</span><span class="params">(self, services, hosts)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Returns recommendation of which hosts various service components should be installed on.</div><div class="line"> </div><div class="line">    This function takes as input all details about services being installed, and hosts</div><div class="line">    they are being installed into, to generate hostname assignments to various components</div><div class="line">    of each service.</div><div class="line"> </div><div class="line"> </div><div class="line">    @type services: dictionary</div><div class="line">    @param services: Dictionary containing all information about services selected by the user.</div><div class="line">    @type hosts: dictionary</div><div class="line">    @param hosts: Dictionary containing all information about hosts in this cluster</div><div class="line">    @rtype: dictionary</div><div class="line">    @return: Layout recommendation of service components on cluster hosts in Ambari Blueprints friendly format.</div><div class="line">        Example: &#123;</div><div class="line">          "resources" : [</div><div class="line">            &#123;</div><div class="line">              "hosts" : [</div><div class="line">                "c6402.ambari.apache.org",</div><div class="line">                "c6401.ambari.apache.org"</div><div class="line">              ],</div><div class="line">              "services" : [</div><div class="line">                "HDFS"</div><div class="line">              ],</div><div class="line">              "recommendations" : &#123;</div><div class="line">                "blueprint" : &#123;</div><div class="line">                  "host_groups" : [</div><div class="line">                    &#123;</div><div class="line">                      "name" : "host-group-2",</div><div class="line">                      "components" : [</div><div class="line">                        &#123; "name" : "JOURNALNODE" &#125;,</div><div class="line">                        &#123; "name" : "ZKFC" &#125;,</div><div class="line">                        &#123; "name" : "DATANODE" &#125;,</div><div class="line">                        &#123; "name" : "SECONDARY_NAMENODE" &#125;</div><div class="line">                      ]</div><div class="line">                    &#125;,</div><div class="line">                    &#123;</div><div class="line">                      "name" : "host-group-1",</div><div class="line">                      "components" :</div><div class="line">                        &#123; "name" : "HDFS_CLIENT" &#125;,</div><div class="line">                        &#123; "name" : "NAMENODE" &#125;,</div><div class="line">                        &#123; "name" : "JOURNALNODE" &#125;,</div><div class="line">                        &#123; "name" : "ZKFC" &#125;,</div><div class="line">                        &#123; "name" : "DATANODE" &#125;</div><div class="line">                      ]</div><div class="line">                    &#125;</div><div class="line">                  ]</div><div class="line">                &#125;,</div><div class="line">                "blueprint_cluster_binding" : &#123;</div><div class="line">                  "host_groups" : [</div><div class="line">                    &#123;</div><div class="line">                      "name" : "host-group-1",</div><div class="line">                      "hosts" : [ &#123; "fqdn" : "c6401.ambari.apache.org" &#125; ]</div><div class="line">                    &#125;,</div><div class="line">                    &#123;</div><div class="line">                      "name" : "host-group-2",</div><div class="line">                      "hosts" : [ &#123; "fqdn" : "c6402.ambari.apache.org" &#125; ]</div><div class="line">                    &#125;</div><div class="line">                  ]</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          ]</div><div class="line">        &#125;</div><div class="line">    """</div><div class="line">    <span class="keyword">pass</span></div><div class="line"> </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">validateComponentLayout</span><span class="params">(self, services, hosts)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Returns array of Validation issues with service component layout on hosts</div><div class="line"> </div><div class="line"> </div><div class="line">    This function takes as input all details about services being installed along with</div><div class="line">    hosts the components are being installed on (hostnames property is populated for</div><div class="line">    each component). </div><div class="line"> </div><div class="line">    @type services: dictionary</div><div class="line">    @param services: Dictionary containing information about services and host layout selected by the user.</div><div class="line">    @type hosts: dictionary</div><div class="line">    @param hosts: Dictionary containing all information about hosts in this cluster</div><div class="line">    @rtype: dictionary</div><div class="line">    @return: Dictionary containing array of validation items</div><div class="line">        Example: &#123;</div><div class="line">          "items": [</div><div class="line">            &#123;</div><div class="line">              "type" : "host-group",</div><div class="line">              "level" : "ERROR",</div><div class="line">              "message" : "NameNode and Secondary NameNode should not be hosted on the same machine",</div><div class="line">              "component-name" : "NAMENODE",</div><div class="line">              "host" : "c6401.ambari.apache.org"</div><div class="line">            &#125;,</div><div class="line">            ...</div><div class="line">          ]</div><div class="line">        &#125; </div><div class="line">    """</div><div class="line">    <span class="keyword">pass</span></div><div class="line"> </div><div class="line"> </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">recommendConfigurations</span><span class="params">(self, services, hosts)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Returns recommendation of service configurations based on host-specific layout of components.</div><div class="line"> </div><div class="line">    This function takes as input all details about services being installed, and hosts</div><div class="line">    they are being installed into, to recommend host-specific configurations.</div><div class="line"> </div><div class="line"> </div><div class="line">    @type services: dictionary</div><div class="line">    @param services: Dictionary containing all information about services and component layout selected by the user.</div><div class="line">    @type hosts: dictionary</div><div class="line">    @param hosts: Dictionary containing all information about hosts in this cluster</div><div class="line">    @rtype: dictionary</div><div class="line">    @return: Layout recommendation of service components on cluster hosts in Ambari Blueprints friendly format.</div><div class="line">        Example: &#123;</div><div class="line">         "services": [</div><div class="line">          "HIVE",</div><div class="line">          "TEZ",</div><div class="line">          "YARN"</div><div class="line">         ],</div><div class="line">         "recommendations": &#123;</div><div class="line">          "blueprint": &#123;</div><div class="line">           "host_groups": [],</div><div class="line">           "configurations": &#123;</div><div class="line">            "yarn-site": &#123;</div><div class="line">             "properties": &#123;</div><div class="line">              "yarn.scheduler.minimum-allocation-mb": "682",</div><div class="line">              "yarn.scheduler.maximum-allocation-mb": "2048",</div><div class="line">              "yarn.nodemanager.resource.memory-mb": "2048"</div><div class="line">             &#125;</div><div class="line">            &#125;,</div><div class="line">            "tez-site": &#123;</div><div class="line">             "properties": &#123;</div><div class="line">              "tez.am.java.opts": "-server -Xmx546m -Djava.net.preferIPv4Stack=true -XX:+UseNUMA -XX:+UseParallelGC",</div><div class="line">              "tez.am.resource.memory.mb": "682"</div><div class="line">             &#125;</div><div class="line">            &#125;,</div><div class="line">            "hive-site": &#123;</div><div class="line">             "properties": &#123;</div><div class="line">              "hive.tez.container.size": "682",</div><div class="line">              "hive.tez.java.opts": "-server -Xmx546m -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC",</div><div class="line">              "hive.auto.convert.join.noconditionaltask.size": "238026752"</div><div class="line">             &#125;</div><div class="line">            &#125;</div><div class="line">           &#125;</div><div class="line">          &#125;,</div><div class="line">          "blueprint_cluster_binding": &#123;</div><div class="line">           "host_groups": []</div><div class="line">          &#125;</div><div class="line">         &#125;,</div><div class="line">         "hosts": [</div><div class="line">          "c6401.ambari.apache.org",</div><div class="line">          "c6402.ambari.apache.org",</div><div class="line">          "c6403.ambari.apache.org"</div><div class="line">         ]</div><div class="line">        &#125;</div><div class="line">    """</div><div class="line">    <span class="keyword">pass</span></div><div class="line"> </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">validateConfigurations</span><span class="params">(self, services, hosts)</span>:</span></div><div class="line">    <span class="string">""""</span></div><div class="line">    Returns array of Validation issues with configurations provided by user</div><div class="line">    This function takes as input all details about services being installed along with</div><div class="line">    configuration values entered by the user. These configurations can be validated against</div><div class="line">    service requirements, or host hardware to generate validation issues.</div><div class="line"> </div><div class="line"> </div><div class="line">    @type services: dictionary</div><div class="line">    @param services: Dictionary containing information about services and user configurations.</div><div class="line">    @type hosts: dictionary</div><div class="line">    @param hosts: Dictionary containing all information about hosts in this cluster</div><div class="line">    @rtype: dictionary</div><div class="line">    @return: Dictionary containing array of validation items</div><div class="line">        Example: &#123;</div><div class="line">         "items": [</div><div class="line">          &#123;</div><div class="line">           "config-type": "yarn-site",</div><div class="line">           "message": "Value is less than the recommended default of 682",</div><div class="line">           "type": "configuration",</div><div class="line">           "config-name": "yarn.scheduler.minimum-allocation-mb",</div><div class="line">           "level": "WARN"</div><div class="line">          &#125;</div><div class="line">         ]</div><div class="line">       &#125;</div><div class="line">    """</div><div class="line">    <span class="keyword">pass</span></div></pre></td></tr></table></figure></p>
<h4 id="Examples-3"><a href="#Examples-3" class="headerlink" title="Examples:"></a>Examples:</h4><blockquote>
<p><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/stack_advisor.py#L23" title="Stack Advisor interface" target="_blank" rel="external">Stack Advisor interface</a><br><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/stack_advisor.py#L303" title="Default Stack Advisor implementation - for all stacks" target="_blank" rel="external">Default Stack Advisor implementation - for all stacks</a><br><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L28" title="HDP(2.0.6) Default Stack Advisor implementation" target="_blank" rel="external">HDP(2.0.6) Default Stack Advisor implementation</a><br><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L807" title="YARN container size calculate" target="_blank" rel="external">YARN container size calculate</a><br>Recommended configurations - <a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L222" target="_blank" rel="external">HDFS</a>，<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L133" target="_blank" rel="external">YARN</a>，<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L148" target="_blank" rel="external">MapReduce2</a>, <a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L245" target="_blank" rel="external">HBase</a> (HDP-2.0.6)，<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L148" target="_blank" rel="external">HBase</a> (HDP-2.3)<br><a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.3/services/stack_advisor.py#L272" target="_blank" rel="external">Delete HBase Bucket Cache configs on smaller machines</a><br><a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.3/services/stack_advisor.py#L184" target="_blank" rel="external">Specify maximum value for Tez config</a></p>
</blockquote>
<h3 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h3><p>与stack的配置类似，大多属性都是在service级定义，然而也可以在stack-version级别定义全局属性来影响所有的services。<br>一些例子：<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/properties/stack_tools.json#L2" target="_blank" rel="external">stack-selector and conf-selector</a> 或 <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/properties/stack_features.json#L5" target="_blank" rel="external">stack versions certain stack features</a>。这里的大多属性都是在Ambari 2.4版本引入的以影响stack信息参数化和促进common-service代码重用。<br>这些属性可以定义在stack的properties文件中的.json文件中。<br>stack属性的更多信息可以在<a href="https://cwiki.apache.org/confluence/x/pgPiAw" target="_blank" rel="external">Stack Properties section</a>找到。</p>
<h3 id="Widgets-1"><a href="#Widgets-1" class="headerlink" title="Widgets"></a>Widgets</h3><p>暂无</p>
<h3 id="Kerberos-1"><a href="#Kerberos-1" class="headerlink" title="Kerberos"></a>Kerberos</h3><p>之前我们已经在service级别介绍了Kerberos。<br>stack-version级别定义的Kerberos为所有的service提供了身份描述。</p>
<p>Examples：<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.0.6/kerberos.json" target="_blank" rel="external">Smoke test user and SPNEGO user define in HDP-2.0.6</a></p>
<h3 id="Stack-Upgrades"><a href="#Stack-Upgrades" class="headerlink" title="Stack Upgrades"></a>Stack Upgrades</h3><p>暂无</p>
<h1 id="Writing-metainfo-xml"><a href="#Writing-metainfo-xml" class="headerlink" title="Writing metainfo.xml"></a>Writing metainfo.xml</h1><p>metainfo.xml是Ambari管理的service的定义，它描述了service的内容。它是service定义中最重要的文件。这一章来介绍metainfo.xml中的各个片段。</p>
<h2 id="Structure-1"><a href="#Structure-1" class="headerlink" title="Structure"></a>Structure</h2><p>不重要的字段使用斜体显示。<br>描述一个service的顶级字段如下：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Field</th>
<th style="text-align:left">What is it used for</th>
<th style="text-align:left">Sample Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>name</strong></td>
<td style="text-align:left">service的名字。这个名字必须是service所在Stack范围内唯一的。</td>
<td style="text-align:left">HDFS</td>
</tr>
<tr>
<td style="text-align:left"><strong>displayName</strong></td>
<td style="text-align:left">service在UI中显示的名字。</td>
<td style="text-align:left">HDFS</td>
</tr>
<tr>
<td style="text-align:left"><strong>version</strong></td>
<td style="text-align:left">service的版本。名字和版本一起确定了唯一的service。</td>
<td style="text-align:left">2.1.0.2.0</td>
</tr>
<tr>
<td style="text-align:left"><strong>components</strong></td>
<td style="text-align:left">service的组件列表</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><strong>osSpecifics</strong></td>
<td style="text-align:left">指定service运行所需的操作系统</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><em>commandScript</em></td>
<td style="text-align:left">定义service check脚本</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><em>comment</em></td>
<td style="text-align:left">service的简短描述</td>
<td style="text-align:left">Apache Hadoop Distributed File System</td>
</tr>
<tr>
<td style="text-align:left"><em>requiredServices</em></td>
<td style="text-align:left">该服务所需的前置服务</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><em>configuration-dependencies</em></td>
<td style="text-align:left">service所需的其他配置文件（这些配置文件本身属于其他service）</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><em>restartRequiredAfterRackChange</em></td>
<td style="text-align:left">Rack变更后是否必须重启</td>
<td style="text-align:left">true / false</td>
</tr>
<tr>
<td style="text-align:left"><em>configuration-dir</em></td>
<td style="text-align:left">如果配置目录不是默认的configuration，则需要使用该项来指定</td>
<td style="text-align:left">-</td>
</tr>
</tbody>
</table>
<h3 id="service-components-一个service包含多个components。与component有关的字段有："><a href="#service-components-一个service包含多个components。与component有关的字段有：" class="headerlink" title="service/components - 一个service包含多个components。与component有关的字段有："></a>service/components - 一个service包含多个components。与component有关的字段有：</h3><table>
<thead>
<tr>
<th style="text-align:left">Field</th>
<th style="text-align:left">What is it used it</th>
<th style="text-align:left">Sample Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>name</strong></td>
<td style="text-align:left">component的名字。</td>
<td style="text-align:left">NameNode</td>
</tr>
<tr>
<td style="text-align:left"><strong>dsplayName</strong></td>
<td style="text-align:left">component的显示名。</td>
<td style="text-align:left">NameNode</td>
</tr>
<tr>
<td style="text-align:left"><strong>category</strong></td>
<td style="text-align:left">component的类型。可选值为MASTER、SLAVE或CLIENT。</td>
<td style="text-align:left">-</td>
</tr>
<tr>
<td style="text-align:left"><strong>commandScript</strong></td>
<td style="text-align:left">应用的命令。</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><em>cardinality</em></td>
<td style="text-align:left">允许的实例个数。</td>
<td style="text-align:left">MASTER一般设置为1-2， SLAVE一般设置为1+</td>
</tr>
<tr>
<td style="text-align:left"><em>reassignAllowed</em></td>
<td style="text-align:left">是否允许component被重新分配或移动到另外的主机。</td>
<td style="text-align:left">true / false</td>
</tr>
<tr>
<td style="text-align:left"><em>versionAdvertised</em></td>
<td style="text-align:left">component是否显示它的版本信息。回滚/升级时使用。</td>
<td style="text-align:left">true / false</td>
</tr>
<tr>
<td style="text-align:left"><em>timelineAppid</em></td>
<td style="text-align:left">metrics收集时用来进行区分的id。</td>
<td style="text-align:left">HDFS</td>
</tr>
<tr>
<td style="text-align:left"><em>dependencies</em></td>
<td style="text-align:left">component所依赖的其他component列表。</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><em>customCommands</em></td>
<td style="text-align:left">组件的自定义命令，有别与标准命令。</td>
<td style="text-align:left">RESTART_LLAP (Check out HIVE metainfo)</td>
</tr>
</tbody>
</table>
<h3 id="service-osSpecifics-操作系统包的名"><a href="#service-osSpecifics-操作系统包的名" class="headerlink" title="service/osSpecifics - 操作系统包的名"></a>service/osSpecifics - 操作系统包的名</h3><table>
<thead>
<tr>
<th style="text-align:left">Field</th>
<th style="text-align:left">What is it used for</th>
<th style="text-align:left">Sample Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>osFamily</strong></td>
<td style="text-align:left">service对应的操作系统</td>
<td style="text-align:left">any =&gt; all， amazon2015、redhat6、debian7</td>
</tr>
<tr>
<td style="text-align:left"><strong>packages</strong></td>
<td style="text-align:left">部署这个service所需的packages列表</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><strong>package/name</strong></td>
<td style="text-align:left">package的名字(会被yum\apt等命令使用)</td>
<td style="text-align:left">如 hadoop-lzo</td>
</tr>
</tbody>
</table>
<h3 id="service-commandScript-service检查的脚本"><a href="#service-commandScript-service检查的脚本" class="headerlink" title="service/commandScript - service检查的脚本"></a>service/commandScript - service检查的脚本</h3><table>
<thead>
<tr>
<th style="text-align:left">Field</th>
<th style="text-align:left">What is it used for</th>
<th style="text-align:left">Sample Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>script</strong></td>
<td style="text-align:left">脚本的相对路径</td>
<td style="text-align:left">scripts/service_check.py</td>
</tr>
<tr>
<td style="text-align:left"><strong>scriptType</strong></td>
<td style="text-align:left">脚本的类型，当前纸支持PYTHON</td>
<td style="text-align:left">PYTHON</td>
</tr>
<tr>
<td style="text-align:left"><strong>timeout</strong></td>
<td style="text-align:left">命令的超时时间</td>
<td style="text-align:left">300</td>
</tr>
</tbody>
</table>
<h3 id="service-component-customCommand-component的自定义命令"><a href="#service-component-customCommand-component的自定义命令" class="headerlink" title="service/component/customCommand - component的自定义命令"></a>service/component/customCommand - component的自定义命令</h3><blockquote>
<p><strong>name:</strong> 自定义命令的名字<br><strong>commandScript:</strong> 实现自定义命令的脚本信息，它包含其他片段。<br><strong>commandScript/script:</strong> 脚本的相对路径<br><strong>commandScript/scriptType:</strong> 脚本的类型，目前只支持PYTHON。<br><strong>commandScript/timeout:</strong> 命令的超时时间。</p>
</blockquote>
<h2 id="Sample-metainfo-xml"><a href="#Sample-metainfo-xml" class="headerlink" title="Sample metainfo.xml"></a>Sample metainfo.xml</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">metainfo</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">schemaVersion</span>&gt;</span>2.0<span class="tag">&lt;/<span class="name">schemaVersion</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">services</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">service</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>HBASE<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">displayName</span>&gt;</span>HBase<span class="tag">&lt;/<span class="name">displayName</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">comment</span>&gt;</span>Non-relational distributed database and centralized service for configuration management &amp;amp;</div><div class="line"> synchronization</div><div class="line">      <span class="tag">&lt;/<span class="name">comment</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.96.0.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">components</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">component</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>HBASE_MASTER<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">displayName</span>&gt;</span>HBase Master<span class="tag">&lt;/<span class="name">displayName</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">category</span>&gt;</span>MASTER<span class="tag">&lt;/<span class="name">category</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">cardinality</span>&gt;</span>1+<span class="tag">&lt;/<span class="name">cardinality</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">versionAdvertised</span>&gt;</span>true<span class="tag">&lt;/<span class="name">versionAdvertised</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">timelineAppid</span>&gt;</span>HBASE<span class="tag">&lt;/<span class="name">timelineAppid</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>HDFS/HDFS_CLIENT<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">scope</span>&gt;</span>host<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">auto-deploy</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></div><div class="line">              <span class="tag">&lt;/<span class="name">auto-deploy</span>&gt;</span></div><div class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>ZOOKEEPER/ZOOKEEPER_SERVER<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">scope</span>&gt;</span>cluster<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">auto-deploy</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">co-locate</span>&gt;</span>HBASE/HBASE_MASTER<span class="tag">&lt;/<span class="name">co-locate</span>&gt;</span></div><div class="line">              <span class="tag">&lt;/<span class="name">auto-deploy</span>&gt;</span></div><div class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">commandScript</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined">scripts/hbase_master.py</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">scriptType</span>&gt;</span>PYTHON<span class="tag">&lt;/<span class="name">scriptType</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">timeout</span>&gt;</span>1200<span class="tag">&lt;/<span class="name">timeout</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">commandScript</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">customCommands</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">customCommand</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>DECOMMISSION<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">commandScript</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined">scripts/hbase_master.py</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">scriptType</span>&gt;</span>PYTHON<span class="tag">&lt;/<span class="name">scriptType</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">timeout</span>&gt;</span>600<span class="tag">&lt;/<span class="name">timeout</span>&gt;</span></div><div class="line">              <span class="tag">&lt;/<span class="name">commandScript</span>&gt;</span></div><div class="line">            <span class="tag">&lt;/<span class="name">customCommand</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">customCommands</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">component</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>HBASE_REGIONSERVER<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">displayName</span>&gt;</span>RegionServer<span class="tag">&lt;/<span class="name">displayName</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">category</span>&gt;</span>SLAVE<span class="tag">&lt;/<span class="name">category</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">cardinality</span>&gt;</span>1+<span class="tag">&lt;/<span class="name">cardinality</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">versionAdvertised</span>&gt;</span>true<span class="tag">&lt;/<span class="name">versionAdvertised</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">timelineAppid</span>&gt;</span>HBASE<span class="tag">&lt;/<span class="name">timelineAppid</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">commandScript</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined">scripts/hbase_regionserver.py</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">scriptType</span>&gt;</span>PYTHON<span class="tag">&lt;/<span class="name">scriptType</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">commandScript</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">component</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>HBASE_CLIENT<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">displayName</span>&gt;</span>HBase Client<span class="tag">&lt;/<span class="name">displayName</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">category</span>&gt;</span>CLIENT<span class="tag">&lt;/<span class="name">category</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">cardinality</span>&gt;</span>1+<span class="tag">&lt;/<span class="name">cardinality</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">versionAdvertised</span>&gt;</span>true<span class="tag">&lt;/<span class="name">versionAdvertised</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">commandScript</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined">scripts/hbase_client.py</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">scriptType</span>&gt;</span>PYTHON<span class="tag">&lt;/<span class="name">scriptType</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">commandScript</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">configFiles</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">configFile</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">type</span>&gt;</span>xml<span class="tag">&lt;/<span class="name">type</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">fileName</span>&gt;</span>hbase-site.xml<span class="tag">&lt;/<span class="name">fileName</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">dictionaryName</span>&gt;</span>hbase-site<span class="tag">&lt;/<span class="name">dictionaryName</span>&gt;</span></div><div class="line">            <span class="tag">&lt;/<span class="name">configFile</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">configFile</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">type</span>&gt;</span>env<span class="tag">&lt;/<span class="name">type</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">fileName</span>&gt;</span>hbase-env.sh<span class="tag">&lt;/<span class="name">fileName</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">dictionaryName</span>&gt;</span>hbase-env<span class="tag">&lt;/<span class="name">dictionaryName</span>&gt;</span></div><div class="line">            <span class="tag">&lt;/<span class="name">configFile</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">configFiles</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">components</span>&gt;</span></div><div class="line"></div><div class="line">      <span class="tag">&lt;<span class="name">osSpecifics</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">osSpecific</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">osFamily</span>&gt;</span>any<span class="tag">&lt;/<span class="name">osFamily</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">packages</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">package</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">            <span class="tag">&lt;/<span class="name">package</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">packages</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">osSpecific</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">osSpecifics</span>&gt;</span></div><div class="line"></div><div class="line">      <span class="tag">&lt;<span class="name">commandScript</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined">scripts/service_check.py</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">scriptType</span>&gt;</span>PYTHON<span class="tag">&lt;/<span class="name">scriptType</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">timeout</span>&gt;</span>300<span class="tag">&lt;/<span class="name">timeout</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">commandScript</span>&gt;</span></div><div class="line">      </div><div class="line">      <span class="tag">&lt;<span class="name">requiredServices</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">service</span>&gt;</span>ZOOKEEPER<span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">service</span>&gt;</span>HDFS<span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">requiredServices</span>&gt;</span></div><div class="line"></div><div class="line">      <span class="tag">&lt;<span class="name">configuration-dependencies</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">config-type</span>&gt;</span>core-site<span class="tag">&lt;/<span class="name">config-type</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">config-type</span>&gt;</span>hbase-site<span class="tag">&lt;/<span class="name">config-type</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">config-type</span>&gt;</span>ranger-hbase-policymgr-ssl<span class="tag">&lt;/<span class="name">config-type</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">config-type</span>&gt;</span>ranger-hbase-security<span class="tag">&lt;/<span class="name">config-type</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">configuration-dependencies</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">services</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">metainfo</span>&gt;</span></div></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/13/ambari/" itemprop="url">
                  Ambari
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-13T10:42:44+08:00" content="2018-08-13">
              2018-08-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">hadoop</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文用来记录Ambari的学习</p>
<h1 id="Ambari的简单介绍"><a href="#Ambari的简单介绍" class="headerlink" title="Ambari的简单介绍"></a>Ambari的简单介绍</h1><p>从Ambari的作用来说，它是用来创建、管理、监控Hadoop生态（例如hadoop、hive、hbase、Sqoop以及Zookeeper）集群的工具。Ambari就是为了让Hadoop已经相关的大数据软件更容易使用的一个工具。Ambari支持的平台组建也越来越多，如流行的Spark、Storm等计算框架，已经资源调度平台YARN等，都可以通过Ambari来轻松部署。</p>
<p>Ambari自称也是一个分布式架构的软件，主要由两部分组成：Ambari Server和Ambari Agent。用户通过Ambari Server来通知Ambari Agent来安装对应的软件；Agent会定时的发送各个机器每个软件模块的状态给Ambari Server，最终这些信息会呈现在Ambari的GUI中，方便用户了解集群中各个模块的状态，并进行维护。</p>
<h1 id="Ambari的架构和工作原理"><a href="#Ambari的架构和工作原理" class="headerlink" title="Ambari的架构和工作原理"></a>Ambari的架构和工作原理</h1><p>Ambari Server会读取Stack和Service的配置文件。当用Ambari创建集群的时候，Ambari Server传送Stack和Service的配置文件配以及Service生命周期的控制脚本到Ambari Agent。Agent拿到配置文件后，会下载安装公共资源里的软件包。安装完成后，Ambari Server会通知Agent去启动Service。之后，Ambari Server会定时发送命令道Agent检查Service的状态，Agent上报给Server并显示在Ambari的UI上。<br>Ambari Server支持其他API，这样能够很容易的扩展或定制Ambari。<br>如果有安全方面的要求，Ambari支持Kerberos认证的hadoop集群。</p>
<blockquote>
<p>Ambari web：用户交互界面，通过HTTP发送使用Rest API与Ambari Server进行交互。<br>Ambari Server：Ambari服务器，用于和Web、Agent进行交互，并且包含了Agent的所有控制逻辑，Server产生的数据存储在DB中。<br>Ambari Agent：守护进程，主要包含节点状态与执行结果信息汇报给Server，以及接受Server操作命令的两个消息队列。<br>Host：安装实际大数据服务组件的物理机器，每台机器都有Ambari Agent和Metrcis Monitor守护进程服务。<br>Metrics Collector：主要包括将Metrics monitor汇报的监控信息存储到Hbase，以及提供给Ambari Server的查询接口。</p>
</blockquote>
<h1 id="Ambari的自定义命令"><a href="#Ambari的自定义命令" class="headerlink" title="Ambari的自定义命令"></a>Ambari的自定义命令</h1><p>在Ambari的Stack中，每个Service都有start、stop、status和configure这样的命令，我们称为生命周期的控制命令。Service的每个模块必须实现这几个命令。为了让用户可以更好的控制每个service以及模块，Ambari支持了自定义命令。<br>具体的自定义命令配置在每个Service的metainfo.xml中。不过不同的模块类型，呈现在GUI的方式是不一样的。当一个service的Master模块增加一个自定义命令时，该命令会显示在该Service的Service Action List中。如果点击这个命令，Ambari Server就会通知Master所在机器的Agent，Agent就会执行该自定义命令的逻辑。当增加一个自定义命令给Slave或Client类型的Component，该命令会呈现在机器的Component页面。在哪个机器的Component页面点击该命令，Ambari Server就会通知对应机器的Agent调用这个自定义的命令接口。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/10/spark-2-3-1-SparkSQL-DataFramesAndDatasetsGuide/" itemprop="url">
                  Spark 2.3.1 Spark SQL DataFrames and DatasetsGuide
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-10T15:21:30+08:00" content="2018-08-10">
              2018-08-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Spark-SQL-DataFrames-and-Dataset-Guide"><a href="#Spark-SQL-DataFrames-and-Dataset-Guide" class="headerlink" title="Spark SQL, DataFrames and Dataset Guide"></a>Spark SQL, DataFrames and Dataset Guide</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Spark SQL是一个用于结构化数据处理的Spark模块。与Spark RDD API不同，由Spark SQL提供的这些接口在结构化数据和结构化计算执行方面提供了更多信息。在内部，Spark SQL使用了这个额外信息来执行额外的优化。有几种与Spark SQL交互的方法，包括SQL和Dataset API。当计算一个结果时，相同的计算引擎会被使用，与你执行计算使用的API／语言无关。这种统一意味着开发者能够轻松在那些提供更加原始的方式处理给定转换的不同API之间进行来回切换。<br>本篇中所有例子使用的样例数据包含在Spark中，并能够使用spark-shell、pyspark shell或sparkR shell来运行。</p>
<h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p>Spark SQL的一种用法时执行SQL查询。Spark SQL还能够被用来从Hive实例中读取数据。关于如何配置这个特性，请参考<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables" title="Hive Tables " target="_blank" rel="external">Hive Tables</a>。当在另一种编程语言中执行SQL时，结果会作为一个Dataset/DataFrame来返回。你还能够使用<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-spark-sql-cli" title="command-line" target="_blank" rel="external">command-line</a>或<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server" title="JDBC/ODBC" target="_blank" rel="external">JDBC/ODBC</a>的方式与SQL接口进行交互。</p>
<h3 id="Datasets-and-DataFrames"><a href="#Datasets-and-DataFrames" class="headerlink" title="Datasets and DataFrames"></a>Datasets and DataFrames</h3><p>一个Dataset就是一个分布式数据集。Dataset作为一个新接口在Spark 1.6中被添加，它提供了RDD的优点（强类型、能够使用强大的lambda函数）和Spark SQL的优化执行引擎的有点。一个Dataset能够根据JVM对象来构造，然后使用函数转换（map、flatMap、filter）进行操作。Dataset的API在Scala和Java中时可用的。Python还不支持Dataset API。但是因为Python的动态特性，Dataset API的很多优点已经可用了（例如你可以很自然的通过名称来访问某一行的一个字段 row.columnName）。对于R语言也是如此。<br>一个DataFrame是一个带有列名的数据集。它在概念上等同于关系数据库中的一个表或者一个是R语言或Python语言中data frame，但是底层具更加优化。DataFrame可以根据各种资源进行构建，例如：结构化的数据文件、Hive中的表、外部数据库以及已经存在的RDD。DataFrame API在Scala、Java、Python和R语言中都可用。在Scala和Java中，一个DataFrame相当于一个有很多行的Dataset。在<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" title="Scala API" target="_blank" rel="external">Scala API</a>中，DataFrame相当于一个Dataset[Row]类型。而在<a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html" title="Java API" target="_blank" rel="external">Java API</a>中，用户需要使用Dataset<row>来表述一个DataFrame。<br>在本文中，我们将经常引用Scala/Java由有Row组成的Dataset来表述DataFrame。</row></p>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><h3 id="Starting-Point-SparkSession"><a href="#Starting-Point-SparkSession" class="headerlink" title="Starting Point: SparkSession"></a>Starting Point: SparkSession</h3><p>Spark中，所有功能的切入点是<a href="http://spark.apache.org/docs/latest/api/java/index.html#org.apache.spark.sql.SparkSession" title="SparkSession" target="_blank" rel="external">SparkSession</a>类。要创建一个基本的SparkSession，只需要使用SparkSession.builder()<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"></div><div class="line">SparkSession spark = SparkSession</div><div class="line">  .builder()</div><div class="line">  .appName(<span class="string">"Java Spark SQL basic example"</span>)</div><div class="line">  .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</div><div class="line">  .getOrCreate();</div></pre></td></tr></table></figure></p>
<p>在Spark库的“examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java”目录下，查看完整的示例代码。<br>SparkSession是Spark 2.0的内置功能，用于提供Hive特性，包括用来写HiveQL查询、<br>访问Hive UDFs已经从Hive表中读取数据。要使用这些特性，你不需要配置Hive。</p>
<h3 id="Creating-DataFrames"><a href="#Creating-DataFrames" class="headerlink" title="Creating DataFrames"></a>Creating DataFrames</h3><p>使用SparkSession，application能够从一个已经存在的RDD、一个Hive表或<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources" title="Spark data sources" target="_blank" rel="external">Spark data sources</a>来创建DataFrame。<br>作为一个例子，下面的代码根据一个JSON文件中的内容来创建一个DataFrame：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></div><div class="line">df.show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看Spark库中的“examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java”。</p>
<h3 id="Untyped-Dataset-Operations-aka-DataFrame-Operations"><a href="#Untyped-Dataset-Operations-aka-DataFrame-Operations" class="headerlink" title="Untyped Dataset Operations(aka DataFrame Operations)"></a>Untyped Dataset Operations(aka DataFrame Operations)</h3><p>在Scala、Java、Python和R语言中，DataFrames针对不同的语言提供不同的结构化数据操作。正如上面提到的，在Spark2.0中，在Scala和Java的API中，DataFrames是以Dataset<row>来表述的。这些操作也被称为“无类型转换”，与强类型转换的Scala/Java Dataset的类型形成对比。<br>这里，我们展示了使用Dataset进行结构化数据处理的基本示例：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// col("...") is preferable to df.col("...")</span></div><div class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.spark.sql.functions.col;</div><div class="line"></div><div class="line"><span class="comment">// Print the schema in a tree format</span></div><div class="line">df.printSchema();</div><div class="line"><span class="comment">// root</span></div><div class="line"><span class="comment">// |-- age: long (nullable = true)</span></div><div class="line"><span class="comment">// |-- name: string (nullable = true)</span></div><div class="line"></div><div class="line"><span class="comment">// Select only the "name" column</span></div><div class="line">df.select(<span class="string">"name"</span>).show();</div><div class="line"><span class="comment">// +-------+</span></div><div class="line"><span class="comment">// |   name|</span></div><div class="line"><span class="comment">// +-------+</span></div><div class="line"><span class="comment">// |Michael|</span></div><div class="line"><span class="comment">// |   Andy|</span></div><div class="line"><span class="comment">// | Justin|</span></div><div class="line"><span class="comment">// +-------+</span></div><div class="line"></div><div class="line"><span class="comment">// Select everybody, but increment the age by 1</span></div><div class="line">df.select(col(<span class="string">"name"</span>), col(<span class="string">"age"</span>).plus(<span class="number">1</span>)).show();</div><div class="line"><span class="comment">// +-------+---------+</span></div><div class="line"><span class="comment">// |   name|(age + 1)|</span></div><div class="line"><span class="comment">// +-------+---------+</span></div><div class="line"><span class="comment">// |Michael|     null|</span></div><div class="line"><span class="comment">// |   Andy|       31|</span></div><div class="line"><span class="comment">// | Justin|       20|</span></div><div class="line"><span class="comment">// +-------+---------+</span></div><div class="line"></div><div class="line"><span class="comment">// Select people older than 21</span></div><div class="line">df.filter(col(<span class="string">"age"</span>).gt(<span class="number">21</span>)).show();</div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// |age|name|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// | 30|Andy|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"></div><div class="line"><span class="comment">// Count people by age</span></div><div class="line">df.groupBy(<span class="string">"age"</span>).count().show();</div><div class="line"><span class="comment">// +----+-----+</span></div><div class="line"><span class="comment">// | age|count|</span></div><div class="line"><span class="comment">// +----+-----+</span></div><div class="line"><span class="comment">// |  19|    1|</span></div><div class="line"><span class="comment">// |null|    1|</span></div><div class="line"><span class="comment">// |  30|    1|</span></div><div class="line"><span class="comment">// +----+-----+</span></div></pre></td></tr></table></figure></row></p>
<p>完整的样例代码，查看Spark库的examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java。<br>在Dataset上能够执行的操作类型列表，可以查看<a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html" title="API Document" target="_blank" rel="external">API Document</a>。<br>除了简单的列引用和计算外，Dataset还有一个丰富的函数库，包括字符串的操作、日期的计算以及常用的数学操作等。完整的列表可以在<a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html" title="DataFrame Function Reference" target="_blank" rel="external">DataFrame Function Reference</a>找到。</p>
<h3 id="Running-SQL-Queries-Programmatically"><a href="#Running-SQL-Queries-Programmatically" class="headerlink" title="Running SQL Queries Programmatically"></a>Running SQL Queries Programmatically</h3><p>SparkSession上的sql函数使application能够执行SQL查询，并返回一个Dataset<row>作为结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">import org.apache.spark.sql.Dataset;</div><div class="line">import org.apache.spark.sql.Row;</div><div class="line"></div><div class="line">// Register the DataFrame as a SQL temporary view</div><div class="line">df.createOrReplaceTempView(&quot;people&quot;);</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(&quot;SELECT * FROM people&quot;);</div><div class="line">sqlDF.show();</div><div class="line">// +----+-------+</div><div class="line">// | age|   name|</div><div class="line">// +----+-------+</div><div class="line">// |null|Michael|</div><div class="line">// |  30|   Andy|</div><div class="line">// |  19| Justin|</div><div class="line">// +----+-------+</div></pre></td></tr></table></figure></row></p>
<p>完整的代码，请查看Spark库中的 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h3 id="Global-Temporary-View"><a href="#Global-Temporary-View" class="headerlink" title="Global Temporary View"></a>Global Temporary View</h3><p>在Spark SQL中，临时视图是session范围的，将会伴随着创建它的那个session的终止而消失。如果你想要跨session共享一个临时视图，并让它存活到application终止，你可以创建一个全局临时视图。全局视图与一个名为‘global_temp’的由系统保护的数据库进行绑定，我们必须使用这个特殊的名字来引用它，如：SELECT * FROM global_temp.view1。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Register the DataFrame as a global temporary view</span></div><div class="line">df.createGlobalTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Global temporary view is tied to a system preserved database `global_temp`</span></div><div class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"></div><div class="line"><span class="comment">// Global temporary view is cross-session</span></div><div class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看“examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java”。</p>
<h3 id="Creating-Datasets"><a href="#Creating-Datasets" class="headerlink" title="Creating Datasets"></a>Creating Datasets</h3><p>Dataset与RDD类似，不同的是它没有使用Java序列化或Kryo，它们使用了一个特殊的<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder" title="Encoder" target="_blank" rel="external">Encoder</a>来序列化对象，以便这些对象的处理或跨网络传输。虽然encoder和标准序列化器都能够将一个对象转换为字节，encoder是动态编码产生的，并且使用一种格式来允许Spark执行很多操作(filtering， sorting 和 hashing)，而不需要讲字节反编译为对象。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.Arrays;</div><div class="line"><span class="keyword">import</span> java.util.Collections;</div><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> String name;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> age;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> name;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.name = name;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> age;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.age = age;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Create an instance of a Bean class</span></div><div class="line">Person person = <span class="keyword">new</span> Person();</div><div class="line">person.setName(<span class="string">"Andy"</span>);</div><div class="line">person.setAge(<span class="number">32</span>);</div><div class="line"></div><div class="line"><span class="comment">// Encoders are created for Java beans</span></div><div class="line">Encoder&lt;Person&gt; personEncoder = Encoders.bean(Person.class);</div><div class="line">Dataset&lt;Person&gt; javaBeanDS = spark.createDataset(</div><div class="line">  Collections.singletonList(person),</div><div class="line">  personEncoder</div><div class="line">);</div><div class="line">javaBeanDS.show();</div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// |age|name|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// | 32|Andy|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"></div><div class="line"><span class="comment">// Encoders for most common types are provided in class Encoders</span></div><div class="line">Encoder&lt;Integer&gt; integerEncoder = Encoders.INT();</div><div class="line">Dataset&lt;Integer&gt; primitiveDS = spark.createDataset(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), integerEncoder);</div><div class="line">Dataset&lt;Integer&gt; transformedDS = primitiveDS.map(</div><div class="line">    (MapFunction&lt;Integer, Integer&gt;) value -&gt; value + <span class="number">1</span>,</div><div class="line">    integerEncoder);</div><div class="line">transformedDS.collect(); <span class="comment">// Returns [2, 3, 4]</span></div><div class="line"></div><div class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping based on name</span></div><div class="line">String path = <span class="string">"examples/src/main/resources/people.json"</span>;</div><div class="line">Dataset&lt;Person&gt; peopleDS = spark.read().json(path).as(personEncoder);</div><div class="line">peopleDS.show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div></pre></td></tr></table></figure></p>
<p>完整的示例，请查看 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h3 id="Interoperating-with-RDDs"><a href="#Interoperating-with-RDDs" class="headerlink" title="Interoperating with RDDs"></a>Interoperating with RDDs</h3><p>Spark SQL支持两种不同方法来将存在的RDD转换为Dataset。第一种方法是使用反射来推导包含特殊类型对象的RDD的模式。这种反射的方法代码更加简单，而且如果在你写Spark application时已经知道了模式时，工作的会很好。<br>第二种方法是通过一个程序接口来创建Dataset，这个程序接口允许你构建一个模式，并且将它应用到一个已经存在的RDD上。但是这个方法比较冗长，它允许你只有在运行时才知道列和列类型时来构造Dataset。</p>
<h4 id="Inferring-the-Schema-Using-Reflection"><a href="#Inferring-the-Schema-Using-Reflection" class="headerlink" title="Inferring the Schema Using Reflection"></a>Inferring the Schema Using Reflection</h4><p>Spark SQL支持自动将一个JavaBean的RDD转换为一个DataFrame。BeanInfo使用反射机制获得，定义了表的模式。当前，Spark SQL不支持那些包含了Map类型字段的JavaBean，但是对于嵌套的JavaBean以及嵌套了List或Array类型的字段给予了充分的支持。你可以通过创建一个实现了Serializable接口以及为所有字段生成getter和setter方法的类来创建一个JavaBean。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"></div><div class="line"><span class="comment">// Create an RDD of Person objects from a text file</span></div><div class="line">JavaRDD&lt;Person&gt; peopleRDD = spark.read()</div><div class="line">  .textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</div><div class="line">  .javaRDD()</div><div class="line">  .map(line -&gt; &#123;</div><div class="line">    String[] parts = line.split(<span class="string">","</span>);</div><div class="line">    Person person = <span class="keyword">new</span> Person();</div><div class="line">    person.setName(parts[<span class="number">0</span>]);</div><div class="line">    person.setAge(Integer.parseInt(parts[<span class="number">1</span>].trim()));</div><div class="line">    <span class="keyword">return</span> person;</div><div class="line">  &#125;);</div><div class="line"></div><div class="line"><span class="comment">// Apply a schema to an RDD of JavaBeans to get a DataFrame</span></div><div class="line">Dataset&lt;Row&gt; peopleDF = spark.createDataFrame(peopleRDD, Person.class);</div><div class="line"><span class="comment">// Register the DataFrame as a temporary view</span></div><div class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></div><div class="line">Dataset&lt;Row&gt; teenagersDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The columns of a row in the result can be accessed by field index</span></div><div class="line">Encoder&lt;String&gt; stringEncoder = Encoders.STRING();</div><div class="line">Dataset&lt;String&gt; teenagerNamesByIndexDF = teenagersDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</div><div class="line">    stringEncoder);</div><div class="line">teenagerNamesByIndexDF.show();</div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |       value|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |Name: Justin|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"></div><div class="line"><span class="comment">// or by field name</span></div><div class="line">Dataset&lt;String&gt; teenagerNamesByFieldDF = teenagersDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.&lt;String&gt;getAs(<span class="string">"name"</span>),</div><div class="line">    stringEncoder);</div><div class="line">teenagerNamesByFieldDF.show();</div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |       value|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |Name: Justin|</span></div><div class="line"><span class="comment">// +------------+</span></div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h4 id="Programmatically-Specifying-the-Schema"><a href="#Programmatically-Specifying-the-Schema" class="headerlink" title="Programmatically Specifying the Schema"></a>Programmatically Specifying the Schema</h4><p>当JavaBean无法提前定义时（例如，记录的结构被编码为一个字符串，或者一个文本数据集将被解析，但是其中的字段可能根据不同的用户而不一样），Dataset<row>能够通过三个步骤来创建。</row></p>
<blockquote>
<p>1、根据原生的RDD创建一个RDD<row>。<br>2、创建一个与第一步骤RDD中Row结构匹配的StructType来描述的模式。<br>3、通过由SparkSession提供的createDataFrame方法，将这个模式应用到RDD<row>。</row></row></p>
</blockquote>
<p>例如：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</div><div class="line"></div><div class="line"><span class="comment">// Create an RDD</span></div><div class="line">JavaRDD&lt;String&gt; peopleRDD = spark.sparkContext()</div><div class="line">  .textFile(<span class="string">"examples/src/main/resources/people.txt"</span>, <span class="number">1</span>)</div><div class="line">  .toJavaRDD();</div><div class="line"></div><div class="line"><span class="comment">// The schema is encoded in a string</span></div><div class="line">String schemaString = <span class="string">"name age"</span>;</div><div class="line"></div><div class="line"><span class="comment">// Generate the schema based on the string of schema</span></div><div class="line">List&lt;StructField&gt; fields = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (String fieldName : schemaString.split(<span class="string">" "</span>)) &#123;</div><div class="line">  StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, <span class="keyword">true</span>);</div><div class="line">  fields.add(field);</div><div class="line">&#125;</div><div class="line">StructType schema = DataTypes.createStructType(fields);</div><div class="line"></div><div class="line"><span class="comment">// Convert records of the RDD (people) to Rows</span></div><div class="line">JavaRDD&lt;Row&gt; rowRDD = peopleRDD.map((Function&lt;String, Row&gt;) record -&gt; &#123;</div><div class="line">  String[] attributes = record.split(<span class="string">","</span>);</div><div class="line">  <span class="keyword">return</span> RowFactory.create(attributes[<span class="number">0</span>], attributes[<span class="number">1</span>].trim());</div><div class="line">&#125;);</div><div class="line"></div><div class="line"><span class="comment">// Apply the schema to the RDD</span></div><div class="line">Dataset&lt;Row&gt; peopleDataFrame = spark.createDataFrame(rowRDD, schema);</div><div class="line"></div><div class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></div><div class="line">peopleDataFrame.createOrReplaceTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// SQL can be run over a temporary view created using DataFrames</span></div><div class="line">Dataset&lt;Row&gt; results = spark.sql(<span class="string">"SELECT name FROM people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations</span></div><div class="line"><span class="comment">// The columns of a row in the result can be accessed by field index or by field name</span></div><div class="line">Dataset&lt;String&gt; namesDS = results.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</div><div class="line">    Encoders.STRING());</div><div class="line">namesDS.show();</div><div class="line"><span class="comment">// +-------------+</span></div><div class="line"><span class="comment">// |        value|</span></div><div class="line"><span class="comment">// +-------------+</span></div><div class="line"><span class="comment">// |Name: Michael|</span></div><div class="line"><span class="comment">// |   Name: Andy|</span></div><div class="line"><span class="comment">// | Name: Justin|</span></div><div class="line"><span class="comment">// +-------------+</span></div></pre></td></tr></table></figure></p>
<p>完整的示例，请查看 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h3 id="Aggregations"><a href="#Aggregations" class="headerlink" title="Aggregations"></a>Aggregations</h3><p>内置的DataFrame函数提供了常用的聚合操作，如count()、countDistinct()、avg()、max()、min()等。然而这些函数是为了DataFrame设计的，Spark SQL同样由类型安全的版本，以便其中一些被用到Scala和Java的强类型Dataset。此外，Spark没有限制用户预定义聚合函数，可以自己来创建聚合函数。</p>
<h4 id="Untyped-User-Defined-Aggregate-Functions"><a href="#Untyped-User-Defined-Aggregate-Functions" class="headerlink" title="Untyped User-Defined Aggregate Functions"></a>Untyped User-Defined Aggregate Functions</h4><p>用户要实现无类型聚合函数，则需要继承<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.UserDefinedAggregateFunction" title="UserDefinedAggregateFunction" target="_blank" rel="external">UserDefinedAggregateFunction</a>抽象类。例如，你一个用户自定义的平均数函数，看起来像这样：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.MutableAggregationBuffer;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.UserDefinedAggregateFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataType;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">private</span> StructType inputSchema;</div><div class="line">  <span class="keyword">private</span> StructType bufferSchema;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="title">MyAverage</span><span class="params">()</span> </span>&#123;</div><div class="line">    List&lt;StructField&gt; inputFields = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">    inputFields.add(DataTypes.createStructField(<span class="string">"inputColumn"</span>, DataTypes.LongType, <span class="keyword">true</span>));</div><div class="line">    inputSchema = DataTypes.createStructType(inputFields);</div><div class="line"></div><div class="line">    List&lt;StructField&gt; bufferFields = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">    bufferFields.add(DataTypes.createStructField(<span class="string">"sum"</span>, DataTypes.LongType, <span class="keyword">true</span>));</div><div class="line">    bufferFields.add(DataTypes.createStructField(<span class="string">"count"</span>, DataTypes.LongType, <span class="keyword">true</span>));</div><div class="line">    bufferSchema = DataTypes.createStructType(bufferFields);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Data types of input arguments of this aggregate function</span></div><div class="line">  <span class="function"><span class="keyword">public</span> StructType <span class="title">inputSchema</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> inputSchema;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Data types of values in the aggregation buffer</span></div><div class="line">  <span class="function"><span class="keyword">public</span> StructType <span class="title">bufferSchema</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> bufferSchema;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// The data type of the returned value</span></div><div class="line">  <span class="function"><span class="keyword">public</span> DataType <span class="title">dataType</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> DataTypes.DoubleType;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Whether this function always returns the same output on the identical input</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">deterministic</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to</span></div><div class="line">  <span class="comment">// standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides</span></div><div class="line">  <span class="comment">// the opportunity to update its values. Note that arrays and maps inside the buffer are still</span></div><div class="line">  <span class="comment">// immutable.</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(MutableAggregationBuffer buffer)</span> </span>&#123;</div><div class="line">    buffer.update(<span class="number">0</span>, <span class="number">0L</span>);</div><div class="line">    buffer.update(<span class="number">1</span>, <span class="number">0L</span>);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Updates the given aggregation buffer `buffer` with new input data from `input`</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">(MutableAggregationBuffer buffer, Row input)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</div><div class="line">      <span class="keyword">long</span> updatedSum = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>);</div><div class="line">      <span class="keyword">long</span> updatedCount = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span>;</div><div class="line">      buffer.update(<span class="number">0</span>, updatedSum);</div><div class="line">      buffer.update(<span class="number">1</span>, updatedCount);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Merges two aggregation buffers and stores the updated buffer values back to `buffer1`</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(MutableAggregationBuffer buffer1, Row buffer2)</span> </span>&#123;</div><div class="line">    <span class="keyword">long</span> mergedSum = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>);</div><div class="line">    <span class="keyword">long</span> mergedCount = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>);</div><div class="line">    buffer1.update(<span class="number">0</span>, mergedSum);</div><div class="line">    buffer1.update(<span class="number">1</span>, mergedCount);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Calculates the final result</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">evaluate</span><span class="params">(Row buffer)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) buffer.getLong(<span class="number">0</span>)) / buffer.getLong(<span class="number">1</span>);</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Register the function to access it</span></div><div class="line">spark.udf().register(<span class="string">"myAverage"</span>, <span class="keyword">new</span> MyAverage());</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">"examples/src/main/resources/employees.json"</span>);</div><div class="line">df.createOrReplaceTempView(<span class="string">"employees"</span>);</div><div class="line">df.show();</div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |   name|salary|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |Michael|  3000|</span></div><div class="line"><span class="comment">// |   Andy|  4500|</span></div><div class="line"><span class="comment">// | Justin|  3500|</span></div><div class="line"><span class="comment">// |  Berta|  4000|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"></div><div class="line">Dataset&lt;Row&gt; result = spark.sql(<span class="string">"SELECT myAverage(salary) as average_salary FROM employees"</span>);</div><div class="line">result.show();</div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |average_salary|</span></div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |        3750.0|</span></div><div class="line"><span class="comment">// +--------------+</span></div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考 examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java 。</p>
<h4 id="Type-Safe-User-Defined-Aggregate-Functions"><a href="#Type-Safe-User-Defined-Aggregate-Functions" class="headerlink" title="Type-Safe User-Defined Aggregate Functions"></a>Type-Safe User-Defined Aggregate Functions</h4><p>强类型Dataset的用户自定义聚合围绕着<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator ‘Aggregator’" target="_blank" rel="external">Aggregator</a>抽象类来解决。例如，一个类型安全的用户自定义平均数看起来是这样：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.TypedColumn;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.Aggregator;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> String name;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">long</span> salary;</div><div class="line"></div><div class="line">  <span class="comment">// Constructors, getters, setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Average</span> <span class="keyword">implements</span> <span class="title">Serializable</span>  </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">long</span> sum;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">long</span> count;</div><div class="line"></div><div class="line">  <span class="comment">// Constructors, getters, setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>&lt;<span class="title">Employee</span>, <span class="title">Average</span>, <span class="title">Double</span>&gt; </span>&#123;</div><div class="line">  <span class="comment">// A zero value for this aggregation. Should satisfy the property that any b + zero = b</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">zero</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Average(<span class="number">0L</span>, <span class="number">0L</span>);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span></div><div class="line">  <span class="comment">// and return it instead of constructing a new object</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">reduce</span><span class="params">(Average buffer, Employee employee)</span> </span>&#123;</div><div class="line">    <span class="keyword">long</span> newSum = buffer.getSum() + employee.getSalary();</div><div class="line">    <span class="keyword">long</span> newCount = buffer.getCount() + <span class="number">1</span>;</div><div class="line">    buffer.setSum(newSum);</div><div class="line">    buffer.setCount(newCount);</div><div class="line">    <span class="keyword">return</span> buffer;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Merge two intermediate values</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">merge</span><span class="params">(Average b1, Average b2)</span> </span>&#123;</div><div class="line">    <span class="keyword">long</span> mergedSum = b1.getSum() + b2.getSum();</div><div class="line">    <span class="keyword">long</span> mergedCount = b1.getCount() + b2.getCount();</div><div class="line">    b1.setSum(mergedSum);</div><div class="line">    b1.setCount(mergedCount);</div><div class="line">    <span class="keyword">return</span> b1;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Transform the output of the reduction</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">finish</span><span class="params">(Average reduction)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) reduction.getSum()) / reduction.getCount();</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Specifies the Encoder for the intermediate value type</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Encoder&lt;Average&gt; <span class="title">bufferEncoder</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> Encoders.bean(Average.class);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Specifies the Encoder for the final output value type</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Encoder&lt;Double&gt; <span class="title">outputEncoder</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> Encoders.DOUBLE();</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">Encoder&lt;Employee&gt; employeeEncoder = Encoders.bean(Employee.class);</div><div class="line">String path = <span class="string">"examples/src/main/resources/employees.json"</span>;</div><div class="line">Dataset&lt;Employee&gt; ds = spark.read().json(path).as(employeeEncoder);</div><div class="line">ds.show();</div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |   name|salary|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |Michael|  3000|</span></div><div class="line"><span class="comment">// |   Andy|  4500|</span></div><div class="line"><span class="comment">// | Justin|  3500|</span></div><div class="line"><span class="comment">// |  Berta|  4000|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"></div><div class="line">MyAverage myAverage = <span class="keyword">new</span> MyAverage();</div><div class="line"><span class="comment">// Convert the function to a `TypedColumn` and give it a name</span></div><div class="line">TypedColumn&lt;Employee, Double&gt; averageSalary = myAverage.toColumn().name(<span class="string">"average_salary"</span>);</div><div class="line">Dataset&lt;Double&gt; result = ds.select(averageSalary);</div><div class="line">result.show();</div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |average_salary|</span></div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |        3750.0|</span></div><div class="line"><span class="comment">// +--------------+</span></div></pre></td></tr></table></figure></p>
<p>完整的示例，请看 examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java 。</p>
<h2 id="Data-Sources"><a href="#Data-Sources" class="headerlink" title="Data Sources"></a>Data Sources</h2><p>Spark SQL通过DataFrame接口支持多种数据源的操作。DataFrame能够使用关系转换进行操作，也可以被用来创建一个临时视图。将DataFrame注册为一个临时视图，将允许你在视图的数据上运行SQL查询。这一章节描述了使用Spark Data Sources加载和保存数据的一般方法，然后介绍内置数据源可用的详细参数。</p>
<h3 id="Generic-Load-Save-Functions"><a href="#Generic-Load-Save-Functions" class="headerlink" title="Generic Load/Save Functions"></a>Generic Load/Save Functions</h3><p>最简单的格式，默认数据源（默认是parquet， 除非通过spark.sql.soiurces.default配置修改过）将被用于所有操作。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; usersDF = spark.read().load(<span class="string">"examples/src/main/resources/users.parquet"</span>);</div><div class="line">usersDF.select(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>).write().save(<span class="string">"namesAndFavColors.parquet"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考 examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java 。</p>
<h4 id="Manually-Specifying-Options"><a href="#Manually-Specifying-Options" class="headerlink" title="Manually Specifying Options"></a>Manually Specifying Options</h4><p>你还可以手动指定想要使用的数据源，以及传递给数据源任何额外的参数。数据源可以通过它的完整限定名（如：org.apache.spark.sql.parquet）来指定，但是对于内置的数据源，你也能够使用它的短名字（json、parquet、jdbc、orc、libsvm、csv、text）。从任何类型数据源加载的DataFrames，通过使用这个语句都可以转为其他类型。<br>要加载一个JSON文件，你可以使用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; peopleDF = spark.read().format(<span class="string">"json"</span>).load(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line">peopleDF.select(<span class="string">"name"</span>, <span class="string">"age"</span>).write().format(<span class="string">"parquet"</span>).save(<span class="string">"namesAndAges.parquet"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考：xamples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>要加载一个CSV文件，你可以使用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; peopleDFCsv = spark.read().format(<span class="string">"csv"</span>)</div><div class="line">  .option(<span class="string">"sep"</span>, <span class="string">";"</span>)</div><div class="line">  .option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)</div><div class="line">  .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</div><div class="line">  .load(<span class="string">"examples/src/main/resources/people.csv"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考：xamples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Run-SQL-on-files-directly"><a href="#Run-SQL-on-files-directly" class="headerlink" title="Run SQL on files directly"></a>Run SQL on files directly</h4><p>除了使用read API加载文件到DataFrame然后查询它之外，你还可以使用SQL直接查询那个文件。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; sqlDF =</div><div class="line">  spark.sql(<span class="string">"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考：xamples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Save-Modes"><a href="#Save-Modes" class="headerlink" title="Save Modes"></a>Save Modes</h4><p>保存操作可以选择一种SaveMode，它指定了如何处理存在的数据。一件非常重要的事情是这些保存模式没有利用任何锁，并且它们不是原子操作。另外，当执行Overwrite模式时，已有的数据将会在写出新数据之前被删掉。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Scala/Java</th>
<th style="text-align:left">Any Language</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">SaveMode.ErrorIfExists(default)</td>
<td style="text-align:left">“error” or “errorifexists” (default)</td>
<td style="text-align:left">当保存一个DataFrame到一个数据源时，如果数据已经存在，预计将抛出一个异常</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Append</td>
<td style="text-align:left">“append”</td>
<td style="text-align:left">当保存一个DataFrame到一个数据源时，如果数据或表格已经存在，DataFrame的内容将被追加到已存在数据</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Overwrite</td>
<td style="text-align:left">“overwrite”</td>
<td style="text-align:left">Overwrite模式意味着，当保存一个DataFrame到一个数据源时，如果数据或表格已经存在，已存在的数据将会被DataFrame的内容所覆盖</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Ignore</td>
<td style="text-align:left">“ignore”</td>
<td style="text-align:left">Ignore模式意味着当保存一个DataFrame到一个数据源时，如果数据已经存在，保存操作将不会保存DataFrame的内容，并且不会修改已经存在的数据。这个操作类似 CREATE TABLE IF NOT EXISTS</td>
</tr>
</tbody>
</table>
<h4 id="Saving-to-Persistent-Tables"><a href="#Saving-to-Persistent-Tables" class="headerlink" title="Saving to Persistent Tables"></a>Saving to Persistent Tables</h4><p>使用saveAsTable命令，DataFrames也可以作为持久化表被保存到Hive metastore中。注意，使用这个功能不需要现有Hive的部署。Spark将会为你创建一个默认的本地Hive metastore(使用Derby)。与createOrReplaceTempView命令不同，saveAsTable将显示DataFrame的内容并创建一个指向Hive metastore中数据的指针。持久化表将在你的Spark程序重启之后持续存在，只要你维持你的连接在相同的metastore。通过在SparkSession上调用table方法（并传递表的名字），就能根据持久化表创建对应的DataFrame。<br>对于基于文件的数据源，如：text、parquet、json等。通过path选项，你可以指定一个自定义表路径，如:df.write.option(“path”, “/some/path”).saveAsTable(“t”)。当这个表被删除，自定义表路径将不会被移除，并且表数据依然存在。如果没有指定自定义表路径，Spark将会把数据写到仓库目录下的默认表路径。当这个表被删除时，默认表路径也会一并被删除。<br>从Spark2.1开始，持久化数据源表格在Hive metastore中有独立的元数据。这样做又一些优点：</p>
<blockquote>
<p>因为metastore只返回查询所需的partition，因此表上的首次查询就不需要查找所有的aprtition。<br>Hive DDL（如ALTER TABLE PARTITION … SET LOCATION），对于使用Datasource APi来创建表都是可用的。</p>
</blockquote>
<p>注意，当创建外部数据源表时（那些带有path选项的），分区信息默认是不会被收集的。要同步分区信息到metastore中，你可以执行MSCK REPAIR TABLE。</p>
<h4 id="Bucketing-Sorting-and-Partitioning"><a href="#Bucketing-Sorting-and-Partitioning" class="headerlink" title="Bucketing, Sorting and Partitioning"></a>Bucketing, Sorting and Partitioning</h4><p>对于基于文件的数据源，还可以对输出进行分组并排序或分组并分区。分组并排序只对持久化表适用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">peopleDF.write().bucketBy(<span class="number">42</span>, <span class="string">"name"</span>).sortBy(<span class="string">"age"</span>).saveAsTable(<span class="string">"people_bucketed"</span>);</div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>当使用Dataset API时，partitioning能够和save以及saveAsTable一起使用。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">usersDF</div><div class="line">  .write()</div><div class="line">  .partitionBy(<span class="string">"favorite_color"</span>)</div><div class="line">  .format(<span class="string">"parquet"</span>)</div><div class="line">  .save(<span class="string">"namesPartByColor.parquet"</span>);</div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>可以对单个表使用partitioning和bucketing：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">peopleDF</div><div class="line">  .write()</div><div class="line">  .partitionBy(<span class="string">"favorite_color"</span>)</div><div class="line">  .bucketBy(<span class="number">42</span>, <span class="string">"name"</span>)</div><div class="line">  .saveAsTable(<span class="string">"people_partitioned_bucketed"</span>);</div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>partitionBy创建了一个在<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#partition-discovery" title="Partition Discovery" target="_blank" rel="external">Partition Discovery</a>章节中描述的目录结构。因此，它对具有高基数的列的适用性有限。相比之下，BucketBy会跨固定数量的bucket来分布部署数据，and can be used when a number of unique values is unbounded.（！！！无法理解）</p>
<h3 id="Parquet-Files"><a href="#Parquet-Files" class="headerlink" title="Parquet Files"></a>Parquet Files</h3><p>Parquet时一种列式文件格式，它被很多其他数据处理系统所支持。Spark SQL对Parquet文件提供了读写支持，并能够自动保护原始数据的模式。当写Parquet文件时，为了兼容的原因，所有列被自动转换为nullable。</p>
<h4 id="Loading-Data-Programmatically"><a href="#Loading-Data-Programmatically" class="headerlink" title="Loading Data Programmatically"></a>Loading Data Programmatically</h4><p>使用上面例子中的数据：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; peopleDF = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line"></div><div class="line"><span class="comment">// DataFrames can be saved as Parquet files, maintaining the schema information</span></div><div class="line">peopleDF.write().parquet(<span class="string">"people.parquet"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Read in the Parquet file created above.</span></div><div class="line"><span class="comment">// Parquet files are self-describing so the schema is preserved</span></div><div class="line"><span class="comment">// The result of loading a parquet file is also a DataFrame</span></div><div class="line">Dataset&lt;Row&gt; parquetFileDF = spark.read().parquet(<span class="string">"people.parquet"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Parquet files can also be used to create a temporary view and then used in SQL statements</span></div><div class="line">parquetFileDF.createOrReplaceTempView(<span class="string">"parquetFile"</span>);</div><div class="line">Dataset&lt;Row&gt; namesDF = spark.sql(<span class="string">"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19"</span>);</div><div class="line">Dataset&lt;String&gt; namesDS = namesDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</div><div class="line">    Encoders.STRING());</div><div class="line">namesDS.show();</div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |       value|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |Name: Justin|</span></div><div class="line"><span class="comment">// +------------+</span></div></pre></td></tr></table></figure></p>
<p>完整示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Partition-Discovery"><a href="#Partition-Discovery" class="headerlink" title="Partition Discovery"></a>Partition Discovery</h4><p>在像Hive这样的系统中，常用的优化方法时进行表分区。在分区表中，数据通常存储在不同的目录中，根据分区列的值，编码到每个分区目录的路径中。所有内置文件源（包括Text/CSV/JSON/ORC/Parquet）都能够自动发现并推断分区信息。例如，我们能够将我们之前使用的数据存储到如下目录结构的分区表中，这个分区表使用两个额外的字段gender和country来作为分区字段：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">path</div><div class="line">└── to</div><div class="line">    └── table</div><div class="line">        ├── gender=male</div><div class="line">        │   ├── ...</div><div class="line">        │   │</div><div class="line">        │   ├── country=US</div><div class="line">        │   │   └── data.parquet</div><div class="line">        │   ├── country=CN</div><div class="line">        │   │   └── data.parquet</div><div class="line">        │   └── ...</div><div class="line">        └── gender=female</div><div class="line">            ├── ...</div><div class="line">            │</div><div class="line">            ├── country=US</div><div class="line">            │   └── data.parquet</div><div class="line">            ├── country=CN</div><div class="line">            │   └── data.parquet</div><div class="line">            └── ...</div></pre></td></tr></table></figure></p>
<p>通过将path/to/table传递给SparkSession.read.parquet或SparkSession.read.load，Spark SQL将自动从路径中获取分区信息。现在返回的DataFrame的模式变成：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">root</div><div class="line">|-- name: string (nullable = true)</div><div class="line">|-- age: long (nullable = true)</div><div class="line">|-- gender: string (nullable = true)</div><div class="line">|-- country: string (nullable = true)</div></pre></td></tr></table></figure></p>
<p>注意，分区列的数据类型是自动推断的。当前支持数字数据类型、日期、时间戳和字符串类型。有些时候，用户可能不想自动推导分区列的数据类型。对于这种情况，自动类型推导能够通过配置项spark.sql.sources.partitionColumnTypeInference.enabled来配置，该配置默认值为True。当类型推导被禁用后，分区列将使用字符串类型。<br>从Spark1.6开始，分区发现默认只能查找给定路径下的。因此，对于上面的那个例子，如果用户传递path/to/table/gender=male给SparkSession.read.parquet或SparkSession.read.load，那么gender将不会被当成一个分区列。如果用户想要具体说明分区开始查找的基本目录，可以在数据源选项中设置basePath。例如，当数据目录为path/to/table/gender=male时，并且设置了basePath为path/to/table/，那么gender将会是一个分区列。</p>
<h4 id="Schema-Merging"><a href="#Schema-Merging" class="headerlink" title="Schema Merging"></a>Schema Merging</h4><p>和ProtocolBuffer、Avro以及Thrift一样，Parquet也支持模式演化。用户可以先从一个简单的schema开始，然后根据需要逐渐增加更多的列。通过这种方式，用户可能最终会得到不同但相互兼容的多个Parquet文件。Parquet数据源能够自动发现这种情况，并合并这些文件的schemas。<br>因为合并schema是一个成本相当高的操作，而且在很多情况是不必要的，因此从1.5.0开始，该功能默认是关闭的。你可以通过以下来启用它：</p>
<blockquote>
<p>当你读区Parquet文件时，设置数据源选项 mergeSchema为true（下面的列子将展示）或者<br>设置全局SQL选项 spark.sql.parquet.mergeSchema为true。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.Arrays;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Square</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> value;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> square;</div><div class="line"></div><div class="line">  <span class="comment">// Getters and setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Cube</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> value;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> cube;</div><div class="line"></div><div class="line">  <span class="comment">// Getters and setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line">List&lt;Square&gt; squares = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> value = <span class="number">1</span>; value &lt;= <span class="number">5</span>; value++) &#123;</div><div class="line">  Square square = <span class="keyword">new</span> Square();</div><div class="line">  square.setValue(value);</div><div class="line">  square.setSquare(value * value);</div><div class="line">  squares.add(square);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Create a simple DataFrame, store into a partition directory</span></div><div class="line">Dataset&lt;Row&gt; squaresDF = spark.createDataFrame(squares, Square.class);</div><div class="line">squaresDF.write().parquet(<span class="string">"data/test_table/key=1"</span>);</div><div class="line"></div><div class="line">List&lt;Cube&gt; cubes = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> value = <span class="number">6</span>; value &lt;= <span class="number">10</span>; value++) &#123;</div><div class="line">  Cube cube = <span class="keyword">new</span> Cube();</div><div class="line">  cube.setValue(value);</div><div class="line">  cube.setCube(value * value * value);</div><div class="line">  cubes.add(cube);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Create another DataFrame in a new partition directory,</span></div><div class="line"><span class="comment">// adding a new column and dropping an existing column</span></div><div class="line">Dataset&lt;Row&gt; cubesDF = spark.createDataFrame(cubes, Cube.class);</div><div class="line">cubesDF.write().parquet(<span class="string">"data/test_table/key=2"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Read the partitioned table</span></div><div class="line">Dataset&lt;Row&gt; mergedDF = spark.read().option(<span class="string">"mergeSchema"</span>, <span class="keyword">true</span>).parquet(<span class="string">"data/test_table"</span>);</div><div class="line">mergedDF.printSchema();</div><div class="line"></div><div class="line"><span class="comment">// The final schema consists of all 3 columns in the Parquet files together</span></div><div class="line"><span class="comment">// with the partitioning column appeared in the partition directory paths</span></div><div class="line"><span class="comment">// root</span></div><div class="line"><span class="comment">//  |-- value: int (nullable = true)</span></div><div class="line"><span class="comment">//  |-- square: int (nullable = true)</span></div><div class="line"><span class="comment">//  |-- cube: int (nullable = true)</span></div><div class="line"><span class="comment">//  |-- key: int (nullable = true)</span></div></pre></td></tr></table></figure>
<p>完整示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Hive-metastore-Parquet-table-conversion"><a href="#Hive-metastore-Parquet-table-conversion" class="headerlink" title="Hive metastore Parquet table conversion"></a>Hive metastore Parquet table conversion</h4><p>当我们向Hive metastore Parquet table写数据或从中读数据时，Spark SQL奖尝试使用自己的Parquet支持来代替Hive SerDe以获取更好的性能。这个行为通过spark.sql.hive.converMetastoreParquet来配置，并且默认为打开的。</p>
<h5 id="Hive-Parquet-Schema-Reconciliation"><a href="#Hive-Parquet-Schema-Reconciliation" class="headerlink" title="Hive/Parquet Schema Reconciliation"></a>Hive/Parquet Schema Reconciliation</h5><p>从表schema处理的角度来看，Hive和Parquet有两个主要区别：</p>
<blockquote>
<p>1、Hive是不区分大小写的，而Parquet是区分大小写的。<br>2、Hive认为所有列nullable，而nullable在Parquet中很重要。</p>
</blockquote>
<p>因为上面的原因，当我们将一个Hive metastore Parquet table转换为一个Spark SQL Parquet table时，我们必须将Hive metastore schema与Parquet schema调整一致。调整的规则为：</p>
<blockquote>
<p>1、两个schema中相同名称的字段不管是否为空必须具有相同的数据类型。调整好的字段应当具有Parquet端的数据类型，因此nullable是具有意义的。<br>2、调整后的schema必须包含Hive metastore schema中定义的字段。<br>    1）只出现在Parquet schema中的字段将从调整后的schema中删掉。<br>    2）只出现在Hive metastore schema中的字段将被作为nullable字段添加到调整后的schema中。</p>
</blockquote>
<h5 id="Metadata-Refreshing"><a href="#Metadata-Refreshing" class="headerlink" title="Metadata Refreshing"></a>Metadata Refreshing</h5><p>Spark SQL为了更好的性能而缓存了Parquet metadata。当Hive metastore Parquet表转换启用时，那些被转换的表的metadata也会被缓存。如果这些表被Hive或其他外部工具更新了，你需要手动刷新它们以保证metadata的一致。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// spark is an existing SparkSession</span></div><div class="line">spark.catalog().refreshTable(<span class="string">"my_table"</span>);</div></pre></td></tr></table></figure></p>
<h4 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h4><p>Parquet的配置可以通过两种方式完成，在SparkSession上使用setConf方法或使用SQL运行SET key=value。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.parquet.binaryAsString</td>
<td style="text-align:left">false</td>
<td style="text-align:left">一些其他产生Parquet的系统，主要是Impala、Hive以及老版本的Spark SQL，这些系统在写Parquet schema时不区分二进制数据和字符串。这个标记告诉Spark SQL为这些系统将二进制数据按照字符串来进行兼容。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.int96AsTimestamp</td>
<td style="text-align:left">true</td>
<td style="text-align:left">一些其他产生Parquet的系统，特别是Impala和Hive，它们使用INT96来存储时间戳。这个标记告诉Spark SQL将INT96按照时间戳来解析，以便为那些系统提供兼容。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.compressio.codec</td>
<td style="text-align:left">snappy</td>
<td style="text-align:left">设置写Parquet文件的压缩编码器。如果没有在表详情的选项/属性中指定”compression”或”parquet.compression”。根据优先级排序：compression &gt; parquet.compression &gt; spark.sql.parquet.compression.codec。该选项可以使用的值有：none、uncompressed、snappy、gzip或lzo。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.filterPushdown.</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为True时，启用Parquet过滤器的push-down优化。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.converMetastoreParquet</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为false时，Spark SQL将对parquet table使用Hive SerDe，而不是使用内置支持。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.mergeSchema</td>
<td style="text-align:left">false</td>
<td style="text-align:left">当设置为true时，Parquet数据源合并从所有数据文件收集的schema，如果是false，将从摘要文件中挑选schema，如果没有摘要文件可用，则随机选择一个文件。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.optimizer.metadataOnly.</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为true时，启用metadata-only查询优化，这个优化使用表的metadata来产生分区列，而不是通过对表扫描。当所有扫描过的列示分区列，且查询操作有一个满足distinct语意的聚合操作时，适用。</td>
</tr>
</tbody>
</table>
<h3 id="ORC-Files"><a href="#ORC-Files" class="headerlink" title="ORC Files"></a>ORC Files</h3><p>从Spark 2.3开始，Spark支持向量ORC reader，这个reader使用新的ORC文件格式来读取ORC文件。因此新增了如下配置。当spark.sql.orc.impl被设置为native且spark.sql.orc.enableVectorizedReader被设置为true时，向量读取器将用于读区原生的ORC表（这些表使用USING ORC语句创建）。对于Hive ORC serde表（使用USING HIVE OPTIONS），当spark.sql.hive.convertMetastoreOrc也被设置为true时，向量reader被使用。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.orc.impl</td>
<td style="text-align:left">hive</td>
<td style="text-align:left">ORC实现类的名字。可以是native和hive中的一个。native意味着对构建于Apache ORC 1.4.1上的原生ORC支持。hive意味着对Hive 1.2.1中的ORC库进行支持。</td>
</tr>
<tr>
<td style="text-align:left">spark.slql.orc.enableVectorizedReader</td>
<td style="text-align:left">true</td>
<td style="text-align:left">在native实现中启用向量化orc编码。如果为false，一个新的非向量化ORC reader被用于native实现。对于hive实现，本项可以忽略。</td>
</tr>
</tbody>
</table>
<h3 id="JSON-Datasets"><a href="#JSON-Datasets" class="headerlink" title="JSON Datasets"></a>JSON Datasets</h3><p>Spark SQL能够自动推导一个JSON dataset的schema并将它加载为一个Dataset<row>。这个转换能够在一个Dataset<string>上或一个JSON文件上使用SparkSession.read().json()来完成。<br>注意，提供的json文件不是一个典型的JSON文件。每一行必须是一个独立有效的JSON对象（其实这句话的意思就是，一个json数据必须独立一行，不能跨多行）。关于更多的信息，请查看<a href="http://jsonlines.org/" title="JSON Lines text format, also called newline-delimited JSON" target="_blank" rel="external">JSON Lines text format, also called newline-delimited JSON</a>.<br>要想解析多行JSON文件，需要设置multiLine选项为true。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line"><span class="comment">// A JSON dataset is pointed to by path.</span></div><div class="line"><span class="comment">// The path can be either a single text file or a directory storing text files</span></div><div class="line">Dataset&lt;Row&gt; people = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The inferred schema can be visualized using the printSchema() method</span></div><div class="line">people.printSchema();</div><div class="line"><span class="comment">// root</span></div><div class="line"><span class="comment">//  |-- age: long (nullable = true)</span></div><div class="line"><span class="comment">//  |-- name: string (nullable = true)</span></div><div class="line"></div><div class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></div><div class="line">people.createOrReplaceTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></div><div class="line">Dataset&lt;Row&gt; namesDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>);</div><div class="line">namesDF.show();</div><div class="line"><span class="comment">// +------+</span></div><div class="line"><span class="comment">// |  name|</span></div><div class="line"><span class="comment">// +------+</span></div><div class="line"><span class="comment">// |Justin|</span></div><div class="line"><span class="comment">// +------+</span></div><div class="line"></div><div class="line"><span class="comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span></div><div class="line"><span class="comment">// a Dataset&lt;String&gt; storing one JSON object per string.</span></div><div class="line">List&lt;String&gt; jsonData = Arrays.asList(</div><div class="line">        <span class="string">"&#123;\"name\":\"Yin\",\"address\":&#123;\"city\":\"Columbus\",\"state\":\"Ohio\"&#125;&#125;"</span>);</div><div class="line">Dataset&lt;String&gt; anotherPeopleDataset = spark.createDataset(jsonData, Encoders.STRING());</div><div class="line">Dataset&lt;Row&gt; anotherPeople = spark.read().json(anotherPeopleDataset);</div><div class="line">anotherPeople.show();</div><div class="line"><span class="comment">// +---------------+----+</span></div><div class="line"><span class="comment">// |        address|name|</span></div><div class="line"><span class="comment">// +---------------+----+</span></div><div class="line"><span class="comment">// |[Columbus,Ohio]| Yin|</span></div><div class="line"><span class="comment">// +---------------+----+</span></div></pre></td></tr></table></figure></string></row></p>
<p>完整的示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h3 id="Hive-Tables"><a href="#Hive-Tables" class="headerlink" title="Hive Tables"></a>Hive Tables</h3><p>Spark SQL还支持对Apache Hive读写数据。然而，因为Hive有很多的依赖，而这些依赖默认没有包含在Spark的发布中。如果Hive的依赖能够在classpath中找到，Spark将自动加载它们。注意，这些Hive依赖也必须在所有worker节点上存在，因为它们需要访问Hive的序列化和反序列化库（SerDes）以便访问Hive上存储的数据。<br>Hive的配置是通过替换conf/目录下的hive-site.xml、core-site.xml（安全配置）和hdfs-sit.xml（HDFS配置）来完成的。<br>当使用Hive工作时，必须实例化支持Hive的SparkSession，包括连接到已有的Hive metastore、支持Hive serdes以及Hive自定义函数。即使没有Hive环境也能够启用Hive支持。当没有通过hive-site.xml进行配置时，context自动在当前目录创建metastore_db，并创建一个由spark.sql.warehouse.dir配置指定的目录，默认目录在Spark application启动的当前目录中的spark-warehouse。注意hive-site.xml中的hive.metastore.warehouse.dir属性在Spark2.0.0中废弃了，取而代之是使用spark.sql.warehouse.dir来指定数据库在仓库中的位置。你可以需要为启动Spark appliction的用户开放写权限。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.File;</div><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> key;</div><div class="line">  <span class="keyword">private</span> String value;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getKey</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> key;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setKey</span><span class="params">(<span class="keyword">int</span> key)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.key = key;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getValue</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> value;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setValue</span><span class="params">(String value)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.value = value;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// warehouseLocation points to the default location for managed databases and tables</span></div><div class="line">String warehouseLocation = <span class="keyword">new</span> File(<span class="string">"spark-warehouse"</span>).getAbsolutePath();</div><div class="line">SparkSession spark = SparkSession</div><div class="line">  .builder()</div><div class="line">  .appName(<span class="string">"Java Spark Hive Example"</span>)</div><div class="line">  .config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation)</div><div class="line">  .enableHiveSupport()</div><div class="line">  .getOrCreate();</div><div class="line"></div><div class="line">spark.sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive"</span>);</div><div class="line">spark.sql(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Queries are expressed in HiveQL</span></div><div class="line">spark.sql(<span class="string">"SELECT * FROM src"</span>).show();</div><div class="line"><span class="comment">// +---+-------+</span></div><div class="line"><span class="comment">// |key|  value|</span></div><div class="line"><span class="comment">// +---+-------+</span></div><div class="line"><span class="comment">// |238|val_238|</span></div><div class="line"><span class="comment">// | 86| val_86|</span></div><div class="line"><span class="comment">// |311|val_311|</span></div><div class="line"><span class="comment">// ...</span></div><div class="line"></div><div class="line"><span class="comment">// Aggregation queries are also supported.</span></div><div class="line">spark.sql(<span class="string">"SELECT COUNT(*) FROM src"</span>).show();</div><div class="line"><span class="comment">// +--------+</span></div><div class="line"><span class="comment">// |count(1)|</span></div><div class="line"><span class="comment">// +--------+</span></div><div class="line"><span class="comment">// |    500 |</span></div><div class="line"><span class="comment">// +--------+</span></div><div class="line"></div><div class="line"><span class="comment">// The results of SQL queries are themselves DataFrames and support all normal functions.</span></div><div class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(<span class="string">"SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The items in DataFrames are of type Row, which lets you to access each column by ordinal.</span></div><div class="line">Dataset&lt;String&gt; stringsDS = sqlDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Key: "</span> + row.get(<span class="number">0</span>) + <span class="string">", Value: "</span> + row.get(<span class="number">1</span>),</div><div class="line">    Encoders.STRING());</div><div class="line">stringsDS.show();</div><div class="line"><span class="comment">// +--------------------+</span></div><div class="line"><span class="comment">// |               value|</span></div><div class="line"><span class="comment">// +--------------------+</span></div><div class="line"><span class="comment">// |Key: 0, Value: val_0|</span></div><div class="line"><span class="comment">// |Key: 0, Value: val_0|</span></div><div class="line"><span class="comment">// |Key: 0, Value: val_0|</span></div><div class="line"><span class="comment">// ...</span></div><div class="line"></div><div class="line"><span class="comment">// You can also use DataFrames to create temporary views within a SparkSession.</span></div><div class="line">List&lt;Record&gt; records = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> key = <span class="number">1</span>; key &lt; <span class="number">100</span>; key++) &#123;</div><div class="line">  Record record = <span class="keyword">new</span> Record();</div><div class="line">  record.setKey(key);</div><div class="line">  record.setValue(<span class="string">"val_"</span> + key);</div><div class="line">  records.add(record);</div><div class="line">&#125;</div><div class="line">Dataset&lt;Row&gt; recordsDF = spark.createDataFrame(records, Record.class);</div><div class="line">recordsDF.createOrReplaceTempView(<span class="string">"records"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Queries can then join DataFrames data with data stored in Hive.</span></div><div class="line">spark.sql(<span class="string">"SELECT * FROM records r JOIN src s ON r.key = s.key"</span>).show();</div><div class="line"><span class="comment">// +---+------+---+------+</span></div><div class="line"><span class="comment">// |key| value|key| value|</span></div><div class="line"><span class="comment">// +---+------+---+------+</span></div><div class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></div><div class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></div><div class="line"><span class="comment">// |  4| val_4|  4| val_4|</span></div><div class="line"><span class="comment">// ...</span></div></pre></td></tr></table></figure></p>
<p>完整示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java。</p>
<h4 id="Specifying-storage-format-for-Hive-table"><a href="#Specifying-storage-format-for-Hive-table" class="headerlink" title="Specifying storage format for Hive table"></a>Specifying storage format for Hive table</h4><p>当你创建一个Hive表时，你需要指定这个表应该如何从文件系统读写数据，例如”input format”和”output format”。你还需要定义这个表应该如何将data反序列化为row，或者如何将row序列化为data，如”serde”。下面的选项可以被用来指定存储格式（”serde”、”input format”、”output format”），如：CREATE TABLE src(id int) USING hive OPTIONS(fileFormat ‘parquet’)。默认情况下，我们将以简单文本的格式读取table。值得注意的是，在创建table的时候，存储handler还不被支持，你可以在Hive端使用存储handler来创建一个table，然后使用Spark SQL来读区它。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">fileFormat</td>
<td style="text-align:left">用来说明文件格式的存储格式包，包括”serde”、”input format”和”output format”。当前我们支持6中文件格式：sequencefile、rcfile、orc、parquet、textfile和avro。</td>
</tr>
<tr>
<td style="text-align:left">inputFormat\outputFormat</td>
<td style="text-align:left">这两个选项用来指定”InputFormat“和”OutputFormat“类的名字，例如：org.apache.hadoop.hive.qllio.orc.OrcInputFormat。这两个选项应该成对出现，如果你设置了”fileFormat”选项，那么你不能分别指定它们。</td>
</tr>
<tr>
<td style="text-align:left">serde</td>
<td style="text-align:left">这个选项指定了一个serde类。当设置了‘fileFormat’选项时，如果给定的‘fileFormat’已经包含了serde信息，那么不要设置这个选项。目前，“sequencefile”、“textfile”和“rcfile”不包含serde信息，因此你可以为这3种文件格式设置此选项。</td>
</tr>
<tr>
<td style="text-align:left">fieldDelim, escapeDelim, collectionDelim, mapkeyDelim, lineDelim</td>
<td style="text-align:left">这个选项只能被用于”textfile”的文件格式。它们定义了文件的换行符。</td>
</tr>
</tbody>
</table>
<p>其他属性使用OPTIONS进行定义，将作为Hive serde属性来考虑。</p>
<h4 id="Interacting-with-Different-Versions-of-Hive-Metastore"><a href="#Interacting-with-Different-Versions-of-Hive-Metastore" class="headerlink" title="Interacting with Different Versions of Hive Metastore"></a>Interacting with Different Versions of Hive Metastore</h4><p>Spark SQL的Hive支持的最重要部分是与Hive metastore的交互，它使Spark SQL能够访问Hive表中的metadata。从Spark 1.4.0开始，使用下面描述的配置，Spark有一个独立的包用来访问不同版本的Hive metadata。注意，无论要去访问的metastore的Hive是什么版本，在Spark SQL内部将针对Hive 1.2.1进行编译，并使用这些类作为内部执行（serdes、UDFs、UDAFs等）。<br>下面的选项能够被用来配置获取metadata的Hive的版本：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.hive.metastore.version</td>
<td style="text-align:left">1.2.1</td>
<td style="text-align:left">Hive metadata的版本。可用的选项从0.12.0到1.2.1</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.metastore.jars</td>
<td style="text-align:left">builtin</td>
<td style="text-align:left">被用于实例化HiveMetastoreClient的jar的位置。这个属性有三个选项：<br>1）builtin： 使用Hive 1.2.1，当-Phive被启用时，它与Spark assembly绑定。当这个选择了这个选项，spark.sql.hive.metastore.version必须是1.2.1或为定义。<br> 2) maven：从Maven库中下载指定版本的Hive jars。这个配置对于生产环境通常不推荐。<br> 3）JVM的标准classpath格式。这个classpath必须包含了Hive和它的依赖，以及对应的版本的Hadoop。这些jar只需要存在于driver上，但是如果你实在yarn资源管理器的集群上，那么你必须确保它们和你的application一起被打包。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.metastore. sharedPrefixes</td>
<td style="text-align:left">com.mysql.jdbc,org.postgresql, com.microsoft.sqlserver,oracla.jdbc</td>
<td style="text-align:left">那些需要使用类加载器加载的用于在Spark SQL和指定版本的Hive之间共享的类前缀，类前缀是一个逗号分隔的列表。一个需要被共享的类就是JDBC driver，它需要访问metastore。其他需要共享的类是那些需要与已经共享类交互的类。例如，由log4j使用的自定义appender。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.metastore. barrierPrefixes</td>
<td style="text-align:left">(empty)</td>
<td style="text-align:left">Spark SQL所连接的每个版本的Hive都应明确加载的类的前缀，列表以逗号分隔。例如，通常需要被共享的Hive UDFs在一个前缀中被声明（如，org.apache.spark.*）</td>
</tr>
</tbody>
</table>
<h3 id="JDBC-To-Other-Databases"><a href="#JDBC-To-Other-Databases" class="headerlink" title="JDBC To Other Databases"></a>JDBC To Other Databases</h3><p>Spark SQL还有一个数据源，可以使用JDBC从其他数据库读取数据。这个功能比使用<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.JdbcRDD" title="jdbcRDD" target="_blank" rel="external">jdbcRDD</a>更加受欢迎。这是因为结果是作为一个DataFrame被返回，这样很容易的使用Spark SQL进行处理或与其他数据源相连接。JDBC数据源在Java或Python中使用起来也很容易，因为它不需要用户提供一个ClassTag。（注意，这不同于Spark SQL JDBC Server，Spark SQL JDBC Server允许其他application使用Spark SQL运行查询）<br>你需要在spark classpath中添加对应数据库的JDBC driver。例如，要从Spark shell连接到postgres，你应该运行如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar</div></pre></td></tr></table></figure></p>
<p>使用Data Source API，远程数据库中的表可以被加载为一个DataFrame或Spark SQL临时视图。用户可以在数据源选项中指定JDBC的连接属性。连接通畅需要提供user和password属性，来登陆数据源。除了连接属性外，Spark还支持如下的选项，这些选项忽略大小写：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">url</td>
<td style="text-align:left">进行连接的JDBC URL。特定数据源的连接属性可能会在URL中设置。如：jdbc:postgresql://localhost/test?user=fred&amp;password=secret。</td>
</tr>
<tr>
<td style="text-align:left">dbtable</td>
<td style="text-align:left">要读取的JDBC表。注意，在SQL查询中的From子句中有效的任何东西，都能使用。例如，你可以在括号中使用子查询来代替全表。</td>
</tr>
<tr>
<td style="text-align:left">driver</td>
<td style="text-align:left">连接URL的JDBC driver的类名。</td>
</tr>
<tr>
<td style="text-align:left">partitionColumn, lowerBound, upperBound</td>
<td style="text-align:left">这些选项中的一个被指定，那么所有的都必须被指定。此外，numPartitions必须被指定。它们描述了多个worker并行读取表数据时，应该如何分区。partitionColumn必须是表中的数值列。注意，lowerBound和upperBound仅仅用来决定分区的幅度，而不是过滤表中的行。因此表中的所有行都将被分区并返回。这个选项只能被用于读取。</td>
</tr>
<tr>
<td style="text-align:left">numPartitions</td>
<td style="text-align:left">并行读写表的最大分区数。这也确定了JDBC连接的最大并发。如果写的分区数量超过了这个限制，我们可以在写数据之前调用coalesce(numPartitions)来减少它。</td>
</tr>
<tr>
<td style="text-align:left">fetchsize</td>
<td style="text-align:left">JDBC的提取大小，它确定了每次通信能够取得多少行。它能够帮助提升那些默认fetch size低的JDBC dirver的性能（比如，Orache的fetch size为10）。这个选项只能用于读操作。</td>
</tr>
<tr>
<td style="text-align:left">batch</td>
<td style="text-align:left">JDBC的batch大小，它确定了每次通信能够插入多少行。这能够帮助提升JDBC dirver的性能。这个选项只能用于写操作。默认值为1000。</td>
</tr>
<tr>
<td style="text-align:left">isolationLevel</td>
<td style="text-align:left">事务的隔离级别，应用于当前连接。它可以是：NONE\READ_COMMITTED\ READ_UNCOMMITTED\REPEATABLE_READ\SERIALIZABLE中的一个，通过JDBC连接对象来定义标准事务的隔离级别，默认为READ_UNCOMMITTED。这个选项只能用于写操作。请参考java.sql.Connection文档。</td>
</tr>
<tr>
<td style="text-align:left">sessionInitStatement</td>
<td style="text-align:left">session初始化声明。在到远程数据库的session被打开之后，开始读取数据之前，这个选项执行一个自定义语句（PL/SQL块）。使用这个来实现session的初始化代码。例如：option(“色上司哦那I逆天S塔特闷它”， “”“BEGIN execute immediate ‘alter session set “_serial_direct_read”=true’; END; “””)</td>
</tr>
<tr>
<td style="text-align:left">truncate</td>
<td style="text-align:left">这是一个与JDBC writer相关操作。当启用了SaveMode.Overwrite，这个选项控制删除已存在的表，而不是先drop表然后再创建表。这个更加有效率，并且避免了表的metadata被删除。然而在某些情况下，它无法工作，如新数据有不同的schema。该选项默认值为false。这个选项只用于写操作。</td>
</tr>
<tr>
<td style="text-align:left">createTableOptions</td>
<td style="text-align:left">这是一个与JDBC writer相关的操作。如果设置，该选项允许在创建表的时候设置特定数据库表和分区的选项（如，CREATE TABLE T(name string) ENGINE=InnoDB）。这个选项只能被用于写操作。</td>
</tr>
<tr>
<td style="text-align:left">createTableColumnTypes</td>
<td style="text-align:left">当创建表时，用来代替默认的数据库列类型。数据类型信息使用与CREATE TABLE columns语句（如：”name CHAR(64), comments VARCHAR(1024)”）相同的格式被指定。被指定的数据类型应该是有效的spark sql数据类型。本选项只能用于写操作。</td>
</tr>
<tr>
<td style="text-align:left">customSchema</td>
<td style="text-align:left">自定义schema用于从JDBC连接中读取数据。例如，”id DECIMAL(38, 0), name STRING”。你也可以指定部分字段，其他的时候默认类型映射。例如：”id DECIMAL(38, 0)”。列名称应该与JDBC表的相关列名称一致。用户可以指定Spark SQL的相关数据类型，而不是使用默认的。这个选项只能被用于读操作。</td>
</tr>
</tbody>
</table>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span></div><div class="line"><span class="comment">// Loading data from a JDBC source</span></div><div class="line">Dataset&lt;Row&gt; jdbcDF = spark.read()</div><div class="line">  .format(<span class="string">"jdbc"</span>)</div><div class="line">  .option(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>)</div><div class="line">  .option(<span class="string">"dbtable"</span>, <span class="string">"schema.tablename"</span>)</div><div class="line">  .option(<span class="string">"user"</span>, <span class="string">"username"</span>)</div><div class="line">  .option(<span class="string">"password"</span>, <span class="string">"password"</span>)</div><div class="line">  .load();</div><div class="line"></div><div class="line">Properties connectionProperties = <span class="keyword">new</span> Properties();</div><div class="line">connectionProperties.put(<span class="string">"user"</span>, <span class="string">"username"</span>);</div><div class="line">connectionProperties.put(<span class="string">"password"</span>, <span class="string">"password"</span>);</div><div class="line">Dataset&lt;Row&gt; jdbcDF2 = spark.read()</div><div class="line">  .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</div><div class="line"></div><div class="line"><span class="comment">// Saving data to a JDBC source</span></div><div class="line">jdbcDF.write()</div><div class="line">  .format(<span class="string">"jdbc"</span>)</div><div class="line">  .option(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>)</div><div class="line">  .option(<span class="string">"dbtable"</span>, <span class="string">"schema.tablename"</span>)</div><div class="line">  .option(<span class="string">"user"</span>, <span class="string">"username"</span>)</div><div class="line">  .option(<span class="string">"password"</span>, <span class="string">"password"</span>)</div><div class="line">  .save();</div><div class="line"></div><div class="line">jdbcDF2.write()</div><div class="line">  .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</div><div class="line"></div><div class="line"><span class="comment">// Specifying create table column data types on write</span></div><div class="line">jdbcDF.write()</div><div class="line">  .option(<span class="string">"createTableColumnTypes"</span>, <span class="string">"name CHAR(64), comments VARCHAR(1024)"</span>)</div><div class="line">  .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</div></pre></td></tr></table></figure>
<p>完整示例代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h3 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h3><blockquote>
<p>1、JDBC dirver类对于client session和所有executor的主类加载器是可访问的。这是因为Java的DriverManager会做一个安全检查，当DriverManager要打开一个连接时，检查结果会忽略所有主类加载器无法访问的driver。一个简便的方法是修改所有worker节点的compute_classpath.sh来包含你的driver JAR。<br>2、一些数据库，如H2，需要将所有名字转换为大写。你需要在Spark SQL中使用大写来引用那些名字。</p>
</blockquote>
<h2 id="Performance-Tuning"><a href="#Performance-Tuning" class="headerlink" title="Performance Tuning"></a>Performance Tuning</h2><p>通过将数据缓存到内存或开启一些创新选项，一些工作量是可以优化提升性能的。</p>
<h3 id="Caching-Data-In-Memory"><a href="#Caching-Data-In-Memory" class="headerlink" title="Caching Data In Memory"></a>Caching Data In Memory</h3><p>通过调用spark.catalog.cacheTable(“tableName”)或dataFrame.cache()，Spark能够使用内存中列式格式来缓存表。Spark SQL将扫描需要的列，并自动调整压缩，以达到最小的内存使用和GC压力。你可以使用spark.catalog.uncacheTable(“tableName”)，将table从内存中移除。<br>配置内存缓存可以通过两种方式来实现：在SparkSession上调用setConf方法，或者使用SQL来执行SET key=value命令。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql. inMemoryColumnarStorage.compressed</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为true的时候，Spark SQL将基于数据的统计自动为每一列选择一种压缩编码器。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql. imMemoryColumnarStorage.batchSize</td>
<td style="text-align:left">10000</td>
<td style="text-align:left">控制列式缓存的批量大小。较大的批量size会影响内存会提高内存的利用率和压缩，但是会产生内存溢出的风险。</td>
</tr>
</tbody>
</table>
<h3 id="Other-Configuration-Options"><a href="#Other-Configuration-Options" class="headerlink" title="Other Configuration Options"></a>Other Configuration Options</h3><p>下面的选项也能够被用来提高查询的效率。随着Spark的优化，这些选项在未来可能会被废弃。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.files.maxPartitionbytes</td>
<td style="text-align:left">134217728 (128 MB)</td>
<td style="text-align:left">读取文件时，单个分区的最大字节数。</td>
</tr>
<tr>
<td style="text-align:left">saprk.sql.files.openCostInBytes</td>
<td style="text-align:left">4194304 (4 MB)</td>
<td style="text-align:left">打开一个文件的成本，通过在同一时间能够扫描的字节数来测量。当推送多个文件到一个partition时非常有用。提高这个值会更好，这样写小文件的partition要比写大文件的partition更加快（写小文件的partitin优先调度）。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.broadcastTimeout</td>
<td style="text-align:left">300</td>
<td style="text-align:left">broadcast连接的等待时间，以秒为单位。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.broadcastJoinThreshold</td>
<td style="text-align:left">10485760 (10 MB)</td>
<td style="text-align:left">当执行join操作时，为那些需要广播到所有worker节点的表设置最大字节数。通过设置这个值为-1，广播操作可以被禁用。注意，当前的统计只支持那些运行了ANALYZE TABLE <tablename> COMPUTE STATISTICS命令的Hive Metastore表。</tablename></td>
</tr>
<tr>
<td style="text-align:left">spark.sql.shuffle.partitions</td>
<td style="text-align:left">200</td>
<td style="text-align:left">当为join或aggregation操作而混洗数据时，用来配置使用partitions的数量。</td>
</tr>
</tbody>
</table>
<h3 id="Broadcast-Hint-for-SQL-Queries"><a href="#Broadcast-Hint-for-SQL-Queries" class="headerlink" title="Broadcast Hint for SQL Queries"></a>Broadcast Hint for SQL Queries</h3><p>BROADCAST hint指导Spark在使用其他表或视图join指定表时，如何广播指定表。在Spark决定join方法时，broadcast hash join被优先考虑，即使统计高于spark.sql.autoBroadcastJoinThreshold的配置。当join两边都被指定了，Spark广播具有较低统计的那边。注意Spark不保证BHJ（broadcast hash join）总是被选择，因为不是所有的情况都支持BHJ。当broadcast nested loop join被选择时，我们仍然最重提示。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.spark.sql.functions.broadcast;</div><div class="line">broadcast(spark.table(<span class="string">"src"</span>)).join(spark.table(<span class="string">"records"</span>), <span class="string">"key"</span>).show();</div></pre></td></tr></table></figure></p>
<h2 id="Distributed-SQL-Engine"><a href="#Distributed-SQL-Engine" class="headerlink" title="Distributed SQL Engine"></a>Distributed SQL Engine</h2><p>使用Spark SQL的JDBC/ODBC或command-line interface，Spark SQL也能够具有分布式查询引擎的行为。在这种模式中，终端用户或application能够直接与Spark SQL交互来运行SQL查询，而不需要写任何的代码。</p>
<h3 id="Running-the-Thrift-JDBC-ODBC-server"><a href="#Running-the-Thrift-JDBC-ODBC-server" class="headerlink" title="Running the Thrift JDBC/ODBC server"></a>Running the Thrift JDBC/ODBC server</h3><p>Thrift JDBC/ODBC server实现了相当于Hive 1.2.1中的HiveServer2。你可以使用Spark或Hive1.2.1的beeline脚本来测试JDBC server。<br>要启动JDBC/ODBC server，在Spark目录中运行如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./sbin/start-thriftserver.sh</div></pre></td></tr></table></figure></p>
<p>这个脚本接受所有bin/spark-submit命令的行的参数，并增加了一个–hiveconf选项用来指定Hive属性。你可以执行 ./sbin/start-thriftserver.sh –help来获取完整的可用属性列表。默认，这个server监听的是本地的10000端口。要想重写这个丢昂扣，你可以修改环境变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">export HIVE_SERVER2_THRIFT_PORT=&lt;listening-port&gt;</div><div class="line">export HIVE_SERVER2_THRIFT_BIND_HOST=&lt;listening-host&gt;</div><div class="line">./sbin/start-thriftserver.sh \</div><div class="line">  --master &lt;master-uri&gt; \</div><div class="line">  ...</div></pre></td></tr></table></figure></p>
<p>或者修改系统属性：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">./sbin/start-thriftserver.sh \</div><div class="line">  --hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \</div><div class="line">  --hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \</div><div class="line">  --master &lt;master-uri&gt;</div><div class="line">  ...</div></pre></td></tr></table></figure></p>
<p>现在，你可以使用beeline来测试Thrift JDBC/ODBC server：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/beeline</div></pre></td></tr></table></figure></p>
<p>在beeline中连接JDBC/ODBC server可以使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">beeline&gt; !connect jdbc:hive2://localhost:10000</div></pre></td></tr></table></figure></p>
<p>beeline将会询问你用户名和密码。在非安全模式中，输入你机器的用户名和空白的密码。对于安全模式，请遵循<a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients" title="beeline documentation" target="_blank" rel="external">beeline documentation</a>的指导。</p>
<p>通过替换conf/中hive-site.xml、core-site.mxl和hdfs-site.xml来完成Hive的配置。</p>
<p>你可能还需要使用Hive提供的beeline脚本。</p>
<p>Thrift JDBC server还支持通过HTTP协议发送thrift RPC messages。要启用HTTP模式，可以如下修改系统属性，或者修改conf中的hive-site.xml：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive.server2.transport.mode - Set this to value: http</div><div class="line">hive.server2.thrift.http.port - HTTP port number to listen on; default is 10001</div><div class="line">hive.server2.http.endpoint - HTTP endpoint; default is cliservice</div></pre></td></tr></table></figure></p>
<p>要进行测试，使用beeline以http模式连接到JDBC/ODBC server：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">beeline&gt; !connect jdbc:hive2://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?hive.server2.transport.mode=http;hive.server2.thrift.http.path=&lt;http_endpoint&gt;</div></pre></td></tr></table></figure></p>
<h4 id="Running-the-Spark-SQL-CLI"><a href="#Running-the-Spark-SQL-CLI" class="headerlink" title="Running the Spark SQL CLI"></a>Running the Spark SQL CLI</h4><p>Spark SQL CLI是一个方便的工具用来在本地模式中运行Hive metastore服务并执行来自命令的查询输入。注意，Spark SQL CLI不能与Thrift JDBC server通信，<br>要启动Spark SQL  CLI，在Spark目录中运行如下脚本：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-sql</div></pre></td></tr></table></figure></p>
<p>通过替换conf/中hive-site.xml、core-site.mxl和hdfs-site.xml来完成Hive的配置。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><h3 id="Data-Types"><a href="#Data-Types" class="headerlink" title="Data Types"></a>Data Types</h3><p>Spark SQL和DataFrame支持如下数据类型：</p>
<blockquote>
<p>1、Numeric types<br>        ByteType：声明一个一个字节的有符号的整型。数值范围从-128到127。<br>        ShortType：声明一个两字节的有符号的整型。数值范围从-32768到32767。<br>        IntegerType：声明一个四字节的有符号的整型。数值范围从-2147483648到2147483647。<br>        LongType：声明一个八个字节的有符号的整型。数值范围从-9223372036854775808到9223372036854775807。<br>        FloatType：声明一个四字节的单精度浮点数值。<br>        DoubleType：声明一个八字节的双精度浮点数。<br>        DecimlType：声明一个任意精度的有符号的十进制数值。内部由java.math.BigDecimal支持。一个DecimlType由一个任意精度的不能整型值和一个32位的整型组成。<br>2、Strubg type<br>        声明一个字符串值。<br>3、Binary type<br>        BinaryType：声明一个字节序列值。<br>4、Boolean type<br>        BooleanType：声明一个boolean值。<br>5、Datetime type<br>        TimestampType：声明一个由year、month、day、hour、minute和second字段的值组成。<br>        DateType：声明一个由year、month和day字段的值组成。<br>6、Complex types<br>        ArrayType(elementType, containsNull)：声明一个elementType类型序列。containsNull用来检测ArrayType中是否包含null的值。<br>        MapType(keyType, valueType, valueContainsNull)：由一组key-value对组成。key的数据类型由KeyType来描述，value的数据类型由valueType来描述。对于MapType的一个值，keys不允许为null。valueContainsNull<br>        被用来检测MapTypte的values中是否包含null值。<br>        StructType(fields)：StructFields(fields)序列。<br>                StructField(name, datatype, nullable): StructType类型的字段。字段的名称通过name指定。字段的数据类型通过datatype来指定。nullable用来决定这个fields的values是否可以有null。</p>
</blockquote>
<p>Spark SQL的所有数据类型都位于org.apache.spark.sql.types包中。要访问或创建一种数据类型，请使用org.apache.spark.sql.types.DataTypes中提供的接口方法。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Data type</th>
<th style="text-align:left">Value type in Java</th>
<th style="text-align:left">API to access or create a data type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ByteType</td>
<td style="text-align:left">byte or Byte</td>
<td style="text-align:left">DataTypes.ByteType</td>
</tr>
<tr>
<td style="text-align:left">ShortType</td>
<td style="text-align:left">short or Short</td>
<td style="text-align:left">DataTypes.ShortType</td>
</tr>
<tr>
<td style="text-align:left">IntegerType</td>
<td style="text-align:left">int or Integer</td>
<td style="text-align:left">DataTypes.IntegerType</td>
</tr>
<tr>
<td style="text-align:left">LongType</td>
<td style="text-align:left">long or Long</td>
<td style="text-align:left">DataTypes.LongType</td>
</tr>
<tr>
<td style="text-align:left">FloatType</td>
<td style="text-align:left">float or Float</td>
<td style="text-align:left">DataTypes.FloatType</td>
</tr>
<tr>
<td style="text-align:left">DoubleType</td>
<td style="text-align:left">double or Double</td>
<td style="text-align:left">DataTypes.DoubleType</td>
</tr>
<tr>
<td style="text-align:left">DecimalType</td>
<td style="text-align:left">java.math.BigDecimal</td>
<td style="text-align:left">DataTypes.createDecimalType() DataTypes.createDecimalType(precision, scale)</td>
</tr>
<tr>
<td style="text-align:left">StringType</td>
<td style="text-align:left">String</td>
<td style="text-align:left">DataTypes.StringType</td>
</tr>
<tr>
<td style="text-align:left">BinaryType</td>
<td style="text-align:left">byte[]</td>
<td style="text-align:left">DataTypes.BinaryType</td>
</tr>
<tr>
<td style="text-align:left">BooleanType</td>
<td style="text-align:left">boolean or Boolean</td>
<td style="text-align:left">DataTypes.BooleanType</td>
</tr>
<tr>
<td style="text-align:left">TimestampType</td>
<td style="text-align:left">java.sql.Timestamp</td>
<td style="text-align:left">DataTypes.TimestampType</td>
</tr>
<tr>
<td style="text-align:left">DateType</td>
<td style="text-align:left">java.sql.Date</td>
<td style="text-align:left">DateTypes.DateType</td>
</tr>
<tr>
<td style="text-align:left">ArrayType</td>
<td style="text-align:left">java.util.List</td>
<td style="text-align:left">DataTypes.createArrayType(elementType) 注意：containsNull的值为true。</td>
</tr>
<tr>
<td style="text-align:left">MapType</td>
<td style="text-align:left">java.util.Map</td>
<td style="text-align:left">DataTypes.createMapType(keyType, valueType) 注意，valueContainsNull的值将为true</td>
</tr>
<tr>
<td style="text-align:left">StructType</td>
<td style="text-align:left">org.apache.spark.sql.Row</td>
<td style="text-align:left">DataTypes.createStructType(fields)</td>
</tr>
<tr>
<td style="text-align:left">StructField</td>
<td style="text-align:left">The value type in Java of the data type of this field</td>
<td style="text-align:left">DataTypes.createStructField(name, dataType, nullable)</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/10/spark-2-3-1-QuickStart/" itemprop="url">
                  spark_2.3.1_QuickStart
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-10T15:20:56+08:00" content="2018-08-10">
              2018-08-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h1><p>本指南快速的介绍如何使用Spark。我们将通过Spark的交互式shell（用Python或Scala）首先引入API，然后展示如何用Java、Scala和Python写application。<br>要遵循这个指南，首先需要从Spark的网站上下载Spark包。因为我们不使用HDFS，因此你可以现在任何版本的Hadoop。<br>注意，在Spark 2.0之前，Spark的主要程序接口是Resillent Distributed Dataset(RDD)。在Spark 2.0之后，RDD被Dataset所代替，Dataset类似于RDD的强类型，但是底层有更佳丰富的优化。RDD接口仍然被支持，你可以在<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html" title="RDD programming guide" target="_blank" rel="external">RDD programming guide</a>。然而，我们高度推荐你使用Dataset，它比RDD有更好的性能。查看<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" title="SQL programming guide" target="_blank" rel="external">SQL programming guide</a> 以获取更多关于Dataset的详细信息。</p>
<h2 id="Interactive-Analysis-with-the-Spak-Shell"><a href="#Interactive-Analysis-with-the-Spak-Shell" class="headerlink" title="Interactive Analysis with the Spak Shell"></a>Interactive Analysis with the Spak Shell</h2><h3 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h3><p>Spark的shell提供了简单的方式来学习API，以及一种强大的工具来交互式的分析数据。可以通过Scala（它运行在Java虚拟机上，因此它是学习已有Java库的很好方式）或Python来使用。通过在Spark目录下运行如下脚本来启动：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-shell</div></pre></td></tr></table></figure></p>
<p>Spark的主要抽象是一个名为Dataset的分布式项目（数据条目–一条条的数据）集合。Dataset可以通过Hadoop InputFormates（如HDFS文件）来创建，或者由其他Dataset来转换。我们根据Spark源目录下README文件中的文本来创建一个新的Dataset：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val textFile = spark.read.textFile(&quot;README.md&quot;)</div><div class="line">textFile: org.apache.spark.sql.Dataset[String] = [value: string]</div></pre></td></tr></table></figure></p>
<p>通过调用一些action，你可以直接冲Dataset获取值，或者将这个Dataset转换为另一个新的Dataset。对于更多的细节，请查看<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset &#39;API doc" target="_blank" rel="external">API doc</a>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; textFile.count() // Number of items in this Dataset</div><div class="line">res0: Long = 126 // May be different from yours as README.md will change over time, similar to other outputs</div><div class="line"></div><div class="line">scala&gt; textFile.first() // First item in this Dataset</div><div class="line">res1: String = # Apache Spark</div></pre></td></tr></table></figure></p>
<p>现在，我们将这个Dataset转换为一个新的。我们调用filter，将会返回一个包含文件子集合的新的Dataset。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val linesWithSpark = textFile.filter(line =&gt; line.contains(&quot;Spark&quot;))</div><div class="line">linesWithSpark: org.apache.spark.sql.Dataset[String] = [value: string]</div></pre></td></tr></table></figure></p>
<p>我们可以将转换和action串联在一起：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; textFile.filter(line =&gt; line.contains(&quot;Spark&quot;)).count() // How many lines contain &quot;Spark&quot;?</div><div class="line">res3: Long = 15</div></pre></td></tr></table></figure></p>
<h2 id="More-on-Dataset-Operations"><a href="#More-on-Dataset-Operations" class="headerlink" title="More on Dataset Operations"></a>More on Dataset Operations</h2><p>Dataset的转换和action可以被用于更加复杂的计算。假设我们要找出含有打你最多的一行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; textFile.map(line =&gt; line.split(&quot; &quot;).size).reduce((a, b) =&gt; if (a &gt; b) a else b)</div><div class="line">res4: Long = 15</div></pre></td></tr></table></figure></p>
<p>它首先将一个行映射为一个数值，这创建了一个新的Dataset。reduce在Dataset上被调用，用来找到最大的数。map和reduce的参数是Scala的函数（闭包），也可以使用任何语言的特性或Scala/Java库。例如，我们在任意地方调用函数的声明（引入）。我们将使用Math.max()函数来使代码更加容易理解：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; import java.lang.Math</div><div class="line">import java.lang.Math</div><div class="line"></div><div class="line">scala&gt; textFile.map(line =&gt; line.split(&quot; &quot;).size).reduce((a, b) =&gt; Math.max(a, b))</div><div class="line">res5: Int = 15</div></pre></td></tr></table></figure></p>
<p>一个常用的数据流是MapReduce。Spark能够很轻松的实现MapReduce流：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val wordCounts = textFile.flatMap(line =&gt; line.split(&quot; &quot;)).groupByKey(identity).count()</div><div class="line">wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]</div></pre></td></tr></table></figure></p>
<p>这里，我们调用flatMap将行的Dataset转换为一个单词的Dataset，接着利用groupbyKey和count的组合来计算每个单词在文件中出现的次数(String, Long对)从而生成一个新的Dataset。要在shell中收集单词的数量，我们可以调用collect：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; wordCounts.collect()</div><div class="line">res6: Array[(String, Int)] = Array((means,1), (under,2), (this,3), (Because,1), (Pyhon,2), (agree,1), (cluster.,1), ...)</div></pre></td></tr></table></figure></p>
<h2 id="Caching"><a href="#Caching" class="headerlink" title="Caching"></a>Caching</h2><p>Spark还支持将数据集合缓存到集群端内存缓存中。这在数据被反复访问时非常有用，例如当查询一个非常热门的数据集时，又或是在运行一个类似PageRank这样的迭代算法时。作为一个简单的例子，我们将linesWithSpark数据进行缓存：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">scala&gt; linesWithSpark.cache()</div><div class="line">res7: linesWithSpark.type = [value: string]</div><div class="line"></div><div class="line">scala&gt; linesWithSpark.count()</div><div class="line">res8: Long = 15</div><div class="line"></div><div class="line">scala&gt; linesWithSpark.count()</div><div class="line">res9: Long = 15</div></pre></td></tr></table></figure></p>
<p>使用Spark来分析并缓存一个100行的文本开起来很愚蠢。有意思的是，这些相同的函数可以被用在非常大的数据集上，即使它们跨越数十个甚至数百个节点。你可以通过连接bin/spark-shell到一个集群来进行交互式操作，就像<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#using-the-shell" title="RDD programming guide" target="_blank" rel="external">RDD programming guide</a>中描述的。</p>
<h2 id="Self-Contained-Applications"><a href="#Self-Contained-Applications" class="headerlink" title="Self-Contained Applications"></a>Self-Contained Applications</h2><p>假设我们想要使用Spark API写一个自包含的application。我们将使用Scala(利用sbt)、Java(利用Maven)和Pyton(利用pip)来实现一个简单的application。<br>这里我们将使用Maven来构建一个application JAR，其他类似的构建系统也可以。<br>我们将创建一个非常简单的Spark application，SimpleApp.java<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">/* SimpleApp.java */</div><div class="line">import org.apache.spark.sql.SparkSession;</div><div class="line">import org.apache.spark.sql.Dataset;</div><div class="line"></div><div class="line">public class SimpleApp &#123;</div><div class="line">  public static void main(String[] args) &#123;</div><div class="line">    String logFile = &quot;YOUR_SPARK_HOME/README.md&quot;; // Should be some file on your system</div><div class="line">    SparkSession spark = SparkSession.builder().appName(&quot;Simple Application&quot;).getOrCreate();</div><div class="line">    Dataset&lt;String&gt; logData = spark.read().textFile(logFile).cache();</div><div class="line"></div><div class="line">    long numAs = logData.filter(s -&gt; s.contains(&quot;a&quot;)).count();</div><div class="line">    long numBs = logData.filter(s -&gt; s.contains(&quot;b&quot;)).count();</div><div class="line"></div><div class="line">    System.out.println(&quot;Lines with a: &quot; + numAs + &quot;, lines with b: &quot; + numBs);</div><div class="line"></div><div class="line">    spark.stop();</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这段代码用来计算Spark README文件中包含’a’的行数，和包含’b’的行数。注意你需要将YOUR_SPARK_HOME替换为Spark的安装位置。和之前使用Spark shell不同，Spark shell会初始化它自己的SparkSession，而在代码中初始化SparkSession是程序的一部分。<br>要构建这个程序，我们还需要写一个Maven的pom.xml文件，在这个文件中列出Spark的依赖。注意Spark的依赖和Scala的版本要对应。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&lt;project&gt;</div><div class="line">  &lt;groupId&gt;edu.berkeley&lt;/groupId&gt;</div><div class="line">  &lt;artifactId&gt;simple-project&lt;/artifactId&gt;</div><div class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</div><div class="line">  &lt;name&gt;Simple Project&lt;/name&gt;</div><div class="line">  &lt;packaging&gt;jar&lt;/packaging&gt;</div><div class="line">  &lt;version&gt;1.0&lt;/version&gt;</div><div class="line">  &lt;dependencies&gt;</div><div class="line">    &lt;dependency&gt; &lt;!-- Spark dependency --&gt;</div><div class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">      &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</div><div class="line">      &lt;version&gt;2.3.1&lt;/version&gt;</div><div class="line">    &lt;/dependency&gt;</div><div class="line">  &lt;/dependencies&gt;</div><div class="line">&lt;/project&gt;</div></pre></td></tr></table></figure></p>
<p>我们根据规范列出了Maven的目录结构：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ find .</div><div class="line">./pom.xml</div><div class="line">./src</div><div class="line">./src/main</div><div class="line">./src/main/java</div><div class="line">./src/main/java/SimpleApp.java</div></pre></td></tr></table></figure></p>
<p>现在我们可以使用Maven进行打包，并使用./bin/spark-submit来执行它。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># Package a JAR containing your application</div><div class="line">$ mvn package</div><div class="line">...</div><div class="line">[INFO] Building jar: &#123;..&#125;/&#123;..&#125;/target/simple-project-1.0.jar</div><div class="line"></div><div class="line"># Use spark-submit to run your application</div><div class="line">$ YOUR_SPARK_HOME/bin/spark-submit \</div><div class="line">  --class &quot;SimpleApp&quot; \</div><div class="line">  --master local[4] \</div><div class="line">  target/simple-project-1.0.jar</div><div class="line">...</div><div class="line">Lines with a: 46, Lines with b: 23</div></pre></td></tr></table></figure></p>
<h2 id="Where-to-Go-from-Here"><a href="#Where-to-Go-from-Here" class="headerlink" title="Where to Go from Here"></a>Where to Go from Here</h2><p>恭喜你运行了自己的第一个Spark application！</p>
<blockquote>
<p>对于API的更深了解，可以从<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html" title="RDD programming guide" target="_blank" rel="external">RDD programming guide</a>和<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" title="SQL programming guide" target="_blank" rel="external">SQL programming guide</a>或者查看 ‘Programming Guides’菜单来了解其他组件。<br>想要在集群上运行application，去<a href="http://spark.apache.org/docs/latest/cluster-overview.html" title="deployment overview" target="_blank" rel="external">deployment overview</a>。<br>最后，Spark在examples目录中包含了一些例子（Scala, Java, Python, R）。你可以如下运行它们：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># For Scala and Java, use run-example:</div><div class="line">./bin/run-example SparkPi</div><div class="line"></div><div class="line"># For Python examples, use spark-submit directly:</div><div class="line">./bin/spark-submit examples/src/main/python/pi.py</div><div class="line"></div><div class="line"># For R examples, use spark-submit directly:</div><div class="line">./bin/spark-submit examples/src/main/r/dataframe.R</div></pre></td></tr></table></figure></p>
</blockquote>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/10/spark-2-3-1-MonitoringAndInstrumentation/" itemprop="url">
                  spark_2.3.1_MonitoringAndInstrumentation
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-10T15:20:35+08:00" content="2018-08-10">
              2018-08-10
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/10/spark-2-3-1-ClusterModeOverview/" itemprop="url">
                  Spark 2.3.1 Cluster Mode Overview
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-10T11:17:30+08:00" content="2018-08-10">
              2018-08-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Cluster-Mode-Overview"><a href="#Cluster-Mode-Overview" class="headerlink" title="Cluster Mode Overview"></a>Cluster Mode Overview</h1><p>本文档对Spark如何在集群上运行给出了一个简短的浏览，以便更加容易理解相关组件。通过看application submission guide来学习关于在集群上启动一个applicaiton的信息。</p>
<h2 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h2><p>Spark的application作为一组独立的进程在集群上运行，通过你主程序（被称为driver）中的SparkContext对象来协调合作。<br>具体来说，要在集群上运行application，SparkContext能够连接到某种类型的集群管理器（Spark自己的standalone集群管理器、Mesos或YARN），集群管理器能够跨application分配资源。一旦连接成功，Spark得到集群节点上的executor，这些executor为你的application执行计算以及存储数据。接下来SparkContext发送你的application代码（由传递给你SparkContext的JAR或Python文件定义）到executor。最终，SparkContext发送任务到executor来运行。</p>
<p>此处是图片<img src=""><br>关于这个结构，有一些有用的东西需要注意：</p>
<blockquote>
<p>1、每个application会得到自己的executor进程，这些executor在这个application持续期间保持不变并以多线程运行任务。这样的好处是application彼此隔离，无论是在调度方面（每个driver调度它自己的任务）还是executor方面（来自不同application的任务运行在不同的JVM中）。因此，这也意味着数据在不同的Spark application之间是不能共享的，除非借助其他外部存储。<br>2、Spark与底层集群管理器无关。只要它能够得到executor进程，并且它们能够彼此通信，这样即使是在支持其他application的集群管理器上也能相对简单的运行。<br>3、在driver程序整个生命周期内，它必须监听并接受来自它的executor的连接（查看网路配置章节spark.driver.port）。因此，driver程序对于它的worker节点来说必须是可以迅指的。<br>4、因为driver在集群上调度任务，因此它应该靠近worker节点，最好是在相同的局域网内。如果你想要远程向集群发送请求，最好为你的driver打开一个RPC，让它就近提交，而不是在远离worker节点的地方运行driver。</p>
</blockquote>
<h2 id="Cluster-Manager-Types"><a href="#Cluster-Manager-Types" class="headerlink" title="Cluster Manager Types"></a>Cluster Manager Types</h2><p>系统当前支持3种集群管理器：</p>
<blockquote>
<p>Stangalone - Spark自带的一种集群管理器，使用它能够很容易的构建集群。<br>Apache Mesos - 一个很普遍的集群管理器，它还能够运行Hadoop的MapReduce和服务应用。<br>Hadoop YARN -    Hadoop2种的资源管理器。<br>Kubernetes - 一个开源的系统，用于自动部署、扩展以及管理application。</p>
</blockquote>
<h2 id="Submitting-Applications"><a href="#Submitting-Applications" class="headerlink" title="Submitting Applications"></a>Submitting Applications</h2><p>使用spark-submit脚本可以将application提交到任何类型集群。<a href="/blog/2018/08/09/spark-2-3-1-submit-applications">application submission guide</a>描述了应该如何做。</p>
<h2 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h2><p>每个driver程序都有一个Web UI，端口一般是4040，这个web UI展示了运行的任务、executor已经存储的使用情况。通过在浏览器中输入http://<driver-node>:4040就可以访问这个UI。<a href="http://spark.apache.org/docs/latest/monitoring.html" title="monitoring guide" target="_blank" rel="external">monitoring guide</a>描述了其他的监控项。</driver-node></p>
<h2 id="Job-Scheduling"><a href="#Job-Scheduling" class="headerlink" title="Job Scheduling"></a>Job Scheduling</h2><p>spark给出了两种资源分配，一种是跨applications（在集群管理器级别上），一种是application中（在相同SparkContext上出现多次计算的情况）。<a href="http://spark.apache.org/docs/latest/job-scheduling.html" title="job scheduling overview" target="_blank" rel="external">job scheduling overview</a>描述了详细信息。</p>
<h2 id="Glossary"><a href="#Glossary" class="headerlink" title="Glossary"></a>Glossary</h2><p>下面的表格列出了常用的一些集群概念：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Application</td>
<td style="text-align:left">在Spark上构建的用户程序。由一个driver程序和集群上的executors组成。</td>
</tr>
<tr>
<td style="text-align:left">Application jar</td>
<td style="text-align:left">一个包含了用户的Spark application的jar。在某些情况下用户可能想要创建一个”uber jar”来包含他们的application以及依赖。用户的jar应当不要包含Hadoop或Spark的库，因为这些库会在运行时自动被添加。</td>
</tr>
<tr>
<td style="text-align:left">Driver program</td>
<td style="text-align:left">运行application的main函数并创建SparkContext的进程。</td>
</tr>
<tr>
<td style="text-align:left">Cluster</td>
<td style="text-align:left">一个额外的服务，用来获取集群上的资源（如standalong manager、Mesos或YARN）。</td>
</tr>
<tr>
<td style="text-align:left">Deploy mode</td>
<td style="text-align:left">用来区分在哪里运行driver进程。在“cluster”模式中，系统在集群内部启动driver。在“client”模式中，在集群之外启动driver。</td>
</tr>
<tr>
<td style="text-align:left">Worker node</td>
<td style="text-align:left">集群中任何可以运行application的节点。</td>
</tr>
<tr>
<td style="text-align:left">Executor</td>
<td style="text-align:left">在worker节点上启动的用来处理application的进程，它执行任务并在内存或磁盘上保存数据。每个application都有自己的exectors。</td>
</tr>
<tr>
<td style="text-align:left">Task</td>
<td style="text-align:left">发送给executor的一个工作单元。</td>
</tr>
<tr>
<td style="text-align:left">Job</td>
<td style="text-align:left">由多个tasks组成的一个并行计算，并为一个spark action产生结果（如 save、collect）。 你将会在driver的日志中看到它们。</td>
</tr>
<tr>
<td style="text-align:left">Stage</td>
<td style="text-align:left">每个job被划分为一更小的task，称为stage，这些stage相互依赖（类似MapReduce中map阶段和reduce阶段）。你将会在driver的日志中看到他们。</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/09/spark-2-3-1-submit-applications/" itemprop="url">
                  Spark 2.3.1 Submit Applications
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-09T15:33:36+08:00" content="2018-08-09">
              2018-08-09
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Submitting-Applications"><a href="#Submitting-Applications" class="headerlink" title="Submitting Applications"></a>Submitting Applications</h1><p>Spark的bin目录下的spark-submit脚本用于在一个集群上启动一个应用。通过一个统一的接口，它可以使用所有Spark支持的集群管理器，因此你不需要针对每种集群管理器来单独配置你的应用。</p>
<h2 id="Bunding-Your-Application’s-Dependencies"><a href="#Bunding-Your-Application’s-Dependencies" class="headerlink" title="Bunding Your Application’s Dependencies"></a>Bunding Your Application’s Dependencies</h2><p>如果你的代码依赖其他项目，那么你需要将它们和你的应用一并打包，以便分发代码到一个spark集群。要完成这些，需要创建一个assembly jar(uber jar)来包含你的代码和代码的依赖。sbt和Maven都有assembly插件。当创建assembly jar时，排除Spark和Hadoop提供的依赖，因为这些不需要绑定，因为这些将由集群管理器在运行时提供。一旦你弄好了assembly jar，你就可以如下所示在调用 bin/spark-submit脚本是传递你的jar。<br>对于Python，你可以使用spark-submit的–py-files参数来添加.py、.zip或.egg文件，让他们和你的应用一起分发。如果你依赖多个python文件，我们推荐将他们打到一个.zip或.egg包中。</p>
<h2 id="Launching-Applications-with-spark-submit"><a href="#Launching-Applications-with-spark-submit" class="headerlink" title="Launching Applications with spark-submit"></a>Launching Applications with spark-submit</h2><p>一旦一个用户应用被绑定，就可以使用bin/spark-submit脚本来启动这个应用。这个脚本负责设置Spark的classpath和它依赖，而且脚本支持由Spark支持的不同的集群管理器和部署模式。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit \</div><div class="line">  --class &lt;main-class&gt; \</div><div class="line">  --master &lt;master-url&gt; \</div><div class="line">  --deploy-mode &lt;deploy-mode&gt; \</div><div class="line">  --conf &lt;key&gt;=&lt;value&gt; \</div><div class="line">  ... # other options</div><div class="line">  &lt;application-jar&gt; \</div><div class="line">  [application-arguments]</div></pre></td></tr></table></figure></p>
<p>一些常用的选项：</p>
<blockquote>
<p>–class：你应用的执行程序(如：org.apache.spark.example.SparkPi)<br>–master：集群的master URL（如：spark://23.195.26.187:7077）<br>–deploy-mode：在worker节点(cluster)上部署你的driver还是在本地作为一个额外的客户端(client)来部署。默认是client。<br>–conf：以key=velue格式配置的任意的Spark配置属性。对于包含空格的值，使用双引号包含起来，如”key=value”。<br>application-jar：指向你的应用程序和它依赖的jar的路径。这个URL必须是你集群内部全局可见，例如，一个hdfs://路径或一个在所有节点上都存在的file://路径。<br>application-arguments：任何需要传递给你的主类的主方法的参数。</p>
</blockquote>
<p>常见的部署策略是，在一个与你的worker机位置相同的gateway机器上提交你的应用。在这种设置中，client模式是合适的，driver在spark-submit进程中被直接启动，这种方式像是集群的一个client。这个应用的输入和输出被打印到控制台。因此这种模式特别适合那些涉及REPL的应用。</p>
<p>此外，如果你的应用是用一个远离worker机器的机器上提交的，通常使用cluster模式来降低drivers和executors之间的网络传输。目前，standalone模式还不能够为Python应用提供cluster模式。</p>
<p>对于Python应用，在<application-jar>处传递一个.py来代替一个jar，在–py-files中添加.zip、.egg或.py，作为搜索目录。</application-jar></p>
<p>这里有一些选项可用，用来指定使用的集群管理器。例如，对于cluster部署模式的standalone管理器管理的Saprk集群，你可以指定 –supervise 来保证在非0退出代码时，driver被自动重启。要枚举spark-submit所有可用的选项，使用–help运行spark-submit。这里有些常用的选项：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"># Run application locally on 8 cores</div><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master local[8] \</div><div class="line">  /path/to/examples.jar \</div><div class="line">  100</div><div class="line"></div><div class="line"># Run on a Spark standalone cluster in client deploy mode</div><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master spark://207.184.161.138:7077 \</div><div class="line">  --executor-memory 20G \</div><div class="line">  --total-executor-cores 100 \</div><div class="line">  /path/to/examples.jar \</div><div class="line">  1000</div><div class="line"></div><div class="line"># Run on a Spark standalone cluster in cluster deploy mode with supervise</div><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master spark://207.184.161.138:7077 \</div><div class="line">  --deploy-mode cluster \</div><div class="line">  --supervise \</div><div class="line">  --executor-memory 20G \</div><div class="line">  --total-executor-cores 100 \</div><div class="line">  /path/to/examples.jar \</div><div class="line">  1000</div><div class="line"></div><div class="line"># Run on a YARN cluster</div><div class="line">export HADOOP_CONF_DIR=XXX</div><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master yarn \</div><div class="line">  --deploy-mode cluster \  # can be client for client mode</div><div class="line">  --executor-memory 20G \</div><div class="line">  --num-executors 50 \</div><div class="line">  /path/to/examples.jar \</div><div class="line">  1000</div><div class="line"></div><div class="line"># Run a Python application on a Spark standalone cluster</div><div class="line">./bin/spark-submit \</div><div class="line">  --master spark://207.184.161.138:7077 \</div><div class="line">  examples/src/main/python/pi.py \</div><div class="line">  1000</div><div class="line"></div><div class="line"># Run on a Mesos cluster in cluster deploy mode with supervise</div><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master mesos://207.184.161.138:7077 \</div><div class="line">  --deploy-mode cluster \</div><div class="line">  --supervise \</div><div class="line">  --executor-memory 20G \</div><div class="line">  --total-executor-cores 100 \</div><div class="line">  http://path/to/examples.jar \</div><div class="line">  1000</div><div class="line"></div><div class="line"># Run on a Kubernetes cluster in cluster deploy mode</div><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master k8s://xx.yy.zz.ww:443 \</div><div class="line">  --deploy-mode cluster \</div><div class="line">  --executor-memory 20G \</div><div class="line">  --num-executors 50 \</div><div class="line">  http://path/to/examples.jar \</div><div class="line">  1000</div></pre></td></tr></table></figure></p>
<h2 id="Master-URLs"><a href="#Master-URLs" class="headerlink" title="Master URLs"></a>Master URLs</h2><p>传递给Spark的master URL可以是下面格式中的一个：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Master URL</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">local</td>
<td style="text-align:left">使用单个线程以本地模式运行Spark</td>
</tr>
<tr>
<td style="text-align:left">local[K]</td>
<td style="text-align:left">使用K个线程以本地模式运行Spark</td>
</tr>
<tr>
<td style="text-align:left">local[K, F]</td>
<td style="text-align:left">使用K个线程以本地模式运行Spark，允许最多F个失败</td>
</tr>
<tr>
<td style="text-align:left">local[*]</td>
<td style="text-align:left">使用与你机器逻辑核数相同的线程，以本地模式运行Spark</td>
</tr>
<tr>
<td style="text-align:left">local[*, F]</td>
<td style="text-align:left">使用于你机器逻辑核数相同的香橙，以本地模式运行Spark，允许最多F个失败</td>
</tr>
<tr>
<td style="text-align:left">spark://HOST:PORT</td>
<td style="text-align:left">连接到给定的以standalone模式运行的集群的Master。端口必须是你的master所配置使用的，默认为7077</td>
</tr>
<tr>
<td style="text-align:left">spark://HOST1:PORT1,HOST2:PORT2</td>
<td style="text-align:left">连接到使用了Zookeeper以standalone模式运行的带有standby master的集群。这个列表必须包含了使用Zookeeper配置的高可用集群的所有master的host。端口必须是你的master所配置使用的，默认为7077。</td>
</tr>
<tr>
<td style="text-align:left">mesos://HOST:PORT</td>
<td style="text-align:left">连接到给定的以MESOS模式运行的集群。端口必须是你的配置中使用的，默认为5050。或者，对于使用了Zookeeper的Mesos集群，使用mesos://zk://…配合–deploy-mode cluster来提交，HOST:PORT应该被配置为连接到MesosClusterDispatcher。</td>
</tr>
<tr>
<td style="text-align:left">yarn</td>
<td style="text-align:left">以cluster或client模式连接到yarn集群，连接模式通过 –deploy-mode来指定。这个集群的位置将基于HADOOP_CONF_DIR或YARN_CONF_DIR变量来找到。</td>
</tr>
<tr>
<td style="text-align:left">k8s://HOST:PORT</td>
<td style="text-align:left">以cluster模式连接到Kubernetes集群。Client模式当前还不支持，将会在未来被支持。HOST和PORT指向[Kubernetes API Server]。默认使用TLS连接。想要强制使用不安全的连接，你可以使用k8s://<a href="http://HOST:PORT。" target="_blank" rel="external">http://HOST:PORT。</a></td>
</tr>
</tbody>
</table>
<h2 id="Loading-Configuration-from-a-File"><a href="#Loading-Configuration-from-a-File" class="headerlink" title="Loading Configuration from a File"></a>Loading Configuration from a File</h2><p>spark-submit脚本能够从一个属性文件中加载默认的Spark配置属性值，并传递它们到你的应用。默认它将从<br>Spark目录的conf/spark-defaults.conf中读取选项。<br>加载默认Spark配置，这种方式可以避免给spark-submit设置有确切值的选项（有些选项的值是固定的）。例如，如果设置了spark.master属性，你就可以在spark-submit中忽略–master项了。通常，在SparkConf中设置的值具有最高优先级，其次是传递给spark-submit的值，最后是默认文件里的值。</p>
<p>如果你无法确认配置项的值来自哪里，你可以在运行spark-submit是使用-verbose选项，将细粒度的调试信息打印出来。</p>
<h2 id="Advanced-Dependency-Management"><a href="#Advanced-Dependency-Management" class="headerlink" title="Advanced Dependency Management"></a>Advanced Dependency Management</h2><p>在使用spark-submit的时候，应用程序jar以及使用–jars选项包含的人和jar将会自动传输到集群。–jars后面提供的URLs必须以逗号分隔。那个列表被包含在driver和executor的classpath中。目录范围在–jars中不起作用。<br>Spark使用如下的URL模式来允许不同的策略传递jar：</p>
<blockquote>
<p>file: 绝对路径，并且file:/ URLs由driver的HTTP文件服务提供服务，每个executor从driver的HTTP服务拉取文件。<br>hdfs:、http:、https:、ftp: 这些按照期望的那样从URI拉取文件和Jars。<br>local: 一个以local:/开头的URI，希望作为每个worker节点上的本地文件而存在。这意味着将不会发生网络IO。这种适用于将较大文件或jar推送到每个worker或通过NFS、GlusterFS等共享较大文件或Jar的方式。</p>
</blockquote>
<p>注意，JARs和文件会为每个运行在executor节点上的SparkContext拷贝一份到工作目录。随着时间的推移，这将耗费大量的空间，因此需要清理。对于使用YARN的方式，清理将会自动方式；对于使用standalone方式的，自动清理工作可以通过spark.worker.cleanup.appDataTtl属性配置。</p>
<p>用户还可以通过使用-packages提供以逗号分隔的Maven坐标列表来包含任何其他依赖。使用此命令时，所有传递的依赖都将被处理。另外，使用–repositories选项，还可以用来添加maven库。多个库之间使用逗号分隔。这些命令可以被pyspark、spark-shell以及spark-submit来使用来包含Saprk包。<br>对于Python，–py-files选项可以被用来分发.egg、.zip以及.py文件到executors。</p>
<h2 id="More-Information"><a href="#More-Information" class="headerlink" title="More Information"></a>More Information</h2><p>一旦你部署了你的应用，cluster mode overview 描述了分布式执行中的各个组件，以及如何监控和调试应用。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/09/spark-2-3-1-overview/" itemprop="url">
                  Spark 2.3.1 Overview
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-09T12:45:35+08:00" content="2018-08-09">
              2018-08-09
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Spark-Overview"><a href="#Spark-Overview" class="headerlink" title="Spark Overview"></a>Spark Overview</h1><p>Apache Spark是一个很快的用于一般目的的集群计算系统。它在Java、Python和R语言上提供了高级别的API，并且提供了一个支持一般图计算的优化引擎。它还提供了一组丰富的高级别的工具，包括SQL和结构化数据处理所需要的Spark SQL、机器学习所需要的MLlib、图处理所需要的GraphX以及Spark Streaming。</p>
<h2 id="Downloading"><a href="#Downloading" class="headerlink" title="Downloading"></a>Downloading</h2><p>从项目网站的下载页获取Spark。这个文档为是针对的Spark2.3.1版本。Spark为了使用HDFS和YARN使用了Hadoop客户端库。这个下载中预置了一些常用的Hadoop版本。用户还可以下载一个”Hadoop free”库通过Spark的classpath指定Hadoop版本来运行Spark。Scala和Java用户可以在自己的项目的中使用Spark的Mave依赖来包含Spark，而Python用户在未来也可以从PyPI中安装Spark。</p>
<p>如果你喜欢从源码构建Spark，可以通过这个链接来操作。</p>
<p>Spark能够运行在Windowns和类UNIX的系统上。在一台机器上以本地模式运行很容易–你需要做的事情就是在你的系统路径中安装java或者在环境变量JAVA_HOME中指向Java的安装。</p>
<p>Spark运行在Java 8+， Python 2.7+/3.4+或R 3.1+上。对于Scala API，Spark2.3.1使用的是Scala2.11。你需要使用一个合适Scala版本（Scala2.11+）。</p>
<p>注意，对于Java 7、Python 2.6以及2.6.5以前的Hadoop版本的支持，已经在Spark 2.2.0中移除。对于Scala2.10版本的支持在Spark 2.3.0中移除了。</p>
<h2 id="Running-the-Examples-and-Shell"><a href="#Running-the-Examples-and-Shell" class="headerlink" title="Running the Examples and Shell"></a>Running the Examples and Shell</h2><p>Spark带有一些简单的样例程序。Scala、Java和R的样例都在examples/src/main目录下。想要运行一个Java或Scala样例程序，需要使用顶级Spark目录下bin/run-example <class> [params]。如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/run-example SparkPi 10</div></pre></td></tr></table></figure></class></p>
<p>你还可以通过Scala shell的一个修改版，以交互的方式运行Saprk。这对于学习这个框架是很好的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-shell --master local[2]</div></pre></td></tr></table></figure></p>
<p>其中的–master选项指定了一个分布式集群的master的URL，或者使用一个线程以本地模式运行，或者local[N]表示使用N个线程以本地模式运行。你可以从使用本地模式做测试来开始。对于选项的全部列表，使用使用–help选项来运行Spark shell。<br>Spark还提供了一个Python的API。想要在Python解析器中以交互方式运行Spark，可以使用 bin/pyspark：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/pyspark --master local[2]</div></pre></td></tr></table></figure></p>
<p>样例application也提供了Python版本。如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit examples/src/main/python/pi.py 10</div></pre></td></tr></table></figure></p>
<p>从Spark1.4开始Spark也提供了R API的样例。要以R解析器中以交互方式运行Spark，可以使用 bin/sparkR:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/sparkR --master local[2]</div></pre></td></tr></table></figure></p>
<p>样例程序同样也提供了R语言版本的，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit examples/src/main/r/dataframe.R</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/07/13/HiveStudy/" itemprop="url">
                  Hive Study
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-07-13T15:53:05+08:00" content="2018-07-13">
              2018-07-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">bigdata</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文用来记录自己在使用Hive Sql方面的一些经验。</p>
<h1 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h1><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 创建了一个带有两个分区的表，这个表按照partition_date和hour进行分区</div><div class="line">CREATE EXTERNAL TABLE `user.user_action`(</div><div class="line">     `action` string COMMENT '&#123;"chs_name":"", "description":"","etl":"","value":"","remark":""&#125;',</div><div class="line">     `num` double comment '&#123;"chs_name":"", "description":"","etl":"","value":"","remark":""&#125;'</div><div class="line">  )</div><div class="line">PARTITIONED BY ( `partition_date` string COMMENT '分区日期',  `hour` string COMMENT '小时')</div><div class="line">ROW FORMAT DELIMITED</div><div class="line">    --TODO: 导入MYSQL的表建议'\t'分隔</div><div class="line">    FIELDS TERMINATED BY '\t'</div><div class="line">    COLLECTION ITEMS TERMINATED BY '\002'</div><div class="line">    MAP KEYS TERMINATED BY '\003'</div><div class="line">    LINES TERMINATED BY '\n'</div><div class="line">STORED as textfile;</div></pre></td></tr></table></figure>
<h1 id="查询数据并将数据写入到表中"><a href="#查询数据并将数据写入到表中" class="headerlink" title="查询数据并将数据写入到表中"></a>查询数据并将数据写入到表中</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> user.user_action</div><div class="line"><span class="keyword">partition</span>(partition_date = <span class="string">'20180602'</span>, <span class="keyword">hour</span>=<span class="string">'0'</span>)</div><div class="line"><span class="keyword">select</span> <span class="keyword">action</span>,</div><div class="line">       <span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> n</div><div class="line">  <span class="keyword">from</span> (</div><div class="line">        <span class="keyword">select</span> <span class="keyword">action</span>,</div><div class="line">               <span class="keyword">num</span></div><div class="line">          <span class="keyword">from</span> (</div><div class="line">               <span class="keyword">select</span> momo_id, </div><div class="line">                      event_num_map </div><div class="line">                 <span class="keyword">from</span> db.event_summary </div><div class="line">                <span class="keyword">where</span> partition_date = <span class="string">'20180602'</span></div><div class="line">                  <span class="keyword">and</span> <span class="keyword">size</span>(event_num_map)&gt;<span class="number">0</span></div><div class="line">          )a</div><div class="line">          LATERAL <span class="keyword">VIEW</span> EXPLODE(event_num_map)t <span class="keyword">AS</span> <span class="keyword">action</span>, <span class="keyword">num</span></div><div class="line">    </div><div class="line">  )b</div><div class="line">  <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">action</span>;</div></pre></td></tr></table></figure>
<p>该表从db.event_summary中查询数据然后吸入到user.user_action表中。需要注意db.event_summay中的event_num_map字段是一个map，map的key是action，value是action的数量。这里使用了一个函数LATERAL VIEW EXPLODE，用来map展开。</p>
<h1 id="一些常用函数"><a href="#一些常用函数" class="headerlink" title="一些常用函数"></a>一些常用函数</h1><h2 id="ROW-NUMBER-OVER-函数"><a href="#ROW-NUMBER-OVER-函数" class="headerlink" title="ROW_NUMBER() OVER()函数"></a>ROW_NUMBER() OVER()函数</h2><p>ROW_NUMBER() OVER()函数用来为每条记录返回一个行号，可以用来对记录进行排序并返回该序号，需要从1开始排序。<br>OVER()是一个聚合函数，可以对记录进行分组和排序。ROW_NUMBER()不能单独使用，必须搭配OVER()才能使用，否则会报错。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select *, row_number() over() as r from mytable;</div></pre></td></tr></table></figure></p>
<p>配合partition by/order by<br>按照某个字段排序后返回行号<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select *, row_number() over(partition by aaaaab order by num desc) r from mytable;</div></pre></td></tr></table></figure></p>
<p>按照aaaaab分组后，并根据aaaaaab进行倒序排列。</p>
<h1 id="SQL中的类型转换"><a href="#SQL中的类型转换" class="headerlink" title="SQL中的类型转换"></a>SQL中的类型转换</h1><p>需要使用cast()函数进行类型转换。</p>
<blockquote>
<p>cast(str_column as int) </p>
</blockquote>
<h1 id="一些经验的总结"><a href="#一些经验的总结" class="headerlink" title="一些经验的总结"></a>一些经验的总结</h1><h2 id="一个表中分时段记录内容的统一查询"><a href="#一个表中分时段记录内容的统一查询" class="headerlink" title="一个表中分时段记录内容的统一查询"></a>一个表中分时段记录内容的统一查询</h2><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>遇到的情况是这样的，有一个表A，表A中有24个字段（event_0_map … event_24_map）用来记录对应小时内每个用户各自发生的一些事情的数量。表结构如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">id string = 1000010</div><div class="line">event_0_map = &#123;&apos;event0&apos;:200, &apos;event2&apos;:100&#125;</div><div class="line">...</div><div class="line">event_24_map = &#123;&apos;event0&apos;:500, &apos;event2&apos;:800&#125;</div><div class="line">partition = &apos;20180101&apos;</div></pre></td></tr></table></figure></p>
<p>现在有一个需求：需要统计每个小时发生事件最多的前100个事件</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>因为是每个小时执行的任务，而且每个小时的数据是存放在不同的字段里面，而字段名在SQL中是不可以拼接的，如：event_24_map，无法来拼接，因此有两种方案。</p>
<h4 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h4><p>生成24个任务，每个任务的SQL都一样，只是查询的字段不一样<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">select action,</div><div class="line">       num</div><div class="line">  from (</div><div class="line">       select id, </div><div class="line">              event_0_map </div><div class="line">         from online.tableA </div><div class="line">        where partition = &apos;$&#123;partition_date&#125;&apos;</div><div class="line">          and size(event_0_map)&gt;0</div><div class="line">  )a0</div><div class="line">  LATERAL VIEW EXPLODE(event_0_map)t AS action, num</div></pre></td></tr></table></figure></p>
<h4 id="方案二-推荐"><a href="#方案二-推荐" class="headerlink" title="方案二(推荐)"></a>方案二(推荐)</h4><p>将所有的字段同时解析，生成一个大表，再对大表进行过滤查询<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">select action,</div><div class="line">       num</div><div class="line">  from (</div><div class="line">		select action,</div><div class="line">		       num,</div><div class="line">		       &apos;00&apos; as hour</div><div class="line">		  from (</div><div class="line">		       select id, </div><div class="line">		              event_0_map </div><div class="line">		         from online.tableA </div><div class="line">		        where partition = &apos;$&#123;partition_date&#125;&apos;</div><div class="line">		          and size(event_0_map)&gt;0</div><div class="line">		  )a0</div><div class="line">		  LATERAL VIEW EXPLODE(event_0_map)t AS action, num</div><div class="line">		union all</div><div class="line">		select action,</div><div class="line">		       num,</div><div class="line">		       &apos;01&apos; as hour</div><div class="line">		  from (</div><div class="line">		       select id, </div><div class="line">		              event_2_map </div><div class="line">		         from online.tableA </div><div class="line">		        where partition = &apos;$&#123;partition_date&#125;&apos;</div><div class="line">		          and size(event_2_map)&gt;0</div><div class="line">		  )a1</div><div class="line">		  LATERAL VIEW EXPLODE(event_2_map)t AS action, num</div><div class="line">	) data</div><div class="line"> where hour = &apos;$&#123;partition_hour&#125;&apos;</div></pre></td></tr></table></figure></p>
<h2 id="表的删除和恢复"><a href="#表的删除和恢复" class="headerlink" title="表的删除和恢复"></a>表的删除和恢复</h2><p>在使用Hive的表的过程中，难免会有对表进行删除的情况，其实把表删除后，数据文件还是存在的，那么如何将数据按照新表的结构恢复一下呢？可以如下操作，但是需要注意的是，对于新增的字段，值是NULL。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">drop table db.table_test;</div><div class="line">...</div><div class="line">create table xxx...</div><div class="line">...</div><div class="line">MSCK REPAIR TABLE db.table_test;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/07/10/easyUseMapreduce/" itemprop="url">
                  Easy Use Mapreduce
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-07-10T15:19:55+08:00" content="2018-07-10">
              2018-07-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">bigdata</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文用来记录MR的使用，已经遇到的一些问题和解决方法</p>
<p>#使用Python执行MR</p>
<h2 id="Mapper的写法"><a href="#Mapper的写法" class="headerlink" title="Mapper的写法"></a>Mapper的写法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#! /usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="comment">#加载编码</span></div><div class="line">reload(sys)</div><div class="line">sys.setdefaultencoding(<span class="string">'utf-8'</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">    j = json.loads(line.strip())</div><div class="line">    <span class="keyword">print</span> <span class="string">"%s\t%s"</span> % (j.get(<span class="string">"name"</span>), j.get(<span class="string">"age"</span>))</div></pre></td></tr></table></figure>
<h2 id="Reducer的写法"><a href="#Reducer的写法" class="headerlink" title="Reducer的写法"></a>Reducer的写法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#! /usr/bin/env python</span></div><div class="line">  <span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line">  </div><div class="line">  <span class="keyword">import</span> sys</div><div class="line">  <span class="keyword">import</span> json</div><div class="line">  <span class="keyword">import</span> time</div><div class="line">  </div><div class="line">  <span class="comment">#加载编码</span></div><div class="line">  reload(sys)</div><div class="line">  sys.setdefaultencoding(<span class="string">'utf-8'</span>)</div><div class="line">  </div><div class="line">  uri_count = &#123;&#125;</div><div class="line">  <span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">      data = line.strip().split(<span class="string">"\t"</span>)</div><div class="line">      key = <span class="string">"%s-%s"</span> % (data[<span class="number">0</span>], data[<span class="number">1</span>])</div><div class="line">      c = uri_count.get(key, <span class="number">0</span>)</div><div class="line">      uri_count[key] = c + <span class="number">1</span></div><div class="line">  </div><div class="line">  <span class="keyword">for</span> key <span class="keyword">in</span> uri_count:</div><div class="line">      <span class="keyword">print</span> <span class="string">"%s\t%s"</span> % (key, uri_count.get(key, <span class="number">0</span>))</div></pre></td></tr></table></figure>
<p>从上面的代码可以看出来，python的脚本需要从标准输入(sys.stdin)中接入数据。</p>
<h2 id="执行Mapreduce"><a href="#执行Mapreduce" class="headerlink" title="执行Mapreduce"></a>执行Mapreduce</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">/home/hadoop/yarn-2.8.1/bin/hadoop jar /home/hadoop/yarn-2.8.1/share/hadoop/tools/lib/hadoop-streaming-2.8.1.jar \</div><div class="line">-D mapreduce.job.queuename=bigdata.queue \</div><div class="line">-input hdfs://nameservice1/data/mylogs/api_request/2018/07/03/*/* \</div><div class="line">-output /tmp/20180703SpecialUri \</div><div class="line">-mapper &quot;specialUriMapper.py&quot; \</div><div class="line">-reducer &quot;specialUriReducer.py&quot; \</div><div class="line">-file /home/hadoop/script/user_action/specialUriMapper.py \</div><div class="line">-file /home/hadoop/script/user_action/specialUriReducer.py \</div><div class="line">-file /home/hadoop/script/user_action/kickA.log</div></pre></td></tr></table></figure>
<p>参数说明：</p>
<blockquote>
<p>-D mapreduce.job.queuename用指定需要运行MR的队列<br>-input MR的输入<br>-output MR的输出<br>-mapper 指定执行MR中Mapper的程序<br>-reducer 指定执行MR中Reducer的程序<br>-file 需要一起上传的文件，如果python程序中使用了其他的数据文件，可以通过这个参数一起上传。</p>
</blockquote>
<p>其他一些参数：</p>
<blockquote>
<p>-D mapreduce.job.name Job的名称<br>-D mapreduce.job.user.name<br>-D mapreduce.job.node-label-expression<br>-D mapreduce.job.queuename<br>-D mapreduce.map.memory.mb<br>-D mapreduce.reduce.memory.mb</p>
</blockquote>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/06/29/netty-study/" itemprop="url">
                  netty-study
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-06-29T11:15:01+08:00" content="2018-06-29">
              2018-06-29
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/netty/" itemprop="url" rel="index">
                    <span itemprop="name">netty</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>NETTY学习笔记</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/05/24/kafka-script/" itemprop="url">
                  kafka-script
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-05-24T17:53:42+08:00" content="2017-05-24">
              2017-05-24
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文主要讨论kafka服务的相关启动和关闭脚本。</p>
<h1 id="kafka-server-start-sh"><a href="#kafka-server-start-sh" class="headerlink" title="kafka-server-start.sh"></a>kafka-server-start.sh</h1><p>Kafka服务的启动脚本，正确的用法为 kafka-server-start.sh [-daemon] server.properties<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"># 如果执行脚本时传入的参数小于1个，则退出执行并提示用户需要指定服务属性配置文件， 此处也说明了执行kafka-server-start.sh的正确用法</div><div class="line">if [ $# -lt 1 ];</div><div class="line">then</div><div class="line">	echo &quot;USAGE: $0 [-daemon] server.properties&quot;</div><div class="line">	exit 1</div><div class="line">fi</div><div class="line"></div><div class="line"># $0 表示的是当前shell的文件名，dirname用来获取当前shell文件的所在目录</div><div class="line">base_dir=$(dirname $0)</div><div class="line"></div><div class="line"># 读取环境变量中的KAFKA_LOG4J_OPTS的信息，如果没有配置该环境变量，则将kafka目录下conf中的log4j.properties作为配置添加到环境变量中，配置给KAFKA_LOG4J_OPTS</div><div class="line">if [ &quot;x$KAFKA_LOG4J_OPTS&quot; = &quot;x&quot; ]; then</div><div class="line">    export KAFKA_LOG4J_OPTS=&quot;-Dlog4j.configuration=file:$base_dir/../config/log4j.properties&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 读取环境变量中KAFKA_HEAP_OPTS的信息，如果没有配置该环境变量，则使用默认配置&quot;-Xmx1G -Xms1G&quot;来配置，并添加到环境变量&quot;KAFKA_HEAP_OPTS&quot;中</div><div class="line">if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then</div><div class="line">    export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 定义一个额外的参数 name，为kafka服务指定了进程名</div><div class="line">EXTRA_ARGS=&quot;-name kafkaServer -loggc&quot;</div><div class="line"></div><div class="line"># 如果服务要作为后台进程运行，则需要添加-daemon参数，而且这个参数必须是第一个参数，如果第一个参数是-daemon，则为进程添加自定义的名称</div><div class="line">COMMAND=$1</div><div class="line">case $COMMAND in</div><div class="line">  -daemon)</div><div class="line">    EXTRA_ARGS=&quot;-daemon &quot;$EXTRA_ARGS</div><div class="line">    shift</div><div class="line">    ;;</div><div class="line">  *)</div><div class="line">    ;;</div><div class="line">esac</div><div class="line"></div><div class="line"># 启动kafka服务，由此处也可以看出来，可以使用kafka-run-class.sh来执行相关的类，其中$@表示的是命令行传入的所有参数，这里要启动的类名为kafka.Kafka</div><div class="line">exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka $@</div></pre></td></tr></table></figure></p>
<h1 id="kafka-server-stop-sh"><a href="#kafka-server-stop-sh" class="headerlink" title="kafka-server-stop.sh"></a>kafka-server-stop.sh</h1><p>Kafka服务的停止脚本，其实就是查找KafkaServer对应的进程号，并kill。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 在进程中过滤包含&quot;kafka.Kafka&quot;且不包含&quot;grep&quot;的java进程，截取进程号kill掉</div><div class="line">ps ax | grep -i &apos;kafka\.Kafka&apos; | grep java | grep -v grep | awk &apos;&#123;print $1&#125;&apos; | xargs kill -SIGTERM</div></pre></td></tr></table></figure></p>
<h1 id="kafka-run-class-sh"><a href="#kafka-run-class-sh" class="headerlink" title="kafka-run-class.sh"></a>kafka-run-class.sh</h1><p>kafka-run-class.sh是用来运行class的脚本。正确的用法为 kafka-run-class.sh [-daemon] [-name servicename] [-loggc] classname [opts]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div></pre></td><td class="code"><pre><div class="line"># 验证kafka-run-class脚本的参数</div><div class="line">if [ $# -lt 1 ];</div><div class="line">then</div><div class="line">  echo &quot;USAGE: $0 [-daemon] [-name servicename] [-loggc] classname [opts]&quot;</div><div class="line">  exit 1</div><div class="line">fi</div><div class="line"></div><div class="line"># 获取Kafka的基目录，就是当前目录（bin）的上一层目录</div><div class="line">base_dir=$(dirname $0)/..</div><div class="line"></div><div class="line"># 创建Kafka的日志目录，首先从环境变量“LOG_DIR”中读取，如果没有配置LOG_DIR，则使用Kafka基目录下的logs目录作为日志目录</div><div class="line"># create logs directory</div><div class="line">if [ &quot;x$LOG_DIR&quot; = &quot;x&quot; ]; then</div><div class="line">    LOG_DIR=&quot;$base_dir/logs&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 如果日志目录目存在则创建日志目录</div><div class="line">if [ ! -d &quot;$LOG_DIR&quot; ]; then</div><div class="line">    mkdir -p &quot;$LOG_DIR&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 获取Scala的版本号，首先从环境变量 SCALA_VERSION 中读取，如果没有配置，则使用默认值 2.10.4</div><div class="line">if [ -z &quot;$SCALA_VERSION&quot; ]; then</div><div class="line">	SCALA_VERSION=2.10.4</div><div class="line">fi</div><div class="line"></div><div class="line"># 获取Scala库的版本号，首先从环境变量 SCALA_BINARY_VERSION 中读取，如果没有配置，则使用默认值 2.10</div><div class="line">if [ -z &quot;$SCALA_BINARY_VERSION&quot; ]; then</div><div class="line">	SCALA_BINARY_VERSION=2.10</div><div class="line">fi</div><div class="line"></div><div class="line"># 这里开始加载各种依赖的jar包，并将这些jar包添加到CLASSPATH环境变量中，由此也可以看出运行完整的Kafka服务（支持各种consumer／producer）需要依赖的jar包</div><div class="line"># run ./gradlew copyDependantLibs to get all dependant jars in a local dir</div><div class="line"></div><div class="line"># 将Kafka依赖Scala的jar包添加到CLASSPATH中</div><div class="line">for file in $base_dir/core/build/dependant-libs-$&#123;SCALA_VERSION&#125;*/*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka的示例jar添加到CLASSPATH中</div><div class="line">for file in $base_dir/examples/build/libs//kafka-examples*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将kafka的hadoop consumer相关jar包添加到CLASSPATH中</div><div class="line">for file in $base_dir/contrib/hadoop-consumer/build/libs//kafka-hadoop-consumer*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka的hadoop producer相关jar包添加到CLASSPATH中</div><div class="line">for file in $base_dir/contrib/hadoop-producer/build/libs//kafka-hadoop-producer*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka客户端相关的jar包添加到CLASSPATH中</div><div class="line">for file in $base_dir/clients/build/libs/kafka-clients*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka的libs下的jar包添加到CLASSPATH中</div><div class="line"># classpath addition for release</div><div class="line">for file in $base_dir/libs/*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka依赖的Scala对应版本的库添加到CLASSPATH中</div><div class="line">for file in $base_dir/core/build/libs/kafka_$&#123;SCALA_BINARY_VERSION&#125;*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 以下是Java管理扩展的设置</div><div class="line"># 如果没有在环境变量中设置KAFKA_JMX_OPTS，则将Kafka的JMX配置关闭</div><div class="line"># JMX settings</div><div class="line">if [ -z &quot;$KAFKA_JMX_OPTS&quot; ]; then</div><div class="line">  KAFKA_JMX_OPTS=&quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false  -Dcom.sun.management.jmxremote.ssl=false &quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 如果设置了KAFKA_JMX_OPTS环境变量，则利用这个值来设置变量KAFKA_JMX_OPTS的值，该值用于指定虚拟机的信息</div><div class="line"># JMX port to use</div><div class="line">if [  $JMX_PORT ]; then</div><div class="line">  KAFKA_JMX_OPTS=&quot;$KAFKA_JMX_OPTS -Dcom.sun.management.jmxremote.port=$JMX_PORT &quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># Log4j的配置</div><div class="line"># Log4j settings 如果环境变量中没有设置KAFKA_LOG4J_OPTS，则使用Kafka基目录下conf/tools-log4j.properties来设置KAFKA_LOG4J_OPTS变量</div><div class="line">if [ -z &quot;$KAFKA_LOG4J_OPTS&quot; ]; then</div><div class="line">  KAFKA_LOG4J_OPTS=&quot;-Dlog4j.configuration=file:$base_dir/config/tools-log4j.properties&quot;</div><div class="line">fi</div><div class="line"># 根据环境变量LOG_DIR和KAFKA_LOG4J_OPTS来生成变量KAFKA_LOG4J_OPTS的新的值</div><div class="line">KAFKA_LOG4J_OPTS=&quot;-Dkafka.logs.dir=$LOG_DIR $KAFKA_LOG4J_OPTS&quot;</div><div class="line"></div><div class="line"># 判断环境变量KAFKA_OPTS是否有相关设置</div><div class="line"># Generic jvm settings you want to add</div><div class="line">if [ -z &quot;$KAFKA_OPTS&quot; ]; then</div><div class="line">  KAFKA_OPTS=&quot;&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 判断环境变量JAVA_HOME中是否有值，如果不存在则使用默认的java，如果有，则使用该目录下指定的java</div><div class="line"># Which java to use</div><div class="line">if [ -z &quot;$JAVA_HOME&quot; ]; then</div><div class="line">  JAVA=&quot;java&quot;</div><div class="line">else</div><div class="line">  JAVA=&quot;$JAVA_HOME/bin/java&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># Kafka的内存配置，如果环境变量KAFKA_HEAP_OPTS的值为空，则设置值为默认值-Xmx256M</div><div class="line"># Memory options</div><div class="line">if [ -z &quot;$KAFKA_HEAP_OPTS&quot; ]; then</div><div class="line">  KAFKA_HEAP_OPTS=&quot;-Xmx256M&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 如果没有设置环境变量KAFKA_JVM-PERFORMANCE_OPTS，则使用默认值进行配置</div><div class="line"># JVM performance options</div><div class="line">if [ -z &quot;$KAFKA_JVM_PERFORMANCE_OPTS&quot; ]; then</div><div class="line">  KAFKA_JVM_PERFORMANCE_OPTS=&quot;-server -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -Djava.awt.headless=true&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"></div><div class="line"># 这里对脚本传入的参数进行解析，提取守护进程名／是否后台运行／GC日志这个三个信息</div><div class="line"># 第一个case，如果循环到了-name参数，则读取-name的下一参数，下一个参数必定是后台进程的名字，而且控制台的输出日志文件也是该名字</div><div class="line"># 第二个case，如果循环到了-loggc，则表示要记录GC日志，记录GC日志的另一个要求是配置KAFKA_GC_LOG_OPTS环境变量</div><div class="line"># 第三个case，如果循环到了-daemon，则表示服务以后台进程的方式运行</div><div class="line">while [ $# -gt 0 ]; do</div><div class="line">  COMMAND=$1</div><div class="line">  case $COMMAND in</div><div class="line">    -name)</div><div class="line">      DAEMON_NAME=$2</div><div class="line">      CONSOLE_OUTPUT_FILE=$LOG_DIR/$DAEMON_NAME.out</div><div class="line">      shift 2</div><div class="line">      ;;</div><div class="line">    -loggc)</div><div class="line">      if [ -z &quot;$KAFKA_GC_LOG_OPTS&quot;] ; then</div><div class="line">        GC_LOG_ENABLED=&quot;true&quot;</div><div class="line">      fi</div><div class="line">      shift</div><div class="line">      ;;</div><div class="line">    -daemon)</div><div class="line">      DAEMON_MODE=&quot;true&quot;</div><div class="line">      shift</div><div class="line">      ;;</div><div class="line">    *)</div><div class="line">      break</div><div class="line">      ;;</div><div class="line">  esac</div><div class="line">done</div><div class="line"></div><div class="line"></div><div class="line"># 如果启用了GC日志，GC日志的名字为后台进程的名字[-name指定]-gc.log。</div><div class="line"># GC options</div><div class="line">GC_FILE_SUFFIX=&apos;-gc.log&apos;</div><div class="line">GC_LOG_FILE_NAME=&apos;&apos;</div><div class="line">if [ &quot;x$GC_LOG_ENABLED&quot; = &quot;xtrue&quot; ]; then</div><div class="line">  GC_LOG_FILE_NAME=$DAEMON_NAME$GC_FILE_SUFFIX</div><div class="line">  KAFKA_GC_LOG_OPTS=&quot;-Xloggc:$LOG_DIR/$GC_LOG_FILE_NAME -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps &quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 启动Java进程，将上面的所有信息整合在一起，使用指定的Java，还有各种参数，这里区分了运行模式，其实就是将进程作为后台进程运行还是前台进程运行而已</div><div class="line"># Launch mode</div><div class="line">if [ &quot;x$DAEMON_MODE&quot; = &quot;xtrue&quot; ]; then</div><div class="line">  nohup $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS &quot;$@&quot; &gt; &quot;$CONSOLE_OUTPUT_FILE&quot; 2&gt;&amp;1 &lt; /dev/null &amp;</div><div class="line">else</div><div class="line">  exec $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS &quot;$@&quot;</div><div class="line">fi</div></pre></td></tr></table></figure></p>
<h1 id="kafka-topics-sh"><a href="#kafka-topics-sh" class="headerlink" title="kafka-topics.sh"></a>kafka-topics.sh</h1><p>kafka-topics.sh是用来操作Kafka的Topic的脚本，其内部通过kafka-run-class.sh脚本来调用kafka.admin.TopicCommand来实现Topic的操作。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">exec $(dirname $0)/kafka-run-class.sh kafka.admin.TopicCommand $@</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/03/08/redis-lua/" itemprop="url">
                  redis_lua
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-08T10:32:34+08:00" content="2017-03-08">
              2017-03-08
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/blog/page/2/">2</a><a class="page-number" href="/blog/page/3/">3</a><a class="extend next" rel="next" href="/blog/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/blog/uploads/avatar.png"
               alt="baimoon" />
          <p class="site-author-name" itemprop="name">baimoon</p>
          <p class="site-description motion-element" itemprop="description">Baimoon's blog</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/blog/archives">
              <span class="site-state-item-count">45</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/blog/categories">
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/blog/tags">
                <span class="site-state-item-count">23</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/baimoon" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://gallery.xrange.org" title="xrange" target="_blank">xrange</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016-07 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">baimoon</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/blog/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/blog/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/blog/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/blog/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
