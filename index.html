<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/blog/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/blog/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/blog/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico?v=5.0.1" />






<meta name="description" content="Baimoon&apos;s blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Baimoon's Note">
<meta property="og:url" content="http://baimoon.github.io/index.html">
<meta property="og:site_name" content="Baimoon's Note">
<meta property="og:description" content="Baimoon&apos;s blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Baimoon's Note">
<meta name="twitter:description" content="Baimoon&apos;s blog">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://baimoon.github.io/"/>

  <title> Baimoon's Note </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/blog/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Baimoon's Note</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/blog/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/blog/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/05/24/kafka-script/" itemprop="url">
                  kafka-script
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-05-24T17:53:42+08:00" content="2017-05-24">
              2017-05-24
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文主要讨论kafka服务的相关启动和关闭脚本。</p>
<h1 id="kafka-server-start-sh"><a href="#kafka-server-start-sh" class="headerlink" title="kafka-server-start.sh"></a>kafka-server-start.sh</h1><p>Kafka服务的启动脚本，正确的用法为 kafka-server-start.sh [-daemon] server.properties<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"># 如果执行脚本时传入的参数小于1个，则退出执行并提示用户需要指定服务属性配置文件， 此处也说明了执行kafka-server-start.sh的正确用法</div><div class="line">if [ $# -lt 1 ];</div><div class="line">then</div><div class="line">	echo &quot;USAGE: $0 [-daemon] server.properties&quot;</div><div class="line">	exit 1</div><div class="line">fi</div><div class="line"></div><div class="line"># $0 表示的是当前shell的文件名，dirname用来获取当前shell文件的所在目录</div><div class="line">base_dir=$(dirname $0)</div><div class="line"></div><div class="line"># 读取环境变量中的KAFKA_LOG4J_OPTS的信息，如果没有配置该环境变量，则将kafka目录下conf中的log4j.properties作为配置添加到环境变量中，配置给KAFKA_LOG4J_OPTS</div><div class="line">if [ &quot;x$KAFKA_LOG4J_OPTS&quot; = &quot;x&quot; ]; then</div><div class="line">    export KAFKA_LOG4J_OPTS=&quot;-Dlog4j.configuration=file:$base_dir/../config/log4j.properties&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 读取环境变量中KAFKA_HEAP_OPTS的信息，如果没有配置该环境变量，则使用默认配置&quot;-Xmx1G -Xms1G&quot;来配置，并添加到环境变量&quot;KAFKA_HEAP_OPTS&quot;中</div><div class="line">if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then</div><div class="line">    export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 定义一个额外的参数 name，为kafka服务指定了进程名</div><div class="line">EXTRA_ARGS=&quot;-name kafkaServer -loggc&quot;</div><div class="line"></div><div class="line"># 如果服务要作为后台进程运行，则需要添加-daemon参数，而且这个参数必须是第一个参数，如果第一个参数是-daemon，则为进程添加自定义的名称</div><div class="line">COMMAND=$1</div><div class="line">case $COMMAND in</div><div class="line">  -daemon)</div><div class="line">    EXTRA_ARGS=&quot;-daemon &quot;$EXTRA_ARGS</div><div class="line">    shift</div><div class="line">    ;;</div><div class="line">  *)</div><div class="line">    ;;</div><div class="line">esac</div><div class="line"></div><div class="line"># 启动kafka服务，由此处也可以看出来，可以使用kafka-run-class.sh来执行相关的类，其中$@表示的是命令行传入的所有参数，这里要启动的类名为kafka.Kafka</div><div class="line">exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka $@</div></pre></td></tr></table></figure></p>
<h1 id="kafka-server-stop-sh"><a href="#kafka-server-stop-sh" class="headerlink" title="kafka-server-stop.sh"></a>kafka-server-stop.sh</h1><p>Kafka服务的停止脚本，其实就是查找KafkaServer对应的进程号，并kill。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 在进程中过滤包含&quot;kafka.Kafka&quot;且不包含&quot;grep&quot;的java进程，截取进程号kill掉</div><div class="line">ps ax | grep -i &apos;kafka\.Kafka&apos; | grep java | grep -v grep | awk &apos;&#123;print $1&#125;&apos; | xargs kill -SIGTERM</div></pre></td></tr></table></figure></p>
<h1 id="kafka-run-class-sh"><a href="#kafka-run-class-sh" class="headerlink" title="kafka-run-class.sh"></a>kafka-run-class.sh</h1><p>kafka-run-class.sh是用来运行class的脚本。正确的用法为 kafka-run-class.sh [-daemon] [-name servicename] [-loggc] classname [opts]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div></pre></td><td class="code"><pre><div class="line"># 验证kafka-run-class脚本的参数</div><div class="line">if [ $# -lt 1 ];</div><div class="line">then</div><div class="line">  echo &quot;USAGE: $0 [-daemon] [-name servicename] [-loggc] classname [opts]&quot;</div><div class="line">  exit 1</div><div class="line">fi</div><div class="line"></div><div class="line"># 获取Kafka的基目录，就是当前目录（bin）的上一层目录</div><div class="line">base_dir=$(dirname $0)/..</div><div class="line"></div><div class="line"># 创建Kafka的日志目录，首先从环境变量“LOG_DIR”中读取，如果没有配置LOG_DIR，则使用Kafka基目录下的logs目录作为日志目录</div><div class="line"># create logs directory</div><div class="line">if [ &quot;x$LOG_DIR&quot; = &quot;x&quot; ]; then</div><div class="line">    LOG_DIR=&quot;$base_dir/logs&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 如果日志目录目存在则创建日志目录</div><div class="line">if [ ! -d &quot;$LOG_DIR&quot; ]; then</div><div class="line">    mkdir -p &quot;$LOG_DIR&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 获取Scala的版本号，首先从环境变量 SCALA_VERSION 中读取，如果没有配置，则使用默认值 2.10.4</div><div class="line">if [ -z &quot;$SCALA_VERSION&quot; ]; then</div><div class="line">	SCALA_VERSION=2.10.4</div><div class="line">fi</div><div class="line"></div><div class="line"># 获取Scala库的版本号，首先从环境变量 SCALA_BINARY_VERSION 中读取，如果没有配置，则使用默认值 2.10</div><div class="line">if [ -z &quot;$SCALA_BINARY_VERSION&quot; ]; then</div><div class="line">	SCALA_BINARY_VERSION=2.10</div><div class="line">fi</div><div class="line"></div><div class="line"># 这里开始加载各种依赖的jar包，并将这些jar包添加到CLASSPATH环境变量中，由此也可以看出运行完整的Kafka服务（支持各种consumer／producer）需要依赖的jar包</div><div class="line"># run ./gradlew copyDependantLibs to get all dependant jars in a local dir</div><div class="line"></div><div class="line"># 将Kafka依赖Scala的jar包添加到CLASSPATH中</div><div class="line">for file in $base_dir/core/build/dependant-libs-$&#123;SCALA_VERSION&#125;*/*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka的示例jar添加到CLASSPATH中</div><div class="line">for file in $base_dir/examples/build/libs//kafka-examples*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将kafka的hadoop consumer相关jar包添加到CLASSPATH中</div><div class="line">for file in $base_dir/contrib/hadoop-consumer/build/libs//kafka-hadoop-consumer*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka的hadoop producer相关jar包添加到CLASSPATH中</div><div class="line">for file in $base_dir/contrib/hadoop-producer/build/libs//kafka-hadoop-producer*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka客户端相关的jar包添加到CLASSPATH中</div><div class="line">for file in $base_dir/clients/build/libs/kafka-clients*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka的libs下的jar包添加到CLASSPATH中</div><div class="line"># classpath addition for release</div><div class="line">for file in $base_dir/libs/*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 将Kafka依赖的Scala对应版本的库添加到CLASSPATH中</div><div class="line">for file in $base_dir/core/build/libs/kafka_$&#123;SCALA_BINARY_VERSION&#125;*.jar;</div><div class="line">do</div><div class="line">  CLASSPATH=$CLASSPATH:$file</div><div class="line">done</div><div class="line"></div><div class="line"># 以下是Java管理扩展的设置</div><div class="line"># 如果没有在环境变量中设置KAFKA_JMX_OPTS，则将Kafka的JMX配置关闭</div><div class="line"># JMX settings</div><div class="line">if [ -z &quot;$KAFKA_JMX_OPTS&quot; ]; then</div><div class="line">  KAFKA_JMX_OPTS=&quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false  -Dcom.sun.management.jmxremote.ssl=false &quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 如果设置了KAFKA_JMX_OPTS环境变量，则利用这个值来设置变量KAFKA_JMX_OPTS的值，该值用于指定虚拟机的信息</div><div class="line"># JMX port to use</div><div class="line">if [  $JMX_PORT ]; then</div><div class="line">  KAFKA_JMX_OPTS=&quot;$KAFKA_JMX_OPTS -Dcom.sun.management.jmxremote.port=$JMX_PORT &quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># Log4j的配置</div><div class="line"># Log4j settings 如果环境变量中没有设置KAFKA_LOG4J_OPTS，则使用Kafka基目录下conf/tools-log4j.properties来设置KAFKA_LOG4J_OPTS变量</div><div class="line">if [ -z &quot;$KAFKA_LOG4J_OPTS&quot; ]; then</div><div class="line">  KAFKA_LOG4J_OPTS=&quot;-Dlog4j.configuration=file:$base_dir/config/tools-log4j.properties&quot;</div><div class="line">fi</div><div class="line"># 根据环境变量LOG_DIR和KAFKA_LOG4J_OPTS来生成变量KAFKA_LOG4J_OPTS的新的值</div><div class="line">KAFKA_LOG4J_OPTS=&quot;-Dkafka.logs.dir=$LOG_DIR $KAFKA_LOG4J_OPTS&quot;</div><div class="line"></div><div class="line"># 判断环境变量KAFKA_OPTS是否有相关设置</div><div class="line"># Generic jvm settings you want to add</div><div class="line">if [ -z &quot;$KAFKA_OPTS&quot; ]; then</div><div class="line">  KAFKA_OPTS=&quot;&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 判断环境变量JAVA_HOME中是否有值，如果不存在则使用默认的java，如果有，则使用该目录下指定的java</div><div class="line"># Which java to use</div><div class="line">if [ -z &quot;$JAVA_HOME&quot; ]; then</div><div class="line">  JAVA=&quot;java&quot;</div><div class="line">else</div><div class="line">  JAVA=&quot;$JAVA_HOME/bin/java&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># Kafka的内存配置，如果环境变量KAFKA_HEAP_OPTS的值为空，则设置值为默认值-Xmx256M</div><div class="line"># Memory options</div><div class="line">if [ -z &quot;$KAFKA_HEAP_OPTS&quot; ]; then</div><div class="line">  KAFKA_HEAP_OPTS=&quot;-Xmx256M&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 如果没有设置环境变量KAFKA_JVM-PERFORMANCE_OPTS，则使用默认值进行配置</div><div class="line"># JVM performance options</div><div class="line">if [ -z &quot;$KAFKA_JVM_PERFORMANCE_OPTS&quot; ]; then</div><div class="line">  KAFKA_JVM_PERFORMANCE_OPTS=&quot;-server -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -Djava.awt.headless=true&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"></div><div class="line"># 这里对脚本传入的参数进行解析，提取守护进程名／是否后台运行／GC日志这个三个信息</div><div class="line"># 第一个case，如果循环到了-name参数，则读取-name的下一参数，下一个参数必定是后台进程的名字，而且控制台的输出日志文件也是该名字</div><div class="line"># 第二个case，如果循环到了-loggc，则表示要记录GC日志，记录GC日志的另一个要求是配置KAFKA_GC_LOG_OPTS环境变量</div><div class="line"># 第三个case，如果循环到了-daemon，则表示服务以后台进程的方式运行</div><div class="line">while [ $# -gt 0 ]; do</div><div class="line">  COMMAND=$1</div><div class="line">  case $COMMAND in</div><div class="line">    -name)</div><div class="line">      DAEMON_NAME=$2</div><div class="line">      CONSOLE_OUTPUT_FILE=$LOG_DIR/$DAEMON_NAME.out</div><div class="line">      shift 2</div><div class="line">      ;;</div><div class="line">    -loggc)</div><div class="line">      if [ -z &quot;$KAFKA_GC_LOG_OPTS&quot;] ; then</div><div class="line">        GC_LOG_ENABLED=&quot;true&quot;</div><div class="line">      fi</div><div class="line">      shift</div><div class="line">      ;;</div><div class="line">    -daemon)</div><div class="line">      DAEMON_MODE=&quot;true&quot;</div><div class="line">      shift</div><div class="line">      ;;</div><div class="line">    *)</div><div class="line">      break</div><div class="line">      ;;</div><div class="line">  esac</div><div class="line">done</div><div class="line"></div><div class="line"></div><div class="line"># 如果启用了GC日志，GC日志的名字为后台进程的名字[-name指定]-gc.log。</div><div class="line"># GC options</div><div class="line">GC_FILE_SUFFIX=&apos;-gc.log&apos;</div><div class="line">GC_LOG_FILE_NAME=&apos;&apos;</div><div class="line">if [ &quot;x$GC_LOG_ENABLED&quot; = &quot;xtrue&quot; ]; then</div><div class="line">  GC_LOG_FILE_NAME=$DAEMON_NAME$GC_FILE_SUFFIX</div><div class="line">  KAFKA_GC_LOG_OPTS=&quot;-Xloggc:$LOG_DIR/$GC_LOG_FILE_NAME -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps &quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># 启动Java进程，将上面的所有信息整合在一起，使用指定的Java，还有各种参数，这里区分了运行模式，其实就是将进程作为后台进程运行还是前台进程运行而已</div><div class="line"># Launch mode</div><div class="line">if [ &quot;x$DAEMON_MODE&quot; = &quot;xtrue&quot; ]; then</div><div class="line">  nohup $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS &quot;$@&quot; &gt; &quot;$CONSOLE_OUTPUT_FILE&quot; 2&gt;&amp;1 &lt; /dev/null &amp;</div><div class="line">else</div><div class="line">  exec $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS &quot;$@&quot;</div><div class="line">fi</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/03/29/unity3D/" itemprop="url">
                  unity3D
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-29T21:44:39+08:00" content="2017-03-29">
              2017-03-29
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/unity/" itemprop="url" rel="index">
                    <span itemprop="name">unity</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文是学习Unity3D过程中的一些学习笔记，主要为个人学习使用，如能帮助别人纯属意外。</p>
<h2 id="游戏对象以及基本操作"><a href="#游戏对象以及基本操作" class="headerlink" title="游戏对象以及基本操作"></a>游戏对象以及基本操作</h2><p>在Unity3D中，在Unity游戏中，一个游戏由多个场景所组成，每个场景又是由多个游戏对象构成，一个游戏对象可以包含多个组件用来实现不同的功能。在场景中，一个角色、一个模型、一个特效，都属于游戏对象。每个游戏对象可以有多个组件，从而使对象具备某种特性、实现相应的功能。</p>
<p>在Unity3D中，对一个游戏对象进行的基本操作有：</p>
<blockquote>
<p>场景拖动<br>游戏对象的拖动 （对象拖动，可以按照轴拖动，也可以按照面拖动）<br>游戏对象的旋转 （旋转可以按照轴旋转）<br>游戏对象的缩放 （只能按照轴来缩放）<br>选中对象，按F或双击，可以快速定位对象到场景中心。</p>
</blockquote>
<p>（对应快捷键为q、w、e、r）。</p>
<p>视角的操作：</p>
<blockquote>
<p>滚轮可以用来拉近或拉远视角。<br>按下鼠标右键旋转，以当前位置旋转视角。<br>按下Alt和鼠标左键旋转，是根据选中的对象，进行视角的旋转。<br>按下鼠标右键的同时，如果按A、W、S、D键，可以进行场景漫游。</p>
</blockquote>
<h2 id="Terrain地形"><a href="#Terrain地形" class="headerlink" title="Terrain地形"></a>Terrain地形</h2><p>需要创建一个名为Terrain的3D模型。然后编辑Terrain组件。<br>组件中包含一个地形编辑工具列表：<br>第一个，提升或沉降地形的工具，点击左键，来提升地形，按下左键并按下shift键，则沉降。<br>第二个，平坦地形工具。刷出来的是一个平面。<br>第三个，平滑地形工具。使得地形的过渡更加自然。<br>第四个，绘制地形表面纹理工具。<br>第五个，植树工具。<br>第六个，添加地表细节工具。</p>
<h2 id="3D开发基础"><a href="#3D开发基础" class="headerlink" title="3D开发基础"></a>3D开发基础</h2><h3 id="摄像机"><a href="#摄像机" class="headerlink" title="摄像机"></a>摄像机</h3><p>摄像机决定了游戏的最终显示效果。而且一个场景中可以存在多个摄像机。<br>将摄像机以当前游戏对象视角来运行，可以选中摄像机，然后按Ctrl + Shift + F来设定。</p>
<h3 id="坐标系"><a href="#坐标系" class="headerlink" title="坐标系"></a>坐标系</h3><p>世界坐标系和本地坐标系，世界坐标系是整个3D场景的坐标系，本地坐标系（会根据游戏对象的旋转而旋转）是某个游戏对象内独立的坐标系。可以通过左上角的按钮来切换。</p>
<h3 id="网格"><a href="#网格" class="headerlink" title="网格"></a>网格</h3><p>网格勾画了一个模型的基本框架。</p>
<h3 id="纹理"><a href="#纹理" class="headerlink" title="纹理"></a>纹理</h3><p>纹理可以使物体的表面富有细节。添加纹理，需要先创建一个新的材质，然后将材质添加到对象上面。然后通过材质来修改材质，材质属性中，有纹理属性，可以用来设置纹理，纹理可以是一张图片。</p>
<h3 id="材质和着色器"><a href="#材质和着色器" class="headerlink" title="材质和着色器"></a>材质和着色器</h3><p>材质能够将纹理应用到模型上，着色器界定纹理呈现出的最终效果。</p>
<h3 id="工程与应用程序"><a href="#工程与应用程序" class="headerlink" title="工程与应用程序"></a>工程与应用程序</h3><p>一个Unity工程一般包含下面几个文件目录：</p>
<blockquote>
<p>Assets 存放的是项目所需的资源。<br>Library 存放的是所需要的库文件。<br>ProjectSettings 里面存放的是工程设置文件。<br>Temp 里面存放的是临时文件。</p>
</blockquote>
<p>游戏对象的组件<br>Transform：控制对象的位置、旋转和缩放。以及游戏对象的父子关系。<br>Mesh Filter：显示网格。<br>xxx Collider：给物体添加碰撞器。<br>Mesh Rendered：渲染器。可以给物体添加材质、纹理以及渲染方式。<br>Script：脚本也是组件。如果一个脚本想要挂载到一个游戏对象上面，那么它必须继承MonoBehaviour。</p>
<h3 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h3><p>首先，一个脚本属于一个游戏对象的一个组件。如果一个脚本想要能够挂载到一个游戏对象上，那么这个脚本必须是要继承MonoBehaviour的。MonoBehaviour为脚本提供了很多已经实现的方法和属性。</p>
<h4 id="获取脚本所挂载到的对象。"><a href="#获取脚本所挂载到的对象。" class="headerlink" title="获取脚本所挂载到的对象。"></a>获取脚本所挂载到的对象。</h4><p>在脚本中，想要获取当前脚本所挂载的对象，只需要调用属性gameObject（注意大小写）即可得到。通过这个gameObject对象，就可以获得游戏对象的各个属性和其他的组件。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gameObject.name  <span class="comment">//获取游戏对象的名字</span></div></pre></td></tr></table></figure></p>
<h4 id="获取脚本挂载对象的Transform组件"><a href="#获取脚本挂载对象的Transform组件" class="headerlink" title="获取脚本挂载对象的Transform组件"></a>获取脚本挂载对象的Transform组件</h4><p>每个游戏对象都会包含一个名为Transform组件，在继承了MonoBehaviour类的脚本中，可以通过transform属性来获取脚本所挂载到对象的Transform组件。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">transform.position.x <span class="comment">//游戏对象位置的x坐标</span></div></pre></td></tr></table></figure></p>
<h4 id="获取脚本挂载对象的其他组件"><a href="#获取脚本挂载对象的其他组件" class="headerlink" title="获取脚本挂载对象的其他组件"></a>获取脚本挂载对象的其他组件</h4><p>要获取那些不常用的组件，需要使用GetComponent方法。如：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Test t = GetComponent &lt;Test&gt; (); <span class="comment">//Test为组件名称，这里是一个名为Test的Script组件。GetComponent是gameObject的一个方法。</span></div><div class="line">t.age = <span class="number">24</span>;</div></pre></td></tr></table></figure></p>
<h4 id="脚本的生命周期"><a href="#脚本的生命周期" class="headerlink" title="脚本的生命周期"></a>脚本的生命周期</h4><p>脚本的生命周期包含：</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/03/08/redis-lua/" itemprop="url">
                  redis_lua
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-08T10:32:34+08:00" content="2017-03-08">
              2017-03-08
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/02/23/shell-study/" itemprop="url">
                  Shell Study
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-23T22:31:10+08:00" content="2017-02-23">
              2017-02-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/unix/" itemprop="url" rel="index">
                    <span itemprop="name">unix</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <hr>
<p>本文记录一些自己在使用shell进行批量操作、任务调度等工作时用到的一些shell的基础知识，在此记录以备翻阅和查找。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2017/02/23/shell-study/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/02/09/machine-learning-note/" itemprop="url">
                  Machine Learning Notebook
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-09T16:00:37+08:00" content="2017-02-09">
              2017-02-09
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/Maching-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Maching learning</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文是自己学习机器学习过程的一些笔记。</p>
<p>决策面 -&gt; 线性决策面</p>
<p>学习的过程就是将data -&gt; DS（决策面）</p>
<p>朴素贝叶斯(Naive bayes)</p>
<p>sklearn（scikit-learn）使用入门<br>可以在google中搜索sklearn naive bayes，Gaussian Naive Bayes</p>
<p>高斯朴素贝叶斯的使用<br>sklearn.naive_bayes.GaussianNB<br>例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X = np.array([[<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">-2</span>, <span class="number">-1</span>], [<span class="number">-3</span>, <span class="number">-2</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>]])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>Y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf = GaussianNB()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>clf.fit(X, Y) <span class="comment">#X是特征， Y是标签</span></div><div class="line">GaussianNB(priors=<span class="keyword">None</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(clf.predict([[<span class="number">-0.8</span>, <span class="number">-1</span>]]))</div><div class="line">[<span class="number">1</span>]</div></pre></td></tr></table></figure></p>
<p>评估分类器的分类准确性<br>准确率是指分类器正确分类的数据点个数占测试集中所有被分类点的比率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</div><div class="line"></div><div class="line"><span class="comment">### create classifier</span></div><div class="line">clf = GaussianNB()</div><div class="line"></div><div class="line"><span class="comment">### fit the classifier on the training features and labels</span></div><div class="line">clf.fit(features_train, labels_train)</div><div class="line"></div><div class="line"><span class="comment">### use the trained classifier to predict labels for the test features</span></div><div class="line">pred = clf.predict(features_test)</div><div class="line"></div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</div><div class="line">accuracy = clf.score(features_test, labels_test)</div><div class="line"><span class="keyword">return</span> accuracy</div><div class="line"></div><div class="line"><span class="comment">#或者 </span></div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</div><div class="line"><span class="keyword">print</span> accuracy_score(pred, lables_test)</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/02/08/java-execute-command/" itemprop="url">
                  Java execute command
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-08T11:27:12+08:00" content="2017-02-08">
              2017-02-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/Java/" itemprop="url" rel="index">
                    <span itemprop="name">Java</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>在Java的使用过程中，难免需要去执行linux命令（执行shell也是linux命令），那么应该如何做呢？本文将进行一些演示。</p>
<h1 id="所依赖的相关类"><a href="#所依赖的相关类" class="headerlink" title="所依赖的相关类"></a>所依赖的相关类</h1><p>要在Java中执行linux命令有两种方式，依赖于三个类。我们先介绍这三个类，然后在使用这三个类，组合两种方案来进行说明。</p>
<h2 id="java-lang-Process"><a href="#java-lang-Process" class="headerlink" title="java.lang.Process"></a>java.lang.Process</h2><h3 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h3><p>ProcessBuilder.start()和Runtime.exec方法创建一个本地进程，并返回一个Process子类的实例，该实例用来控制进程并获得相关信息。Process类提供了执行<br>当Process对象没有更多的引用时，不是删除子进程，而是继续异步执行子进程。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2017/02/08/java-execute-command/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/02/04/python-datetime/" itemprop="url">
                  Python Datetime
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-04T18:38:47+08:00" content="2017-02-04">
              2017-02-04
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>在使用python写调度任务的时候，离不开的必然有日期和时间的处理；最常见的有根据字符串生成时间、将时间生成指定格式的字符串、日期时间的计算（加减）等等。在python中对日期时间进行操作的包为datetime。下面就对该包的一些常用操作和对应的参数进行介绍。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2017/02/04/python-datetime/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/02/04/python-subprocess/" itemprop="url">
                  Python Subprocess
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-02-04T18:38:47+08:00" content="2017-02-04">
              2017-02-04
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在平时python的使用过程中，难免会遇到调用服务器命令的时候。直接调用普通的命令基本上都没有什么问题，令人比较麻烦的是带有控制台的命令，例如python、beeline、spark-shell。虽然向python、spark都有相关的脚本文件或者jar来避免直接使用控制台命令的调用，然后有些时候还是不免会用到控制台的方式，那么对于带有控制台的命令行应该如何实现呢？本文将使用subprocess，并以beeline为背景来实现使用python执行带有控制台的命令行命令。<br>首先看看参考代码，代码是以执行Hive的beeline命令行为例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">beeline</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"# 1 建立连接"</span></div><div class="line">        self.p = subprocess.Popen([<span class="string">'apache-hive-0.14.0-bin/bin/beeline'</span>], stdin=subprocess.PIPE,</div><div class="line">                             stdout=subprocess.PIPE)</div><div class="line">        <span class="keyword">print</span> &gt;&gt; self.p.stdin, <span class="string">'!connect jdbc:hive2://hdfs001:2181,hdfs002:2181,hdfs003:2181,hdfs004:2181,hdfs005:2181/;serviceDiscoveryMode=zookeeper userName password\n'</span></div><div class="line">        self.p.stdin.flush()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">submit</span><span class="params">(self, hql)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"# 2 输入命令"</span></div><div class="line">        <span class="keyword">print</span> &gt;&gt; self.p.stdin, hql</div><div class="line">        self.p.stdin.flush()</div><div class="line"></div><div class="line">        <span class="keyword">print</span> <span class="string">"# 3 等待关闭"</span></div><div class="line">        <span class="keyword">print</span> &gt;&gt; self.p.stdin, <span class="string">"!q"</span></div><div class="line">        self.p.wait()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hadoop_get</span><span class="params">(self, from_, to_)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"# 4 下载数据"</span></div><div class="line">        (status, output) = commands.getstatusoutput(<span class="string">" "</span>.join((<span class="string">"hadoop-2.6.0/bin/hadoop fs -text"</span>, from_+<span class="string">'*'</span>, <span class="string">'&gt;'</span>, to_)))</div><div class="line">        <span class="keyword">print</span> status, output</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">queryDataByDate</span><span class="params">(start_date, end_date, local_path)</span>:</span></div><div class="line">    sql = <span class="string">"""</span></div><div class="line">    create table database.table_%s_%s</div><div class="line">       ROW FORMAT DELIMITED</div><div class="line">       FIELDS TERMINATED BY '-@-'</div><div class="line">       NULL DEFINED AS '...'</div><div class="line">    STORED AS TEXTFILE</div><div class="line">    AS</div><div class="line">    SELECT * FROM DB.TABLE_NAME;</div><div class="line">"""</div><div class="line">    b = beeline()</div><div class="line">    s = sql % (start_date, end_date, start_date, end_date)</div><div class="line">    b.submit((sql % (start_date, end_date, start_date, end_date)))</div><div class="line"></div><div class="line">    fileName = <span class="string">'feed_%s_%s'</span> % (start_date, end_date)</div><div class="line">    b.hadoop_get((<span class="string">"HDFS_PATH/%s/"</span> % (fileName)), (<span class="string">"LOCAL_PATH/%s"</span> % fileName))</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="keyword">if</span> len(sys.argv) &lt; <span class="number">2</span>:</div><div class="line">        <span class="keyword">print</span> <span class="string">"请输入要获取feed的开始日期和结束日志，如：20160105"</span></div><div class="line">        exit(<span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"正在执行%s文件，来查询%s-%s之间的数据:"</span> % (sys.argv[<span class="number">0</span>], sys.argv[<span class="number">1</span>], sys.argv[<span class="number">2</span>])</div><div class="line">    queryDataByDate(sys.argv[<span class="number">1</span>], sys.argv[<span class="number">2</span>], <span class="string">"/data/"</span>)</div></pre></td></tr></table></figure></p>
<p>该代码块的主要流程是，在初始化beeline对象时调用beeline命令，并进行连接（<strong>init</strong>方法中实现了全部的操作）;然后是提交需要beeline执行的sql（submit方法中实现）;最后是将sql执行的结果从HDFS中取到本地（hadoop_get方法中实现）。queryDataByData方法就是对beeline类中各个方法的一个集成调用。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2017/01/13/doubleArray-trie/" itemprop="url">
                  Double-Array trie
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-01-13T16:01:54+08:00" content="2017-01-13">
              2017-01-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/数据结构/" itemprop="url" rel="index">
                    <span itemprop="name">数据结构</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文主要用来学习Double-Array trie的相关知识。</p>
<p><a href="https://github.com/digitalstain/DoubleArrayTrie" title="Double-Array trie" target="_blank" rel="external">源码的github地址</a></p>
<p>Double-Array trie是Trie结构的压缩形式，仅用两个数组来表示Trie，这个结构有效的结合数字搜索树(Digital Search Tree)检索时间高效的特点和链式表示的Trie空间结构紧凑的特点。双数组Trie的本质是一个确定有限状态自动机(DFA)，每个节点代表自动机的一个状态，根据不同的变量，进行状态转移，当到达结束状态或无法转移时，完成一次查询操作。在双数组所有键中，包含的字符之间的联系都是通过简单的数学加法运算表示的，不仅提高了检索速度，而且省去了链式结构中使用的大量指针，节省了存储空间。</p>
<p>在了解Double-Array trie之前，我们先了解一下“确定有限状态自动机”。在数学理论中，确定有限状态自动机或确定有限自动机（deterministic finite automation, DFA）是一个能实现状态转移的自动机。对于一个给定的属于该自动机的状态和一个属于该自动机字母表的字符，它能够根据实现给定的函数转移到下一个状态。简单的说，就是当前状态根据一个公式和状态的确定值，能够到达另外一个状态，而且要到达的状态是确定的。如图：<br><a href="&quot;确定有限自动机&quot;">确定有限自动状态机</a><br>图中的每个字代表一个状态，并且每个状态都有一个固定的变量。</p>
<p>在了解了确定优先状态自动机之后，我们来了解一下Double-Array trie。Double-Array trie的核心是使用两个整型数组base和check来分别存储状态以及前驱状态。说的简单一些，base用来存储状态，check用来验证。在状态的转移过程中，有如下公式：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">check[t]=s</div><div class="line">base[s]+c=t <span class="comment">//其中t和s是数组下标</span></div></pre></td></tr></table></figure></p>
<p>上面的公式表示 base[s]的值 + 状态的变量 = t下标，check[t]的值 = s下标。</p>
<p>举例来说明：</p>
<p>在学习Douoble-Array trie和看DoubleArrayTrie源码的时候，参考了以下文章，在此表示感谢：<br><a href="http://www.hankcs.com/program/java/%E5%8F%8C%E6%95%B0%E7%BB%84trie%E6%A0%91doublearraytriejava%E5%AE%9E%E7%8E%B0.html" title="双数组Trie树(DoubleArrayTrie)Java实现" target="_blank" rel="external">双数组Trie树(DoubleArrayTrie)Java实现</a><br><a href="http://www.cnblogs.com/zhangchaoyang/articles/4508266.html" title="Double Array Trie" target="_blank" rel="external">Double Array Trie</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/12/30/JPinYin/" itemprop="url">
                  JPinYin
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-12-30T16:28:15+08:00" content="2016-12-30">
              2016-12-30
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文主要介绍JPinYin的使用和配置，<a href="https://github.com/stuxuhai/jpinyin" title="Jpinyin">github的地址</a>。</p>
<h1 id="简单介绍"><a href="#简单介绍" class="headerlink" title="简单介绍"></a>简单介绍</h1><p>Jpinyin是一个开源的用于将汉字转换为拼音的java库。</p>
<h2 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h2><p>1、准确、完善的字库：Unicode编码从4E00-9FA5范围及3007(〇)的20903个汉字中，除了46个异体字（不存在标准拼音）Jpinyin都能转换。<br>2、拼音转换速度快：经测试，转换Unicode编码范围的20902个汉字，Jpinyin耗时约为100毫秒。<br>3、支持多种拼音格式：Jpinyin支持多种拼音输出格式：带声调、不带声调、数字表示声调以及拼音首字母格式输出。<br>4、常见多音字识别：Jpinyin支持常见多音字的识别，其中包括词组、成语、地名等；<br>5、简繁体中文互转。<br>6、支持用户自定义字典。</p>
<h2 id="Maven依赖"><a href="#Maven依赖" class="headerlink" title="Maven依赖"></a>Maven依赖</h2><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">   &lt;groupId&gt;com.github.stuxuhai&lt;/groupId&gt;</div><div class="line">   &lt;artifactId&gt;jpinyin&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;1.1.8&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure>
<h2 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">String str = <span class="string">"你好世界"</span>;</div><div class="line">PinyinHelper.convertToPinyinString(str, <span class="string">","</span>, PinyinFormat.WITH_TONE_MARK); <span class="comment">// nǐ,hǎo,shì,jiè</span></div><div class="line">PinyinHelper.convertToPinyinString(str, <span class="string">","</span>, PinyinFormat.WITH_TONE_NUMBER); <span class="comment">// ni3,hao3,shi4,jie4</span></div><div class="line">PinyinHelper.convertToPinyinString(str, <span class="string">","</span>, PinyinFormat.WITHOUT_TONE); <span class="comment">// ni,hao,shi,jie</span></div><div class="line">PinyinHelper.getShortPinyin(str); <span class="comment">// nhsj</span></div><div class="line">PinyinHelper.addPinyinDict(<span class="string">"user.dict"</span>);  <span class="comment">// 添加用户自定义字典</span></div></pre></td></tr></table></figure>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/12/30/JPinYin/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/12/19/scikitImage-ACrashCourseOnNumPyForImages/" itemprop="url">
                  scikit image - a crash course on NumPy for images
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-12-19T12:43:26+08:00" content="2016-12-19">
              2016-12-19
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/图像处理/" itemprop="url" rel="index">
                    <span itemprop="name">图像处理</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文用来学习scikit-image的官方文档的<a href="http://scikit-image.org/docs/stable/user_guide/numpy_images.html" title="a crash course on NumPy for images">a crash course on NumPy for images</a>，<a href="http://scikit-image.org/docs/stable/user_guide/numpy_images.html" title="a crash course on NumPy for images">原链接</a></p>
<h1 id="A-crash-course-on-NumPy-for-images"><a href="#A-crash-course-on-NumPy-for-images" class="headerlink" title="A crash course on NumPy for images"></a>A crash course on NumPy for images</h1><p>scikit-image是以NumPy数组的方式来操作图像。因此图像很大一部分的操作将是使用NumPy：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> skimage <span class="keyword">import</span> data</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera = data.camera()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>type(camera)</div><div class="line">&lt;type <span class="string">'numpy.ndarray'</span>&gt;</div></pre></td></tr></table></figure></p>
<p>检索图像的几何以及像素数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.shape</div><div class="line">(<span class="number">512</span>, <span class="number">512</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.size</div><div class="line"><span class="number">262144</span></div></pre></td></tr></table></figure></p>
<p>检索关于灰度值的统计信息：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.min(), camera.max()</div><div class="line">(<span class="number">0</span>, <span class="number">255</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.mean()</div><div class="line"><span class="number">118.31400299072266</span></div></pre></td></tr></table></figure></p>
<p>代表图片的NumPy数组可以是浮点数类型的不同整数。查看<a href="http://scikit-image.org/docs/stable/user_guide/data_types.html#data-types" title="Image data type and what the mean">Image data type and what the mean</a>获取关于这些类型的更多信息，以及scikit-image如何处理它们。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/12/19/scikitImage-ACrashCourseOnNumPyForImages/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/12/16/scikitImage-gettingStarted/" itemprop="url">
                  scikit image - getting started
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-12-16T16:42:31+08:00" content="2016-12-16">
              2016-12-16
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/图像处理/" itemprop="url" rel="index">
                    <span itemprop="name">图像处理</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文用来学习scikit-image的官方文档的入门手册，<a href="http://scikit-image.org/docs/stable/user_guide/getting_started.html" title="Getting started" target="_blank" rel="external">原链接</a></p>
<h1 id="Getting-started"><a href="#Getting-started" class="headerlink" title="Getting started"></a>Getting started</h1><p>scikit-image是一个图像处理的Python包，它使用numpy数组来工作。这个包作为skimage被引入：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> skimage</div></pre></td></tr></table></figure></p>
<p>skimage的大多数函数将在子模块中找到：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> skimage <span class="keyword">import</span> data</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera = data.camera()</div></pre></td></tr></table></figure></p>
<p>一个包含子模块和函数的web页面可以在<a href="http://scikit-image.org/docs/stable/api/api.html" title="API Reference" target="_blank" rel="external">API reference</a>中找到。<br>在scikit-image中，图片相当于一个NumPy数组，例如，一个2-D的数组表示了一个灰度的2-D图片<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>type(camera)</div><div class="line">&lt;type <span class="string">'numpy.ndarray'</span>&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># An image with 512 rows and 512 columns</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>camera.shape</div><div class="line">(<span class="number">512</span>, <span class="number">512</span>)</div></pre></td></tr></table></figure></p>
<p>skimage.data模块提供了一组返回示例图片的函数，这些图片可以用来快速学习scikit-image的函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>coins = data.coins()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> skimage <span class="keyword">import</span> filters</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>threshold_value = filters.threshold_otsu(coins)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>threshold_value</div><div class="line"><span class="number">107</span></div></pre></td></tr></table></figure></p>
<p>当然，还可以使用skimage.io.imread()从图片文件来加载自己的图片信息，加载后的图片也是作为一个NumPy数组：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> os</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>filename = os.path.join(skimage.data_dir, <span class="string">'moon.png'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> skimage <span class="keyword">import</span> io</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>moon = io.imread(filename)</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/11/10/log4j/" itemprop="url">
                  Log4j Architecture
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-11-10T15:25:48+08:00" content="2016-11-10">
              2016-11-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/Logging/" itemprop="url" rel="index">
                    <span itemprop="name">Logging</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文主要是针对Log4j的2.x版本的文档的，<a href="http://logging.apache.org/log4j/2.x/" title="Log4j">链接</a></p>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><h2 id="Main-Components"><a href="#Main-Components" class="headerlink" title="Main Components"></a>Main Components</h2><p>Log4j使用的类下面图表中展示。<br><img src="http://oaavtz33a.bkt.clouddn.com/Log4jClasses.jpg" alt="Log4j Classes" title="class Log4j Classes"><br>使用Log4j的applications讲需要使用一个特定的名称向LogManager请求一个Logger。LogManager会定位到适当的LoggerContext，然后从LoggerContext中获取Logger。如果Logger必须被创建，它将与包含了如下内容的LoggerConfig进行关联：1）与Logger相同名称的LoggerConfig；b）父级package的名称的LoggerConfig；3）根级LoggerConfig。LoggerConfig对象根据配置中Logger的声明进行创建。LoggerConfig与日志事件的实际输出源联系在一起。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/11/10/log4j/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/11/10/log4j-configuration/" itemprop="url">
                  Log4j Configuration
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-11-10T15:25:48+08:00" content="2016-11-10">
              2016-11-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/Logging/" itemprop="url" rel="index">
                    <span itemprop="name">Logging</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文用来学习关于Log4j的配置（通过配置文件的方式），<a href="http://logging.apache.org/log4j/2.x/manual/configuration.html" title="Configuration">原文档连接</a></p>
<h1 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h1><p>将日志请求插入到application代码中需要相当多的计划和努力。观察表明，大约百分之四的代码用于记录日志。因此，即使是一般大小的application也将会有数以千计的日志片段嵌套在代码中。考虑到这个数量，如何不需要手动修改就能管理这些日志片段就显得十分重要。<br>配置Log4j 2版本，能够通过下面四种方法中的任意一种来完成：<br>1、通过一个以XML、JSON、YAML或properties格式的配置文件。<br>2、编程方式，通过创建一个ConfigurationFactory和Configuration实现。<br>3、编程方式，通过调用在Configuration接口中的APIs来添加组件到默认配置。<br>4、编程方式，通过调用内部Logger类的方法。<br>本文主要关注通过一个配置文件来配置Log4j。对于通过编程方式来配置Log4j，可以在<a href="http://logging.apache.org/log4j/2.x/manual/extending.html" title="Extending Log4j 2">Extending Log4j 2</a>和<a href="http://logging.apache.org/log4j/2.x/manual/customconfig.html" title="Programmatic Log4j Configuration">Programmatic Log4j Configuration</a>。<br>注意，不同于Log4j 1.x，Log4j 2的公共API没有公开关于添加、修改和移除appenders和filter的方法，或者以任何方式来操作配置。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/11/10/log4j-configuration/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/09/28/kafka-document/" itemprop="url">
                  Apache Kafka
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-28T15:46:13+08:00" content="2016-09-28">
              2016-09-28
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/Kafka/" itemprop="url" rel="index">
                    <span itemprop="name">Kafka</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是Kafka 0.10.0文档的翻译，主要用于自学。</p>
<h1 id="1-Getting-Started"><a href="#1-Getting-Started" class="headerlink" title="1 Getting Started"></a>1 Getting Started</h1><h2 id="1-1-Introduction"><a href="#1-1-Introduction" class="headerlink" title="1.1 Introduction"></a>1.1 Introduction</h2><p>Kafka是一个分布式的、分区的、备份的提交日志服务。它提供了一个消息传输系统的功能，但是使用了一个独特的设计。<br>那意味着什么？<br>首先我们浏览一下基本的消息队列术语：</p>
<ul>
<li>Kafka以一种类型持续messages的提供称为topics。</li>
<li>我们称那些publish message到一个Kafka topic的进程为producers。</li>
<li>我们称那些subscribe到topics并处理被publish的message的进程为consumers。</li>
<li>kafka作为一个集群而运行，集群由一个或多个server组成，每个server成为一个broker。</li>
</ul>
<p>因此，整体来看，producers通过网络发送messages到Kafka集群，同样Kafka又为consumers服务，像这样：<br><img src="http://oaavtz33a.bkt.clouddn.com/producer_consumer.png" alt="producer_consumer" title="producer and consumer"><br>clients和servers之间的通信是通过一个简单的、高性能的、跨语言的TCP协议完成的。我们为Kafka提供了一个Java client，但是clients在很多语言中都可用。</p>
<h3 id="Topics-and-Logs"><a href="#Topics-and-Logs" class="headerlink" title="Topics and Logs"></a>Topics and Logs</h3><p>首先我们学习由Kafka提供的高级别的抽象 - topic。<br>一个topic是一种或一个提供的名称，用来publish message。对于每个topic，Kafka集群维持着一个分区日志，看起来像这样：<br><img src="http://oaavtz33a.bkt.clouddn.com/log_anatomy.png" alt="Anatomy of a Topic" title="Anatomy of a Topic"><br>每个partition是一个顺序的、不可变的连续添加的消息队列。partitions中的每个message分配一个序列id号，称为offset，用来唯一标识partition中的每条message。<br>Kafka集群保存所有publish过来的message-不管它们是否被消费，保存时长可配置。例如，如果日志保存设置为两天，那么一个message在publish后两天内是可以被消费的，但是两天之后，它将被删除以释放空间。kafka的性能对于不同数据大小是恒定有效的，因此很多的数据不是个问题。<br>实际上每个consumer仅有被保存的元数据是consumer在日志中的位置，称为offset。这个offset由consumer控制：通常一个consumer按照它读取的message，线性的推进它的offset，但是这个位置由consumer控制并且consumer能够以任意顺序消费message。例如一个consumer可以重置到一个原来的offset来重新处理。<br>这个特征的组合意味着Kafka consumer是非常廉价的 - 它们能够自由的来去，而不会影响集群或其他consumer。例如，你可以使用我们的命令行工具来tail任何topic的内容，而不需要任何已经存在的consumers改变它所消费的内容。<br>日志中的partitions有几个用途。首先，它们允许日志扩展到单个server所能容纳的日志大小之外。一个partition必须位于它所属的server上，但是一个topic可能有很多的partitions，因此它能够持有任意数量的数据。其次，它们的行为类似一个并行单元 - 汇聚更多于一点。</p>
<h3 id="Distribution"><a href="#Distribution" class="headerlink" title="Distribution"></a>Distribution</h3><p>日志的partitions在Kafka集群中跨server分布，每个server为一个共享的partition处理数据和请求。每个partition为了容灾，跨server保存多个备份，备份的数量可以配置。<br>每个partition有一个server扮演”leader”的角色，并有零个或多个servers扮演”followers”的角色。leader为partition处理所有的读和写的请求，而follower只是被动的复制leader。如果leader失效了，follower中的一个将自动成为新的leader。每个server为它自己一些的partitions扮演一个leader角色，为其他的partition扮演一个follower的角色，因此每个server在集群中的负载是很均衡的。</p>
<h3 id="Producers"><a href="#Producers" class="headerlink" title="Producers"></a>Producers</h3><p>Producers将数据publish到它们选择的topics中。Producer负责哪些message被分配到topic的哪些partition中。这可以通过简单的轮转来完成以平衡负载，或者可以一致性的partition定义函数来完成（例如基于message的某些key）。在分区上用的更多的是第二种。</p>
<h3 id="Consumers"><a href="#Consumers" class="headerlink" title="Consumers"></a>Consumers</h3><p>传统的消息传输有两种模式：queuing和publish-subscribe。在队列方式中，一个consumers池从一个server中读取，每条message只会到达某个consumer；在发布-订阅方式中，message广播给所有的consumers。Kafka提供了单个consumer抽象，它概括了上面两种方式 - consumer group。<br>consumers使用一个consumer群名称来标识它们自己，每个publish到一个topic的message被传递到每个订阅了topic的consumer组的一个consumer实例。consumer实例能够在单独的进程或单独的机器上。<br>如果所有的consumer实例拥有相同的consumer组，那么工作方式与一个传统的跨consumers负载均衡的队列类似。<br>如果所有的consumer实例都有不同的consumer组，那么工作方式与发布-订阅类似，所有的message会广播给所有的consumer。<br>更常见的，尽管我们发现那些topics有少数量的consumer组，然而每个都是一个逻辑订阅者。每个组由多个consumer实例组成，这样具有扩展性和容灾性。这也是publish-subscrib的定义，只不过subscriber是一个consumer群，而不是单个进程。<br>相对于传统消息传输系统，Kafka有更强的顺序保证。<br><img src="http://oaavtz33a.bkt.clouddn.com/consumer-groups.png" alt="consumer-groups" title="一个有两个server组成的Kafka集群有四个partitions(P0-P3)和两个consumer组。consumer组A有两个consumer实例，而组B有四个实例"><br>一个传统队列在server上按顺序保存messages，如果多个consumer从队列中消费数据，那么server以message存储的顺序拿出message。然而，虽然server按照顺序拿出message，但是message以异步方式投递给consumers，因此它们可能在不同的consumer上以不同的顺序到达。这意味着在并行消费的情况中消息的顺序丢失了。消息传输系统通过一个”exclusive consumer”的概念来解决这个问题，它只允许一个进程从队列中消费，但是这意味着没有并行处理。<br>Kafka做的更好一些。通过一个并行概念-partition-在topics中，Kafka能够在一个consumer进程池上同时提供顺序保证和负载均衡。这是通过将topic中的partitions分配给consumer组中的consumers来完成的，因此每个partition有组中确切的一个consumer来消费。通过这样，我们确保consumer值读取那一个partition，并以顺序消费数据。因为有很多partitions在很多consumer实例上是均衡负载的。注意，一个consumer组中的consumer实例不能多余partitions的数量。<br>Kafka只是在一个partition中提供了一个整体的顺序，而不是在一个topic的不同partition之间。对于大多applications，每个分区的排序联合根据key划分数据的能力是充分的。如果你要求在message上有整体的顺序，这可以通过使用一个topic只有一个partition来完成，这也意味着每个consuemr组只有一个consumer进程。</p>
<h3 id="Guarantees"><a href="#Guarantees" class="headerlink" title="Guarantees"></a>Guarantees</h3><p>在高层次上，Kafka给了如下的保证：</p>
<ul>
<li>由一个producer发送到一个特定topic partition的Messages将会以它们被发送的顺序添加。那就是，如果一个message M1与发送message M2的producer是一个，并且M1先被发送，那么M1将有一个比M2小的offset，并且要比M2更早的添加到日志中。</li>
<li>一个consumer看到messages的顺序是messages存储的顺序。</li>
<li>对于使用了复制因子为N的topic，在不丢失任何提交到log的messages丢失，我们允许最多N-1个server故障。</li>
</ul>
<p>这些保证的更多细节在文档的design章节中给出。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/09/28/kafka-document/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/09/21/flume-install/" itemprop="url">
                  Flume Install
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-21T21:01:12+08:00" content="2016-09-21">
              2016-09-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/Flume/" itemprop="url" rel="index">
                    <span itemprop="name">Flume</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文主要介绍自己在生产中使用flume的实际配置，以便以后查询。如果能够为他人提供参考，荣幸之至。<br>在Flume中分为三个部分source、channel和sink。source主要用于接收数据，sink用于写出数据，channel作为source和sink的连接、保存和转发使用。其中非常好用的是，channel可以使用Kafka，从而使得Flume具有了超强的存储能力，如果在加上可靠的source和sink，完全可以保证数据零丢失。<br>本文使用的例子中采用的是内存channel，这种channel的缺点是存储长度有限，重启数据丢失，有点就是速度快，低延迟。至于source，使用的是avro。最后是sink，因为我得目的是将数据写入到HDFS中，以后Hadoop集群或Spark集群进行计算，因此使用的hdfs类型的sink。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/09/21/flume-install/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/09/18/ganglia-installAndConfig/" itemprop="url">
                  Ganglia Install And Config
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-18T17:30:07+08:00" content="2016-09-18">
              2016-09-18
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/ganglia/" itemprop="url" rel="index">
                    <span itemprop="name">ganglia</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是ganglia的安装和配置的笔记</p>
<h1 id="Ganglia的安装"><a href="#Ganglia的安装" class="headerlink" title="Ganglia的安装"></a>Ganglia的安装</h1><p>首先，ganglia由gmond、gmetad和gweb三部分组成。</p>
<h2 id="gmond"><a href="#gmond" class="headerlink" title="gmond"></a>gmond</h2><p>gmond（Ganglia Monitoring Daemon）是一种轻量级服务，安装在每台需要手机指标数据的节点主机上。它通过侦听/通告协议与集群内其他节点共享数据。<br>gmond的安装很简单，其所依赖的库，libconfuse、pkgconfig、PCRE和APR等在大多数现行的linux上都有安装。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum install ganglia-gmond</div></pre></td></tr></table></figure></p>
<h2 id="gmetad"><a href="#gmetad" class="headerlink" title="gmetad"></a>gmetad</h2><p>gmetad （Ganglia Meta Daemon）是一种从其他gmetad或gmond源收集指标数据，并将数据以RRD格式存储到磁盘的服务。gmetad为从主机组收集的特定指标信息提供了简单的查询机制，并支持分级授权，使得创建联合检测域成为可能。<br>gmetad除了需要安装gmond所需的依赖之外，还需要RDDtool库。它用来存储和显示从其他gmetad和gmond源收集的时间序列数据。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum install ganglia-gmetad</div></pre></td></tr></table></figure></p>
<h2 id="gweb"><a href="#gweb" class="headerlink" title="gweb"></a>gweb</h2><p>完整的Ganglia不能缺少网络接口：gweb（Ganglia Web）。gweb是一种利用浏览器显示gmetad所存储数据的PHP前端。<br>Ganglia 3.4的Web接口是一个独立的发布包，其源代码也是独立的。gweb 3.4支持gmond/gmetad 3.4.x及以上版本；gweb未来版本可能需要与gmond/gmetad未来版本相匹配。建议安装或更新gweb的时候查看安装文档，以获取更多信息。<br>安装gweb需要如下需求：</p>
<ul>
<li>Apache Web Server</li>
<li>PHP 5.2级更新版本</li>
<li>PHP JSON扩展的安装和启用</li>
</ul>
<p>首先安装Apache和PHP<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum install httpd php</div></pre></td></tr></table></figure></p>
<p>用户还需要启用PHP的JSON扩展，通过检查/etc/php.d/json.ini文件来检查JSON的扩展状态，如果已经启用扩展，文件中应该包含下面的语句：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">extension=json.ini</div></pre></td></tr></table></figure></p>
<p>下载最新的gweb(<a href="https://sourceforge.net/projects/ganglia/files/gweb/)，然后编译Makefile来安装gweb2">https://sourceforge.net/projects/ganglia/files/gweb/)，然后编译Makefile来安装gweb2</a>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tar -xvzf ganglia-web-major.minor.release.tar.gz</div><div class="line"><span class="built_in">cd</span> ganglia-web-major.minor.release</div></pre></td></tr></table></figure></p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/09/18/ganglia-installAndConfig/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/09/10/spark-tuningSpark/" itemprop="url">
                  Tuning Spark
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-10T23:28:56+08:00" content="2016-09-10">
              2016-09-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是Tuning Spark文档的翻译，原文档<a href="http://spark.apache.org/docs/latest/tuning.html" title="Tuning Spark">请参考</a>，本文主要用于个人学习。</p>
<h1 id="Tuning-Spark"><a href="#Tuning-Spark" class="headerlink" title="Tuning Spark"></a>Tuning Spark</h1><p>因为大多数Spark计算是内存中的计算，因此集群中的任何资源都能够成为Spark程序的瓶颈：CPU、网络带宽或内存。通常，如果数据装载到内存中，瓶颈可能是网络带宽，但是有些时候，你还需要做一些调整，例如以序列化格式存储RDDs，以降低内存的使用。本指南覆盖了两个主题：数据序列化和内存调整，其中数据序列化对好的网络性能是至关重要的，并且还可以降低内存的使用。我们还会概述其他一些小的主题。</p>
<h2 id="Data-Serialization"><a href="#Data-Serialization" class="headerlink" title="Data Serialization"></a>Data Serialization</h2><p>序列化在任何分布式application的执行中扮演了很重要的角色。那些序列化缓慢的对象或消费很大数量byte的格式将极大的减慢计算。通常，这是优化一个Spark application首先要调整的东西。Spark的目标是在方便（允许你在你的操作中使用任何Java类型）和性能之间达到一个平衡。它提供了两种序列化库：</p>
<ul>
<li>Java serialization: 默认，Spark使用Java的ObjectOutputStream框架进行序列化对象操作，能够和任何你实现了java.io.Serializable接口的类型一起工作。通过继承java.io.Externalizable，你能够更加近的控制你的序列化的执行。Java序列化是灵活的，但是通常是慢的，这导致对于很多类会有很大的序列化格式。</li>
<li>Kryo serialization: Spark能够使用Kryo库（版本2）来更快的序列化对象。相对于Java序列化Kryo显然是更快且更加简单的（通常是10倍还多），但是不支持所有的可序列化类型，并且需要你在程序中将你要使用的类进行注册以便获取更好的执行。</li>
</ul>
<p>通过使用SparkConf初始化你的job，并调用conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”)来转换为使用Kryo。这个设置不止为了wroker节点混洗数据配置序列化生成器，当序列化RDDs到磁盘时也有用。Kryo不作为默认序列化生成器的唯一原因是需要自定义注册，但是我们推荐在任何网络集中型application中使用它。<br>Spark自动的很多常用的核心Scala类包含Kryo序列化生成器，涵盖在Twitter chill库的AllScalaRegistrar中。<br>要使用Kryo注册你自己的自定义类，使用registerKryoClasses方法。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</div><div class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</div><div class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div></pre></td></tr></table></figure></p>
<p><a href="https://github.com/EsotericSoftware/kryo" title="Kryo">Kryo documntation</a>描述了更加高级的注册选项，如添加自定义序列化编码器。<br>如果你的对象很大，你可能需要增加spark.kryoserializer.buffer配置的值。这个值需要足够大以便保存你要序列化的最大对象。<br>最后，如果你没有注册你的自定义，Kryo将仍然能够工作，但是它将存储每个对象的全类名，这样损耗很大。</p>
<h2 id="Memory-Tuning"><a href="#Memory-Tuning" class="headerlink" title="Memory Tuning"></a>Memory Tuning</h2><p>在调整内存的用法中，有三种值得考虑：被你的对象使用的内存的总量（你可能想要将整个数据集装配到内存中）、访问这些对象的开销和垃圾回收的开销（如果你在这些对象）。<br>默认，Java对象是快速访问的，但是在它们字段的内部很容消费掉比原始数据多2-5倍的空间 。这是因为有如下原因：</p>
<ul>
<li>每个不同的Java对象有一个”object header”，它大概是是16个字节，并包含信息，诸如一个指向它的class的指针。对于一个带有非常少数据的对象（假设一个Int字段），这可能要比数据大很多。</li>
<li>Java Strings在原生的string数据上有一个大概40字节的开销（因为它们被存储到一个Chars数组中，并且保存了额外的数据，如长度），并且每个字符以两个字节存储，因为String内部使用的UTF-16进行编码。因此一个包含10个字符的字符串很容易消耗掉60个字节。</li>
<li>常见的集合类，如HashMap和LinkedList，使用linked数据结构，每个数据项是一个包装对象（如Map.Entry）。这种对象不只有header，而且还有指针（通常是8个字节）指向列表中的下一个对象。</li>
<li>原始类型常常以包装对象来存储，诸如java.lang.Integer。</li>
</ul>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/09/10/spark-tuningSpark/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/09/08/spark-streaming-kafka/" itemprop="url">
                  Spark Streaming + Kafka Integration Guide
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-08T16:45:09+08:00" content="2016-09-08">
              2016-09-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文是Spark Streaming + Kafka Integration Guide文档的翻译，原文请<a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html" title="Spark Streaming + Kafka Integration Guide" target="_blank" rel="external">参考</a>。另外，本文主要用于个人学习使用。</p>
<h1 id="Spark-Streaming-Kafka-Integration-Guide"><a href="#Spark-Streaming-Kafka-Integration-Guide" class="headerlink" title="Spark Streaming + Kafka Integration Guide"></a>Spark Streaming + Kafka Integration Guide</h1><p><a href="http://kafka.apache.org/" title="Apache Kafka" target="_blank" rel="external">Apache Kafka</a>是一个发布-订阅的消息队列，作为一个分布式的、分片的、副本提交的日志服务。这里解释如何配置Spark Streaming来从Kafka接收数据。有两种方法来达到这个目的 - 老的方法是使用Receiver和Kafka的高级API，还有一个新的实验性解决方法（在Spark 1.3版本中引入），不需要使用Receiver。它们有不同的编程模式、执行特征和语义保证，因此请仔细阅读。</p>
<h2 id="Approach-1-Receiver-based-Approach"><a href="#Approach-1-Receiver-based-Approach" class="headerlink" title="Approach 1: Receiver-based Approach"></a>Approach 1: Receiver-based Approach</h2><p>这个方法使用一个receiver来接收数据。这个Receiver使用Kafka高级别consumer API来实现。和所有receivers一样，通过一个Receiver从Kafka接收到的数据被存储到Spark executors中，然后Spark Streaming启动jobs来处理数据。<br>然而，根据默认配置，这个解决方法会因为故障而丢失数据（查看<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#receiver-reliability" title="Receiver Reliability" target="_blank" rel="external">receiver reliabillity</a>。要确保零数据丢失，你需要额外的在Spark Streaming中启用Write Ahead Logs(从Spark 1.2版本中引入)。这将同步的将从Kafka接收到的数据到以写ahead日志的方式波存到分布式文件系统中（如HDFS），因此所有的数据能够从故障中恢复。查看Streaming programming guide中<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#deploying-applications" title="Deploying Applications" target="_blank" rel="external">Deploying section</a>获取关于Write Ahead Logs的详细信息）。<br>接下来，我们讨论如何在你的streaming application中使用这个方法。</p>
<p>1、<strong>Linking:</strong> 对于使用SBT或Maven项目描述的Scala或Java application，使用如下的坐标链接你的streaming application（查看programming guide中的[Linking section]获取更多信息）。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">groupId = org.apache.spark</div><div class="line">artifactId = spark-streaming-kafka<span class="number">-0</span><span class="number">-8</span>_2<span class="number">.11</span></div><div class="line">version = <span class="number">2.0</span><span class="number">.0</span></div></pre></td></tr></table></figure></p>
<p>对于Python application，你需要在部署你的application时添加上面的库以及它的依赖。查看Deploying分项的内容。<br>2、<strong>Programming:</strong> 在streaming application代码中，导入KafkaUtils并创建一个输入DStream，如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka._</div><div class="line"></div><div class="line"><span class="keyword">val</span> kafkaStream = <span class="type">KafkaUtils</span>.createStream(streamingContext, </div><div class="line">    [<span class="type">ZK</span> quorum], [consumer group id], [per-topic number of <span class="type">Kafka</span> partitions to consume])</div></pre></td></tr></table></figure></p>
<p>你还可以指定key和value的类以及它们使用createStream的变体的相关解码类。查看<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.kafka.KafkaUtils$" title="KafkaUtils" target="_blank" rel="external">API docs</a>和<a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/KafkaWordCount.scala" title="KafkaWordCount" target="_blank" rel="external">example</a>。</p>
<h4 id="需要注意的几点："><a href="#需要注意的几点：" class="headerlink" title="需要注意的几点："></a>需要注意的几点：</h4><ul>
<li>Kafka中topic的partitions与Spark Streaming中RDDs生成的partitions没有关联。因此在KafkaUtils.createStream()中增加特定于topic的partition的数量只是增加使用的线程的数量，使用这些线程在单个receiver中对topic进行消费。它不会增加Spark数据的并发处理。参考主要文档获取它的更多信息。</li>
<li>多个Kafka输入DStream能够使用不同的group和topics来创建，以便使用多个receiver来并行接收数据。</li>
<li>如果你使用一个可靠的文件系统（像HDFS）启用了Write Ahead logs，接收到的数据已经被复制到日志中。因此，对于输入流存储的存储级别为StorageLevel.MEMORY_AND_DISK_SER（那就是KafkaUtils.createStream(…, StorageLevel.MEMORY_AND_DISK_SER)）。</li>
</ul>
<p>3、<strong>Deploying:</strong> 对于任何Spark application，spark-submit被使用来启动你的application。然而，对于Scala/Java application和Python application之间有轻微的不同。<br>对于Scala和Java application，如果你使用SBT或Maven作为项目管理，那么打包spark-streaming-kafka-0-8_2.11和它的依赖到application的JAR中。确保spark-core_2.11和spark-streaming_2.11作为依赖进行标记，因为他们已经安装到Spark中了。然后，使用spark-submit来启动你的application（查看主要编程指南中的<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#deploying-applications" title="Deploying Applications" target="_blank" rel="external">Deploying section</a>）。<br>对于Python application，它缺少SBT/Maven项目管理，spark-streaming-kafka-0-8_2.11和它的依赖可以使用–packages直接添加到spark-submit（查看<a href="http://spark.apache.org/docs/latest/submitting-applications.html" title="Submitting Applications" target="_blank" rel="external">Application Submission Guide</a>）。这样：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0 ...</div></pre></td></tr></table></figure></p>
<p>或者，你能够从<a href="http://search.maven.org/#search|ga|1|a%3A%22spark-streaming-kafka-0-8-assembly_2.11%22%20AND%20v%3A%222.0.0%22" target="_blank" rel="external">Maven repository</a>下载Maven坐标的JAR并使用–jars将其添加到spark-submit。</p>
<h2 id="Approach-2-Direct-Approach-No-Receivers"><a href="#Approach-2-Direct-Approach-No-Receivers" class="headerlink" title="Approach 2: Direct Approach (No Receivers)"></a>Approach 2: Direct Approach (No Receivers)</h2><p>这个新的五receiver的直接解决方法已经在Spark 1.3版本中引入，来确保健壮的端对端的保证。代替receivers来接收数据，这个方法周期性的查询Kafka来获取每个topic + partition中最后的offset，然后相应的定义每个batch中处理offset的范围。当处理数据的job启动时，使用Kafka的简单API从Kafka中读取定义的offset范围（类似从文件系统中读取文件）。注意这是一个在Spark 1.3中引入的实验性特征，用于Scala和Java API，在Spark 1.4中才有了Python的API。<br>这个解决方案在基于receiver的解决方案上有如下优势：</p>
<ul>
<li><em>Simplified Parallelism:</em> 不需要创建多个输入Kafka streams以及联合他们。使用directStream，Spark Streaming将会创建和要消费的Kafka partitions数量相同的RDD partitions，这样将病习惯你的从Kafka读取数据。因此在Kafka和RDD partitions之间有一个一对一的映射，这样更加容易理解和调整。</li>
<li><em>Efficiency:</em> 在第一个解决方案中要实现零数据丢失需要将数据存储到Write Ahead Log中，这种方式是进一步的复制数据。这实际上是效率低的，因为数据实际上复制了两次-一次被Kafka，另一次被Write Ahead Log。第二种解决方案消除了这个问题，因为没有了receiver，因此不需要Write Ahead Log。只要你有足够的Kafka保留时间，message能够从Kafka中恢复。</li>
<li><em>Exactly-once semantics:</em> 第一种解决方案使用Kafka的高级API将消费掉的offset存储到Zookeeper中。这是从Kafka消费数据的传统方式。而这种解决方案（和write ahead log混合）能够保证零数据丢失（例如至少一次的语义），在一些故障下，有很小的可能性是一些数据会消费两次。这种存在是因为由Spark Streaming可靠的接收的数据和由Zookeeper跟踪的offsets之间的矛盾造成的。因此，在第二解决方案中，我们使用了简单的Kafka API，简单的API没有使用Zookeeper。通过Spark Streaming中它的checkpoints来跟踪offset。这消除了Spark Streaming和Zookeeper/Kafka之间的差异，因此尽管有故障，但是被Spark Streaming接收到的记录实际上只有一次。为了达到输出你的结果只有一次的语义，你的输出操作将数据保存到外部存储中必须是幂等或原子事务的，这样来保存结果和offset（查看主编程指南中<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#semantics-of-output-operations" title="Semantics of output operations" target="_blank" rel="external">Semantics of output operations</a>获取更多信息）。</li>
</ul>
<p>注意，这种解决方案中一个不利条件是不会在Zookeeper中更新offset，因此那些基于Zookeeper的监控工具将不会更新进度。然而，你能够在这种解决方案中访问每个batch中处理过的offsets并自己更新到Zookeeper（参考下面）。<br>接下来，我们讨论如何在你的application中使用这种解决方案。</p>
<ul>
<li><p><strong>Linking:</strong> 这种解决方案在Scala和Java application中支持。使用如下的坐标来连接你的SBT/Maven项目（查看主要编程指南中<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#linking" title="Linking" target="_blank" rel="external">Linking section</a>获取更多信息）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">groupId = org.apache.spark</div><div class="line">artifactId = spark-streaming-kafka<span class="number">-0</span><span class="number">-8</span>_2<span class="number">.11</span></div><div class="line">version = <span class="number">2.0</span><span class="number">.0</span></div></pre></td></tr></table></figure>
</li>
<li><p><strong>Programming:</strong> 在Streaming application代码中，引入KafkaUtils并如下创建一个输入DStream。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka._</div><div class="line"></div><div class="line"><span class="keyword">val</span> directKafkaStream = <span class="type">KafkaUtils</span>.createDirectStream[</div><div class="line">    [key <span class="class"><span class="keyword">class</span>], [value class], [key decoder class], [value decoder class] ](<span class="params"></span></span></div><div class="line">    streamingContext, [map of <span class="type">Kafka</span> parameters], [set of topics to consume])</div></pre></td></tr></table></figure>
</li>
</ul>
<p>你还可以传递一个<em>messageHandler</em>到<em>createDirectStream</em>来访问<em>MessageAndMetadata</em>，MessageAndMetadata包含了关于当前message的元数据和要将它转换成的目标类型。查看<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.kafka.KafkaUtils$" title="KafkaUtils" target="_blank" rel="external">API docs</a>和<a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala" title="DirectKafkaWordCount" target="_blank" rel="external">examples</a>。<br>在Kafka的参数中，你必须指定metadata.broker.list或bootstrap.servers。默认，它将从每个Kafka partition的最后的offset处开始消费。如果你在Kafka参数中设置auto.offset.reset为smallest，那么它将从最小的offset处开始消费。<br>使用KafkaUtils.createDirectStream的其他变量，你还能够从任意offset处开始消费。此外，如果你想要访问每个batch中已经消费的Kafka offset，你可以如下这么做。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Hold a reference to the current offset ranges, so it can be used downstream</span></div><div class="line"><span class="keyword">var</span> offsetRanges = <span class="type">Array</span>[<span class="type">OffsetRange</span>]()</div><div class="line"></div><div class="line">directKafkaStream.transform &#123; rdd =&gt;</div><div class="line">  offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</div><div class="line">  rdd</div><div class="line">&#125;.map &#123;</div><div class="line">          ...</div><div class="line">&#125;.foreachRDD &#123; rdd =&gt;</div><div class="line">  <span class="keyword">for</span> (o &lt;- offsetRanges) &#123;</div><div class="line">    println(<span class="string">s"<span class="subst">$&#123;o.topic&#125;</span> <span class="subst">$&#123;o.partition&#125;</span> <span class="subst">$&#123;o.fromOffset&#125;</span> <span class="subst">$&#123;o.untilOffset&#125;</span>"</span>)</div><div class="line">  &#125;</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>如果你想要基于Zookeeper的Kafka监控工具来展示streaming application的进度，你还可以使用这个来更新Zookeeper。<br>注意到HasOffsetRanges的类型转换，将只有它在第一个方法在directKafkaStream上调用完成后才会成功，不会晚于一个方法链（不知道这句是否正确）。你能够使用transform()方法来代替foreachRDD()方法来作为你调用的第一个方法以便访问offsets，然后调用进一步的Spark方法。然而，需要注意的是在任何shuffle或repartition方法之后，RDD partition和Kafka partition之间的一对一映射将不再维持，例如reduceByKey()方法或window()方法。<br>另一个需要注意的是，因为这个解决方案没有使用receiver，标准的receiver-related（spark.streaming.receiver.<em>格式的配置）将不会应用到由这种解决方案生成的输入DStreams上（但是会应用到其他输入DStream上）。相反，会是用spark.streaming.kafka.</em>的配置。一个重要的东西是spark.streaming.kafka.maxRatePerPartition，它将限制该API的从每个Kafka partition读取数据的速度（每秒message的条数）。</p>
<ul>
<li><strong>Deploying:</strong> 这跟第一种解决方案中的相同。</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2016/08/17/spark-streaming/" itemprop="url">
                  Spark Streaming Programming Guide
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-17T16:40:48+08:00" content="2016-08-17">
              2016-08-17
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文是Spark Streaming手册的翻译文档，会随着自己的实现进行更新，官方文档请<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" title="Spark Streming Programming Guide">参考</a>。</p>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>Spark Streaming是核心Spark API的一个延伸，它对实时数据流进行可扩展的、高吞吐量的、容灾的进行处理。数据可以从很多源（如Kafka、Flume、Kinesis或TCP socket）进行提取，然后被复杂的算法组合处理，这些复杂的算法可以使用高级别的函数，如map、reduce、join和window。最后，被处理过的数据可以推出到外部文件系统、数据库和实时图表中。实际上你可以在数据流上应用Spark的<a href="http://spark.apache.org/docs/latest/ml-guide.html" title="Machine Learning Library (MLlib) Guide">机器学习</a>和<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html" title="GraphX Programming Guide">图处理</a>。<br><img src="http://oaavtz33a.bkt.clouddn.com/streaming-arch.png" alt="spark streaming architecture" title="spark streaming architecture"><br>在内部，它如下工作。Spark Streaming接收实时的输入数据流，并将数据划分到批次中，然后在批次中数据被Spark引擎处理并生成最终的结果流。<br><img src="http://oaavtz33a.bkt.clouddn.com/streaming-flow.png" alt="Spark Streaming data flow" title="Spark Streaming data flow"><br>Spark Streaming提供了一个高级别的抽象，叫做discretized stream或DStream，它代表了一个连续的数据流。DStream可以从来自数据源（如Kafka、Flume和Kinesis）的输入数据流创建，也可以通过在其他DStream上应用高级别的操作来创建，一个DStream以一个<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD" title="RDD">RDDs</a>序列来表示。<br>本指南展示了如何开始使用DStreams来编写Spark Streaming程序。你可以使用Scala、Java或Python（从Spark1.2中引入）来编写Spark Streaming程序，这些语言的代码都会在本指南中提供。你会发现tabs在本指南中随处可见，是你可以在不同语言的代码片段之间任意选择。<br><strong>注意：</strong>在Python中有少量的APIs是不同或不可用的。贯穿整个指南，你会发现<em>Python API</em>标签高亮了这些不同。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/blog/2016/08/17/spark-streaming/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/blog/page/2/">2</a><a class="extend next" rel="next" href="/blog/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/blog/uploads/avatar.png"
               alt="baimoon" />
          <p class="site-author-name" itemprop="name">baimoon</p>
          <p class="site-description motion-element" itemprop="description">Baimoon's blog</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/blog/archives">
              <span class="site-state-item-count">29</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/blog/categories">
                <span class="site-state-item-count">15</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/blog/tags">
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/baimoon" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://gallery.xrange.org" title="xrange" target="_blank">xrange</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016-07 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">baimoon</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/blog/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/blog/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/blog/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/blog/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
