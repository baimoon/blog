<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/blog/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/blog/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/blog/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico?v=5.0.1" />






<meta name="description" content="Baimoon&apos;s blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Baimoon's Note">
<meta property="og:url" content="http://baimoon.github.io/index.html">
<meta property="og:site_name" content="Baimoon's Note">
<meta property="og:description" content="Baimoon&apos;s blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Baimoon's Note">
<meta name="twitter:description" content="Baimoon&apos;s blog">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://baimoon.github.io/"/>

  <title> Baimoon's Note </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/blog/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Baimoon's Note</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/blog/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/blog/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2019/01/31/spark-2-11-metircs/" itemprop="url">
                  spark-2-11-metircs
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2019-01-31T16:05:32+08:00" content="2019-01-31">
              2019-01-31
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-11/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.11</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文按照Metric的基本认识、Spark对Metrics system的配置、源码分析MetricsSystem的顺序进行学习</p>
<h1 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h1><p>目前最流行的metrics库是dropwizard/metircs，spark使用的也是这个库。下面我们介绍一下dropwizard/metircs的概念和用法。</p>
<h2 id="Maven依赖"><a href="#Maven依赖" class="headerlink" title="Maven依赖"></a>Maven依赖</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&lt;dependencies&gt;</div><div class="line">    &lt;dependency&gt;</div><div class="line">        &lt;groupId&gt;io.dropwizard.metrics&lt;/groupId&gt;</div><div class="line">        &lt;artifactId&gt;metrics-core&lt;/artifactId&gt;</div><div class="line">        &lt;version&gt;$&#123;metrics.version&#125;&lt;/version&gt;</div><div class="line">    &lt;/dependency&gt;</div><div class="line">&lt;/dependencies&gt;</div></pre></td></tr></table></figure>
<h2 id="Metric的基本使用"><a href="#Metric的基本使用" class="headerlink" title="Metric的基本使用"></a>Metric的基本使用</h2><p>MetricRegistry类是Metrics的核心，他是存放应用中所有metrics的容器，也是我们使用Metrics的第一步：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">MetricRegistry metricRegistry = <span class="keyword">new</span> MetricsRegistry();</div></pre></td></tr></table></figure></p>
<p>每个Metrics都有一个唯一的名字，我们可以通过MetricRegistry.name来生成：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">metricName = MetricRegistry.name(<span class="string">"name"</span>, <span class="string">"namesecond"</span>, <span class="string">"namethrid"</span>); <span class="comment">//生成name.namescond.namethrid</span></div></pre></td></tr></table></figure></p>
<p>有了MetricRegistry和Metric之后，接下来需要进行注册<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">metricRegistry.register(metricName, MetricType)</div></pre></td></tr></table></figure></p>
<p>注册的时候，需要指定Metric的类型，详细的类型，后面会介绍。<br>以下是Spark中的一些使用参考：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="keyword">val</span> metricRegistry = <span class="keyword">new</span> <span class="type">MetricRegistry</span>()</div><div class="line"></div><div class="line">metricRegistry.register(<span class="type">MetricRegistry</span>.name(<span class="string">"threadpool"</span>, <span class="string">"activeTasks"</span>), <span class="keyword">new</span> <span class="type">Gauge</span>[<span class="type">Int</span>] &#123;</div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>: <span class="type">Int</span> = threadPool.getActiveCount()</div><div class="line">&#125;)</div></pre></td></tr></table></figure></p>
<h2 id="Metrics类型"><a href="#Metrics类型" class="headerlink" title="Metrics类型"></a>Metrics类型</h2><p>Metrics有五种类型，分别是Gauges、Counters、Meters、Histograms和Timers。</p>
<h3 id="Gauges"><a href="#Gauges" class="headerlink" title="Gauges"></a>Gauges</h3><p>Gauges是最简单的Metrics类型，该对象中只有一个方法getValue用于返回统计的值，下面是Gauges接口的定义：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Gauge</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Metric</span> </span>&#123;</div><div class="line">    <span class="function">T <span class="title">getValue</span><span class="params">()</span></span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>从方法可以看出，Gauge中的getValue可以返回与Gauge定义类型相同的任意类型。<br>以下是Spark中Gauges类型Metric的定义：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">metricRegistry.register(<span class="type">MetricRegistry</span>.name(<span class="string">"threadpool"</span>, <span class="string">"activeTasks"</span>), <span class="keyword">new</span> <span class="type">Gauge</span>[<span class="type">Int</span>] &#123;</div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>: <span class="type">Int</span> = threadPool.getActiveCount()</div><div class="line">&#125;)</div><div class="line"></div><div class="line"><span class="comment">// Gauge for executor thread pool's approximate total number of tasks that have been completed</span></div><div class="line">metricRegistry.register(<span class="type">MetricRegistry</span>.name(<span class="string">"threadpool"</span>, <span class="string">"completeTasks"</span>), <span class="keyword">new</span> <span class="type">Gauge</span>[<span class="type">Long</span>] &#123;</div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>: <span class="type">Long</span> = threadPool.getCompletedTaskCount()</div><div class="line">&#125;)</div></pre></td></tr></table></figure></p>
<p>分别定义了一个Int类型和一个Long类型的Gauges。</p>
<h3 id="Counters"><a href="#Counters" class="headerlink" title="Counters"></a>Counters</h3><p>Counter是计数器，既然是计数器，因此可增可减。Counter其实是对AtomicLong的封装。</p>
<h4 id="Meters支持的方法"><a href="#Meters支持的方法" class="headerlink" title="Meters支持的方法"></a>Meters支持的方法</h4><p>inc(): 计数器自加。<br>dec(): 计数器自减。</p>
<h4 id="生成方法"><a href="#生成方法" class="headerlink" title="生成方法"></a>生成方法</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pendingJobs = registry.counter(MetricRegistry.name(Queue.class,<span class="string">"pending-jobs"</span>,<span class="string">"size"</span>));</div></pre></td></tr></table></figure>
<h3 id="Meters"><a href="#Meters" class="headerlink" title="Meters"></a>Meters</h3><p>Meters用来计算事件发生的速率。Meters会统计近1分钟、5分钟、15分钟以及全部时间的速率。</p>
<h4 id="Meters可用的方法"><a href="#Meters可用的方法" class="headerlink" title="Meters可用的方法"></a>Meters可用的方法</h4><p>mark()方法：表示发生一次事件。</p>
<h4 id="生成方法-1"><a href="#生成方法-1" class="headerlink" title="生成方法"></a>生成方法</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Meter meterTps = registry.meter(MetricRegistry.name(MeterTest.class,<span class="string">"request"</span>,<span class="string">"tps"</span>));</div></pre></td></tr></table></figure>
<h3 id="Histograms"><a href="#Histograms" class="headerlink" title="Histograms"></a>Histograms</h3><p>Histograms统计数据的分布情况。比如最大值、最小值、中间值、中位数、75百分位、90百分位、98百分位、99百分位和99.9百分位的值。</p>
<h4 id="Histograms可用的方法"><a href="#Histograms可用的方法" class="headerlink" title="Histograms可用的方法"></a>Histograms可用的方法</h4><p>histogram.update(…): 更新一个数</p>
<h4 id="生成方法-2"><a href="#生成方法-2" class="headerlink" title="生成方法"></a>生成方法</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Histogram histogram - <span class="keyword">new</span> Histogram(...)</div><div class="line">registery.register(MetricRegistry.name(...), histogram)</div></pre></td></tr></table></figure>
<h3 id="Timers"><a href="#Timers" class="headerlink" title="Timers"></a>Timers</h3><p>Timer可以理解为Histogram和Meter的结合。</p>
<h4 id="Timer可用的方法"><a href="#Timer可用的方法" class="headerlink" title="Timer可用的方法"></a>Timer可用的方法</h4><p>timer.time(): 记录时间</p>
<h4 id="生成方法-3"><a href="#生成方法-3" class="headerlink" title="生成方法"></a>生成方法</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Timer timer = registry.timer(MetricRegistry.name(...))</div></pre></td></tr></table></figure>
<h2 id="Metrics的输出"><a href="#Metrics的输出" class="headerlink" title="Metrics的输出"></a>Metrics的输出</h2><h3 id="通过JMX"><a href="#通过JMX" class="headerlink" title="通过JMX"></a>通过JMX</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">final</span> JmxReporter reporter = jmxReporter.forRegistry(registry).build()</div><div class="line">reporter.start()</div></pre></td></tr></table></figure>
<p>一旦reporter被启动，所有在registry中的metrics可以通过JConsole或VisualVM（如果安装了MBeans插件）可见。</p>
<h3 id="其他方式"><a href="#其他方式" class="headerlink" title="其他方式"></a>其他方式</h3><p>除了JMX，Metrics还能够以HTTP、STDOUT、CSV、SLFJ、Ganglia和Graphite的方式输出。 详细可以参考：<a href="https://metrics.dropwizard.io/3.1.0/getting-started" target="_blank" rel="external">https://metrics.dropwizard.io/3.1.0/getting-started</a> 和 <a href="https://metrics.dropwizard.io/3.1.0/manual/core/" target="_blank" rel="external">https://metrics.dropwizard.io/3.1.0/manual/core/</a></p>
<h1 id="Spark中Metrics的配置"><a href="#Spark中Metrics的配置" class="headerlink" title="Spark中Metrics的配置"></a>Spark中Metrics的配置</h1><p>Spark的Metrics的配置，在$SPARK_HOME/conf/metrics.properties文件中配置。如果用户需要制定自己的配置文件，可以通过spark.metrics.conf来指定。默认情况下，driver或executor的root命名空间为spark.app.id，但是在某些情况下，用户想要跨driver或executor跟踪Metrics，对于这种情况，可以使用spark.metrics.namespace来自定义命名空间。</p>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>在Spark中，Metrics根据Spark的组建被划分中不同的实例。对于每个实例，可以配置一组sinks来报告metrics。如下是当前支持的实例：</p>
<table>
<thead>
<tr>
<th style="text-align:left">实例名</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">master</td>
<td style="text-align:left">standalone模式Spark的master进程</td>
</tr>
<tr>
<td style="text-align:left">application</td>
<td style="text-align:left">master中的组件，对不同applications进行报告</td>
</tr>
<tr>
<td style="text-align:left">worker</td>
<td style="text-align:left">standalone模式Spark的worker进程</td>
</tr>
<tr>
<td style="text-align:left">executor</td>
<td style="text-align:left">一个Spark executor</td>
</tr>
<tr>
<td style="text-align:left">driver</td>
<td style="text-align:left">Spark的driver进程</td>
</tr>
<tr>
<td style="text-align:left">shuffleService</td>
<td style="text-align:left">Spark的Shuffle服务</td>
</tr>
<tr>
<td style="text-align:left">applicationMaster</td>
<td style="text-align:left">当在Yarn上运行时，Spark的application master</td>
</tr>
</tbody>
</table>
<h2 id="Sinks"><a href="#Sinks" class="headerlink" title="Sinks"></a>Sinks</h2><p>每个实例能够汇报给0个或多个sink。Sinks包含在org.apache.spark.metrics.sink包中，有如下sink：</p>
<table>
<thead>
<tr>
<th style="text-align:left">sink</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ConsoleSink</td>
<td style="text-align:left">记录metrics信息到 console</td>
</tr>
<tr>
<td style="text-align:left">CSVSink</td>
<td style="text-align:left">以一定的间隔，将metrics数据导出到CSV文件</td>
</tr>
<tr>
<td style="text-align:left">JmxSink</td>
<td style="text-align:left">将metrics注册到JMX console</td>
</tr>
<tr>
<td style="text-align:left">MetricsServlet</td>
<td style="text-align:left">在已有的Spark UI中添加一个Servlet，以JONS数据格式来服务metrics数据</td>
</tr>
<tr>
<td style="text-align:left">GraphiteSink</td>
<td style="text-align:left">将metrics发送给Graphite节点</td>
</tr>
<tr>
<td style="text-align:left">Slf4jSink</td>
<td style="text-align:left">将metrics发送给slf4j</td>
</tr>
<tr>
<td style="text-align:left">StatsdSink</td>
<td style="text-align:left">将metrics发送给StatsD节点</td>
</tr>
<tr>
<td style="text-align:left">GangliaSink</td>
<td style="text-align:left">将metrics发送给Ganglia节点，需要重新编译Spark，将gangliaSink打包进去，默认不包含</td>
</tr>
</tbody>
</table>
<h2 id="metrics-properties"><a href="#metrics-properties" class="headerlink" title="metrics.properties"></a>metrics.properties</h2><p>可以参考：$SPARK_HOME/conf/metrics.properties.template文件，在这个文件中对source、sink等作了详细的解释，并提供了例子。</p>
<h1 id="从Spark源码看Metrics的使用"><a href="#从Spark源码看Metrics的使用" class="headerlink" title="从Spark源码看Metrics的使用"></a>从Spark源码看Metrics的使用</h1><p>Spark中对于Metrics的处理集中在core模块下的org.apache.spark.metrics包中。<br>包结构如下：<br><img src="/attach/5c52acc1de555.png" alt="image.png"><br>其中MetricsSystem是该模块的入口，MetricsConfig为模块的配置管理，Source是Metric系统的数据源，Sink是Metrics的量输出。</p>
<h2 id="MetricsConfig"><a href="#MetricsConfig" class="headerlink" title="MetricsConfig"></a>MetricsConfig</h2><p>MetricsConfig用来加载metrics的配置，并对MetricsSystem进行相关的配置。在MetricsSystem类中，会生成MetricsConfig对象，并进行初始化操作。</p>
<h3 id="首先看看MetricsConfig的一些属性"><a href="#首先看看MetricsConfig的一些属性" class="headerlink" title="首先看看MetricsConfig的一些属性"></a>首先看看MetricsConfig的一些属性</h3><p>正如Spark文档说的，默认的Metrics配置文件为 metrics.properties</p>
<h3 id="initialize方法"><a href="#initialize方法" class="headerlink" title="initialize方法"></a>initialize方法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>() &#123;</div><div class="line">  <span class="comment">// 添加默认属性，用于没有配置文件的情况</span></div><div class="line">  setDefaultProperties(properties)</div><div class="line">  <span class="comment">// 从metrics配置文件中加载metrics配置</span></div><div class="line">  loadPropertiesFromFile(conf.getOption(<span class="string">"spark.metrics.conf"</span>))</div><div class="line"></div><div class="line">  <span class="comment">// 从Spark配置文件中加载metrics配置</span></div><div class="line">  <span class="keyword">val</span> prefix = <span class="string">"spark.metrics.conf."</span></div><div class="line">  conf.getAll.foreach &#123;</div><div class="line">    <span class="keyword">case</span> (k, v) <span class="keyword">if</span> k.startsWith(prefix) =&gt;</div><div class="line">      properties.setProperty(k.substring(prefix.length()), v)</div><div class="line">    <span class="keyword">case</span> _ =&gt;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  perInstanceSubProperties = subProperties(properties, <span class="type">INSTANCE_REGEX</span>)</div><div class="line">  <span class="comment">// 判断是否以 * 开始的实例配置，这种表示是所有实例的配置（称为默认实例配置）</span></div><div class="line">  <span class="keyword">if</span> (perInstanceSubProperties.contains(<span class="type">DEFAULT_PREFIX</span>)) &#123;</div><div class="line">    <span class="comment">// 获取默认实例配置</span></div><div class="line">    <span class="keyword">val</span> defaultSubProperties = perInstanceSubProperties(<span class="type">DEFAULT_PREFIX</span>).asScala</div><div class="line">    <span class="comment">// 循环所有非默认实例配置，将实例配置添加到非实例配置中（注意，实例配置已有的属性不会被覆盖）</span></div><div class="line">    <span class="keyword">for</span> ((instance, prop) &lt;- perInstanceSubProperties <span class="keyword">if</span> (instance != <span class="type">DEFAULT_PREFIX</span>);</div><div class="line">         (k, v) &lt;- defaultSubProperties <span class="keyword">if</span> (prop.get(k) == <span class="literal">null</span>)) &#123;</div><div class="line">      prop.put(k, v)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在initialize方法中，首先会调用setDefaultProperties方法来加载默认配置，这是为了应对没有设置Metrics配置文件的情况（连默认的metrics.properties都没有提供）。<br>然后会调用loadPropertiesFromFile方法，从指定的配置文件中加载Metrics配置。<br>再然后，会读取Spark的所有配置项，从中筛选以”spark.metrics.conf.”为前缀的配置项，并将配置项添加到属性中。需要注意的是，在添加属性的时候，并非以配置项的全名作为属性，而是以子名作为属性名，例如：spark.metrics.conf.xxx，那么会使用xxx作为属性名，而spark.metrics.conf.xxx.x1则会使用xxx.x1作为属性名。<br>最后，该方法还会对实例属性（指定了实例的配置）进行整理。实例属性的配置类似driver.path这样的，driver就是实例，因此这里就有一个问题，对于所有实例通用的配置应该怎么设置呢？答案是<em>.path这样的。有</em>的则表示通用的默认值，没有<em>的配置，第一个dot（.）前的为实例。有实例的配置会覆盖没有实例的配置。代码中的属性private val INSTANCE_REGEX = “^(\</em>|[a-zA-Z]+)\.(.+)”.r，就解释了实例配置的模式，要不就是以“<em>”开头，要么就以字母开头，以字母开头的表示具体实例，以“</em>”开头的表示所有实例通用。<br>因此也就可以理解setDefaultProperties方法中设置默认值的作用。整理的作用就是将默认实例配置（以“*”开头的实例配置）添加到非默认实例配置中（不会覆盖非默认实例配置已有的属性）。<br>比如，有如下属性<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;\\*.class&quot;-&gt;&quot;default_class&quot;, &quot;\\*.path&quot;-&gt;&quot;default_path, &quot;driver.path&quot;-&gt;&quot;driver_path&quot;&#125;</div></pre></td></tr></table></figure></p>
<p>对于driver实例，他最后得到的属性为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;&#123;&quot;driver&quot;-&gt;Map(&quot;path&quot;-&gt;&quot;driver_path&quot;, &quot;class&quot;-&gt;&quot;default_class&quot;&#125;&#125;</div></pre></td></tr></table></figure></p>
<h3 id="setDefaultProperties"><a href="#setDefaultProperties" class="headerlink" title="setDefaultProperties"></a>setDefaultProperties</h3><p>当没有配置metrics配置文件时（是指连默认的配置文件都不存在），采取的默认Metrics配置。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">setDefaultProperties</span></span>(prop: <span class="type">Properties</span>) &#123;</div><div class="line">  <span class="comment">// 默认开启了servlet类型的sink，指定servlet的class为org.apache.spark.metrics.sink.MetricsServlet</span></div><div class="line">  prop.setProperty(<span class="string">"*.sink.servlet.class"</span>, <span class="string">"org.apache.spark.metrics.sink.MetricsServlet"</span>)</div><div class="line">  <span class="comment">// 先为所有实例设置，再为master和applications类型的实例设置</span></div><div class="line">  prop.setProperty(<span class="string">"*.sink.servlet.path"</span>, <span class="string">"/metrics/json"</span>)</div><div class="line">  prop.setProperty(<span class="string">"master.sink.servlet.path"</span>, <span class="string">"/metrics/master/json"</span>)</div><div class="line">  prop.setProperty(<span class="string">"applications.sink.servlet.path"</span>, <span class="string">"/metrics/applications/json"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="loadPropertiesFromFile"><a href="#loadPropertiesFromFile" class="headerlink" title="loadPropertiesFromFile"></a>loadPropertiesFromFile</h3><p>方法需要path参数（配置文件的名字-从Spark的配置项spark.metrics.conf中读取），如果给定的path存在，则从这个文件中加载，如果path不存在，则从classPath目录中加载默认配置文件metrics.properties。</p>
<h2 id="MetricsSystem"><a href="#MetricsSystem" class="headerlink" title="MetricsSystem"></a>MetricsSystem</h2><p>MetricsSystem类是对外提供的一个对象，从调用来看，在Master、SparkEnv和Work中都有创建MetricsSystem的代码。其中Master和Work是对于standalone模式下创建的。SparkEnv中是对driver和executor进行创建的。生成MetricsSystem的时候是需要指定instance参数的。因此从创建MetricsSystem的代码我们也能够知道MetricsSystem都支持哪些instance。<br>TODO 对于applications的创建，有时间再找<br>这里，我们先抛开对MetricsSystem的操作，我们先看看MetricsSystem的实现</p>
<h3 id="创建MetricsSystem"><a href="#创建MetricsSystem" class="headerlink" title="创建MetricsSystem"></a>创建MetricsSystem</h3><p>从所有的生成MetricsSyste的代码来看，MetricsSystem都是通过如下代码创建的：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> metricsSystem = <span class="type">MetricsSystem</span>.createMetricsSystem(<span class="string">"worker"</span>, conf, securityMgr) <span class="comment">// 这里创建的一个work实例的MetricsSystem</span></div></pre></td></tr></table></figure>
<p>而对于createMetricsSystem方法的定义也很多简单，就是new了一个MetricsSystem对象：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createMetricsSystem</span></span>(</div><div class="line">    instance: <span class="type">String</span>, conf: <span class="type">SparkConf</span>, securityMgr: <span class="type">SecurityManager</span>): <span class="type">MetricsSystem</span> = &#123;</div><div class="line">  <span class="keyword">new</span> <span class="type">MetricsSystem</span>(instance, conf, securityMgr)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="注册Source"><a href="#注册Source" class="headerlink" title="注册Source"></a>注册Source</h3><p>创建了MetricsSystem，接下来就需要向它注册source。我们以Worker中MetricsSystem的使用来作为参考。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">metricsSystem.registerSource(workerSource)</div><div class="line">metricsSystem.start()</div><div class="line">metricsSystem.getServletHandlers.foreach(webUi.attachHandler)</div></pre></td></tr></table></figure></p>
<p>因此，我们来看一下MetricsSystem的registerSource方法<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">registerSource</span></span>(source: <span class="type">Source</span>) &#123;</div><div class="line">  sources += source</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">val</span> regName = buildRegistryName(source)</div><div class="line">    registry.register(regName, source.metricRegistry)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">IllegalArgumentException</span> =&gt; logInfo(<span class="string">"Metrics already registered"</span>, e)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>将source添加Source列表中，然后调用buildRegistryName方法生成source注册使用的名字，然后进行注册，那么需要看一下buildRegistryName方法<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">buildRegistryName</span></span>(source: <span class="type">Source</span>): <span class="type">String</span> = &#123;</div><div class="line">  <span class="keyword">val</span> metricsNamespace = conf.get(<span class="type">METRICS_NAMESPACE</span>).orElse(conf.getOption(<span class="string">"spark.app.id"</span>))</div><div class="line"></div><div class="line">  <span class="keyword">val</span> executorId = conf.getOption(<span class="string">"spark.executor.id"</span>)</div><div class="line">  <span class="keyword">val</span> defaultName = <span class="type">MetricRegistry</span>.name(source.sourceName)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (instance == <span class="string">"driver"</span> || instance == <span class="string">"executor"</span>) &#123;</div><div class="line">    <span class="keyword">if</span> (metricsNamespace.isDefined &amp;&amp; executorId.isDefined) &#123;</div><div class="line">      <span class="type">MetricRegistry</span>.name(metricsNamespace.get, executorId.get, source.sourceName)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// Only Driver and Executor set spark.app.id and spark.executor.id.</span></div><div class="line">      <span class="comment">// Other instance types, e.g. Master and Worker, are not related to a specific application.</span></div><div class="line">      <span class="keyword">if</span> (metricsNamespace.isEmpty) &#123;</div><div class="line">        logWarning(<span class="string">s"Using default name <span class="subst">$defaultName</span> for source because neither "</span> +</div><div class="line">          <span class="string">s"<span class="subst">$&#123;METRICS_NAMESPACE.key&#125;</span> nor spark.app.id is set."</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (executorId.isEmpty) &#123;</div><div class="line">        logWarning(<span class="string">s"Using default name <span class="subst">$defaultName</span> for source because spark.executor.id is "</span> +</div><div class="line">          <span class="string">s"not set."</span>)</div><div class="line">      &#125;</div><div class="line">      defaultName</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">else</span> &#123; defaultName &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>buildRegistryName方法，就是根据Source，构建一个向MetricRegistry注册的名字。这个名字是唯一的。其基本思路就是根据namespace（从spark.metrics.namespace或spark.app.id中取）和executorId（从spark.executor.id中取）构建一个注册用的名字，但是需要注意的是，是否使用这两个值作为构建的条件，要依赖实例是否是drvier或executor，否则一律使用source的名字生成。<br>注册好Source，调用MetricsSystem的start方法启动MetricsSystem。</p>
<h3 id="启动start方法"><a href="#启动start方法" class="headerlink" title="启动start方法"></a>启动start方法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123;</div><div class="line">  require(!running, <span class="string">"Attempting to start a MetricsSystem that is already running"</span>)</div><div class="line">  running = <span class="literal">true</span></div><div class="line">  <span class="type">StaticSources</span>.allSources.foreach(registerSource)</div><div class="line">  registerSources()</div><div class="line">  registerSinks()</div><div class="line">  sinks.foreach(_.start)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>start方法代码很少，但是包含的东西却挺多。注册静态Source，所谓的静态Source，其实就是不依赖用户配置的Source。接着调用registerSources方法，这个注册是读取配置的文件中当前实例（instance）的配置，对Source的class进行实例化并注册。然后是调用registerSinks方法来注册Sink，也是将配置文件中当前实例（instance）的配置进行操作，对Sink的class进行实例化，并加入到Sink集合中等待调用（调用sink的report方法）；最后启动所有的sink。</p>
<h2 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h2><p>Source就是Metrics的数据源，我们对Source中的统计数据进行操作，Metrics系统会从已经注册的Source中获取这些数据，然后由Sink报告出去。下面我们以WorkSource来进行分析，看看如何自定义一个Source。</p>
<h3 id="WorkSource"><a href="#WorkSource" class="headerlink" title="WorkSource"></a>WorkSource</h3><p>先看一下WorkSource的定义：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[worker] <span class="class"><span class="keyword">class</span> <span class="title">WorkerSource</span>(<span class="params">val worker: <span class="type">Worker</span></span>) <span class="keyword">extends</span> <span class="title">Source</span> </span>&#123;</div><div class="line">  <span class="keyword">override</span> <span class="keyword">val</span> sourceName = <span class="string">"worker"</span></div><div class="line">  <span class="keyword">override</span> <span class="keyword">val</span> metricRegistry = <span class="keyword">new</span> <span class="type">MetricRegistry</span>()</div><div class="line"></div><div class="line">  metricRegistry.register(<span class="type">MetricRegistry</span>.name(<span class="string">"executors"</span>), <span class="keyword">new</span> <span class="type">Gauge</span>[<span class="type">Int</span>] &#123;</div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>: <span class="type">Int</span> = worker.executors.size</div><div class="line">  &#125;)</div><div class="line"></div><div class="line">  <span class="comment">// Gauge for cores used of this worker</span></div><div class="line">  metricRegistry.register(<span class="type">MetricRegistry</span>.name(<span class="string">"coresUsed"</span>), <span class="keyword">new</span> <span class="type">Gauge</span>[<span class="type">Int</span>] &#123;</div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>: <span class="type">Int</span> = worker.coresUsed</div><div class="line">  &#125;)</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>WorkSource中metric的类型只涉及到了Gauge，也就是这些值不需要用户参与。对于需要用户参与的我们可以参考StaticSources，稍后也会看到。WorkSource的逻辑很简单，就是注册了一些metric，并制定了这些metric如何取值。</p>
<h3 id="StaticSources-HiveCatalogMetrics"><a href="#StaticSources-HiveCatalogMetrics" class="headerlink" title="StaticSources$HiveCatalogMetrics"></a>StaticSources$HiveCatalogMetrics</h3><p>接下来看一下StaticSources$HiveCatalogMetrics的定义：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">HiveCatalogMetrics</span> <span class="keyword">extends</span> <span class="title">Source</span> </span>&#123;</div><div class="line">  <span class="keyword">override</span> <span class="keyword">val</span> sourceName: <span class="type">String</span> = <span class="string">"HiveExternalCatalog"</span></div><div class="line">  <span class="keyword">override</span> <span class="keyword">val</span> metricRegistry: <span class="type">MetricRegistry</span> = <span class="keyword">new</span> <span class="type">MetricRegistry</span>()</div><div class="line"></div><div class="line">  <span class="keyword">val</span> <span class="type">METRIC_PARTITIONS_FETCHED</span> = metricRegistry.counter(<span class="type">MetricRegistry</span>.name(<span class="string">"partitionsFetched"</span>))</div><div class="line">  <span class="keyword">val</span> <span class="type">METRIC_FILES_DISCOVERED</span> = metricRegistry.counter(<span class="type">MetricRegistry</span>.name(<span class="string">"filesDiscovered"</span>))</div><div class="line">  ...</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="type">METRIC_PARTITIONS_FETCHED</span>.dec(<span class="type">METRIC_PARTITIONS_FETCHED</span>.getCount())</div><div class="line">    <span class="type">METRIC_FILES_DISCOVERED</span>.dec(<span class="type">METRIC_FILES_DISCOVERED</span>.getCount())</div><div class="line">    ...</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// clients can use these to avoid classloader issues with the codahale classes</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">incrementFetchedPartitions</span></span>(n: <span class="type">Int</span>): <span class="type">Unit</span> = <span class="type">METRIC_PARTITIONS_FETCHED</span>.inc(n)</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">incrementFilesDiscovered</span></span>(n: <span class="type">Int</span>): <span class="type">Unit</span> = <span class="type">METRIC_FILES_DISCOVERED</span>.inc(n)</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这个类中metric的类型为Counter，reset方法、incrementFetchedPartitions方法和incrementFilesDiscovered方法就是对这些Counter进行操作。</p>
<h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><p>Spark中已经内置了多种Sink，有ConsoleSink、CsvSink、JmxSink、MetricsServlet等，基本可以满足我们的需求了。接下来我们分析一两个常用Sink的实现。</p>
<h3 id="JmxSink"><a href="#JmxSink" class="headerlink" title="JmxSink"></a>JmxSink</h3><p>JmxSink的定义如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">JmxSink</span>(<span class="params">val property: <span class="type">Properties</span>, val registry: <span class="type">MetricRegistry</span>,</span></span></div><div class="line">    securityMgr: <span class="type">SecurityManager</span>) <span class="keyword">extends</span> <span class="title">Sink</span> &#123;</div><div class="line">  <span class="keyword">val</span> reporter: <span class="type">JmxReporter</span> = <span class="type">JmxReporter</span>.forRegistry(registry).build()</div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123;</div><div class="line">    reporter.start()</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">stop</span></span>() &#123;</div><div class="line">    reporter.stop()</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">report</span></span>() &#123; &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>JmxSink继承Sink特征，因此它需要实现start()、stop()和report()方法。另外在JmxSink定义中创建了一个JmxReporter对象，创建reporter对象时需要MetricRegistry对象，这就将Source和Sink中的Report联系了起来（metric注册到Source的MetricRegistry，source注册到MetricsSystem的MetricRegistry，在生成Sink中Reporter的时候会使用MetrcsSystem中的MetricRegistry）。那么Sink是如何报告自己收集的metrics呢？我理解的是，内部的Reporter会自己报告，也可以调用Sink的report方法，report方法再调用reporter的report方法。（—-TODO，Reporter的汇报，会稍后补充）</p>
<h3 id="ConsoleSink"><a href="#ConsoleSink" class="headerlink" title="ConsoleSink"></a>ConsoleSink</h3><p>ConsoleSink的定义：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ConsoleSink</span>(<span class="params">val property: <span class="type">Properties</span>, val registry: <span class="type">MetricRegistry</span>,</span></span></div><div class="line">    securityMgr: <span class="type">SecurityManager</span>) <span class="keyword">extends</span> <span class="title">Sink</span> &#123;</div><div class="line">  <span class="keyword">val</span> <span class="type">CONSOLE_DEFAULT_PERIOD</span> = <span class="number">10</span></div><div class="line">  <span class="keyword">val</span> <span class="type">CONSOLE_DEFAULT_UNIT</span> = <span class="string">"SECONDS"</span></div><div class="line"></div><div class="line">  <span class="keyword">val</span> <span class="type">CONSOLE_KEY_PERIOD</span> = <span class="string">"period"</span></div><div class="line">  <span class="keyword">val</span> <span class="type">CONSOLE_KEY_UNIT</span> = <span class="string">"unit"</span></div><div class="line"></div><div class="line">  <span class="keyword">val</span> pollPeriod = <span class="type">Option</span>(property.getProperty(<span class="type">CONSOLE_KEY_PERIOD</span>)) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(s) =&gt; s.toInt</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="type">CONSOLE_DEFAULT_PERIOD</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">val</span> pollUnit: <span class="type">TimeUnit</span> = <span class="type">Option</span>(property.getProperty(<span class="type">CONSOLE_KEY_UNIT</span>)) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(s) =&gt; <span class="type">TimeUnit</span>.valueOf(s.toUpperCase(<span class="type">Locale</span>.<span class="type">ROOT</span>))</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="type">TimeUnit</span>.valueOf(<span class="type">CONSOLE_DEFAULT_UNIT</span>)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="type">MetricsSystem</span>.checkMinimalPollingPeriod(pollUnit, pollPeriod)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> reporter: <span class="type">ConsoleReporter</span> = <span class="type">ConsoleReporter</span>.forRegistry(registry)</div><div class="line">      .convertDurationsTo(<span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">      .convertRatesTo(<span class="type">TimeUnit</span>.<span class="type">SECONDS</span>)</div><div class="line">      .build()</div><div class="line"></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123;</div><div class="line">    reporter.start(pollPeriod, pollUnit)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">stop</span></span>() &#123;</div><div class="line">    reporter.stop()</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">report</span></span>() &#123;</div><div class="line">    reporter.report()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>ConsoleSink中也实现了三个必须的方法，除此之外还定义了一个ConsoleReporter。而且它实现的三个方法其实也是对ConsoleReporter进行操作的。查看其他的Sink，也能够发现，每个Sink都有一个Reporter（除了MetricsServlet外，它的逻辑与普通Sink不同）。</p>
<h2 id="Source和Sink的操作"><a href="#Source和Sink的操作" class="headerlink" title="Source和Sink的操作"></a>Source和Sink的操作</h2><h3 id="Source的操作"><a href="#Source的操作" class="headerlink" title="Source的操作"></a>Source的操作</h3><p>对于Source的操作，我们就参考StaticSources$HiveCatalogMetrics中相关metric的操作吧。<br>如下是HiveClientImpl的一个方法，他就在操作Source的METRIC_PARTITIONS_FETCHED。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>(</div><div class="line">      table: <span class="type">CatalogTable</span>,</div><div class="line">      spec: <span class="type">Option</span>[<span class="type">TablePartitionSpec</span>]): <span class="type">Seq</span>[<span class="type">CatalogTablePartition</span>] = withHiveState &#123;</div><div class="line">    <span class="keyword">val</span> hiveTable = toHiveTable(table, <span class="type">Some</span>(userName))</div><div class="line">    <span class="keyword">val</span> parts = spec <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; shim.getAllPartitions(client, hiveTable).map(fromHivePartition)</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(s) =&gt;</div><div class="line">        assert(s.values.forall(_.nonEmpty), <span class="string">s"partition spec '<span class="subst">$s</span>' is invalid"</span>)</div><div class="line">        client.getPartitions(hiveTable, s.asJava).asScala.map(fromHivePartition)</div><div class="line">    &#125;</div><div class="line">    <span class="type">HiveCatalogMetrics</span>.incrementFetchedPartitions(parts.length)</div><div class="line">    parts</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<h3 id="Sink的操作"><a href="#Sink的操作" class="headerlink" title="Sink的操作"></a>Sink的操作</h3><p>对于Sink的调用，是在MetricsSystem的report方法中触发的：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">report</span></span>() &#123;</div><div class="line">  sinks.foreach(_.report())</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>对于MetricsSystem中report方法的调用，会在各个组建停止的时候调用，进行强制汇报。</p>
<h2 id="Source、Sink、MetricsSystem以及MetricsConfig之间的关系"><a href="#Source、Sink、MetricsSystem以及MetricsConfig之间的关系" class="headerlink" title="Source、Sink、MetricsSystem以及MetricsConfig之间的关系"></a>Source、Sink、MetricsSystem以及MetricsConfig之间的关系</h2><p><img src="/attach/5c52afb97da92.png" alt="image.png"></p>
<h2 id="自定义Source"><a href="#自定义Source" class="headerlink" title="自定义Source"></a>自定义Source</h2><p>要定义自己的Source对象，需要实现org.apache.spark.metrics.source.Source特征，并实现sourceName(String类型)和metricRegistry(MetricRegistry类型)方法。之后定义自己的metric类型即可，可用的metric类型已经在上面的第一部分进行介绍。如下是DAGScheduulerSource的定义：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[scheduler] <span class="class"><span class="keyword">class</span> <span class="title">DAGSchedulerSource</span>(<span class="params">val dagScheduler: <span class="type">DAGScheduler</span></span>)</span></div><div class="line">    <span class="keyword">extends</span> <span class="type">Source</span> &#123;</div><div class="line">  <span class="keyword">override</span> <span class="keyword">val</span> metricRegistry = <span class="keyword">new</span> <span class="type">MetricRegistry</span>()</div><div class="line">  <span class="keyword">override</span> <span class="keyword">val</span> sourceName = <span class="string">"DAGScheduler"</span></div><div class="line"></div><div class="line">  metricRegistry.register(<span class="type">MetricRegistry</span>.name(<span class="string">"stage"</span>, <span class="string">"failedStages"</span>), <span class="keyword">new</span> <span class="type">Gauge</span>[<span class="type">Int</span>] &#123;</div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>: <span class="type">Int</span> = dagScheduler.failedStages.size</div><div class="line">  &#125;)</div><div class="line"></div><div class="line">  metricRegistry.register(<span class="type">MetricRegistry</span>.name(<span class="string">"stage"</span>, <span class="string">"runningStages"</span>), <span class="keyword">new</span> <span class="type">Gauge</span>[<span class="type">Int</span>] &#123;</div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>: <span class="type">Int</span> = dagScheduler.runningStages.size</div><div class="line">  &#125;)</div><div class="line"></div><div class="line">  metricRegistry.register(<span class="type">MetricRegistry</span>.name(<span class="string">"stage"</span>, <span class="string">"waitingStages"</span>), <span class="keyword">new</span> <span class="type">Gauge</span>[<span class="type">Int</span>] &#123;</div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>: <span class="type">Int</span> = dagScheduler.waitingStages.size</div><div class="line">  &#125;)</div><div class="line"></div><div class="line">  metricRegistry.register(<span class="type">MetricRegistry</span>.name(<span class="string">"job"</span>, <span class="string">"allJobs"</span>), <span class="keyword">new</span> <span class="type">Gauge</span>[<span class="type">Int</span>] &#123;</div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>: <span class="type">Int</span> = dagScheduler.numTotalJobs</div><div class="line">  &#125;)</div><div class="line"></div><div class="line">  metricRegistry.register(<span class="type">MetricRegistry</span>.name(<span class="string">"job"</span>, <span class="string">"activeJobs"</span>), <span class="keyword">new</span> <span class="type">Gauge</span>[<span class="type">Int</span>] &#123;</div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>: <span class="type">Int</span> = dagScheduler.activeJobs.size</div><div class="line">  &#125;)</div><div class="line"></div><div class="line">  <span class="comment">/** Timer that tracks the time to process messages in the DAGScheduler's event loop */</span></div><div class="line">  <span class="keyword">val</span> messageProcessingTimer: <span class="type">Timer</span> =</div><div class="line">    metricRegistry.timer(<span class="type">MetricRegistry</span>.name(<span class="string">"messageProcessingTime"</span>))</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>在这个类的定义中，实现了Source特征，并重写了metricRegistry和sourceName方法。接下来注册了自己要统计的metric，默认是以servlet中展示这些信息，我们可以通过www.xxxxx:8088/proxy/application_id/metrics/json来查看对应的输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line">&quot;application_1530945921317_6019078.driver.DAGScheduler.job.activeJobs&quot;:&#123;&quot;value&quot;:2&#125;,&quot;application_1530945921317_6019078.driver.DAGScheduler.job.allJobs&quot;:&#123;&quot;value&quot;:77&#125;,&quot;application_1530945921317_6019078.driver.DAGScheduler.stage.failedStages&quot;:&#123;&quot;value&quot;:0&#125;,&quot;application_1530945921317_6019078.driver.DAGScheduler.stage.runningStages&quot;:&#123;&quot;value&quot;:2&#125;,&quot;application_1530945921317_6019078.driver.DAGScheduler.stage.waitingStages&quot;:&#123;&quot;value&quot;:0&#125;,</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>自定义的类截图<br><img src="/attach/5c613f9fdbb6f.png" alt="image.png"><br>该类会在DAGScheduler中进行实例化：<br><img src="/attach/5c613fccd71c9.png" alt="image.png"><br>然后在SparkContext中进行注册：<br><img src="/attach/5c613fea40b45.png" alt="image.png"><br>以下是自定义的source注册后，在servlet中的显示<br><img src="/attach/5c613f66f016d.png" alt="image.png"></p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>如果在配置文件中配置自己的source呢？关键是如何定义。。。需要看一下</p>
<p>对于配置文件中metrics的配置格式如下：</p>
<blockquote>
<p>[instance].[sink|source].[name].[options] = xxxx<br>[instance]可以是master、worker、executor、driver或application，也就是明确了实例的类型。如果让所有实例都可以使用，可以使用“*”代替。<br>[sink|source]指定配置项是sink的信息还是source的信息，只能二选一。<br>[name]指定sink或source的名字。<br>[options]指定sink或source的相关属性，如class。</p>
</blockquote>
<p>注意点：如果是需要进行配置的自定义source，有两点需要注意。</p>
<blockquote>
<p>配置中必须通过class属性指定类名，程序会根据类名进行反射。<br>配置的自定义Source必须有不含构造参数的构造方法，否则无法实例化。</p>
</blockquote>
<h2 id="配置自定义Source"><a href="#配置自定义Source" class="headerlink" title="配置自定义Source"></a>配置自定义Source</h2><p>编辑 metrics.properties配置文件<br>增加如下配置即可<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">driver.source.rwmTest.class=org.apache.spark.metrics.source.RWMTestSource</div></pre></td></tr></table></figure></p>
<p>UI中的显示信息<br><img src="/attach/5c626eb6d83e9.png" alt="image.png"></p>
<h2 id="配置项"><a href="#配置项" class="headerlink" title="配置项"></a>配置项</h2><h3 id="Spark-config"><a href="#Spark-config" class="headerlink" title="Spark config"></a>Spark config</h3><table>
<thead>
<tr>
<th style="text-align:left">配置项</th>
<th style="text-align:left">默认值</th>
<th style="text-align:left">意义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.metrics.conf</td>
<td style="text-align:left">metrics.properties</td>
<td style="text-align:left">Metrics系统的配置文件</td>
</tr>
<tr>
<td style="text-align:left">spark.metrics.conf.xxx</td>
<td style="text-align:left">无</td>
<td style="text-align:left">将xxx作为属性，添加到属性中，可以将其理解为 metrics.properties中配置的一种转移，会和metrics.properties中的配置项放在一起，而且此配置优先级较高</td>
</tr>
<tr>
<td style="text-align:left">spark.metrics.namespace</td>
<td style="text-align:left">无</td>
<td style="text-align:left">metrics的命名空间，参与构建Metrics source注册名的生成</td>
</tr>
<tr>
<td style="text-align:left">spark.app.id</td>
<td style="text-align:left">无</td>
<td style="text-align:left">如果spark.metrics.namespace没有指定值，则使用该值作为namespace，参与构建Metrics source注册名的生成</td>
</tr>
<tr>
<td style="text-align:left">spark.executor.id</td>
<td style="text-align:left">无</td>
<td style="text-align:left">executor的id，参数构建Metrics source注册名的生成</td>
</tr>
</tbody>
</table>
<h3 id="Metrics-config"><a href="#Metrics-config" class="headerlink" title="Metrics config"></a>Metrics config</h3><table>
<thead>
<tr>
<th style="text-align:left">配置项</th>
<th style="text-align:left">默认值</th>
<th style="text-align:left">意义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">source.(.+).(.+)</td>
<td style="text-align:left">无</td>
<td style="text-align:left">source的配置信息，就是以source.xxx.xxx形式的，只能有两个dot(.)</td>
</tr>
<tr>
<td style="text-align:left">sink.(.+).(.+)</td>
<td style="text-align:left">无</td>
<td style="text-align:left">sink的配置信息，就是以sink.xxx.xxx形式的，只能有两个dot(.)</td>
</tr>
</tbody>
</table>
<p>对于source和sink的配置，参考：$SPARK_HOME/conf/metrics.properties.template</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2019/01/23/yarn-restApi/" itemprop="url">
                  Yarn RestApi
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2019-01-23T15:04:35+08:00" content="2019-01-23">
              2019-01-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/yarn/" itemprop="url" rel="index">
                    <span itemprop="name">yarn</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文记录自己在工作学习期间使用Yarn rest api的一些记录</p>
<h1 id="Yarn-rest-api-介绍"><a href="#Yarn-rest-api-介绍" class="headerlink" title="Yarn rest api 介绍"></a>Yarn rest api 介绍</h1><h2 id="获取所有的application-id"><a href="#获取所有的application-id" class="headerlink" title="获取所有的application id"></a>获取所有的application id</h2><h3 id="Url的地址"><a href="#Url的地址" class="headerlink" title="Url的地址"></a>Url的地址</h3><p>http://<rm http="" address:port="">/ws/v1/cluster/apps<br>参考地址：<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html#Cluster_Applications_API" target="_blank" rel="external">https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html#Cluster_Applications_API</a></rm></p>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p>可以提供的参数</p>
<table>
<thead>
<tr>
<th style="text-align:left">参数名</th>
<th style="text-align:left">参数意义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">states</td>
<td style="text-align:left">限制application的状态</td>
</tr>
<tr>
<td style="text-align:left">finalStatus</td>
<td style="text-align:left">限制最终状态，如：UNDEFINED</td>
</tr>
<tr>
<td style="text-align:left">user</td>
<td style="text-align:left">用户名</td>
</tr>
<tr>
<td style="text-align:left">queue</td>
<td style="text-align:left">所使用的队列</td>
</tr>
<tr>
<td style="text-align:left">limit</td>
<td style="text-align:left">限制返回的数量</td>
</tr>
<tr>
<td style="text-align:left">startedTimeBegin</td>
<td style="text-align:left">与startedTimeEnd配合限制application的start的时间</td>
</tr>
<tr>
<td style="text-align:left">startedTimeEnd</td>
<td style="text-align:left">与startedTimeBegin配合限制application的start的时间</td>
</tr>
<tr>
<td style="text-align:left">finishedTimeBegin</td>
<td style="text-align:left">与finishedTimeEnd配合限制application的完成时间</td>
</tr>
<tr>
<td style="text-align:left">finishedTimeEnd</td>
<td style="text-align:left">与finishedTimeBegin配合限制application的完成时间</td>
</tr>
<tr>
<td style="text-align:left">applicationTypes</td>
<td style="text-align:left">限制application的类型，如SPARK</td>
</tr>
<tr>
<td style="text-align:left">applicationTags</td>
<td style="text-align:left">不知道</td>
</tr>
<tr>
<td style="text-align:left">deSelects</td>
<td style="text-align:left">不知道</td>
</tr>
</tbody>
</table>
<h3 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h3><p>需要导入requests</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">queryApplications</span><span class="params">(host, port, p)</span>:</span></div><div class="line">    url = <span class="string">'http://%s:%s/ws/v1/cluster/apps'</span> % (host, port)</div><div class="line">    reponse = requests.get(url, params=p)</div><div class="line">    <span class="comment">#print reponse.url</span></div><div class="line">    apps = json.loads(reponse.text)</div><div class="line">    <span class="keyword">if</span> apps:</div><div class="line">        <span class="keyword">return</span> apps.get(<span class="string">"apps"</span>, &#123;&#125;)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">None</span></div><div class="line"><span class="keyword">if</span> <span class="string">"__main__"</span> == __name__:</div><div class="line">    host = <span class="string">'yarn.host.com'</span></div><div class="line">    port = <span class="number">8088</span></div><div class="line">    params = &#123;&#125;</div><div class="line">    params[<span class="string">'startedTimeBegin'</span>] = changeTimeToNumber(changeStrToTime(getHourBegin()))</div><div class="line">    params[<span class="string">'startedTimeEnd'</span>] = changeTimeToNumber(changeStrToTime(getHourEnd())) * <span class="number">1000</span></div><div class="line">    params[<span class="string">'applicationTypes'</span>] = [<span class="string">'SPARK'</span>]</div><div class="line">    apps = queryApplications(host, port, params)</div></pre></td></tr></table></figure>
<h3 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h3><p>返回在此Yarn调度过的applicaiton列表</p>
<h2 id="获取单个Application的信息"><a href="#获取单个Application的信息" class="headerlink" title="获取单个Application的信息"></a>获取单个Application的信息</h2><h3 id="Url的地址-1"><a href="#Url的地址-1" class="headerlink" title="Url的地址"></a>Url的地址</h3><p>http://<rm http="" address:port="">/ws/v1/cluster/apps/{appid}<br>参考地址：<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html#Cluster_Application_API" target="_blank" rel="external">https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html#Cluster_Application_API</a></rm></p>
<h3 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h3><p>没有参数</p>
<h3 id="Python实现-1"><a href="#Python实现-1" class="headerlink" title="Python实现"></a>Python实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">queryApplication</span><span class="params">(appid, host, port)</span>:</span></div><div class="line">    url = <span class="string">'http://%s:%s/ws/v1/cluster/apps/%s'</span> % (host, port, appid)</div><div class="line">    reponse = requests.get(url)</div><div class="line">    <span class="comment">#print reponse.url</span></div><div class="line">    res = json.loads(reponse.text)</div><div class="line">    <span class="keyword">return</span> res</div><div class="line"></div><div class="line"><span class="keyword">if</span> <span class="string">"__main__"</span> == __name__:</div><div class="line">    host = <span class="string">'yarn.host.com'</span></div><div class="line">    port = <span class="number">8088</span></div><div class="line">    app = queryApplication(<span class="string">'application_1476912658570_0002'</span>, host, port)</div></pre></td></tr></table></figure>
<h3 id="返回值-1"><a href="#返回值-1" class="headerlink" title="返回值"></a>返回值</h3><p>返回application的详细信息，可以参考URL处提供的地址</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2019/01/18/spark-2-11-thriftServer/" itemprop="url">
                  spark-2.11-thriftServer
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2019-01-18T15:29:58+08:00" content="2019-01-18">
              2019-01-18
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文记录Thrift server的学习，先从一个服务异常为切入点：</p>
<h1 id="Thrift-Server的本地启动"><a href="#Thrift-Server的本地启动" class="headerlink" title="Thrift Server的本地启动"></a>Thrift Server的本地启动</h1><p>– 感谢韦大侠帮助<br>直接运行 HiveThriftServer2是无法正常启动的，首先需要配置启动参数：<br><img src="/attach/pimg_5c1771760f7a5.png" alt="pimg_5c1771760f7a5.png"><br>在 Program arguments 中添加：-hiveconf spark.master=local[2] -hiveconf spark.driver.bindAddress=127.0.0.1<br>在Working directory中添加：/Users/renweiming/spark/spark-2.x<br>这样启动的话还是会抛出异常信息：<br><img src="/attach/pimg_5c1771cf3f9a6.png" alt="pimg_5c1771cf3f9a6.png"><br>解决缺少类的方法是 File -&gt; Project Structure…<br><img src="/attach/pimg_5c177201e0a2a.png" alt="pimg_5c177201e0a2a.png"><br>打开项目结构对话框<br><img src="/attach/pimg_5c177250538bb.png" alt="pimg_5c177250538bb.png"><br>点击左下角的“➕”来增加Jar。一个小建议，如果你的Spark项目已经构建过了，那么其实Spark所需的jar都已经存在于本地的maven仓库里了，直接去仓库中，将对应的jar添加即可，就不再去下载jar包了。<br>常见的一些所需jar包：<br>| 异常 | 所需jar | 说明 |<br>| ———— | ————- | —————- |<br>| java.lang.ClassNotFoundException: com.google.common.cache.CacheLoader | com.google.guava | 该问题，一般是因为Scope设置的是Provided，修改为Compile即可，见表格下的截图 |<br>| 我擦，一下子启动起来了，以前配置过，以后遇到再补充吧，不给表现机会。。| …| …|<br><img src="/attach/pimg_5c1774288b08d.png" alt="pimg_5c1774288b08d.png"></p>
<h1 id="带着问题学习"><a href="#带着问题学习" class="headerlink" title="带着问题学习"></a>带着问题学习</h1><p>在Thrift server中，出现了如下异常：<br>18/12/17 15:01:00 INFO AbstractService: Service:HiveServer2 is stopped.<br>通过向上追日志，发现还有如下信息：<br>18/12/17 15:00:59 INFO HiveServer2: Shutting down HiveServer2<br>继续向上追：<br>18/12/17 15:00:59 ERROR SparkExecuteStatementOperation: Error running hive query as user : livepm<br>继续向上追：</p>
<p>从代码结构来看：<br>HiveServer2继承CompositeService，继承AbstractService。上面的异常就是从 AbstractService的stop()方法中报出来的。而这个方法调用是从如下的流程中调用的：HiveServer2.stop() -&gt; CompositeService.stop() -&gt; AbstractService.stop()。<br>所以，从代码上来分析，肯定是什么地方调用了stop方法。<br>在HiveServer的init方法中，添加了程序关闭的钩子函数：<br><img src="/attach/pimg_5c1778027340a.png" alt="pimg_5c1778027340a.png"><br>所以肯定是主动调用了stop方法，或者是异常导致HiveServer关闭了。</p>
<p>开始看到什么记什么，稍后在整理<br>Thrift server的UI其实是在Spark的UI上增加了一个tab页，默认端口为4040，可以通过 spark.ui.port 进行自定义。Thrift Server的展示效果，是通过类 ThriftServerPage 和 ThriftServerSessionPage 来定义的。</p>
<p>在初始化HiveThriftServer的时候，会根据Hive的配置项hive.server2.transport.mode的值（可选的有http和binary）来决定生成哪种 ThriftCLIService。如果是http则生成 ThriftHttpCLIService，否则生成 ThriftBinaryCLIService。</p>
<h1 id="启动流程"><a href="#启动流程" class="headerlink" title="启动流程"></a>启动流程</h1><p>入口必然是HiveThriftServer2，HiveThriftServer2的main方法，在方法的开头就生成了一个HiveServer2对象，并调用它的parse方法得到一个ServerOptionsProcessorResponse对象，该对象中包含的ServerOptionsExecutor是 StartOptionExecutor对象。 – 但是这个服务好像就没有被调用，也就是说 HiveServer2并没有被调用。</p>
<p>接下来<br>创建了一个HiveThriftServer2对象，并调用了这个服务的init和start。<br><img src="/attach/pimg_5c18ae94ed28f.png" alt="pimg_5c18ae94ed28f.png"><br>然而start方法只是调用了父类的start，并将自身的启动标示started设置为true。父类（HiveServer2）的start方法，同样又调用了父类（CompositeService）的start方法。在这个类的方法中，它将serviceList中的service（通过addService添加的）分别调用start方法，然后又调用了父类（AbstractService）的start方法。在这个顶级抽象方法中，设置了服务的开始时间、检查了当前状态为INITED，并将服务的状态设置为 STARTED。<br>所以我们的关注点就落在了 serviceList中service是如何添加进去的。我们返回看一下HiveThriftServer2的init方法，就是上图中我们调用的init方法。<br><img src="/attach/pimg_5c18aee6c6ac3.png" alt="pimg_5c18aee6c6ac3.png"><br>这样就添加了两个service，分别是 SparkSQLCLIService 和 ThriftCLIService。针对THriftCLIService，会根据 hive.server2.transport.mode 的配置项（binary 或 http）（我们公司使用的是默认值“binary”，也就是用的 ThriftBinaryCLIService ）具体生成ThriftHttpCLIService或ThriftBinaryCLIService。<br>继续将此方法看完，在init的最后执行的是 initCompositeService(hiveConf)，此方法是ReflectedCompositeService定义的一个方法，具体如下：<br><img src="/attach/pimg_5c18b128e9a91.png" alt="pimg_5c18b128e9a91.png"><br>这个方法，要把它作为HiveThriftServer2的一部分来读，那么它的功能就是初始化父类ServiceList中的service：</p>
<blockquote>
<p>得到父类（CompositeService）的serviceList，并对里面的service进行初始化（执行init方法）<br>设置hiveConf信息，通过反射设置。<br>检查服务的当前状态（确定必须是NOTINITED状态）并设置新的状态（INITED状态）。 –  为啥不直接调用AbstractService的init方法呢？</p>
</blockquote>
<p>继续按着顺序来吧，看一看前面通过addService添加到serviceList中的每个service吧。</p>
<h2 id="首先看看-SparkSQLCLIService-的init"><a href="#首先看看-SparkSQLCLIService-的init" class="headerlink" title="首先看看 SparkSQLCLIService 的init"></a>首先看看 SparkSQLCLIService 的init</h2><p>为父类设置HiveConf<br>生成SparkSQLSessionManager，并将SparkSQLSessionManager通过addService添加到CompositeService的serviceList中。注意这里的CompositeService是SparkSQLCLIService自己的CompositeService。之后又调用了 initCompositeService。相当于将添加进去的service进行初始化。</p>
<h3 id="SparkSQLSessionManager的初始化"><a href="#SparkSQLSessionManager的初始化" class="headerlink" title="SparkSQLSessionManager的初始化"></a>SparkSQLSessionManager的初始化</h3><p>首先，将hiveConf设置给父类（SparkSQLSessionManager的父类 SessionManager ）。<br>然后，判断是否开启了 hive.server2.logging.operation.enabled 配置（默认为true），如果开启了，则执行initOperationLogRootDir来初始化操作日志目录。<br>然后，根据配置项 hive.server2.async.exec.threads 的值，生成一个固定线程数的线程池，并将线程池设置给父类的 backgroundOperationPool。<br>最后，生成一个 SparkSQLOperationManager 对象，并将此对象 设置给父类（SessionManager）的 operationManager。然后对 operationManager 进行初始化。</p>
<h3 id="SparkSQLOperationManager的初始化"><a href="#SparkSQLOperationManager的初始化" class="headerlink" title="SparkSQLOperationManager的初始化"></a>SparkSQLOperationManager的初始化</h3><p>根据配置 hive.server2.logging.operation.enabled 的值（默认为true），来确定是否要启动一个 hive.server2.logging.operation.level 配置级别的Appender。</p>
<h2 id="再看看-ThriftBinaryCLIService"><a href="#再看看-ThriftBinaryCLIService" class="headerlink" title="再看看 ThriftBinaryCLIService"></a>再看看 ThriftBinaryCLIService</h2><p>首先，根据系统环境或配置项 hive.server2.thrift.bind.host 中得到需要绑定的host，如果没有则绑定本地地址。<br>然后，根据配置项 hive.server2.thrift.worker.keepalive.time 获取worker保持活跃的时长（默认为60秒）。<br>继续然后，从系统变量 HIVE_SERVER2_THRIFT_PORT 或配置项 hive.server2.thrift.port 中获取服务绑定的端口号，默认为10000。<br>最后，从配置项 hive.server2.thrift.min.worker.threads 和 hive.server2.thrift.max.worker.threads 中分别读取worker的最小线程数和最大线程数。<br>调用父类（AbastractService）的init方法。<br>worker活跃时长、最大线程数和最小线程数，用来初始化Service内部的线程池。</p>
<p>综上来看，其实就是一层一层的进行初始化，但是为啥不直接调用初始化方法呢？个人推测是覆盖性操作。</p>
<p>继续回到 HiveThriftServer2 初始化的位置，接下来就是启动 HiveThriftServer2，调用start方法，调用链如下：<br>HiveThriftServer.start() -&gt; HiveServer2.start() -&gt; CompositeService.start()(–此方法中会将serviceList中的所有service进行启动)<br>所以，接下来我们对 SparkSQLCLIService 和 ThriftBinaryCLIService 的start方法进行分析。</p>
<h2 id="SparkSQLCLIService的启动"><a href="#SparkSQLCLIService的启动" class="headerlink" title="SparkSQLCLIService的启动"></a>SparkSQLCLIService的启动</h2><p>SparkSQLCLIService类中并没有定义start方法，但是它的父类CLIService中定义了。父类的启动很简单，首先调用父类（CompositeService）的start方法，相当于对serviceList中的各个service进行启动。然后自己内部生成一个 HiveMetaStoreClient，并链接获取默认的数据库（只是为了测试链接）。 至于serviceList中service的启动，需要结合上面的初始化来看看需要具体启动那些service。稍后上图。</p>
<h2 id="ThriftBinaryCLIService的启动"><a href="#ThriftBinaryCLIService的启动" class="headerlink" title="ThriftBinaryCLIService的启动"></a>ThriftBinaryCLIService的启动</h2><p>ThriftBinaryCLIService自身也没有定义start方法，其父类（ThriftCLISService）定义了start方法。具体逻辑如下：调用父类（AbstractService）的start方法。然后，将自己作为参数生成一个Thread，并进行启动（因为ThriftCLIService实现了Runnable接口）。然而，ThriftCLIService自身只是定义了抽象方法run，具体实现由 ThriftBinaryCLIService 或 ThrfitHttpCLIService 来实现。因为我们使用的是 ThriftBinaryCLIService ，因此我们只对此类进行分析。<br>对于ThriftBinaryCLIService的启动，我们进行分析：<br>创建线程池、创建TTransportFactory、创建TProcessorFactory、创建serverSocket，这些是用来创建TThreadPoolServer.Args，然后使用它来创建 TThreadPoolServer，然后调用service的serve方法来启动服务。这里就会输出 Starting … on port … with … … worker threads的日志。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2019/01/17/spark-2-11-shuffleDataReadAndWrite/" itemprop="url">
                  Spark 2.11 shuffle data read and write
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2019-01-17T19:09:23+08:00" content="2019-01-17">
              2019-01-17
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-11/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.11</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文记录对Spark Shuffle过程中对数据读写的梳理。本文将先从数据的读入作为切入点，也就是从ShuffleReader的源码作为切入，然后在逐步的展开。因此本文将从读取和写入两个角度来进行分析，最后在用流程图的方式将读取和写入整合起来。</p>
<h1 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h1><h2 id="ShuffleReader的实现类"><a href="#ShuffleReader的实现类" class="headerlink" title="ShuffleReader的实现类"></a>ShuffleReader的实现类</h2><p>ShuffleRead是定义在org.apache.spark.shuffle包中的一个接口（理解为接口吧，其实是trait），它只定义了一个方法read()，并且这个方法返回一个Iterator（迭代器）。对于ShuffleReader的实现类，只有一个类 BlockStoreShuffleReader，与ShuffleReader定义在同一个包中。</p>
<h3 id="ShuffleReader的read-方法的实现"><a href="#ShuffleReader的read-方法的实现" class="headerlink" title="ShuffleReader的read()方法的实现"></a>ShuffleReader的read()方法的实现</h3><p>read()基本的流程图参考如下如下<br><img src="" alt="image.png"></p>
<p>在read()方法中，首先生成一个ShuffleBlockFetcherIterator对象。该对象的详细介绍会在后面进行。<br>生成的ShuffleBlockFetcherIterator本身就是一个Iterator，但是需要注意这个Iterator中包含的数据都是进行了加密和压缩的，我们在之后分析ShuffleBlockFetcherIterator的时候会了解到。<br>接着，我们会得到序列化器的实例，将ShuffleBlockFectherIterator中的数据进行解析。<br>如果Shuffle的依赖中定义了聚合器，则需要进行一次聚合操作（将上面的迭代器中的数据再次处理）：如果是map端聚合，则使用聚合器的 combineCombinersByKey来处理，否则使用combineValuesByKey来处理。<br>最后，创建一个ExternalSorter并将上一步的数据全部插入到这个sorter中，并使用这个ExternalSorter创建一个CompletionIterator返回。<br>大家在这里会看到聚合器和ExternalSorter，这就实现了Shuffle的基本操作。</p>
<h2 id="ShuffleBlockFetcherIterator的实现与流程"><a href="#ShuffleBlockFetcherIterator的实现与流程" class="headerlink" title="ShuffleBlockFetcherIterator的实现与流程"></a>ShuffleBlockFetcherIterator的实现与流程</h2><p>生成这个对象需要一系列参数：</p>
<table>
<thead>
<tr>
<th>参数名</th>
<th>参数类型</th>
<th>参数的意义和作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>context</td>
<td>TaskContext</td>
<td>Task的上下文</td>
</tr>
<tr>
<td>shuffleClient</td>
<td>ShuffleClient</td>
<td>读取Shuffle数据的Client</td>
</tr>
<tr>
<td>blockManager</td>
<td>BlockManager</td>
<td>Block的管理器，可以获取Block的相关信息</td>
</tr>
<tr>
<td>blocksByAddress</td>
<td>Seq</td>
<td>根据BlockManagerId聚合的Block列表</td>
</tr>
<tr>
<td>streamWrapper</td>
<td>函数</td>
<td>用于将BlockId和对应的InputStream进行包装的方法</td>
</tr>
<tr>
<td>maxBytesInFlight</td>
<td>Long</td>
<td>——</td>
</tr>
<tr>
<td>maxReqsInFlight</td>
<td>Int</td>
<td>同时进行请求发送的最大数</td>
</tr>
<tr>
<td>maxBlocksInFlightPerAddress</td>
<td>Int</td>
<td>同时拉取某个远程地址的block的请求数</td>
</tr>
<tr>
<td>maxReqSizeShuffleToMem</td>
<td>Long</td>
<td>单个请求拉取数据的可用缓存，如果超出这个缓存则使用临时文件来拉取数据</td>
</tr>
<tr>
<td>detectCorrupt</td>
<td>Boolean</td>
<td>—</td>
</tr>
</tbody>
</table>
<p>通过以上参数，生成了我们的ShuffleBlockFetcherIterator。ShuffleBlockFetcherIterator实现了Iterator（迭代器）接口，同时也实现了TempShuffleFileManager（临时Shuffle文件管理器）。实现Iterator就意味着它有迭代器的相关方法（next、hasNext）；实现TempShuffleFileManager，就意味这它能够创建临时Shuffle文件（createTempShuffleFile）、能够注册临时文件的清理（registerTempShuffleFileToClean）。接下来我们分别按照接口的实现来分析，首先看看比较简单的TempShuffleFileManager：</p>
<h3 id="TempShuffleFileManager的实现"><a href="#TempShuffleFileManager的实现" class="headerlink" title="TempShuffleFileManager的实现"></a>TempShuffleFileManager的实现</h3><p>实现这个接口，我们需要实现两个方法 createTempShuffleFile 和 registerTempShuffleFileToClean。</p>
<h4 id="createTempShuffleFile"><a href="#createTempShuffleFile" class="headerlink" title="createTempShuffleFile"></a>createTempShuffleFile</h4><p>这个方法的实现很简单，就是调用BlockManager的DiskBlockManager的createTempLocalBlock来创建一个临时本地Block并返回。</p>
<h4 id="registerTempShuffleFileToCliean"><a href="#registerTempShuffleFileToCliean" class="headerlink" title="registerTempShuffleFileToCliean"></a>registerTempShuffleFileToCliean</h4><p>这个方法的实现也很简单，就是将文件添加到一个名为 shuffleFileSet 的集合中，等到ShuffleBlockFetcherIterator中cleanup方法被执行的时候，就会将 shuffleFileSet 集合中的文件统一删除。</p>
<h3 id="Iterator的实现"><a href="#Iterator的实现" class="headerlink" title="Iterator的实现"></a>Iterator的实现</h3><p>我们知道，ShuffleBlockFetcherIterator 最重要的功能是获取数据，这个过程会涉及到从远程拉取数据。</p>
<h4 id="hasNext"><a href="#hasNext" class="headerlink" title="hasNext"></a>hasNext</h4><p>方法实现很简单，就是判断已经处理的block数量是否达到需要拉取的block的数量（numBlocksProcessed &lt; numBlocksToFetch）。</p>
<h4 id="next"><a href="#next" class="headerlink" title="next"></a>next</h4><p>next方法的实现就复杂了，而且涉及到很多方法之间的调用，我们首先看看流程图，总体说明一下流程，然后在具体分析细节。<br><img src="/attach/5c3f1cf6f0fa4.png" alt="image.png"><br>这个流程图说明了 ShuffleBlockFetcherIterator 内部获取数据时几个方法之间的调用关系，next是对用户的接口，用户通过这个方法获取数据。但是我们都应该想到，shuffle数据是很庞大的，所以肯定不可能一次性都将数据拉取过来缓存，只能根据blockId逐步拉取，fetchUpToMaxBytes就是实现逐步拉取数据的逻辑，内部实现了对数据拉取的控制。当需要拉取数据的时候fetchUpToMaxBytes就会调用send方法，send方法是对sendRequest的一层封装，除了调用sendRequest之外，就是要记录每个远程地址当前正在传输Block的数量。sendRequest就是使用ShuffleClient从远程地址拉取数据的具体实现了。<br>为了便于理解，我们根据上面流程图的倒序进行分析。</p>
<h4 id="sendRequest"><a href="#sendRequest" class="headerlink" title="sendRequest"></a>sendRequest</h4><p>sendRequest是远程拉取数据的实现。从代码上来看，代码主要分成三个部分：记录状态信息（为fetchUpToMaxByte提供逻辑处理的数据）、生成BlockFetchingListender（调用fetchBlocks的时候需要这个监听器作为回调）以及使用shuffleClient拉取数据。<br>基本的流程图如下：<br><img src="/attach/5c3db21e60597.png" alt="image.png"><br>对于ShuffleClient的fetchBlocks方法的实现我们稍后会深入分析，但是这里需要意识到的是这个方法拉取的是多个block。</p>
<h4 id="send"><a href="#send" class="headerlink" title="send"></a>send</h4><p>send方法就是对sendRequest的包装，除了调用sendRequest方法之外，会记录当前远程地址正在传输Block的个数，也画一个图吧：<br><img src="/attach/5c3db673d673f.png" alt="image.png"><br>就是调用sendRequest，然后设置缓存的值。</p>
<h4 id="fetchUpToMaxBytes"><a href="#fetchUpToMaxBytes" class="headerlink" title="fetchUpToMaxBytes"></a>fetchUpToMaxBytes</h4><p>上面我们也简单的提了一下，这个方法就是读取最大限度的数据（但是要整读，不会读取半个block的数据，要么不读，要读取完成的block）。<br>这个方法的实现其实也比较简单，基本的思路就是在准备发送请求之前，先验证一下是否可以发送，如果可以则调用send方法，如果不可以，则将请求加入到延迟队列中。所以从上面的话中，大家可以了解到，这个数据会操作两个队列 fetchRequests 和 deferredFetchRequests，从代码来看，程序会先从deferredFetchRequest队列中拿请求处理，如果这个队列中没有请求或请求都不符合发送才会去fetchRequest队列中拿请求，实际处理刚开始的时候deferredFetchRequest必然是空的，只有当处理fetchRequest中的请求不符合发送条件的时候才会加入到deferredFetchRequest中，但是这两个队列的数据结构还是不一样的，fetchRequest存放的是FetchRequest(BlockManagerId, Seq[(BlockId, Long)])对象，也就是存储的是BlockManagerId-&gt;Block列表的映射关系，BlockManagerId可以简单的理解为存储Block的远程地址；而deferredFetchRequest，存放的是BlockManagerId -&gt; Queue[FetchRequest]的映射，也就是说在添加到deferredFetchRequest的时候根据BlockManagerId进行整合汇总了。<br>此方法的流程图如下：<br><img src="/attach/5c3e9749e5126.png" alt="image.png"><br>从图中，我们也看到了在这个方法对每个地址正在拉取block的数量(numBlocksInFlightPerAddress)、正在拉取数据的数据量(bytesInFlight)和正在拉取数据的请求个数(reqsInFlight)的使用。<br>在进入fetchUpToMaxBytes方法后，首先会遍历deferredFetchRequests集合中的数据，这里需要注意的是，这个数据根据BlockManagerId进行了汇总，key是BlockManagerId，value是一个FetchRequest的Queue。首先调用isRemoteBlockFetchable方法验证队列是否是可拉取的（主要判断当前最大同时数据拉取量和最大同时请求数）。然后使用isRemoteAddressMaxedOut方法验证要发送的请求是否达到了远程地址最大同时拉取数量。如果验证都符合条件，则将FetchRequest交给send方法来进行请求的发送。</p>
<h4 id="next-1"><a href="#next-1" class="headerlink" title="next"></a>next</h4><p>next方法是Iterator接口的方法，也是用户用来获取数据的方法。<br>因为我们已经知道了sendRequest方法的BlockFetchingListener对象在收到数据拉取成功或失败后会向results（LinkedBlockingQueue）中写入一个SuccessFetchResult或FailedFetchResult，如果是SuccessFetchResult，则会在其中包含一个ManagerBuffer，也就是我们想要的数据。所以next方法获取数据就是从results中拉取数据，又因为results是阻塞队列，所以当results中没有数据的时候，就会被take方法所阻塞，直到拿到数据进行处理。<br>如果从results拿到的数据为FailedFetchResult，则会抛出异常信息，导致拉取失败。<br>如果从results拿到的数据为SuccessFetchResult，那么我们就可以进行处理了，首先，因为这个block已经拉取完毕了，所以相关计数器（如正在拉取数据的数据量bytesInFlight）的计数需要进行更新。<br>对相关计数器操作完成，就会从ManagerBuffer中获取输入流，然后调用serializerManager.wrapStream方法进行数据流的加密和压缩。接着会根据配置（数据流确实被加密或压缩了 、数据量较小）来决定是否要将输入流转换为ChunkedByteBufferInputStream。<br>最后，返回blockId以及使用BufferReleasingInputStream包装的输入流（包装后可以带调用ShuffleBlockFetcherIterator中的清理方法来进行清理工作）。<br><img src="/attach/5c3eb7bc0bfcf.png" alt="image.png"></p>
<h4 id="initialize"><a href="#initialize" class="headerlink" title="initialize"></a>initialize</h4><p>上面的介绍的这些方法都是在从远程节点拉取数据。其实ShuffleBlockFetcherIterator也会读取本地的数据，但是在生成ShuffleBlockFetcherIterator的时候会对Block进行拆分。而这一切是从initialize方法开始的。<br><img src="/attach/5c3ee2f7ef0af.png" alt="image.png"><br>方法的逻辑很简单，显示注册一个Task完成事件，用来调用cleanup方法进行清理工作。然后调用splitLocalRemoteBlocks()方法，将远程Block和本地Block进行拆分，并将拆分后的远程Block打散放入fetchRequest队列中（供fetchUpToMaxBytes()使用）。接着分别调用fetchUpToMaxBytes()和fetchLocalBlocks()方法。</p>
<h4 id="splitLocalRemoteBlocks"><a href="#splitLocalRemoteBlocks" class="headerlink" title="splitLocalRemoteBlocks"></a>splitLocalRemoteBlocks</h4><p>方法的逻辑也相对简单，遍历blocksByAddress（类型为Seq[(BlockManagerId, Seq[(BlockId, Long)]]，BlockManagerId代表了一个远程地址，这个地址上存折多个Block，这里以(BlockId，Block的size)来表示）（这里的blocksByAddress是通过mapOutputTracker（通过SparkEnv.get.mapOutputTracker得到）的getMapSizesByExecutorId方法得到的）中的数据。<br>遍历数据时，会拿到BlockManagerId和这个BlockManager上的Block列表。通过BlockManagerId，能够得到executorId，对这个executorId与本地SparkEnv中的BlockManager中的executor进行比较，就可以判断这个BlockManagerId是否是本地的了（BlockManager中的executorId就相当于BlockManager的唯一ID了）。如果是本地的，将Block列表加入到localBlocks集合中（对于size为0的Block直接丢掉，无需拉取），后面fetchLocalBlocks方法会对这个集合进行处理。对于远程，需要对远程BlockManager中的每个Block进行遍历，这样做的目的是为了拆分FetchRequest（可能会将一个BlockManager中的多个Block分不同的请求来拉取）。</p>
<p><img src="/attach/5c3f1bed65ff7.png" alt="image.png"></p>
<p>从代码中对于FetchRequest的拆分，我们可以了解到在配置spark.reducer.maxSizeInFlight参数的时候，这个参数的最小值应该为一个Block的大小，否则后面没法发送请求，因为任何一个数据的大小都会超过限制，而FetchRequest的拆分是以Block为单位的，一个FetchRequest最少含有一个Block。</p>
<h4 id="fetchLocalBlocks"><a href="#fetchLocalBlocks" class="headerlink" title="fetchLocalBlocks"></a>fetchLocalBlocks</h4><p>fetchLocalBlocks的逻辑很简单，基本流程如下：<br><img src="/attach/5c3f1e5e644c7.png" alt="image.png"><br>此方法就是遍历localBlocks中的数据，并调用本地的BlockManger获取数据，然后将包装为一个FetchResult放到results中，供next方法使用。<br>如果在获取本地Block时发生异常，则推送results时推送的是FailureFetchResult，否则推送的是SuccessFetchResult。</p>
<p>至此，ShuffleBlockFetcherIterator的处理逻辑就介绍完了。</p>
<h3 id="shuffleWriter到shuffleReader的调用链"><a href="#shuffleWriter到shuffleReader的调用链" class="headerlink" title="shuffleWriter到shuffleReader的调用链"></a>shuffleWriter到shuffleReader的调用链</h3><p>我们已经知道了BlockStoreShuffleReader作为ShuffleReader进行数据读取，read方法返回Iterator对象。那么调用这个read方法的流程是什么样的呢？简单的流程图如下：<br><img src="/attach/5c3ff1431fbb5.png" alt="image.png"><br>基本流程就是在执行task的时候，会使用ShuffleWriter来写数据，写数据的时候就会调用RDD的iterator来读取数据。在使用iterator读取数据的时候会根据存储级别来确定调用getOrCompute()方法还是computeOrReadCheckpoint()方法，当RDD的存储级别不为NONE的时候就会调用getOrCompute()方法。但是这两个方法都会调用到computeOrReadCheckpoint()方法，然后调用ShuffleRDD的compute()方法，最终调用到了ShuffleReader的read()方法。所以接下来我们要看看ShuffleWriter的流程。</p>
<h3 id="ShuffleWriter"><a href="#ShuffleWriter" class="headerlink" title="ShuffleWriter"></a>ShuffleWriter</h3><p>ShuffleWriter的获取，ShuffleWriter是调用ShuffleManager(SparkEnv.get.shuffleManager)的getWriter方法得到的。调用getWriter方法的时候会传递dep.shuffleHandle作为参数，因为方法中会根据shuffleHandle的类型，生成不同类型的ShuffleWriter。<br><img src="/attach/5c3ff9cfa7f77.png" alt="image.png"><br>接下来，我们具体分析一下ShuffleWriter的write方法（以简单的SortShuffleWriter为例）。</p>
<h4 id="SortShuffleWriter-write"><a href="#SortShuffleWriter-write" class="headerlink" title="SortShuffleWriter.write"></a>SortShuffleWriter.write</h4><p>此方法的逻辑也比较简单。首先生成ExternalSorter，根据dependency.mapSideCombine来确定生成的ExternalSorter是否含有聚合器（如果为true则含有）。<br>然后调用sorter.insertAll将read返回的Iterator插入到上面生成的ExternalSorter中。<br>接着，使用IndexShuffleBlockResolver（其中包含BlockManager，因此可以管理Block），根据shuffleId和partitionId创建shuffle的数据文件，同时创建这个数据文件的临时文件（在数据文件后面加一个随机的后缀）。<br>然后调用sorter.writePartitionedFile方法，将sorter中的数据写到上面的临时文件中。接着根据临时文件写索引文件，并将临时文件调整为正式文件，通过调用shuffleBlockResolver.writeIndexFileAndCommit方法来实现（此方法中会对数据文件和索引文件进行验证）。<br>write方法比较简单，对于分类器ExternalSorter，我们在ShuffleReader和ShuffleWriter中都看到了，shuffle过程数据的混洗也是这样完成的吧。<br>write方法中使用shuffleBlockResolver做了两件事：获取数据文件（getDataFile）和写索引文件（writeIndexFileAndCommit）。接下来我们对这两个方法也一并分析一下。</p>
<h4 id="IndexShuffleBlockResolver-getDataFile"><a href="#IndexShuffleBlockResolver-getDataFile" class="headerlink" title="IndexShuffleBlockResolver.getDataFile"></a>IndexShuffleBlockResolver.getDataFile</h4><p><img src="/attach/5c40545e9e2c3.png" alt="image.png"><br>方法的实现很简单吧。就是调用BlockManager中的DiskBlockManager去获取文件，如果没有，则生成一个文件并返回（关于DiskBlockManager操作文件的逻辑可以参考内存分析章节）。</p>
<h4 id="shuffleBlockResolver-writeIndexFileAndCommit"><a href="#shuffleBlockResolver-writeIndexFileAndCommit" class="headerlink" title="shuffleBlockResolver.writeIndexFileAndCommit"></a>shuffleBlockResolver.writeIndexFileAndCommit</h4><p>这个方法逻辑稍微复杂一些，先是根据shuffleId和partitionId创建索引文件和索引临时文件。然后根据sorter.insertAll方法返回的文件每个分区的长度，写到索引临时文件中。索引文件指定了数据文件中每个partition的起止offset（sorter.insertAll方法，我们有机会在深入分析）。到这里需要注意，刚刚写的数据文件和索引文件，都是写入到临时文件中的，因为数据文件和索引文件可能已经存在了（因为一个task可能有多个尝试在同时执行），所以接下来就会对索引文件和数据文件（注意这里不是临时文件）进行验证（调用checkIndexAndDataFile方法）。验证成功，则表示有task的其他尝试已经完成了文件的写入，直接将索引临时文件和数据临时文件删除即可；如果验证失败，则将已有的数据文件和索引文件删除，将我们上面生成的索引临时文件和数据临时文件重命名为正式的索引文件和数据文件。<br>缺少一个流程图。。。。。。TODO</p>
<p>至此，数据的写入逻辑也就介绍完了，另外两种Writer的write实现，有机会再补充。</p>
<h3 id="ShuffleClient"><a href="#ShuffleClient" class="headerlink" title="ShuffleClient"></a>ShuffleClient</h3><p>在sendRequest方法中，我们调用了ShuffleClient对象来拉取Blocks。有个细节肯定没有忘记，就是会根据此次请求要拉取数据的大小来决定是否会传递TempShuffleFileManager，这个TempShuffleFileManager有什么用，我们会在分析的时候看到它的作用。另外，还有一个问题：ShuffleClient是如何得到的，具体的实现类是哪个？<br>从生成ShuffleBlockFetcherIterator是，我们知道ShuffleClient是调用BlockManager的shuffleClient方法得到的。<br><img src="/attach/5c3f269fe5441.png" alt="image.png"><br>代码逻辑很简单，根据配置“spark.shuffle.service.enabled”来确定使用ExternalShuffleClient作为ShuffleClient，还是使用BlockTransferService。因为我们开启了此参数，所以我们会针对ExternalShuffleClient进行分析。生成ExternalShuffleClient的时候会需要一个SparkTransportConf</p>
<h3 id="整个过程中所使用的配置"><a href="#整个过程中所使用的配置" class="headerlink" title="整个过程中所使用的配置"></a>整个过程中所使用的配置</h3><table>
<thead>
<tr>
<th>配置参数</th>
<th>默认值</th>
<th>参数作用</th>
<th>使用位置</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.reducer.maxSizeInFlight</td>
<td>48M</td>
<td>控制该节点可以同时拉取多大量的数据（所有请求同时拉取数据的字节数）</td>
<td>1、isRemoteBlockFetchable方法中验证正在拉取的数据的数据量。2、在splitLocalRemoteBlocks方法中，用于拆分FetchRequest（当一个地址上的多个Block的总大小超过该值的五分之一的时候，就把这些Block拆到多个FetchRequest中）。</td>
</tr>
<tr>
<td>spark.reducer.maxReqsInFlight</td>
<td>Int.MaxValue</td>
<td>控制节点同时可以发送的请求数，超过这个数的请求，将被放到deferredFetchRequests中</td>
<td>isRemoteBlockFetchable方法中验证当前正在拉取数据的请求数</td>
</tr>
<tr>
<td>spark.reducer.maxBlocksInFlightPerAddress</td>
<td>Int.MaxValue</td>
<td>一个远程节点，可以同时拉取Block的个数</td>
<td>1、isRemoteAddressMaxedOut方法中检查某个远程地址上正在拉取的block的数量是否超出最大值。2、在splitLocalRemoteBlocks方法中，当一个地址（BlockManagerId）上拥有的Block个数超过该值时，为了可以提交send方法，需要将这些Block拆分到多个FetchRequest中</td>
</tr>
<tr>
<td>spark.shuffle.detectCorrupt</td>
<td>true</td>
<td>如果此参数为true，会将ManagerBuffer得到的输入流转换为ChunkedByteBufferInputStream类型</td>
<td>在shuffleBlockFetcherIterator的next方法中</td>
</tr>
<tr>
<td>spark.shuffle.compress</td>
<td>true</td>
<td>shuffle数据是否进行压缩</td>
<td>在next方法调用serializerManager.wrapStream方法时会验证</td>
</tr>
<tr>
<td>spark.shuffle.service.enabled</td>
<td>false</td>
<td>是否启用shuffleService</td>
<td>如果启用了，则生成ExternalShuffleClient对象作为ShuffleClient，否则使用BlockTransferService.我们开启了此参数</td>
</tr>
</tbody>
</table>
<h1 id="一些有趣的东西"><a href="#一些有趣的东西" class="headerlink" title="一些有趣的东西"></a>一些有趣的东西</h1><h2 id="去哪里可以得到shuffle数据的分布？"><a href="#去哪里可以得到shuffle数据的分布？" class="headerlink" title="去哪里可以得到shuffle数据的分布？"></a>去哪里可以得到shuffle数据的分布？</h2><p>答案是MapOutputTracker，通过SparkEnv.get.mapOutputTracker就可以得到MapOutputTracker对象。<br>比如：mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition) 就可以得到shuffleId阶段，根据ExecutorId聚合后的Block信息。</p>
<h1 id="读写流程的整合"><a href="#读写流程的整合" class="headerlink" title="读写流程的整合"></a>读写流程的整合</h1><p><img src="/attach/5c3d87a7dd187.png" alt="image.png"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/12/07/spark-2-11-storage/" itemprop="url">
                  spark-2.11-storage
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-12-07T11:59:01+08:00" content="2018-12-07">
              2018-12-07
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-11/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.11</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Spark存储管理在Spark执行application时担负这数据的传递、存储的重要职责。因此了解Spark的存储机制对于理解Spark的数据操作和性能的优化有着很重要的作用，本文以粗粒度方式来探究一下Spark的存储（最主要是内存）。由于在Spark中任务的执行、数据的传输均是发生在Executor端，因此本文只关注Executor端的存储操作。</p>
<h1 id="1、内存模式-On-heap与Off-heap"><a href="#1、内存模式-On-heap与Off-heap" class="headerlink" title="1、内存模式 On-heap与Off-heap"></a>1、内存模式 On-heap与Off-heap</h1><p>因为Executor是运行在JVM上的，所以Executor最直接的就是操作On-heap内存，但是除此之外，Spark还引入了Off-heap内存的使用，使Spark可以直接操作JVM之外的系统内存，并对此操作进行了优化。<br><img src="/attach/pimg_5c079127d675e.png"><br>该图中关于task共享Executor内存的较少我们将在后续介绍，这里只需要关注On-heap和Off-heap的区别即可。</p>
<h2 id="1-1、堆内内存-On-heap"><a href="#1-1、堆内内存-On-heap" class="headerlink" title="1.1、堆内内存 On-heap"></a>1.1、堆内内存 On-heap</h2><p>堆内内存的大小通过参数 –executor-memory 或 配置spark.executor.memory来控制。Spark对堆内内存的管理是一种计数上的管理，因为对对象的创建和销毁实际是由JVM来具体操作的，Spark无法准确的控制这些，因此它只是从计数（空间使用量）的角度来管理堆内内存。</p>
<h2 id="1-2、堆外内存-Off-heap"><a href="#1-2、堆外内存-Off-heap" class="headerlink" title="1.2、堆外内存 Off-heap"></a>1.2、堆外内存 Off-heap</h2><p>为了进一步优化内存的使用进而提高Shuffle的执行效率，Spark引入了堆外内存（Off-heap），使得Spark可以使用Executor上的系统内存。默认堆外内存是不可用的，可以通过 spark.memory.offHeap.enabled来进行开启。通过JDK Unsafe API，Spark能够直接操作系统内存，因此可以精确的控制堆外内存的申请和销毁。</p>
<h2 id="1-3、内存的统一管理"><a href="#1-3、内存的统一管理" class="headerlink" title="1.3、内存的统一管理"></a>1.3、内存的统一管理</h2><p>Spark对于内存的管理使用统一的抽象接口MemoryManager。它负责Spark的内存申请、释放以及不同用途内存之间的转换。对于内存管理的实现，Spark主要基于两种内存管理：静态内存管理模式（StaticMemoryManager）和统一内存管理模式（unifledMemoryManager）。从Spark 2.0开始，默认使用的内存管理模式为 统一内存管理模式（unifledMemoryManager）。如果想要使用静态内存管理模式，可以将 spark.memory.useLegacyMode 配置设置为true。</p>
<h1 id="2、内存的用途分类"><a href="#2、内存的用途分类" class="headerlink" title="2、内存的用途分类"></a>2、内存的用途分类</h1><p>从内存的使用来看，Spark对内存的使用主要在两个方面：数据执行和数据存储，所以我们堆内存的划分也就分为执行内存和存储内存。执行内存主要用于shuffle、join、sorts和aggregation的消耗，而存储内存主要用于数据的缓存和集群内数据的传输。在Spark内存管理中，执行内存和存储内存的大小是通过参数 spark.memory.storageFraction 来指定的，执行内存和存储内存之和就是Executor的可用内存（对于On-heap内存，含有other部分）。</p>
<h2 id="2-1、堆内内存中执行内存与存储内存的计算"><a href="#2-1、堆内内存中执行内存与存储内存的计算" class="headerlink" title="2.1、堆内内存中执行内存与存储内存的计算"></a>2.1、堆内内存中执行内存与存储内存的计算</h2><p>对于堆内内存，我们首先要介绍几个概念：</p>
<table>
<thead>
<tr>
<th>概念</th>
<th>解释／取值</th>
</tr>
</thead>
<tbody>
<tr>
<td>系统内存</td>
<td>可以理解为JVM的内存，如果设置了spark.testing.memory，则使用，否则Runtime.getRuntime.maxMemory</td>
</tr>
<tr>
<td>系统预留内存</td>
<td>系统预留的内存，依次spark.testing.reservedMemory &gt; spark.testing则为0 取值，默认为300（300 <em> 1024 </em> 1024）</td>
</tr>
<tr>
<td>最小系统内存</td>
<td>系统预留内存的1.5左右</td>
</tr>
<tr>
<td>可用内存</td>
<td>系统内存 - 预留系统内存</td>
</tr>
<tr>
<td>最大内存</td>
<td>可以被Spark使用的内存（执行内存和存储内存之和）。可用内存 * spark.memory.fraction（默认为0.6，我们集群的配置为0.7）</td>
</tr>
</tbody>
</table>
<p>最大内存又分成了存储内存和执行内存。存储内存在最大内存中的占比通过spark.memory.storageFraction（默认为0.5）配置来指定，其余的为执行内存。这里额外介绍一个细节，如果 spark.executor.memory配置的值小于 最小系统内存，executor是无法启动的。<br><img src="/attach/pimg_5c07a49aee1d7.png"></p>
<h2 id="2-2、堆外内存中执行内存和存储内存的计算"><a href="#2-2、堆外内存中执行内存和存储内存的计算" class="headerlink" title="2.2、堆外内存中执行内存和存储内存的计算"></a>2.2、堆外内存中执行内存和存储内存的计算</h2><p>堆外内存的大小是通过spark.memory.offHeap.size配置指定的。堆外内存比较简单，它不存在其他部分的内存分配，内部直接分为存储内存和执行内存。堆外存储内存所占比例同样通过配置spark.memory.storageFraction（默认0.5）来指定。<br><img src="/attach/pimg_5c07a4b15897b.png"><br>对于堆内存储内存和堆外存储内存，彼此之间是相互独立的，执行内存也是如此。</p>
<h2 id="2-3、存储级别"><a href="#2-3、存储级别" class="headerlink" title="2.3、存储级别"></a>2.3、存储级别</h2><p>我们已经知道了内存模式分为Off-heap和On-heap，而根据内存的用途有分为存储内存和执行内存。因此对于数据的存储，Spark从一下几个因素定义了存储级别（StorageLevel）：</p>
<blockquote>
<ol>
<li>磁盘/内存</li>
<li>堆内/堆外</li>
<li>序列化/不序列化</li>
<li>有副本/没有副本</li>
</ol>
</blockquote>
<p>有了存储级别，就能够明确的说明数据存储的位置、数据存储的方式以及数据存储的个数。</p>
<h1 id="3、静态内存管理与统一内存管理"><a href="#3、静态内存管理与统一内存管理" class="headerlink" title="3、静态内存管理与统一内存管理"></a>3、静态内存管理与统一内存管理</h1><p>最初Spark的采用的是静态内存管理，在2.0的版本中，Spark开始默认使用统一内存管理来进行内存管理。静态内存管理和统一内存管理的区别，可以简单的从执行内存和存储内存能否相互借用来区别。因为我们系统也是使用默认的统一内存管理来对内存进行管理，顾暂时不对静态内存管理进行理解。</p>
<h2 id="3-1、统一内存管理"><a href="#3-1、统一内存管理" class="headerlink" title="3.1、统一内存管理"></a>3.1、统一内存管理</h2><p>相对于静态内存，统一内存增加了动态占用机制的优化，其规则如下：</p>
<blockquote>
<p>1、通过配置项spark.memory.storageFraction，对存储内存和执行内存进行基本值的划分。<br>2、当存储内存不够、执行内存充足时，可以增加存储内存的容量，减少执行内存的容量。反之亦然。<br>3、当存储不够且执行也不充足时，存储数据落盘。执行不够且存储也不够时，执行阻塞或失败。<br>4、当存储占用执行时，执行可要求存储归还，存储不够的可以落盘；当执行占用存储时，存储无法要求执行归还，只能删除数据或落盘。</p>
</blockquote>
<p><img src="/attach/pimg_5c07afb32d3a0.png"></p>
<h2 id="3-2-动态占用机制的实现"><a href="#3-2-动态占用机制的实现" class="headerlink" title="3.2 动态占用机制的实现"></a>3.2 动态占用机制的实现</h2><p>上面我们提到了统一内存管理的动态占用机制，它可以更加充分的使用内存，那么这种机制是如何实现的呢？上面我们也说过，Spark其实是无法精确操作内存的，而是使用了类似计数管理的方式来实现的。<br>因此，在Spark的底层实现中，它为每种内存都创建了与之对应的内存池（执行内存池和存储内存池，但是存储模式又分为堆内和堆外，所以共有四种内存池），内存池记录了对应内存的使用量和容量。</p>
<h3 id="3-2-1-MemoryManager"><a href="#3-2-1-MemoryManager" class="headerlink" title="3.2.1 MemoryManager"></a>3.2.1 MemoryManager</h3><p>对于内存池的封装，是由 MemoryManager来实现，在其内部维持着四种内存池的引用。<br><img src="/attach/pimg_5c07bd672b531.png"><br>其中只有相同内存模式的不同内存之间可以动态占用，如：OnHeapStorageMemoryPool只可以和 OnHeapExecutionMemoryPool 相互占用。另外需要注意的是，内存的总大小（执行内存和存储内存之和）一旦确定是无法修改的，虽然可以调整某个内存的大小，但是总的大小是不变的。<br>MemoryManager（UnifiedMemoryManager）主要的职责就是根据需要调整各自内存池的容量、计算各自内存池的当前使用量以及分配使用量。</p>
<h1 id="4、存储内存的管理"><a href="#4、存储内存的管理" class="headerlink" title="4、存储内存的管理"></a>4、存储内存的管理</h1><p>存储内存最主要的使用就是数据缓存（RDD进行持久化保存）和集群内的数据传输（数据的广播）。而且我们前面也介绍了存储级别，还需要介绍一个其他的概念：Block。对于Block的理解，可以先简单的将数据的parition理解为一个Block，但是在存储过程中Block是由类型的（通过BlockId进行验证）：<br><img src="/attach/pimg_5c07c4739522a.png" alt="pimg_5c07c4739522a.png"><br>从上图可以看出，BlockId由众多的子类，而属于哪种类别的BlockId，就是通过字符串模式匹配来决定的。<br>这里我们为什么要介绍Block呢？因为数据缓存就是以Block方式存储的。<br>在Spark中Storage模块负责Spark在计算过程中产生的数据，对数据的读写进行了统一的封装（包括从内存、磁盘、本地、远程）。在代码架构上，BlockManager分为Master和Salve。Dirver上运行的是Master，Executor上运行的是Slave，两者之间相同通信对数据块（Block）进行管理。</p>
<h2 id="4-1、-具体的实现"><a href="#4-1、-具体的实现" class="headerlink" title="4.1、 具体的实现"></a>4.1、 具体的实现</h2><p>在MemoryStore中，保持一个entries对象，它是一个LinkedHashMap[BlockId, MemoryEntry[_]]对象。MemoryEntry是一个接口，它有两个实现：DeserializedMemoryEntry 和 SerializedMemoryEntry，分别处理非序列化数据和序列化数据的保存。当由此，也就明白了存储级别（StorageLevel）中序列化和非序列化的意义了。当数据向内存中缓存数据时，其实就是将数据保存到enties中，但是与普通生成兑现不太一样，他会以连续的内存来保存，也就是说一个Block内的数据，从内存上来看是连续存储的（序列化的数据很好理解，序列化之后，对象就是一串字节数，但是对于非序列化的对象，其内部会有一个转换操作）。</p>
<h1 id="5、执行内存的管理"><a href="#5、执行内存的管理" class="headerlink" title="5、执行内存的管理"></a>5、执行内存的管理</h1><p>执行内存最主要的使用就是shuffle、sorts、aggregate等操作的时候被使用。而排序和聚合其实都是以shuffle的结果来进行操作然后写出数据，所以我们先从Shuffle的存储进行分析。</p>
<h2 id="5-1、-Shuffle执行内存的使用"><a href="#5-1、-Shuffle执行内存的使用" class="headerlink" title="5.1、 Shuffle执行内存的使用"></a>5.1、 Shuffle执行内存的使用</h2><p>shuffle操作是RDD之间的一种数据转换，从上一个RDD中读取，写入到下一个RDD中，因此我们将从读写两个方面来分析一下：</p>
<h2 id="5-1-1、-shufflerReader"><a href="#5-1-1、-shufflerReader" class="headerlink" title="5.1.1、 shufflerReader"></a>5.1.1、 shufflerReader</h2><p>Spark的shuffle操作是由ShuffleManager（由子类SortShuffleManager进行实现）进行操作的。ShuffleManager要读取数据就需要获取Reader，从而得到BlockStoreShuffleReader，BlockStoreShuffleReader调用read()方法进行数据读取。ShuffleManager可以通过配置项spark.shuffle.manager进行设置（默认为sort，可选的值有sort和tungsten-sort）：</p>
<table>
<thead>
<tr>
<th>spark.shuffle.manager的取值</th>
<th>所代表的类</th>
</tr>
</thead>
<tbody>
<tr>
<td>sort</td>
<td>org.apache.spark.shuffle.sort.SortShuffleManager</td>
</tr>
<tr>
<td>tungsten-sort</td>
<td>org.apache.spark.shuffle.sort.SortShuffleManager</td>
</tr>
</tbody>
</table>
<p>这里需要引入以概念：ShuffleClient，它是实际拉取数据的客户端。在Spark内部存在两种ShuffleClient：BlockTransferService和ExternalShuffleClient。如果配置项 spark.shuffle.service.enabled 为true（默认为false），则启用ExternalShuffleClient（比如我们的集群，就启用了这个配置）。<br>在生成ExternalShuffleClient的需要SparkTransportConf，该配置有两个比较重要的配置：</p>
<table>
<thead>
<tr>
<th>配置项</th>
<th>意义</th>
<th>取值</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.shuffle.io.serverThreads</td>
<td>stage之间TransServer的线程数</td>
<td>用户设定，默认与可用的core的数量相同</td>
</tr>
<tr>
<td>spark.shuffle.io.clientThreads</td>
<td></td>
<td>用户设定，默认与可用的core的数量相同</td>
</tr>
</tbody>
</table>
<p>可用core的数量为：用户指定core数、运行时可用core数 以及 数字8 中最小的那个值（如果用户指定的数不是0，则使用用户指定的数和8中最小的值，否则就是可用core数和8中最小的那个）。我们集群没有对此进行配置，因此会使用JVM可用的core数进行设置，但是不会超过8个。<br>ExternalShuffleClient中重要的方法就是fetchBlock方法。在fetchBlock方法，会创建连接到目标host和port的TransportClient，然后利用这个client生成OneForOneBlockFetcher来拉取指定executor上（通过参数execId）指定的block（通过blockIds指定）。&gt;_&lt; 到这里都没有看到内存的使用。。。醉了<br>突然一个不小心，原来OneForOneBlockFetcher中使用了一个参数 TempShuffleFileManager，它是一个接口，实现类为 ShuffleBlockFetcherIterator。这个类中有一个方法 createTempShuffleFile()。那么我们就看看，OneForOneBlockFetcher 是否将数据写到了临时文件吧（山路十八弯呀）。通过跟踪代码，果然是将远程的数据写入到个临时文件中。但是当数据写完之后，这个文件会被用来生成一个ManagedBuffer（具体类为FileSegmentManagerBuffer），对于这个ManagedBuffer的操作会交给listener进行处理，这个linstener就又指向了ShuffleBlockFetcherIterator中的 BlockFetchingListener，调用它的onBlockFetchSuccess方法。在新的方法中，ManagerBuffer作为一个SuccessFetchResult对象被推送到results中（一个LinkedBlockingQueue队列）。我们已经知道这个方法是在 ShuffleBlockFetcherIteraotr中，而这个类本身就是Iterator，所以对上面的队列的读取，就发生在Iterator的next()方法中。继续回到生成ShuffleBlockFetcherIteraotr的地方BlockStoreShuffleReader.read()中。在read方法中，又继续对数据进行了处理，怎么处理的呢，当然从字节流被转换为对象（进行解序列化操作），但是read返回的依旧是一个迭代器（Iterator）。因为shuffle操作肯定对需要一种聚合手段，这里采用了ExternalAppendOnlyMap进行聚合操作。如果还需要排序，则使用进一步使用ExternalSorter对象进行操作。这两个类好复杂，慢慢在看（也就是这两对象的操作会占用内存）。</p>
<h3 id="5-1-2、ShuffleWriter"><a href="#5-1-2、ShuffleWriter" class="headerlink" title="5.1.2、ShuffleWriter"></a>5.1.2、ShuffleWriter</h3><p>shuffleWriter的调用是在ShuffleMapTask的runTask中触发的（这也很好理解，只要在执行task结束的时候才需要写数据呀），而且我们也知道，对于Task只分为两种类型ShuffleMapTask和ResultTask，因为是了解shuffle部分，所以我们只关注ShuffleMapTask，至于ResultTask以后再继续。<br>至于获取ShuffleWriter，是根据ShuffleDependency中shuffleHandle的类型所有决定的，不同的handler会生成不同的Writer：</p>
<table>
<thead>
<tr>
<th>handler类型</th>
<th>与之对应的writer</th>
</tr>
</thead>
<tbody>
<tr>
<td>unsafeShuffleHandle</td>
<td>UnsafeShuffleWriter</td>
</tr>
<tr>
<td>bypassMergeSortHandle</td>
<td>BypassMergeSortShuffleWriter</td>
</tr>
<tr>
<td>其他</td>
<td>SortShuffleWriter</td>
</tr>
</tbody>
</table>
<p>我们选择一个较为简单的Writer吧，就看SortShuffleWriter。对于Writer来说，最重要的方法必然是write。于是我们就在方法中看到了获取数据文件、生成BlockId、写文件的操作。<br><img src="/attach/pimg_5c09e4c7151a3.png" alt="pimg_5c09e4c7151a3.png"><br>从目前来看，shuffle的写操作，写的是文件，而非内存，但是从文档或其他人的文章都提到有写内存的，应该是我还没有看到，会后续补充。</p>
<h1 id="5-2、task执行内存的分配"><a href="#5-2、task执行内存的分配" class="headerlink" title="5.2、task执行内存的分配"></a>5.2、task执行内存的分配</h1><p>Executor内部是以多线程的方式执行task，要启动一个task其实就是将TaskRunner放到Executor内部的线程池中执行。既然，task是在Executor中运行，多task在运行期间，执行内存是如何分配的呢？Spark在执行内存池中维持了一个HashMap用来记录每个task所占用的内存。每个task允许使用的内存范围为 maxPoolSize/2N ~ maxPoolSize/N（N为当前活跃的Task数， maxPoolSize是执行内存池的最大空间），注意该限制只是在申请资源的时候验证，当申请资源的时候，如果可以分配给task的内存小于最小值，则会使申请资源的操作进入等待状态，等到有其他任务释放内存的时候，会被再次唤醒。<br><img src="/attach/pimg_5c08aca616f57.png" alt="pimg_5c08aca616f57.png"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/10/19/spark-2-11-ExecutorRunnable/" itemprop="url">
                  spark-2.11-ExecutorRunnable
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-10-19T11:45:12+08:00" content="2018-10-19">
              2018-10-19
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-11/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.11</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文是对 org.apache.spark.deploy.yarn.ExecutorRunnable 源码进行学习的分析，spark版本为2.11。</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>这个方法就是用来启动container的。准备环境、生成命令，发送给NMClient。</p>
<h1 id="NMClient"><a href="#NMClient" class="headerlink" title="NMClient"></a>NMClient</h1><p>NMClient是Node Manager的客户端。一下是一些常用的方法。</p>
<table>
<thead>
<tr>
<th style="text-align:left">方法名</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">public static NMClient createNMClient()</td>
<td style="text-align:left">创建一个 NMClient实例</td>
</tr>
<tr>
<td style="text-align:left">public void init(Configuration conf)</td>
<td style="text-align:left">初始化 NMClient</td>
</tr>
<tr>
<td style="text-align:left">public void start()</td>
<td style="text-align:left">启动服务</td>
</tr>
<tr>
<td style="text-align:left">public Map<string, bytebuffer=""> startContainer(Container container, ContainerLaunchContext containerLaunchContext) throws YarnException, IOException</string,></td>
<td style="text-align:left">启动一个分配的 contianer</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/10/17/spark-2-11-ApplicationMaster/" itemprop="url">
                  spark 2.11 ApplicationMaster
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-10-17T19:51:13+08:00" content="2018-10-17">
              2018-10-17
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-11/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.11</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文是对 org.apache.spark.deploy.yarn.ApplicationMaster 源码进行学习的分析，spark的版本为2.11。</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>ApplicationMaster 可以说是运行用户程序的入口类。该类的一些行为有解析用户参数、启动dirver、建立与driver的通信、启动reporter线程、启动用户类等。</p>
<h1 id="主要方法分析"><a href="#主要方法分析" class="headerlink" title="主要方法分析"></a>主要方法分析</h1><h2 id="ApplicationMaster伴生类的main方法"><a href="#ApplicationMaster伴生类的main方法" class="headerlink" title="ApplicationMaster伴生类的main方法"></a>ApplicationMaster伴生类的main方法</h2><p>该方法是ApplicationMaster的启动入口方法，方法定义<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="comment">// 注册日志</span></div><div class="line">  <span class="type">SignalUtils</span>.registerLogger(log)</div><div class="line"></div><div class="line">  <span class="comment">// 实例化 ApplicationMasterArguments对应，用来解析传入的参数</span></div><div class="line">  <span class="keyword">val</span> amArgs = <span class="keyword">new</span> <span class="type">ApplicationMasterArguments</span>(args)</div><div class="line"></div><div class="line">  <span class="comment">// 是否设置了 --properties-file 参数，如果设置，则将properties文件中的配置加载到系统参数中</span></div><div class="line">  <span class="keyword">if</span> (amArgs.propertiesFile != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="type">Utils</span>.getPropertiesFromFile(amArgs.propertiesFile).foreach &#123; <span class="keyword">case</span> (k, v) =&gt;</div><div class="line">      sys.props(k) = v</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 以指定的用户运行 ApplicationMaster</span></div><div class="line">  <span class="type">SparkHadoopUtil</span>.get.runAsSparkUser &#123; () =&gt;</div><div class="line">    master = <span class="keyword">new</span> <span class="type">ApplicationMaster</span>(amArgs, <span class="keyword">new</span> <span class="type">YarnRMClient</span>)</div><div class="line">    <span class="comment">// 运行 application master</span></div><div class="line">    <span class="type">System</span>.exit(master.run())</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>从代码可以看出，该方法就是类的启动方法，注册日志、解析传入参数（如果参数传递了properties文件，则将properties中的配置加载到系统中），最后以特殊的用户身份启动ApplicationMaster（调用run方法）。</p>
<h2 id="run"><a href="#run" class="headerlink" title="run"></a>run</h2><p>该方法用来启动applicationMaster，方法定义如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Int</span> = &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// 获取application的id</span></div><div class="line">    <span class="keyword">val</span> appAttemptId = client.getAttemptId()</div><div class="line"></div><div class="line">    <span class="keyword">var</span> attemptID: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></div><div class="line"></div><div class="line">    <span class="comment">// 是否是集群模式，则设置集群模式需要的一些属性</span></div><div class="line">    <span class="keyword">if</span> (isClusterMode) &#123;</div><div class="line">      <span class="comment">// Set the web ui port to be ephemeral for yarn so we don't conflict with</span></div><div class="line">      <span class="comment">// other spark processes running on the same box</span></div><div class="line">      <span class="type">System</span>.setProperty(<span class="string">"spark.ui.port"</span>, <span class="string">"0"</span>)</div><div class="line"></div><div class="line">      <span class="comment">// Set the master and deploy mode property to match the requested mode.</span></div><div class="line">      <span class="type">System</span>.setProperty(<span class="string">"spark.master"</span>, <span class="string">"yarn"</span>)</div><div class="line">      <span class="type">System</span>.setProperty(<span class="string">"spark.submit.deployMode"</span>, <span class="string">"cluster"</span>)</div><div class="line"></div><div class="line">      <span class="comment">// Set this internal configuration if it is running on cluster mode, this</span></div><div class="line">      <span class="comment">// configuration will be checked in SparkContext to avoid misuse of yarn cluster mode.</span></div><div class="line">      <span class="type">System</span>.setProperty(<span class="string">"spark.yarn.app.id"</span>, appAttemptId.getApplicationId().toString())</div><div class="line"></div><div class="line">      attemptID = <span class="type">Option</span>(appAttemptId.getAttemptId.toString)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// &lt;1&gt;</span></div><div class="line"></div><div class="line">    <span class="comment">// 设置调用上下文，从spark.log.callerContext配置中读取</span></div><div class="line">    <span class="keyword">new</span> <span class="type">CallerContext</span>(</div><div class="line">      <span class="string">"APPMASTER"</span>, sparkConf.get(<span class="type">APP_CALLER_CONTEXT</span>),</div><div class="line">      <span class="type">Option</span>(appAttemptId.getApplicationId.toString), attemptID).setCurrentContext()</div><div class="line"></div><div class="line">    logInfo(<span class="string">"ApplicationAttemptId: "</span> + appAttemptId)</div><div class="line"></div><div class="line">    <span class="comment">// This shutdown hook should run *after* the SparkContext is shut down.</span></div><div class="line">    <span class="comment">// 设置钩子函数，以便在 SparkContext 之后调用，进行操作</span></div><div class="line">    <span class="keyword">val</span> priority = <span class="type">ShutdownHookManager</span>.<span class="type">SPARK_CONTEXT_SHUTDOWN_PRIORITY</span> - <span class="number">1</span></div><div class="line">    <span class="comment">//</span></div><div class="line">    <span class="type">ShutdownHookManager</span>.addShutdownHook(priority) &#123; () =&gt;</div><div class="line">      <span class="keyword">val</span> maxAppAttempts = client.getMaxRegAttempts(sparkConf, yarnConf)</div><div class="line">      <span class="keyword">val</span> isLastAttempt = client.getAttemptId().getAttemptId() &gt;= maxAppAttempts</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (!finished) &#123;</div><div class="line">        <span class="comment">// The default state of ApplicationMaster is failed if it is invoked by shut down hook.</span></div><div class="line">        <span class="comment">// This behavior is different compared to 1.x version.</span></div><div class="line">        <span class="comment">// If user application is exited ahead of time by calling System.exit(N), here mark</span></div><div class="line">        <span class="comment">// this application as failed with EXIT_EARLY. For a good shutdown, user shouldn't call</span></div><div class="line">        <span class="comment">// System.exit(0) to terminate the application.</span></div><div class="line">        finish(finalStatus,</div><div class="line">          <span class="type">ApplicationMaster</span>.<span class="type">EXIT_EARLY</span>,</div><div class="line">          <span class="string">"Shutdown hook called before final status was reported."</span>)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (!unregistered) &#123;</div><div class="line">        <span class="comment">// we only want to unregister if we don't want the RM to retry</span></div><div class="line">        <span class="keyword">if</span> (finalStatus == <span class="type">FinalApplicationStatus</span>.<span class="type">SUCCEEDED</span> || isLastAttempt) &#123;</div><div class="line">          unregister(finalStatus, finalMsg)</div><div class="line">          cleanupStagingDir()</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// &lt;2&gt;</span></div><div class="line"></div><div class="line">    <span class="comment">// Call this to force generation of secret so it gets populated into the</span></div><div class="line">    <span class="comment">// Hadoop UGI. This has to happen before the startUserApplication which does a</span></div><div class="line">    <span class="comment">// doAs in order for the credentials to be passed on to the executor containers.</span></div><div class="line">    <span class="comment">// 根据spark配置生成安全管理器</span></div><div class="line">    <span class="keyword">val</span> securityMgr = <span class="keyword">new</span> <span class="type">SecurityManager</span>(sparkConf)</div><div class="line"></div><div class="line">    <span class="comment">// If the credentials file config is present, we must periodically renew tokens. So create</span></div><div class="line">    <span class="comment">// a new AMDelegationTokenRenewer</span></div><div class="line">    <span class="comment">// spark.yarn.credentials.file</span></div><div class="line">    <span class="keyword">if</span> (sparkConf.contains(<span class="type">CREDENTIALS_FILE_PATH</span>.key)) &#123;</div><div class="line">      <span class="comment">// If a principal and keytab have been set, use that to create new credentials for executors</span></div><div class="line">      <span class="comment">// periodically</span></div><div class="line">      credentialRenewer =</div><div class="line">        <span class="keyword">new</span> <span class="type">ConfigurableCredentialManager</span>(sparkConf, yarnConf).credentialRenewer()</div><div class="line">      credentialRenewer.scheduleLoginFromKeytab()</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// &lt;3&gt;</span></div><div class="line"></div><div class="line">    <span class="comment">// 根据不同的集群模式，调用不同的方法</span></div><div class="line">    <span class="keyword">if</span> (isClusterMode) &#123;</div><div class="line">      runDriver(securityMgr)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      runExecutorLauncher(securityMgr)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// &lt;4&gt;</span></div><div class="line"></div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</div><div class="line">      <span class="comment">// catch everything else if not specifically handled</span></div><div class="line">      logError(<span class="string">"Uncaught exception: "</span>, e)</div><div class="line">      finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>,</div><div class="line">        <span class="type">ApplicationMaster</span>.<span class="type">EXIT_UNCAUGHT_EXCEPTION</span>,</div><div class="line">        <span class="string">"Uncaught exception: "</span> + e)</div><div class="line">  &#125;</div><div class="line">  exitCode</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><1> 这部分用来设置一些系统属性，以便后面使用。</1></p>
<p><2> 设置CurrentContext以及钩子方法，以便在sparkContext销毁之后进行清理操作。钩子函数实际上是交给了 SparkShutdownHookManager 对象进行处理。</2></p>
<p><3> 进行安全方面的一些设置，需要后续仔细看。</3></p>
<p><4> 进行服务的启动，这里区分是集群模式（cluster）还是客户端模式（client）。其内部执行的总体步骤是一样的，只是每个步骤的做的方式不同。<br>接下来，先从集群模式来看，然后再看客户端模式。</4></p>
<h2 id="runDriver"><a href="#runDriver" class="headerlink" title="runDriver"></a>runDriver</h2><p>运行deiver，只有集群模式，才会执行这个方法，方法定义如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runDriver</span></span>(securityMgr: <span class="type">SecurityManager</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="comment">// 添加IP过滤器</span></div><div class="line">  addAmIpFilter()</div><div class="line">  <span class="comment">// 启动用户Application，就是利用反射机制，运行用户指定的class中的main方法</span></div><div class="line">  userClassThread = startUserApplication()</div><div class="line"></div><div class="line">  <span class="comment">// This a bit hacky, but we need to wait until the spark.driver.port property has</span></div><div class="line">  <span class="comment">// been set by the Thread executing the user class.</span></div><div class="line">  logInfo(<span class="string">"Waiting for spark context initialization..."</span>)</div><div class="line">  <span class="comment">// spark.yarn.am.waitTime</span></div><div class="line">  <span class="keyword">val</span> totalWaitTime = sparkConf.get(<span class="type">AM_MAX_WAIT_TIME</span>)</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// 等待 SparkContext的生成，最多等待 totalWaitTime 毫秒</span></div><div class="line">    <span class="keyword">val</span> sc = <span class="type">ThreadUtils</span>.awaitResult(sparkContextPromise.future,</div><div class="line">      <span class="type">Duration</span>(totalWaitTime, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>))</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (sc != <span class="literal">null</span>) &#123;</div><div class="line">      rpcEnv = sc.env.rpcEnv</div><div class="line">      <span class="comment">// 启动 driver RPC</span></div><div class="line">      <span class="keyword">val</span> driverRef = runAMEndpoint(</div><div class="line">        sc.getConf.get(<span class="string">"spark.driver.host"</span>),</div><div class="line">        sc.getConf.get(<span class="string">"spark.driver.port"</span>),</div><div class="line">        isClusterMode = <span class="literal">true</span>)</div><div class="line"></div><div class="line">      <span class="comment">// 注册Application master</span></div><div class="line">      registerAM(sc.getConf, rpcEnv, driverRef, sc.ui.map(_.webUrl), securityMgr)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// Sanity check; should never happen in normal operation, since sc should only be null</span></div><div class="line">      <span class="comment">// if the user app did not create a SparkContext.</span></div><div class="line">      <span class="keyword">if</span> (!finished) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"SparkContext is null but app is still running!"</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 等待 用户线程 的完成</span></div><div class="line">    userClassThread.join()</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">SparkException</span> <span class="keyword">if</span> e.getCause().isInstanceOf[<span class="type">TimeoutException</span>] =&gt;</div><div class="line">      logError(</div><div class="line">        <span class="string">s"SparkContext did not initialize after waiting for <span class="subst">$totalWaitTime</span> ms. "</span> +</div><div class="line">         <span class="string">"Please check earlier log output for errors. Failing the application."</span>)</div><div class="line">      finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>,</div><div class="line">        <span class="type">ApplicationMaster</span>.<span class="type">EXIT_SC_NOT_INITED</span>,</div><div class="line">        <span class="string">"Timed out waiting for SparkContext."</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这个方法比较重要，因此需要详细看看。addAmIpFilter方法，从代码来看是给Spark UI增加IP 过滤器的功能，方法中也会对不同的部署模式有不同的区分，对于集群模式，将过滤器信息（过滤器类和参数）设置到系统参数，而对于client模式，则通过RPC服务发送给了driver。由此可见对于集群模式，driver是运行在本地的，而客户端模式，driver是运行在别处的。运行在哪里呢？另外，addAmIpFilter实际上添加的是 org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter 这个Filter。<br>接下来，方法是在独立的线程中启动了用户类（通过–class参数传入的类），启动用户类，其大概的操作就是获取类加载器，利用反射，加载类并最终调用用户类的main方法。<br>接着，获取SparkContext，并通过runAMEndpoint方法得到 driverEndpint，driverEndpoint作为一个参数来向ResourceManager 注册 ApplicationMaster。<br>然后就是向 ResourceManager 注册 ApplcationMaster。<br>最后等待用户类的执行完成。</p>
<h2 id="runExecutorLauncher"><a href="#runExecutorLauncher" class="headerlink" title="runExecutorLauncher"></a>runExecutorLauncher</h2><p>runExecutorLauncher方法与上面的rundirver的地位相同，只是针对client模式的启动方式。方法定义如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runExecutorLauncher</span></span>(securityMgr: <span class="type">SecurityManager</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">val</span> port = sparkConf.get(<span class="type">AM_PORT</span>)</div><div class="line">  rpcEnv = <span class="type">RpcEnv</span>.create(<span class="string">"sparkYarnAM"</span>, <span class="type">Utils</span>.localHostName, port, sparkConf, securityMgr,</div><div class="line">    clientMode = <span class="literal">true</span>)</div><div class="line">  <span class="keyword">val</span> driverRef = waitForSparkDriver()</div><div class="line">  addAmIpFilter()</div><div class="line">  registerAM(sparkConf, rpcEnv, driverRef, sparkConf.getOption(<span class="string">"spark.driver.appUIAddress"</span>),</div><div class="line">    securityMgr)</div><div class="line"></div><div class="line">  <span class="comment">// In client mode the actor will stop the reporter thread.</span></div><div class="line">  reporterThread.join()</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>此方法相对 rundriver来说就简单多了，首先创建了一个用于连接本机的RpcEnv，然后是等待SparkDriver的启动完成，添加IP Filter（通过amEndpoint发送给driver）。最后向ResourceManager 注册 ApplicationMaster。</p>
<h2 id="registerAM"><a href="#registerAM" class="headerlink" title="registerAM"></a>registerAM</h2><p>此方法用来处理向 ResourceManager 注册 ApplicationMaster。通过这个方法，就将ApplicationMaster与YarnRMClient和YarnAllocator联系起来了。方法定义<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerAM</span></span>(</div><div class="line">    _sparkConf: <span class="type">SparkConf</span>,</div><div class="line">    _rpcEnv: <span class="type">RpcEnv</span>,</div><div class="line">    driverRef: <span class="type">RpcEndpointRef</span>,</div><div class="line">    uiAddress: <span class="type">Option</span>[<span class="type">String</span>],</div><div class="line">    securityMgr: <span class="type">SecurityManager</span>) = &#123;</div><div class="line">  <span class="keyword">val</span> appId = client.getAttemptId().getApplicationId().toString()</div><div class="line">  <span class="keyword">val</span> attemptId = client.getAttemptId().getAttemptId().toString()</div><div class="line">  <span class="keyword">val</span> historyAddress =</div><div class="line">    _sparkConf.get(<span class="type">HISTORY_SERVER_ADDRESS</span>)</div><div class="line">      .map &#123; text =&gt; <span class="type">SparkHadoopUtil</span>.get.substituteHadoopVariables(text, yarnConf) &#125;</div><div class="line">      .map &#123; address =&gt; <span class="string">s"<span class="subst">$&#123;address&#125;</span><span class="subst">$&#123;HistoryServer.UI_PATH_PREFIX&#125;</span>/<span class="subst">$&#123;appId&#125;</span>/<span class="subst">$&#123;attemptId&#125;</span>"</span> &#125;</div><div class="line">      .getOrElse(<span class="string">""</span>)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> driverUrl = <span class="type">RpcEndpointAddress</span>(</div><div class="line">    _sparkConf.get(<span class="string">"spark.driver.host"</span>),</div><div class="line">    _sparkConf.get(<span class="string">"spark.driver.port"</span>).toInt,</div><div class="line">    <span class="type">CoarseGrainedSchedulerBackend</span>.<span class="type">ENDPOINT_NAME</span>).toString</div><div class="line"></div><div class="line">  <span class="comment">// Before we initialize the allocator, let's log the information about how executors will</span></div><div class="line">  <span class="comment">// be run up front, to avoid printing this out for every single executor being launched.</span></div><div class="line">  <span class="comment">// Use placeholders for information that changes such as executor IDs.</span></div><div class="line">  logInfo &#123;</div><div class="line">    <span class="keyword">val</span> executorMemory = sparkConf.get(<span class="type">EXECUTOR_MEMORY</span>).toInt</div><div class="line">    <span class="keyword">val</span> executorCores = sparkConf.get(<span class="type">EXECUTOR_CORES</span>)</div><div class="line">    <span class="keyword">val</span> dummyRunner = <span class="keyword">new</span> <span class="type">ExecutorRunnable</span>(<span class="type">None</span>, yarnConf, sparkConf, driverUrl, <span class="string">"&lt;executorId&gt;"</span>,</div><div class="line">      <span class="string">"&lt;hostname&gt;"</span>, executorMemory, executorCores, appId, securityMgr, localResources)</div><div class="line">    dummyRunner.launchContextDebugInfo()</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  allocator = client.register(driverUrl,</div><div class="line">    driverRef,</div><div class="line">    yarnConf,</div><div class="line">    _sparkConf,</div><div class="line">    uiAddress,</div><div class="line">    historyAddress,</div><div class="line">    securityMgr,</div><div class="line">    localResources)</div><div class="line"></div><div class="line">  allocator.allocateResources()</div><div class="line">  <span class="comment">// 在 客户端 模式中 的 runExecutorLauncher 方法中join</span></div><div class="line">  reporterThread = launchReporterThread()</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>该方法主要就是获取各种参数，然后调用client（YarnRMClient）的register方法进行注册（说是向ResourceManager注册的application，但以我来看，是注册的driver），还有就是启动reporter线程。</p>
<h2 id="launchReporterThread"><a href="#launchReporterThread" class="headerlink" title="launchReporterThread"></a>launchReporterThread</h2><p>用于生成并启动 reporter线程。方法定义如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchReporterThread</span></span>(): <span class="type">Thread</span> = &#123;</div><div class="line">  <span class="comment">// The number of failures in a row until Reporter thread give up</span></div><div class="line">  <span class="comment">// 获取配置的 reporter 线程最大失败次数</span></div><div class="line">  <span class="keyword">val</span> reporterMaxFailures = sparkConf.get(<span class="type">MAX_REPORTER_THREAD_FAILURES</span>)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> t = <span class="keyword">new</span> <span class="type">Thread</span> &#123;</div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</div><div class="line">      <span class="keyword">var</span> failureCount = <span class="number">0</span></div><div class="line">      <span class="keyword">while</span> (!finished) &#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          <span class="comment">// 如果 allocator 失败的 executor 个数已经超出了设置的最大值，则 终止 application 的运行（终止用户类的运行）</span></div><div class="line">          <span class="keyword">if</span> (allocator.getNumExecutorsFailed &gt;= maxNumExecutorFailures) &#123;</div><div class="line">            finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>,</div><div class="line">              <span class="type">ApplicationMaster</span>.<span class="type">EXIT_MAX_EXECUTOR_FAILURES</span>,</div><div class="line">              <span class="string">s"Max number of executor failures (<span class="subst">$maxNumExecutorFailures</span>) reached"</span>)</div><div class="line">          &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="comment">// 向YarnAllocator 申请资源</span></div><div class="line">            logDebug(<span class="string">"Sending progress"</span>)</div><div class="line">            allocator.allocateResources()</div><div class="line">          &#125;</div><div class="line">          failureCount = <span class="number">0</span></div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="comment">// 不同的异常不同的处理，中断异常，有可能是 finish 方法抛出来的</span></div><div class="line">          <span class="keyword">case</span> i: <span class="type">InterruptedException</span> =&gt; <span class="comment">// do nothing</span></div><div class="line">          <span class="keyword">case</span> e: <span class="type">ApplicationAttemptNotFoundException</span> =&gt;</div><div class="line">            failureCount += <span class="number">1</span></div><div class="line">            logError(<span class="string">"Exception from Reporter thread."</span>, e)</div><div class="line">            finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>, <span class="type">ApplicationMaster</span>.<span class="type">EXIT_REPORTER_FAILURE</span>,</div><div class="line">              e.getMessage)</div><div class="line">          <span class="comment">// 如果 reporter 线程的尝试次数超过配置的最大值，则终止 用户类的运行</span></div><div class="line">          <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">            failureCount += <span class="number">1</span></div><div class="line">            <span class="keyword">if</span> (!<span class="type">NonFatal</span>(e) || failureCount &gt;= reporterMaxFailures) &#123;</div><div class="line">              finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>,</div><div class="line">                <span class="type">ApplicationMaster</span>.<span class="type">EXIT_REPORTER_FAILURE</span>, <span class="string">"Exception was thrown "</span> +</div><div class="line">                  <span class="string">s"<span class="subst">$failureCount</span> time(s) from Reporter thread."</span>)</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">              logWarning(<span class="string">s"Reporter thread fails <span class="subst">$failureCount</span> time(s) in a row."</span>, e)</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          <span class="keyword">val</span> numPendingAllocate = allocator.getPendingAllocate.size</div><div class="line">          <span class="keyword">var</span> sleepStart = <span class="number">0</span>L</div><div class="line">          <span class="keyword">var</span> sleepInterval = <span class="number">200</span>L <span class="comment">// ms</span></div><div class="line">          allocatorLock.synchronized &#123;</div><div class="line">            <span class="comment">// 计算 reporter 的休眠时长， 根据</span></div><div class="line">            sleepInterval =</div><div class="line">              <span class="keyword">if</span> (numPendingAllocate &gt; <span class="number">0</span> || allocator.getNumPendingLossReasonRequests &gt; <span class="number">0</span>) &#123;</div><div class="line">                <span class="comment">// 进入这里，表示有丢失的container， 也有正在添加的container</span></div><div class="line">                <span class="keyword">val</span> currentAllocationInterval =</div><div class="line">                  math.min(heartbeatInterval, nextAllocationInterval)</div><div class="line">                nextAllocationInterval = currentAllocationInterval * <span class="number">2</span> <span class="comment">// avoid overflow</span></div><div class="line">                currentAllocationInterval</div><div class="line">              &#125; <span class="keyword">else</span> &#123;</div><div class="line">                nextAllocationInterval = initialAllocationInterval</div><div class="line">                heartbeatInterval</div><div class="line">              &#125;</div><div class="line">            sleepStart = <span class="type">System</span>.currentTimeMillis()</div><div class="line">            allocatorLock.wait(sleepInterval)</div><div class="line">          &#125;</div><div class="line">          <span class="keyword">val</span> sleepDuration = <span class="type">System</span>.currentTimeMillis() - sleepStart</div><div class="line"></div><div class="line">          <span class="comment">// 如果符合这个条件，说明上面的 allocatorLock.wait所等待的时间不够，改用Thread.sleep来休眠</span></div><div class="line">          <span class="keyword">if</span> (sleepDuration &lt; sleepInterval) &#123;</div><div class="line">            <span class="comment">// log when sleep is interrupted</span></div><div class="line">            logDebug(<span class="string">s"Number of pending allocations is <span class="subst">$numPendingAllocate</span>. "</span> +</div><div class="line">                <span class="string">s"Slept for <span class="subst">$sleepDuration</span>/<span class="subst">$sleepInterval</span> ms."</span>)</div><div class="line">            <span class="comment">// if sleep was less than the minimum interval, sleep for the rest of it</span></div><div class="line">            <span class="keyword">val</span> toSleep = math.max(<span class="number">0</span>, initialAllocationInterval - sleepDuration)</div><div class="line">            <span class="keyword">if</span> (toSleep &gt; <span class="number">0</span>) &#123;</div><div class="line">              logDebug(<span class="string">s"Going back to sleep for <span class="subst">$toSleep</span> ms"</span>)</div><div class="line">              <span class="comment">// use Thread.sleep instead of allocatorLock.wait. there is no need to be woken up</span></div><div class="line">              <span class="comment">// by the methods that signal allocatorLock because this is just finishing the min</span></div><div class="line">              <span class="comment">// sleep interval, which should happen even if this is signalled again.</span></div><div class="line">              <span class="type">Thread</span>.sleep(toSleep)</div><div class="line">            &#125;</div><div class="line">          &#125; <span class="keyword">else</span> &#123;</div><div class="line">            logDebug(<span class="string">s"Number of pending allocations is <span class="subst">$numPendingAllocate</span>. "</span> +</div><div class="line">                <span class="string">s"Slept for <span class="subst">$sleepDuration</span>/<span class="subst">$sleepInterval</span>."</span>)</div><div class="line">          &#125;</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> e: <span class="type">InterruptedException</span> =&gt;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// setting to daemon status, though this is usually not a good idea.</span></div><div class="line">  t.setDaemon(<span class="literal">true</span>)</div><div class="line">  t.setName(<span class="string">"Reporter"</span>)</div><div class="line">  t.start()</div><div class="line">  logInfo(<span class="string">s"Started progress reporter thread with (heartbeat : <span class="subst">$heartbeatInterval</span>, "</span> +</div><div class="line">          <span class="string">s"initial allocation : <span class="subst">$initialAllocationInterval</span>) intervals"</span>)</div><div class="line">  t</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>该方法看起来很复杂，其实它主要要做的事情就是 向YarnAllocator申请资源（调用allocateResources方法）。其他代码就是判断是否还要申请资源，以及什么时候进行下一次申请。</p>
<h2 id="startUserApplication"><a href="#startUserApplication" class="headerlink" title="startUserApplication"></a>startUserApplication</h2><p>启动用户应用程序，其实就是启动用户通过 –class参数传递过来的类。方法定义<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startUserApplication</span></span>(): <span class="type">Thread</span> = &#123;</div><div class="line">  logInfo(<span class="string">"Starting the user application in a separate Thread"</span>)</div><div class="line"></div><div class="line">  <span class="comment">// 获取用户的类路经</span></div><div class="line">  <span class="keyword">val</span> classpath = <span class="type">Client</span>.getUserClasspath(sparkConf)</div><div class="line">  <span class="keyword">val</span> urls = classpath.map &#123; entry =&gt;</div><div class="line">    <span class="keyword">new</span> <span class="type">URL</span>(<span class="string">"file:"</span> + <span class="keyword">new</span> <span class="type">File</span>(entry.getPath()).getAbsolutePath())</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">val</span> userClassLoader =</div><div class="line">    <span class="keyword">if</span> (<span class="type">Client</span>.isUserClassPathFirst(sparkConf, isDriver = <span class="literal">true</span>)) &#123;</div><div class="line">      <span class="keyword">new</span> <span class="type">ChildFirstURLClassLoader</span>(urls, <span class="type">Utils</span>.getContextOrSparkClassLoader)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">new</span> <span class="type">MutableURLClassLoader</span>(urls, <span class="type">Utils</span>.getContextOrSparkClassLoader)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">  <span class="keyword">var</span> userArgs = args.userArgs</div><div class="line">  <span class="keyword">if</span> (args.primaryPyFile != <span class="literal">null</span> &amp;&amp; args.primaryPyFile.endsWith(<span class="string">".py"</span>)) &#123;</div><div class="line">    <span class="comment">// When running pyspark, the app is run using PythonRunner. The second argument is the list</span></div><div class="line">    <span class="comment">// of files to add to PYTHONPATH, which Client.scala already handles, so it's empty.</span></div><div class="line">    userArgs = <span class="type">Seq</span>(args.primaryPyFile, <span class="string">""</span>) ++ userArgs</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (args.primaryRFile != <span class="literal">null</span> &amp;&amp; args.primaryRFile.endsWith(<span class="string">".R"</span>)) &#123;</div><div class="line">    <span class="comment">// TODO(davies): add R dependencies here</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// 用户类的main方法</span></div><div class="line">  <span class="keyword">val</span> mainMethod = userClassLoader.loadClass(args.userClass)</div><div class="line">    .getMethod(<span class="string">"main"</span>, classOf[<span class="type">Array</span>[<span class="type">String</span>]])</div><div class="line"></div><div class="line">  <span class="keyword">val</span> userThread = <span class="keyword">new</span> <span class="type">Thread</span> &#123;</div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        <span class="comment">// 在新的线程中运行 用户类的 main方法</span></div><div class="line">        mainMethod.invoke(<span class="literal">null</span>, userArgs.toArray)</div><div class="line"></div><div class="line">        <span class="comment">// 设置完成状态（用户类执行完成）</span></div><div class="line">        finish(<span class="type">FinalApplicationStatus</span>.<span class="type">SUCCEEDED</span>, <span class="type">ApplicationMaster</span>.<span class="type">EXIT_SUCCESS</span>)</div><div class="line"></div><div class="line">        <span class="comment">// 记录日志</span></div><div class="line">        logDebug(<span class="string">"Done running users class"</span>)</div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="comment">// 根据异常的原因，设置用户类执行的完成状态（失败状态）</span></div><div class="line">        <span class="comment">// 异常的原因分为</span></div><div class="line">        <span class="comment">// 中断异常</span></div><div class="line">        <span class="comment">// app异常</span></div><div class="line">        <span class="comment">// 代码异常</span></div><div class="line">        <span class="keyword">case</span> e: <span class="type">InvocationTargetException</span> =&gt;</div><div class="line">          e.getCause <span class="keyword">match</span> &#123;</div><div class="line">            <span class="keyword">case</span> _: <span class="type">InterruptedException</span> =&gt;</div><div class="line">              <span class="comment">// Reporter thread can interrupt to stop user class</span></div><div class="line">            <span class="keyword">case</span> <span class="type">SparkUserAppException</span>(exitCode) =&gt;</div><div class="line">              <span class="keyword">val</span> msg = <span class="string">s"User application exited with status <span class="subst">$exitCode</span>"</span></div><div class="line">              logError(msg)</div><div class="line">              finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>, exitCode, msg)</div><div class="line">            <span class="keyword">case</span> cause: <span class="type">Throwable</span> =&gt;</div><div class="line">              logError(<span class="string">"User class threw exception: "</span> + cause, cause)</div><div class="line">              finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>,</div><div class="line">                <span class="type">ApplicationMaster</span>.<span class="type">EXIT_EXCEPTION_USER_CLASS</span>,</div><div class="line">                <span class="string">"User class threw exception: "</span> + cause)</div><div class="line">          &#125;</div><div class="line">          sparkContextPromise.tryFailure(e.getCause())</div><div class="line">      &#125; <span class="keyword">finally</span> &#123;</div><div class="line">        <span class="comment">// Notify the thread waiting for the SparkContext, in case the application did not</span></div><div class="line">        <span class="comment">// instantiate one. This will do nothing when the user code instantiates a SparkContext</span></div><div class="line">        <span class="comment">// (with the correct master), or when the user code throws an exception (due to the</span></div><div class="line">        <span class="comment">// tryFailure above).</span></div><div class="line">        sparkContextPromise.trySuccess(<span class="literal">null</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// 设置线程的 类加载器 、 线程名字， 启动线程</span></div><div class="line">  userThread.setContextClassLoader(userClassLoader)</div><div class="line">  userThread.setName(<span class="string">"Driver"</span>)</div><div class="line">  userThread.start()</div><div class="line">  userThread</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>首先，通过代码我们可以明确一个问题，那就是用户的程序，其实就是driver。对于这个方法实现的功能，简单来说，就是获取类路径、获取类加载器、实例话用户类、执行用户类的main方法。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/10/12/spark-2-11-ApplicationMasterarguments/" itemprop="url">
                  spark 2.11 ApplicationMasterarguments
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-10-12T12:52:13+08:00" content="2018-10-12">
              2018-10-12
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-11/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.11</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文是对 org.apache.spark.deploy.yarn.ApplicationMasterArguments 源码学习的分析，spark的版本为2.11。</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>org.apache.spark.deploy.yarn.ApplicationMasterArguments类主要用来对ApplicationMaster参数进行解析。</p>
<h1 id="主要方法分析"><a href="#主要方法分析" class="headerlink" title="主要方法分析"></a>主要方法分析</h1><h2 id="parseArgs"><a href="#parseArgs" class="headerlink" title="parseArgs"></a>parseArgs</h2><p>该方法就是用来解析参数的。方法的定义如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">parseArgs</span></span>(inputArgs: <span class="type">List</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">val</span> userArgsBuffer = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">String</span>]()</div><div class="line"></div><div class="line">  <span class="keyword">var</span> args = inputArgs</div><div class="line"></div><div class="line">  <span class="comment">// 从这个匹配可以看出，可以使用的参数列在下面，如果包含了其他参数，系统会异常退出</span></div><div class="line">  <span class="comment">// --jar jar包</span></div><div class="line">  <span class="comment">// --class 类</span></div><div class="line">  <span class="comment">// --primary-py-file  PYTHON语言编写的application</span></div><div class="line">  <span class="comment">// --primary-r-file   R语言编写的application</span></div><div class="line">  <span class="comment">// --arg  其他参数，多个参数需要使用多个 --arg 1 --arg "name"</span></div><div class="line">  <span class="comment">// --properties-file 配置文件</span></div><div class="line">  <span class="keyword">while</span> (!args.isEmpty) &#123;</div><div class="line">    <span class="comment">// --num-workers, --worker-memory, and --worker-cores are deprecated since 1.0,</span></div><div class="line">    <span class="comment">// the properties with executor in their names are preferred.</span></div><div class="line">    <span class="comment">// 开始解析 类参数  case ("--jar") :: value :: tail 就是提取参数和参数名 ，并把剩余的参数放到 tail中</span></div><div class="line">    args <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> (<span class="string">"--jar"</span>) :: value :: tail =&gt;</div><div class="line">        userJar = value</div><div class="line">        args = tail</div><div class="line"></div><div class="line">      <span class="keyword">case</span> (<span class="string">"--class"</span>) :: value :: tail =&gt;</div><div class="line">        userClass = value</div><div class="line">        args = tail</div><div class="line"></div><div class="line">      <span class="keyword">case</span> (<span class="string">"--primary-py-file"</span>) :: value :: tail =&gt;</div><div class="line">        primaryPyFile = value</div><div class="line">        args = tail</div><div class="line"></div><div class="line">      <span class="keyword">case</span> (<span class="string">"--primary-r-file"</span>) :: value :: tail =&gt;</div><div class="line">        primaryRFile = value</div><div class="line">        args = tail</div><div class="line"></div><div class="line">      <span class="keyword">case</span> (<span class="string">"--arg"</span>) :: value :: tail =&gt;</div><div class="line">        userArgsBuffer += value</div><div class="line">        args = tail</div><div class="line"></div><div class="line">      <span class="keyword">case</span> (<span class="string">"--properties-file"</span>) :: value :: tail =&gt;</div><div class="line">        propertiesFile = value</div><div class="line">        args = tail</div><div class="line"></div><div class="line">      <span class="keyword">case</span> _ =&gt;</div><div class="line">        printUsageAndExit(<span class="number">1</span>, args)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (primaryPyFile != <span class="literal">null</span> &amp;&amp; primaryRFile != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="comment">// scalastyle:off println</span></div><div class="line">    <span class="type">System</span>.err.println(<span class="string">"Cannot have primary-py-file and primary-r-file at the same time"</span>)</div><div class="line">    <span class="comment">// scalastyle:on println</span></div><div class="line">    <span class="type">System</span>.exit(<span class="number">-1</span>)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  userArgs = userArgsBuffer.toList</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这个方法对ApplicationMaster参数进行解析，通过方法中match…case判断代码，我们可以看出ApplicationMaster允许的参数只有 6 个，如果包含其他名称的参数则会异常退出，并且参数–primary-py-file 和 参数–primary-r-file 不允许同时出现。对于上面的match…case的分析，见章节结尾部分。</p>
<h2 id="printUsageAndExit"><a href="#printUsageAndExit" class="headerlink" title="printUsageAndExit"></a>printUsageAndExit</h2><p>该方法用来将ApplicationMaster的使用参数信息进行打印。方法定义<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">def printUsageAndExit(exitCode: Int, unknownParam: Any = null) &#123;</div><div class="line">  // scalastyle:off println</div><div class="line">  if (unknownParam != null) &#123;</div><div class="line">    System.err.println("Unknown/unsupported param " + unknownParam)</div><div class="line">  &#125;</div><div class="line">  System.err.println("""</div><div class="line">    |Usage: org.apache.spark.deploy.yarn.ApplicationMaster [options]</div><div class="line">    |Options:</div><div class="line">    |  --jar JAR_PATH       Path to your application's JAR file</div><div class="line">    |  --class CLASS_NAME   Name of your application's main class</div><div class="line">    |  --primary-py-file    A main Python file</div><div class="line">    |  --primary-r-file     A main R file</div><div class="line">    |  --arg ARG            Argument to be passed to your application's main class.</div><div class="line">    |                       Multiple invocations are possible, each will be passed in order.</div><div class="line">    |  --properties-file FILE Path to a custom Spark properties file.</div><div class="line">    """.stripMargin)</div><div class="line">  // scalastyle:on println</div><div class="line">  System.exit(exitCode)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h1><h2 id="参数判断的match-…-case"><a href="#参数判断的match-…-case" class="headerlink" title="参数判断的match … case"></a>参数判断的match … case</h2><p>首先看代码<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">args <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> (<span class="string">"--jar"</span>) :: value :: tail =&gt;</div><div class="line">      userJar = value</div><div class="line">      args = tail</div><div class="line"></div><div class="line">    <span class="keyword">case</span> (<span class="string">"--class"</span>) :: value :: tail =&gt;</div><div class="line">      userClass = value</div><div class="line">      args = tail</div></pre></td></tr></table></figure></p>
<p>case中的信息其实就是匹配模式，这里，如“(“–jar”) :: value :: tail”，其实就是在args开头匹配 “–jar” 参数，也就是如果args中的第一个值为”–jar“，那么将args的第二个值赋值给value，最后将剩余的值放到 tail中，但是需要注意的是，这个模式是从args的第一个元素开始的，如果第二元素是“–jar”，是不符合条件的。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/10/11/spark-2-11-YarnRMClient/" itemprop="url">
                  spark 2.11 YarnRMClient
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-10-11T16:23:43+08:00" content="2018-10-11">
              2018-10-11
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-11/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.11</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文是对 org.apache.spark.deploy.yarn.YarnRMClient 源码进行学习的分析，spark的版本为2.11。</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>YarnRMClient主要用来处理application master向Yarn resourceManager的注册和注销。</p>
<h1 id="主要方法分析"><a href="#主要方法分析" class="headerlink" title="主要方法分析"></a>主要方法分析</h1><h2 id="register"><a href="#register" class="headerlink" title="register"></a>register</h2><p>该方法很简单，就是向YARN ResourceManager注册application master，该方法会在 ApplicationMaster的registerAM方法中调用。具体方法实现<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">register</span></span>(</div><div class="line">      driverUrl: <span class="type">String</span>,</div><div class="line">      driverRef: <span class="type">RpcEndpointRef</span>,</div><div class="line">      conf: <span class="type">YarnConfiguration</span>,</div><div class="line">      sparkConf: <span class="type">SparkConf</span>,</div><div class="line">      uiAddress: <span class="type">Option</span>[<span class="type">String</span>],</div><div class="line">      uiHistoryAddress: <span class="type">String</span>,</div><div class="line">      securityMgr: <span class="type">SecurityManager</span>,</div><div class="line">      localResources: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">LocalResource</span>]</div><div class="line">    ): <span class="type">YarnAllocator</span> = &#123;</div><div class="line"></div><div class="line">    <span class="comment">// 调用AMRMClient自身的方法来生成AMRMClient，再使用 Yarn 配置进行初始化，启动AMRMClient</span></div><div class="line">    amClient = <span class="type">AMRMClient</span>.createAMRMClient()</div><div class="line">    amClient.init(conf)</div><div class="line">    amClient.start()</div><div class="line">    <span class="keyword">this</span>.uiHistoryAddress = uiHistoryAddress</div><div class="line"></div><div class="line">    <span class="keyword">val</span> trackingUrl = uiAddress.getOrElse &#123;</div><div class="line">      <span class="keyword">if</span> (sparkConf.get(<span class="type">ALLOW_HISTORY_SERVER_TRACKING_URL</span>)) uiHistoryAddress <span class="keyword">else</span> <span class="string">""</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    logInfo(<span class="string">"Registering the ApplicationMaster"</span>)</div><div class="line">    <span class="comment">// 向 ResourceManager 注册 application master，从代码看出 application master就是本机，TODO 这个本机是啥呢？？？</span></div><div class="line">    synchronized &#123;</div><div class="line">      amClient.registerApplicationMaster(<span class="type">Utils</span>.localHostName(), <span class="number">0</span>, trackingUrl)</div><div class="line">      registered = <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 生成 YarnAllocator</span></div><div class="line">    <span class="comment">// driverUrl和driverRef需要说一下，</span></div><div class="line">    <span class="comment">// driverUrl，是driver运行的地址，会传递给Executor，应该是用于Execurot与driver进行交互</span></div><div class="line">    <span class="comment">// driverRef 在YarnAllocator中使用，用于同步executor的id，以及 发送删除executor的信息</span></div><div class="line">    <span class="keyword">new</span> <span class="type">YarnAllocator</span>(driverUrl, driverRef, conf, sparkConf, amClient, getAttemptId(), securityMgr,</div><div class="line">      localResources, <span class="keyword">new</span> <span class="type">SparkRackResolver</span>())</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>此方法逻辑很简单，一眼就看明白。生成AMRMClient（用于访问ResourceManager），向ResourceManager注册applicationMaster，生成YarnAllocator。但是需要注意生成YarnAllocator的参数。</p>
<h2 id="unregister"><a href="#unregister" class="headerlink" title="unregister"></a>unregister</h2><p>作用与register方法相反，从YARN ResourceManager中注销 application master。具体方法实现<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unregister</span></span>(status: <span class="type">FinalApplicationStatus</span>, diagnostics: <span class="type">String</span> = <span class="string">""</span>): <span class="type">Unit</span> = synchronized &#123;</div><div class="line">	<span class="keyword">if</span> (registered) &#123;</div><div class="line">	  amClient.unregisterApplicationMaster(status, diagnostics, uiHistoryAddress)</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="getMaxRegAttempts"><a href="#getMaxRegAttempts" class="headerlink" title="getMaxRegAttempts"></a>getMaxRegAttempts</h2><p>此方法就是用来定义注册application master的最大尝试次数。具体方法定义<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** 获取注册AM的最大尝试次数 分别从spark配置和yarn配置中读取 如果spark配置中设置了，则使用spark和yarn配置中最小那个值 */</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getMaxRegAttempts</span></span>(sparkConf: <span class="type">SparkConf</span>, yarnConf: <span class="type">YarnConfiguration</span>): <span class="type">Int</span> = &#123;</div><div class="line">  <span class="keyword">val</span> sparkMaxAttempts = sparkConf.get(<span class="type">MAX_APP_ATTEMPTS</span>).map(_.toInt)</div><div class="line">  <span class="keyword">val</span> yarnMaxAttempts = yarnConf.getInt(</div><div class="line">    <span class="type">YarnConfiguration</span>.<span class="type">RM_AM_MAX_ATTEMPTS</span>, <span class="type">YarnConfiguration</span>.<span class="type">DEFAULT_RM_AM_MAX_ATTEMPTS</span>)</div><div class="line">  sparkMaxAttempts <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(x) =&gt; <span class="keyword">if</span> (x &lt;= yarnMaxAttempts) x <span class="keyword">else</span> yarnMaxAttempts</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt; yarnMaxAttempts</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>此方法也很简单，分别从spark配置和yarn配置中读取 如果spark配置中设置了，则使用spark和yarn配置中最小那个值。没有在spark中配置，则使用yarn配置中的。</p>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h2 id="哪里生成YarnRMClient对象"><a href="#哪里生成YarnRMClient对象" class="headerlink" title="哪里生成YarnRMClient对象"></a>哪里生成YarnRMClient对象</h2><p>答案就是在ApplicationMaster的main方法中，代码：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">  </div><div class="line">  ...</div><div class="line">  </div><div class="line">  <span class="type">SparkHadoopUtil</span>.get.runAsSparkUser &#123; () =&gt;</div><div class="line">    master = <span class="keyword">new</span> <span class="type">ApplicationMaster</span>(amArgs, <span class="keyword">new</span> <span class="type">YarnRMClient</span>)</div><div class="line">    <span class="type">System</span>.exit(master.run())</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="哪里调用-YarnRMClient的register方法"><a href="#哪里调用-YarnRMClient的register方法" class="headerlink" title="哪里调用 YarnRMClient的register方法"></a>哪里调用 YarnRMClient的register方法</h2><p>在register方法中看到了YarnAllocator的生成，那么在哪里调用register方法呢？答案就是 org.apache.spark.deploy.yarn.ApplicationMaster中。而且ApplicationMaster含有main方法，是程序的入口。代码：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerAM</span></span>(</div><div class="line">      _sparkConf: <span class="type">SparkConf</span>,</div><div class="line">      _rpcEnv: <span class="type">RpcEnv</span>,</div><div class="line">      driverRef: <span class="type">RpcEndpointRef</span>,</div><div class="line">      uiAddress: <span class="type">Option</span>[<span class="type">String</span>],</div><div class="line">      securityMgr: <span class="type">SecurityManager</span>) = &#123;</div><div class="line">    </div><div class="line">    ...</div><div class="line"></div><div class="line">    allocator = client.register(driverUrl,</div><div class="line">      driverRef,</div><div class="line">      yarnConf,</div><div class="line">      _sparkConf,</div><div class="line">      uiAddress,</div><div class="line">      historyAddress,</div><div class="line">      securityMgr,</div><div class="line">      localResources)</div><div class="line"></div><div class="line">    allocator.allocateResources()</div><div class="line">    reporterThread = launchReporterThread()</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/10/10/spark-2-11-YarnAllocator/" itemprop="url">
                  spark 2.11 YarnAllocator
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-10-10T15:51:42+08:00" content="2018-10-10">
              2018-10-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-11/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.11</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文是对 org.apache.spark.deploy.yarn.YarnAllocator 类源码进行学习的分析，spark的版本为2.11。</p>
<h1 id="总体概述"><a href="#总体概述" class="headerlink" title="总体概述"></a>总体概述</h1><p>YarnAllocator可以理解成一个Container的筛选器。当调用了YarnAllocator.allocateResources()方法后，程序就会进行各种处理，最终调用ExecutorRunnable类来启动Executor。在YarnAllocator类中，最主要的方法有：allocateResources()、updateResourceRequests()、handleAllocatedContainers()、runAllocatedContainers()和processCompletedContainers()。而整个这些方法的调用，是通过allocateResources()来调用的。基本的流程如下图：</p>
<h1 id="主要方法的分析"><a href="#主要方法的分析" class="headerlink" title="主要方法的分析"></a>主要方法的分析</h1><h2 id="allocateResources"><a href="#allocateResources" class="headerlink" title="allocateResources"></a>allocateResources</h2><p>资源分配的入口，首先看方法的定义<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">allocateResources</span></span>(): <span class="type">Unit</span> = synchronized &#123;</div><div class="line">    updateResourceRequests()</div><div class="line"></div><div class="line">    <span class="comment">// 处理指示器</span></div><div class="line">    <span class="keyword">val</span> progressIndicator = <span class="number">0.1</span>f</div><div class="line">    <span class="comment">// Poll the ResourceManager. This doubles as a heartbeat if there are no pending container</span></div><div class="line">    <span class="comment">// requests.</span></div><div class="line">    <span class="comment">// 调用 AMRMClient 分配资源</span></div><div class="line">    <span class="keyword">val</span> allocateResponse = amClient.allocate(progressIndicator)</div><div class="line"></div><div class="line">    <span class="comment">// 得到已经分配的 container</span></div><div class="line">    <span class="keyword">val</span> allocatedContainers = allocateResponse.getAllocatedContainers()</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (allocatedContainers.size &gt; <span class="number">0</span>) &#123;</div><div class="line">      <span class="comment">// 输出日志信息，包括 分配的container数量，正在运行的以及启动的executor数量，以及可用的资源信息</span></div><div class="line">      logDebug((<span class="string">"Allocated containers: %d. Current executor count: %d. "</span> +</div><div class="line">        <span class="string">"Launching executor count: %d. Cluster resources: %s."</span>)</div><div class="line">        .format(</div><div class="line">          allocatedContainers.size,</div><div class="line">          numExecutorsRunning.get,</div><div class="line">          numExecutorsStarting.get,</div><div class="line">          allocateResponse.getAvailableResources))</div><div class="line"></div><div class="line">      <span class="comment">// 处理已经分配的container</span></div><div class="line">      handleAllocatedContainers(allocatedContainers.asScala)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 获取已经执行完成的 container</span></div><div class="line">    <span class="keyword">val</span> completedContainers = allocateResponse.getCompletedContainersStatuses()</div><div class="line">    <span class="keyword">if</span> (completedContainers.size &gt; <span class="number">0</span>) &#123;</div><div class="line">      logDebug(<span class="string">"Completed %d containers"</span>.format(completedContainers.size))</div><div class="line">      <span class="comment">//处理已经完成的container</span></div><div class="line">      processCompletedContainers(completedContainers.asScala)</div><div class="line">      logDebug(<span class="string">"Finished processing %d completed containers. Current running executor count: %d."</span></div><div class="line">        .format(completedContainers.size, numExecutorsRunning.get))</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>这个方法首先就是要更新资源的申请（调用updateResourceRequests()方法，我们稍后再看），然后就是调用AMRMClient（amClient）来分配资源，分配资源的返回值（allocateResponse）会包含三部分信息：已经分配的Container（allocatedContainers）、可用的资源和完成的Container（completedContainers）。对于已经分配的和完成的Container，会有对应的方法去处理；对于可用的资源，只是输出到日志。</p>
<h2 id="updateResourceRequests"><a href="#updateResourceRequests" class="headerlink" title="updateResourceRequests"></a>updateResourceRequests</h2><p>更新资源的请求信息，首先看方法的定义<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateResourceRequests</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="comment">// 得到正在添加的container 并 得到正在添加 container的数量</span></div><div class="line">    <span class="keyword">val</span> pendingAllocate = getPendingAllocate</div><div class="line">    <span class="keyword">val</span> numPendingAllocate = pendingAllocate.size</div><div class="line"></div><div class="line">    <span class="comment">// 计划要启动的executor数量 - 正在请求启动的executor - 已经启动的executor - 已经运行任务的executor = 缺少多少executor</span></div><div class="line">    <span class="comment">// 所以这里是在计算 比预计要启动的executor，还缺少多少个</span></div><div class="line">    <span class="keyword">val</span> missing = targetNumExecutors - numPendingAllocate -</div><div class="line">      numExecutorsStarting.get - numExecutorsRunning.get</div><div class="line">    logDebug(<span class="string">s"Updating resource requests, target: <span class="subst">$targetNumExecutors</span>, "</span> +</div><div class="line">      <span class="string">s"pending: <span class="subst">$numPendingAllocate</span>, running: <span class="subst">$&#123;numExecutorsRunning.get&#125;</span>, "</span> +</div><div class="line">      <span class="string">s"executorsStarting: <span class="subst">$&#123;numExecutorsStarting.get&#125;</span>"</span>)</div><div class="line">    <span class="comment">// &lt;1&gt;</span></div><div class="line"></div><div class="line">    <span class="comment">// missing 可以为正数 也可能为负数，负数则说明 动态分配分配多了，但是没有超过最大个数，这个数是通过计划启动个数算出来的</span></div><div class="line">    <span class="keyword">if</span> (missing &gt; <span class="number">0</span>) &#123;</div><div class="line">      logInfo(<span class="string">s"Will request <span class="subst">$missing</span> executor container(s), each with "</span> +</div><div class="line">        <span class="string">s"<span class="subst">$&#123;resource.getVirtualCores&#125;</span> core(s) and "</span> +</div><div class="line">        <span class="string">s"<span class="subst">$&#123;resource.getMemory&#125;</span> MB memory (including <span class="subst">$memoryOverhead</span> MB of overhead)"</span>)</div><div class="line"></div><div class="line">      <span class="comment">// 将要添加的container请求拆分到三个组：位置匹配列表、位置不匹配列表 和 无位置列表。</span></div><div class="line">      <span class="comment">// 对于位置匹配的 container 请求，将他们放到可用的地方，等待分配</span></div><div class="line">      <span class="comment">// 对于位置不匹配的和无位置的container请求，取消这些container的请求，因为 位置优先权已经变了，</span></div><div class="line">      <span class="keyword">val</span> (localRequests, staleRequests, anyHostRequests) = splitPendingAllocationsByLocality(</div><div class="line">        hostToLocalTaskCounts, pendingAllocate)</div><div class="line">      <span class="comment">// &lt;2&gt;</span></div><div class="line"></div><div class="line">      <span class="comment">// cancel "stale" requests for locations that are no longer needed</span></div><div class="line">      <span class="comment">// 对于位置不匹配的container，进行移除操作，并记录日志， 移除了N个container</span></div><div class="line">      staleRequests.foreach &#123; stale =&gt;</div><div class="line">        amClient.removeContainerRequest(stale)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">val</span> cancelledContainers = staleRequests.size</div><div class="line">      <span class="keyword">if</span> (cancelledContainers &gt; <span class="number">0</span>) &#123;</div><div class="line">        logInfo(<span class="string">s"Canceled <span class="subst">$cancelledContainers</span> container request(s) (locality no longer needed)"</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;3&gt;</span></div><div class="line"></div><div class="line">      <span class="comment">// 计算还可以分配的container的数量，就是 缺少的 + 取消的</span></div><div class="line">      <span class="comment">// 因为cancelledContainers的个数实际上就是从pendingAllocate 中取消的</span></div><div class="line">      <span class="keyword">val</span> availableContainers = missing + cancelledContainers</div><div class="line"></div><div class="line">      <span class="comment">// 计算潜在的container就是 可以分配的container + 上面那些 不限制位置的的contianer的数量</span></div><div class="line">      <span class="keyword">val</span> potentialContainers = availableContainers + anyHostRequests.size</div><div class="line"></div><div class="line">      <span class="comment">// TODO 这是在弄啥 ？？？ 应该是根据 潜在container的数量，生成对应个的contaner的位置引用</span></div><div class="line">      <span class="keyword">val</span> containerLocalityPreferences = <span class="keyword">if</span> (labelExpression.isEmpty) &#123;</div><div class="line">        containerPlacementStrategy.localityOfRequestedContainers(</div><div class="line">          potentialContainers, numLocalityAwareTasks, hostToLocalTaskCounts,</div><div class="line">          allocatedHostToContainersMap, localRequests)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="type">Array</span>.empty[<span class="type">ContainerLocalityPreferences</span>]</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// 根据上面的containerLocalityPreferences 创建container添加请求（注意是请求）</span></div><div class="line">      <span class="comment">// newLocalityRequest 用来记录要添加的container的请求</span></div><div class="line">      <span class="keyword">val</span> newLocalityRequests = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">ContainerRequest</span>]</div><div class="line">      containerLocalityPreferences.foreach &#123;</div><div class="line">        <span class="comment">// createContainerRequest 用来生成container的创建请求</span></div><div class="line">        <span class="keyword">case</span> <span class="type">ContainerLocalityPreferences</span>(nodes, racks) <span class="keyword">if</span> nodes != <span class="literal">null</span> =&gt;</div><div class="line">          newLocalityRequests += createContainerRequest(resource, nodes, racks)</div><div class="line">        <span class="keyword">case</span> _ =&gt;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// 需要再次判断 总的container的数量，如果availableContainers &gt; newLocalityRequests 表示，还不够</span></div><div class="line">      <span class="comment">// 为啥 availableContainers 会大于 newLocalityRequests ？ 因为 labelExpression.isEmpty 为空时，会生成一个空的Array</span></div><div class="line">      <span class="keyword">if</span> (availableContainers &gt;= newLocalityRequests.size) &#123;</div><div class="line">        <span class="comment">// more containers are available than needed for locality, fill in requests for any host</span></div><div class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until (availableContainers - newLocalityRequests.size)) &#123;</div><div class="line">          <span class="comment">// createContainerRequest 用来生成container的创建请求</span></div><div class="line">          newLocalityRequests += createContainerRequest(resource, <span class="literal">null</span>, <span class="literal">null</span>)</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line"></div><div class="line">        <span class="comment">// 这里的 newLocalityRequests 实际上对应的个是 potentialContainers = availableContainers + anyHostRequests.size</span></div><div class="line">        <span class="comment">// 因此，如果 newLocalityRequests &gt; availableContainers 则表示生成多了，且 anyHostRequests 的多了</span></div><div class="line"></div><div class="line">        <span class="keyword">val</span> numToCancel = newLocalityRequests.size - availableContainers</div><div class="line">        <span class="comment">// 因此释放到一些多余的 anyHostRequests</span></div><div class="line">        <span class="comment">// cancel some requests without locality preferences to schedule more local containers</span></div><div class="line">        anyHostRequests.slice(<span class="number">0</span>, numToCancel).foreach &#123; nonLocal =&gt;</div><div class="line">          amClient.removeContainerRequest(nonLocal)</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span> (numToCancel &gt; <span class="number">0</span>) &#123;</div><div class="line">          logInfo(<span class="string">s"Canceled <span class="subst">$numToCancel</span> unlocalized container requests to resubmit with locality"</span>)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;4&gt;</span></div><div class="line"></div><div class="line">      <span class="comment">// AMRMClient 请求添加container</span></div><div class="line">      newLocalityRequests.foreach &#123; request =&gt;</div><div class="line">        amClient.addContainerRequest(request)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;5&gt;</span></div><div class="line"></div><div class="line">      <span class="keyword">if</span> (log.isInfoEnabled()) &#123;</div><div class="line">        <span class="keyword">val</span> (localized, anyHost) = newLocalityRequests.partition(_.getNodes() != <span class="literal">null</span>)</div><div class="line">        <span class="keyword">if</span> (anyHost.nonEmpty) &#123;</div><div class="line">          logInfo(<span class="string">s"Submitted <span class="subst">$&#123;anyHost.size&#125;</span> unlocalized container requests."</span>)</div><div class="line">        &#125;</div><div class="line">        localized.foreach &#123; request =&gt;</div><div class="line">          logInfo(<span class="string">s"Submitted container request for host <span class="subst">$&#123;hostStr(request)&#125;</span>."</span>)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;6&gt;</span></div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (numPendingAllocate &gt; <span class="number">0</span> &amp;&amp; missing &lt; <span class="number">0</span>) &#123;</div><div class="line">      <span class="comment">// 如果不缺少executor，并且还有正在添加的executor</span></div><div class="line"></div><div class="line">      <span class="comment">// 计算要取消的contaner的数量，为什么是最小值，个人这样理解：-missing，其实是多出来的，但是在计算missing的时候，</span></div><div class="line">      <span class="comment">// 已经减去 numPendingAllocate 了，也就是说 numPendingAllocate 认为是已经使用的数量</span></div><div class="line">      <span class="comment">// 因此，如果取最大值，那么当 numPendingAllocate &gt; -missing 时，删除的container就太多了</span></div><div class="line">      <span class="keyword">val</span> numToCancel = math.min(numPendingAllocate, -missing)</div><div class="line">      logInfo(<span class="string">s"Canceling requests for <span class="subst">$numToCancel</span> executor container(s) to have a new desired "</span> +</div><div class="line">        <span class="string">s"total <span class="subst">$targetNumExecutors</span> executors."</span>)</div><div class="line"></div><div class="line">      <span class="comment">// 获取匹配的请求，并且有匹配的数据，则移除这些container（而且我猜测，这里的寻找匹配的请求，是在pending的队列中找的）</span></div><div class="line">      <span class="keyword">val</span> matchingRequests = amClient.getMatchingRequests(<span class="type">RM_REQUEST_PRIORITY</span>, <span class="type">ANY_HOST</span>, resource)</div><div class="line">      <span class="keyword">if</span> (!matchingRequests.isEmpty) &#123;</div><div class="line">        matchingRequests.iterator().next().asScala</div><div class="line">          .take(numToCancel).foreach(amClient.removeContainerRequest)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        logWarning(<span class="string">"Expected to find pending requests, but found none."</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;7&gt;</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p><1> 部分主要就是计算系统是否已经达到了目标个数的executor（container是executor的容器，目前一个container只包含了一个executor），并计算缺少的个数。计算的公式就是val missing = targetNumExecutors - numPendingAllocate - numExecutorsStarting.get - numExecutorsRunning.get。targetNumExecutors是配置中 spark.dynamicAllocation.minExecutors 、spark.dynamicAllocation.initialExecutors 以及 spark.executor.instances 三个配置中的最大值（开启了动态分配的话，如果没开启，则使用 spark.executor.instances 的值），并且要求 spark.dynamicAllocation.maxExecutors &gt;= spark.dynamicAllocation.initialExecutors &gt;= spark.dynamicAllocation.minExecutors（如果没有开启动态分配，则不存在这种要求）。这里涉及了4个配置项。numPendingAllocate的值调用AMRMClient的getMatchingRequests方法，获取location为*的所有请求。</1></p>
<p><2> 这一部分就是对添加中的请求（pendingAllocate）进行分类，分为位置自由的、位置匹配的和位置不匹配的。分类的依据是ContainerRequest.getNodes。如果nodes为空，则认为是位置自由的，如果nodes不空，则判断nodes是否hostToLocalTaskCounts的keyset有交集，如果有则认为是匹配的，否则不匹配。这里的匹配和不匹配，大概就是寻找本地的containerRequest（？？？？？？？？？？？？？？？？）。</2></p>
<p><3> 这一部分是对上面找出来的位置不匹配的请求，进行取消，并记录日志。筛选的规则是优先使用位置符合的，坚决不使用位置不符合的，数量不够的时候，使用位置自由的。</3></p>
<p><4> 这一部分是计算出需要创建的container个数。进行ContainerRequest的创建，首先创建位置符合的，然后创建位置自由的，并且将多余的位置自由的请求取消掉。</4></p>
<p><5> 在这里调用AMRMClient的addContainerRequest方法来增加ContainerRequest。</5></p>
<p><6> 记录日志</6></p>
<p><7> 将超出目标个数的container（位置自由的）取消。<br>所以从总体来看，这个方法其实就是把container的个数控制在目标个数范围内，如果缺少了，则增加，如果多了，则取消一些请求。</7></p>
<h2 id="handleAllocatedContainers"><a href="#handleAllocatedContainers" class="headerlink" title="handleAllocatedContainers"></a>handleAllocatedContainers</h2><p>此方法用来处理申请资源的container。方法的定义如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleAllocatedContainers</span></span>(allocatedContainers: <span class="type">Seq</span>[<span class="type">Container</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="comment">// 用来保存要使用的Containers</span></div><div class="line">    <span class="keyword">val</span> containersToUse = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>](allocatedContainers.size)</div><div class="line"></div><div class="line">    <span class="comment">// Match incoming requests by host 根据host进行匹配，匹配不成功的下面继续匹配</span></div><div class="line">    <span class="keyword">val</span> remainingAfterHostMatches = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]</div><div class="line">    <span class="keyword">for</span> (allocatedContainer &lt;- allocatedContainers) &#123;</div><div class="line">      matchContainerToRequest(allocatedContainer, allocatedContainer.getNodeId.getHost,</div><div class="line">        containersToUse, remainingAfterHostMatches)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// Match remaining by rack 对上面按照host匹配不成功的进行机架匹配</span></div><div class="line">    <span class="keyword">val</span> remainingAfterRackMatches = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]</div><div class="line">    <span class="keyword">for</span> (allocatedContainer &lt;- remainingAfterHostMatches) &#123;</div><div class="line">      <span class="keyword">val</span> rack = resolver.resolve(conf, allocatedContainer.getNodeId.getHost)</div><div class="line">      matchContainerToRequest(allocatedContainer, rack, containersToUse,</div><div class="line">        remainingAfterRackMatches)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 通过上面的两次匹配可以看到，最优先使用的是 host -&gt; rack -&gt; *</span></div><div class="line"></div><div class="line">    <span class="comment">// Assign remaining that are neither node-local nor rack-local</span></div><div class="line">    <span class="keyword">val</span> remainingAfterOffRackMatches = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]</div><div class="line">    <span class="keyword">for</span> (allocatedContainer &lt;- remainingAfterRackMatches) &#123;</div><div class="line">      matchContainerToRequest(allocatedContainer, <span class="type">ANY_HOST</span>, containersToUse,</div><div class="line">        remainingAfterOffRackMatches)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 三次匹配都没有成功的，就释放掉了</span></div><div class="line">    <span class="keyword">if</span> (!remainingAfterOffRackMatches.isEmpty) &#123;</div><div class="line">      logDebug(<span class="string">s"Releasing <span class="subst">$&#123;remainingAfterOffRackMatches.size&#125;</span> unneeded containers that were "</span> +</div><div class="line">        <span class="string">s"allocated to us"</span>)</div><div class="line">      <span class="keyword">for</span> (container &lt;- remainingAfterOffRackMatches) &#123;</div><div class="line">        internalReleaseContainer(container)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 启动所有的container，这些container上经过上面过滤后的可以使用container</span></div><div class="line">    runAllocatedContainers(containersToUse)</div><div class="line"></div><div class="line">    logInfo(<span class="string">"Received %d containers from YARN, launching executors on %d of them."</span></div><div class="line">      .format(allocatedContainers.size, containersToUse.size))</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>从代码来看，这个方法其实就是对allocatedContainers再次进行过滤，根据主机、机架 和 通配主机（*）。最后将不匹配的释放掉（不同于取消请求）。并在方法的最后启动所有分配的container启动。</p>
<h2 id="runAllocatedContainers"><a href="#runAllocatedContainers" class="headerlink" title="runAllocatedContainers"></a>runAllocatedContainers</h2><p>运行 container，方法定义如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runAllocatedContainers</span></span>(containersToUse: <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">for</span> (container &lt;- containersToUse) &#123;</div><div class="line">      <span class="comment">// executorIdConter 应该是同步的，用来生成新的executor的id</span></div><div class="line">      executorIdCounter += <span class="number">1</span></div><div class="line">      <span class="keyword">val</span> executorHostname = container.getNodeId.getHost</div><div class="line">      <span class="keyword">val</span> containerId = container.getId</div><div class="line">      <span class="keyword">val</span> executorId = executorIdCounter.toString</div><div class="line">      assert(container.getResource.getMemory &gt;= resource.getMemory)</div><div class="line">      logInfo(<span class="string">s"Launching container <span class="subst">$containerId</span> on host <span class="subst">$executorHostname</span> "</span> +</div><div class="line">        <span class="string">s"for executor with ID <span class="subst">$executorId</span>"</span>)</div><div class="line">      <span class="comment">//&lt;1&gt;</span></div><div class="line"></div><div class="line"></div><div class="line">      <span class="comment">// 主要是为了更新各种计数</span></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">updateInternalState</span></span>(): <span class="type">Unit</span> = synchronized &#123;</div><div class="line">        <span class="comment">// 正在运行的executor加一</span></div><div class="line">        numExecutorsRunning.incrementAndGet()</div><div class="line">        <span class="comment">// 启动中的executor 减一，因为已经运行了，就不能处于启动中了</span></div><div class="line">        numExecutorsStarting.decrementAndGet()</div><div class="line"></div><div class="line">        <span class="comment">//保存 executor -&gt; container的关系 以及 container -&gt; executorId的关系</span></div><div class="line">        executorIdToContainer(executorId) = container</div><div class="line">        containerIdToExecutorId(container.getId) = executorId</div><div class="line"></div><div class="line">        <span class="comment">// 记录 某个主机上都有哪些 containers</span></div><div class="line">        <span class="keyword">val</span> containerSet = allocatedHostToContainersMap.getOrElseUpdate(executorHostname,</div><div class="line">          <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">ContainerId</span>])</div><div class="line">        containerSet += containerId</div><div class="line">        <span class="comment">// 记录contain运行在那个主机上</span></div><div class="line">        allocatedContainerToHostMap.put(containerId, executorHostname)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;2&gt;</span></div><div class="line"></div><div class="line">      <span class="comment">// 如果正在运行的executor的数量小于 想要启动的executor个数</span></div><div class="line">      <span class="keyword">if</span> (numExecutorsRunning.get &lt; targetNumExecutors) &#123;</div><div class="line"></div><div class="line">        <span class="comment">// 将启动的executor的计数加一</span></div><div class="line">        <span class="comment">// 这里为啥要加一呢？？？</span></div><div class="line">        <span class="comment">// 因为 接下来要启动 container了，没有启动起来之前则认为处于启动中，因此先加一，一旦启动完成，则调用上面的updateInternalState</span></div><div class="line">        <span class="comment">// 方法，在这个方法中会对运行中的executor加一，启动中的executor减一。</span></div><div class="line">        numExecutorsStarting.incrementAndGet()</div><div class="line"></div><div class="line">        <span class="comment">// spark.yarn.launchContainers 配置的值</span></div><div class="line">        <span class="keyword">if</span> (launchContainers) &#123;</div><div class="line">          launcherPool.execute(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</div><div class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">              <span class="keyword">try</span> &#123;</div><div class="line">                <span class="comment">// 调用 ExecutorRunnalbe方法来启动container</span></div><div class="line">                <span class="keyword">new</span> <span class="type">ExecutorRunnable</span>(</div><div class="line">                  <span class="type">Some</span>(container),</div><div class="line">                  conf,</div><div class="line">                  sparkConf,</div><div class="line">                  driverUrl,</div><div class="line">                  executorId,</div><div class="line">                  executorHostname,</div><div class="line">                  executorMemory,</div><div class="line">                  executorCores,</div><div class="line">                  appAttemptId.getApplicationId.toString,</div><div class="line">                  securityMgr,</div><div class="line">                  localResources</div><div class="line">                ).run()</div><div class="line"></div><div class="line">                <span class="comment">// 跟新各种计数</span></div><div class="line">                updateInternalState()</div><div class="line">              &#125; <span class="keyword">catch</span> &#123;</div><div class="line">                <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">                  numExecutorsStarting.decrementAndGet()</div><div class="line">                  <span class="keyword">if</span> (<span class="type">NonFatal</span>(e)) &#123;</div><div class="line">                    logError(<span class="string">s"Failed to launch executor <span class="subst">$executorId</span> on container <span class="subst">$containerId</span>"</span>, e)</div><div class="line">                    <span class="comment">// Assigned container should be released immediately</span></div><div class="line">                    <span class="comment">// to avoid unnecessary resource occupation.</span></div><div class="line">                    <span class="comment">// 启动失败，则释放掉这个container</span></div><div class="line">                    amClient.releaseAssignedContainer(containerId)</div><div class="line">                  &#125; <span class="keyword">else</span> &#123;</div><div class="line">                    <span class="keyword">throw</span> e</div><div class="line">                  &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="comment">// For test only</span></div><div class="line">          updateInternalState()</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        logInfo((<span class="string">"Skip launching executorRunnable as runnning Excecutors count: %d "</span> +</div><div class="line">          <span class="string">"reached target Executors count: %d."</span>).format(</div><div class="line">          numExecutorsRunning.get, targetNumExecutors))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// &lt;3&gt;</span></div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p><1> 获取container和executor的信息，方便下面使用。</1></p>
<p><2> 定义了一个方法，这个方法就是用来更新各种数据关系，有executor运行的数量、executor启动的数量、executorId到container的对应关系、containerId到executorId的对应关系、host上运行的contianer的对应关系、container在哪个host运行的关系。</2></p>
<p><3> 异步生成ExecutorRunnable，并启动。</3></p>
<h2 id="processCompletedContainers"><a href="#processCompletedContainers" class="headerlink" title="processCompletedContainers"></a>processCompletedContainers</h2><p>此方法也在allocateResources()方法中调用，用来处理完成的container，完成的container可能是被kill掉，也可能是正常完成的。方法定义如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[yarn] <span class="function"><span class="keyword">def</span> <span class="title">processCompletedContainers</span></span>(completedContainers: <span class="type">Seq</span>[<span class="type">ContainerStatus</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="comment">// 循环完成的container</span></div><div class="line">    <span class="keyword">for</span> (completedContainer &lt;- completedContainers) &#123;</div><div class="line"></div><div class="line">      <span class="comment">// 获取containerId, 判断container是否在releasedContainers 中，获取 container所在的host</span></div><div class="line">      <span class="keyword">val</span> containerId = completedContainer.getContainerId</div><div class="line">      <span class="keyword">val</span> alreadyReleased = releasedContainers.remove(containerId)</div><div class="line">      <span class="keyword">val</span> hostOpt = allocatedContainerToHostMap.get(containerId)</div><div class="line">      <span class="keyword">val</span> onHostStr = hostOpt.map(host =&gt; <span class="string">s" on host: <span class="subst">$host</span>"</span>).getOrElse(<span class="string">""</span>)</div><div class="line">      <span class="comment">// &lt;1&gt;</span></div><div class="line"></div><div class="line">      <span class="comment">// 如果alreadyReleased为false，则表示releasedContainers 中没有这个container，或者是有但是删除失败</span></div><div class="line">      <span class="keyword">val</span> exitReason = <span class="keyword">if</span> (!alreadyReleased) &#123;</div><div class="line">        <span class="comment">// Decrement the number of executors running. The next iteration of</span></div><div class="line">        <span class="comment">// the ApplicationMaster's reporting thread will take care of allocating.</span></div><div class="line">        numExecutorsRunning.decrementAndGet()</div><div class="line">        logInfo(<span class="string">"Completed container %s%s (state: %s, exit status: %s)"</span>.format(</div><div class="line">          containerId,</div><div class="line">          onHostStr,</div><div class="line">          completedContainer.getState,</div><div class="line">          completedContainer.getExitStatus))</div><div class="line">        <span class="comment">// 获取container的退出状态</span></div><div class="line">        <span class="keyword">val</span> exitStatus = completedContainer.getExitStatus</div><div class="line"></div><div class="line">        <span class="comment">// 根据退出状态来判断是否是由于Application的原因</span></div><div class="line">        <span class="comment">// 以及退出的原因</span></div><div class="line">        <span class="comment">// ContainerExitStatus.SUCCESS 表示是因为YARN事件，不是因为运行的job发生error而导致的</span></div><div class="line">        <span class="comment">// ContainerExitStatus.PREEMPTED 表示主机上的端口被占用了</span></div><div class="line">        <span class="comment">// VMEM_EXCEEDED_EXIT_CODE 表示 虚拟内存的问题</span></div><div class="line">        <span class="comment">// PMEM_EXCEEDED_EXIT_CODE 表示 物理内存的问题</span></div><div class="line">        <span class="comment">// 否则</span></div><div class="line">        <span class="comment">// 除了第一种和第二种，其他的认为都属于 Application的原因</span></div><div class="line">        <span class="keyword">val</span> (exitCausedByApp, containerExitReason) = exitStatus <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="type">ContainerExitStatus</span>.<span class="type">SUCCESS</span> =&gt;</div><div class="line">            (<span class="literal">false</span>, <span class="string">s"Executor for container <span class="subst">$containerId</span> exited because of a YARN event (e.g., "</span> +</div><div class="line">              <span class="string">"pre-emption) and not because of an error in the running job."</span>)</div><div class="line">          <span class="keyword">case</span> <span class="type">ContainerExitStatus</span>.<span class="type">PREEMPTED</span> =&gt;</div><div class="line">            <span class="comment">// Preemption is not the fault of the running tasks, since YARN preempts containers</span></div><div class="line">            <span class="comment">// merely to do resource sharing, and tasks that fail due to preempted executors could</span></div><div class="line">            <span class="comment">// just as easily finish on any other executor. See SPARK-8167.</span></div><div class="line">            (<span class="literal">false</span>, <span class="string">s"Container <span class="subst">$&#123;containerId&#125;</span><span class="subst">$&#123;onHostStr&#125;</span> was preempted."</span>)</div><div class="line">          <span class="comment">// Should probably still count memory exceeded exit codes towards task failures</span></div><div class="line">          <span class="keyword">case</span> <span class="type">VMEM_EXCEEDED_EXIT_CODE</span> =&gt;</div><div class="line">            (<span class="literal">true</span>, memLimitExceededLogMessage(</div><div class="line">              completedContainer.getDiagnostics,</div><div class="line">              <span class="type">VMEM_EXCEEDED_PATTERN</span>))</div><div class="line">          <span class="keyword">case</span> <span class="type">PMEM_EXCEEDED_EXIT_CODE</span> =&gt;</div><div class="line">            (<span class="literal">true</span>, memLimitExceededLogMessage(</div><div class="line">              completedContainer.getDiagnostics,</div><div class="line">              <span class="type">PMEM_EXCEEDED_PATTERN</span>))</div><div class="line">          <span class="keyword">case</span> _ =&gt;</div><div class="line">            <span class="comment">// Enqueue the timestamp of failed executor</span></div><div class="line">            failedExecutorsTimeStamps.enqueue(clock.getTimeMillis())</div><div class="line">            (<span class="literal">true</span>, <span class="string">"Container marked as failed: "</span> + containerId + onHostStr +</div><div class="line">              <span class="string">". Exit status: "</span> + completedContainer.getExitStatus +</div><div class="line">              <span class="string">". Diagnostics: "</span> + completedContainer.getDiagnostics)</div><div class="line"></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// 如果是由于Application的原因退出的，则以警告记录日志，否则以info级别记录日志</span></div><div class="line">        <span class="keyword">if</span> (exitCausedByApp) &#123;</div><div class="line">          logWarning(containerExitReason)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          logInfo(containerExitReason)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// 生成一个ExecutorExited 对象并返回给变量</span></div><div class="line">        <span class="type">ExecutorExited</span>(exitStatus, exitCausedByApp, containerExitReason)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// 如果 alreadyReleased 为true，则表示 container已经被kill掉了，在 internalReleaseContainer 方法中会操作</span></div><div class="line">        <span class="type">ExecutorExited</span>(completedContainer.getExitStatus, exitCausedByApp = <span class="literal">false</span>,</div><div class="line">          <span class="string">s"Container <span class="subst">$containerId</span> exited from explicit termination request."</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// &lt;2&gt;</span></div><div class="line"></div><div class="line">      <span class="keyword">for</span> &#123;</div><div class="line">        host &lt;- hostOpt</div><div class="line">        <span class="comment">// 根据host获取上面所运行的container</span></div><div class="line">        containerSet &lt;- allocatedHostToContainersMap.get(host)</div><div class="line">      &#125; &#123;</div><div class="line">        <span class="comment">// 将container从host所包含的container集合中删除，这样host上的container集合就含有这个container了</span></div><div class="line">        containerSet.remove(containerId)</div><div class="line">        <span class="comment">// 删除完成后，如果 container 集合列表空了，则说明 host上只运行了这一个contianer，则删除host与container的对应关系，否则就更新一下</span></div><div class="line">        <span class="keyword">if</span> (containerSet.isEmpty) &#123;</div><div class="line">          allocatedHostToContainersMap.remove(host)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          allocatedHostToContainersMap.update(host, containerSet)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// 将container 到 host的映射关系也删除</span></div><div class="line">        allocatedContainerToHostMap.remove(containerId)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// 移除 container到executor的对应关系</span></div><div class="line">      containerIdToExecutorId.remove(containerId).foreach &#123; eid =&gt;</div><div class="line">        <span class="comment">// 将executor到container的对应关系也删除</span></div><div class="line">        executorIdToContainer.remove(eid)</div><div class="line"></div><div class="line">        <span class="comment">// 从 pendingLossReasonRequests 移除 executor</span></div><div class="line">        pendingLossReasonRequests.remove(eid) <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="type">Some</span>(pendingRequests) =&gt;</div><div class="line">            <span class="comment">// Notify application of executor loss reasons so it can decide whether it should abort</span></div><div class="line">            pendingRequests.foreach(_.reply(exitReason))</div><div class="line"></div><div class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">            releasedExecutorLossReasons.put(eid, exitReason)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// &lt;3&gt;</span></div><div class="line"></div><div class="line">        <span class="comment">// 如果没有被kill掉</span></div><div class="line">        <span class="keyword">if</span> (!alreadyReleased) &#123;</div><div class="line">          <span class="comment">// The executor could have gone away (like no route to host, node failure, etc)</span></div><div class="line">          <span class="comment">// Notify backend about the failure of the executor</span></div><div class="line">          <span class="comment">// container非异常释放的计数加一</span></div><div class="line">          numUnexpectedContainerRelease += <span class="number">1</span></div><div class="line">          <span class="comment">// 发送删除 execurot的请求</span></div><div class="line">          driverRef.send(<span class="type">RemoveExecutor</span>(eid, exitReason))</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// &lt;4&gt;</span></div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p><1> 获取containerId, 判断container是否在releasedContainers 中，获取 container所在的host，以便后面使用。</1></p>
<p><2> 分析contianer退出的原因，退出的类型分为 App引发的和非App引发的。具体的判断，可以看代码和注释。</2></p>
<p><3> 主要为了清理container各种关系的保存信息。</3></p>
<p><4> 使用Netty RPC发送移除Executor的信息。</4></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从上面的代码的调用过程以及实现我们可以看出，YarnAllocator实际上类似一个过滤器，它会从Resource Manager那里申请资源（通过AMRMClient获取），对获取到资源按照host -&gt; stack -&gt; any的顺序筛选container，并将合适的container启动后，反馈给调用者。<br>这个类功能很简单，但是在整个集群中又比较不太好理解，首先需要确定的是，YarnAllocator自己不管理资源（不对资源进行操作），只是筛选，虽然也会有释放、取消操作，但是这些操作都是调用Resource Manager的api来完成的。<br>另外，YarnAllocator是运行在driver上的，由AM来调用（？？？？？需要再次确认），因此，如果每个application都会有自己的YarnAllocator，它只是为自己的job的运行筛选container，而不是全局为所有的application统一筛选。</p>
<h1 id="AMRMClient的一些方法"><a href="#AMRMClient的一些方法" class="headerlink" title="AMRMClient的一些方法"></a>AMRMClient的一些方法</h1><table>
<thead>
<tr>
<th style="text-align:left">方法名</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">AllocateResponse allocate(float progressIndicator)</td>
<td style="text-align:left">请求额外的container并接收新的container的分配。</td>
</tr>
<tr>
<td style="text-align:left">List&lt;? extends Collection<t>&gt; getMatchingRequests(Priority priority, String resourceName, Resource capability)</t></td>
<td style="text-align:left">获取与给定参数匹配的未完成的请求。</td>
</tr>
<tr>
<td style="text-align:left">void removeContainerRequest(T req)</td>
<td style="text-align:left">移除之前的container请求。</td>
</tr>
<tr>
<td style="text-align:left">void addContainerRequest(T req)</td>
<td style="text-align:left">在调用allocate之前为container申请资源。</td>
</tr>
<tr>
<td style="text-align:left">void releaseAssignedContainer(ContainerId containerId)</td>
<td style="text-align:left">释放由Resource Manager分配的container。</td>
</tr>
<tr>
<td style="text-align:left">RegisterApplicationMasterResponse registerApplicationMaster(String appHostName, int appHostPort, String appTrackingUrl)</td>
<td style="text-align:left">注册application Master</td>
</tr>
</tbody>
</table>
<h1 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h1><h2 id="YarnAllocator是什么时候生成的"><a href="#YarnAllocator是什么时候生成的" class="headerlink" title="YarnAllocator是什么时候生成的"></a>YarnAllocator是什么时候生成的</h2><p>是在YarnRMClient的register方法中，向AMRMClient注册ApplicationMaster（调用AMRMClient.registerApplicationMaster方法）完成之后生成的。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">register</span></span>(</div><div class="line">      driverUrl: <span class="type">String</span>,</div><div class="line">      driverRef: <span class="type">RpcEndpointRef</span>,</div><div class="line">      conf: <span class="type">YarnConfiguration</span>,</div><div class="line">      sparkConf: <span class="type">SparkConf</span>,</div><div class="line">      uiAddress: <span class="type">Option</span>[<span class="type">String</span>],</div><div class="line">      uiHistoryAddress: <span class="type">String</span>,</div><div class="line">      securityMgr: <span class="type">SecurityManager</span>,</div><div class="line">      localResources: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">LocalResource</span>]</div><div class="line">    ): <span class="type">YarnAllocator</span> = &#123;</div><div class="line">    amClient = <span class="type">AMRMClient</span>.createAMRMClient()</div><div class="line">    amClient.init(conf)</div><div class="line">    amClient.start()</div><div class="line">    <span class="keyword">this</span>.uiHistoryAddress = uiHistoryAddress</div><div class="line"></div><div class="line">    <span class="keyword">val</span> trackingUrl = uiAddress.getOrElse &#123;</div><div class="line">      <span class="keyword">if</span> (sparkConf.get(<span class="type">ALLOW_HISTORY_SERVER_TRACKING_URL</span>)) uiHistoryAddress <span class="keyword">else</span> <span class="string">""</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    logInfo(<span class="string">"Registering the ApplicationMaster"</span>)</div><div class="line">    synchronized &#123;</div><div class="line">      amClient.registerApplicationMaster(<span class="type">Utils</span>.localHostName(), <span class="number">0</span>, trackingUrl)</div><div class="line">      registered = <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">new</span> <span class="type">YarnAllocator</span>(driverUrl, driverRef, conf, sparkConf, amClient, getAttemptId(), securityMgr,</div><div class="line">      localResources, <span class="keyword">new</span> <span class="type">SparkRackResolver</span>())</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/09/17/spark-resource-study/" itemprop="url">
                  Spark Resource Study
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-09-17T14:36:19+08:00" content="2018-09-17">
              2018-09-17
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Core"><a href="#Core" class="headerlink" title="Core"></a>Core</h1><h2 id="Partition-分区"><a href="#Partition-分区" class="headerlink" title="Partition 分区"></a>Partition 分区</h2><p>在Partition特征中，只定义了index，用来表示分区在父级RDD中索引。</p>
<h2 id="Dependency-依赖"><a href="#Dependency-依赖" class="headerlink" title="Dependency 依赖"></a>Dependency 依赖</h2><p>依赖的定义在 org.apache.spark.Dependency类中，从代码中可以看出，Dependency有2个子类，分别代表了2中类型的依赖，分别为：NarrowDependency和ShuffleDependency。其中NarrowDependency有两个子类：OneToOneDependency和RangeDependency。</p>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD是Resilient Distributed Dataset的简称，是Spark中的基本抽象。要实例化一个RDD，需要两个参数：SparkContext和Dependency列表。需要SparkContext是因为SparkContext提供了RDD的一些操作（如生成RDD的id，清理RDD的缓存、缓存RDD等），而Dependency则是因为它表示了RDD的继承关系。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RDD</span>[<span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></div><div class="line">    @transient private var _sc: <span class="type">SparkContext</span>,</div><div class="line">    @transient private var deps: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]]</div><div class="line">  ) <span class="keyword">extends</span> <span class="title">Serializable</span> <span class="keyword">with</span> <span class="title">Logging</span> &#123;</div></pre></td></tr></table></figure></p>
<p>此处可以看到RDD的基本构造方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(<span class="meta">@transient</span> oneParent: <span class="type">RDD</span>[_]) =</div><div class="line">    <span class="keyword">this</span>(oneParent.context, <span class="type">List</span>(<span class="keyword">new</span> <span class="type">OneToOneDependency</span>(oneParent)))</div></pre></td></tr></table></figure>
<p>使用已有的RDD构造新的RDD。</p>
<p>此外，RDD定义了一些抽象，需要子类进行实现：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] -- 计算给定的分区，返回一个迭代器</div><div class="line"></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] -- 返回<span class="type">RDD</span>的所有分区</div><div class="line"></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps -- 返回<span class="type">RDD</span>的到父类<span class="type">RDD</span>的所有依赖</div></pre></td></tr></table></figure></p>
<h2 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h2><h1 id="wordCount分析"><a href="#wordCount分析" class="headerlink" title="wordCount分析"></a>wordCount分析</h1><p>了解了一些代码之后，决定从wordCount的案例进行分析，以便了解Spark进行计算时的具体操作。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"master"</span>, <span class="string">"testApplication"</span>);</div><div class="line">sc.hadoopFile(<span class="string">"path"</span>, <span class="number">5</span>).map(_ =&gt; <span class="number">1</span>).count()</div></pre></td></tr></table></figure></p>
<p>因为在Spark中，transform是延迟执行的，也就是说，action之前的transform只有在遇到后面的action之后，才开始执行。因此上面的代码就要从count()开始。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum  </div><div class="line"><span class="comment">//Utils.getIteratorSize方法，在之后的代码块中展示，其实现就是根据参数的Iterator，计算一下这个迭代器中的数据个数（猜测迭代器最终是RDD分区的迭代器）</span></div><div class="line"><span class="comment">//这里的runJob的定义是</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</div><div class="line">    runJob(rdd, func, <span class="number">0</span> until rdd.partitions.length)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//然后又指向</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>, partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</div><div class="line">    <span class="keyword">val</span> cleanedFunc = clean(func)</div><div class="line">    runJob(rdd, (ctx: <span class="type">TaskContext</span>, it: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedFunc(it), partitions)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//然后继续指向</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>, partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</div><div class="line">    <span class="keyword">val</span> results = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">U</span>](partitions.size)</div><div class="line">    runJob[<span class="type">T</span>, <span class="type">U</span>](rdd, func, partitions, (index, res) =&gt; results(index) = res)</div><div class="line">    results</div><div class="line">  &#125;</div><div class="line"></div><div class="line"><span class="comment">//继续指向</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>, partitions: <span class="type">Seq</span>[<span class="type">Int</span>], resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    ...</div><div class="line">    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</div><div class="line">    progressBar.foreach(_.finishAll())</div><div class="line">    rdd.doCheckpoint()</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>最终执行Job落到了 dagScheduler 对象身上<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>, partitions: <span class="type">Seq</span>[<span class="type">Int</span>], callSite: <span class="type">CallSite</span>, resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>, properties: <span class="type">Properties</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> start = <span class="type">System</span>.nanoTime</div><div class="line">    <span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</div><div class="line">    ...</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>dagScheduler 的runJob方法中调用submitJob来提交任务</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>, partitions: <span class="type">Seq</span>[<span class="type">Int</span>], callSite: <span class="type">CallSite</span>, resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>, properties: <span class="type">Properties</span>):<span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</div><div class="line">    ...</div><div class="line">    <span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>(<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</div><div class="line">    eventProcessLoop.post(<span class="type">JobSubmitted</span>(jobId, rdd, func2, partitions.toArray, callSite, waiter, <span class="type">SerializationUtils</span>.clone(properties)))</div><div class="line">    waiter</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>最终，通过eventProcessLoop的post将任务提交到了任务执行队列。 这里需要注意的一个问题，加入任务队列的是一个 JobSubmitted对象。为什么要如此处理呢？需要从eventProcessLoop对象入手。<br>eventProcessLoop是DAGSchedulerEventProcessLoop的实例<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[scheduler] <span class="keyword">val</span> eventProcessLoop = <span class="keyword">new</span> <span class="type">DAGSchedulerEventProcessLoop</span>(<span class="keyword">this</span>)</div></pre></td></tr></table></figure></p>
<p>查看 DAGSchedulerEventProcessLoop 的定义<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[scheduler] <span class="class"><span class="keyword">class</span> <span class="title">DAGSchedulerEventProcessLoop</span>(<span class="params">dagScheduler: <span class="type">DAGScheduler</span></span>) <span class="keyword">extends</span> <span class="title">EventLoop</span>[<span class="type">DAGSchedulerEvent</span>](<span class="params">"dag-scheduler-event-loop"</span>) <span class="keyword">with</span> <span class="title">Logging</span></span></div></pre></td></tr></table></figure></p>
<p>DAGSchedulerEventProcessLoop继承于 EventLoop，EventLoop的内部有一个EventThread的线程，该线程从事件队列中循环获取数据<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> (!stopped.get) &#123;</div><div class="line">  <span class="keyword">val</span> event = eventQueue.take()</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    onReceive(event)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>DAGSchedulerEventProcessLoop的doOnReceive方法的定义如下<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</div><div class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>当从事件队列中获取到数据后，如果JobSubmitted对象，则调用dagScheduler的handleJobSubmitted方法。由此也知道了为什么eventProcessLoop.post()推的数据是 JobSubmitted 对象了。</p>
<p>再看handleJobSubmitted方法的定义：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</div><div class="line">  finalRDD: <span class="type">RDD</span>[_],</div><div class="line">  func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</div><div class="line">  partitions: <span class="type">Array</span>[<span class="type">Int</span>],</div><div class="line">  callSite: <span class="type">CallSite</span>,</div><div class="line">  listener: <span class="type">JobListener</span>,</div><div class="line">  properties: <span class="type">Properties</span>) &#123;</div><div class="line">	<span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></div><div class="line">	<span class="keyword">try</span> &#123;</div><div class="line">	  <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span></div><div class="line">	  <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span></div><div class="line">	  finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</div><div class="line">	&#125; <span class="keyword">catch</span> &#123;</div><div class="line">	  <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</div><div class="line">	    logWarning(<span class="string">"Creating new stage failed due to exception - job: "</span> + jobId, e)</div><div class="line">	    listener.jobFailed(e)</div><div class="line">	    <span class="keyword">return</span></div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, callSite, listener, properties)</div><div class="line">	</div><div class="line">	jobIdToActiveJob(jobId) = job</div><div class="line">	activeJobs += job</div><div class="line">	finalStage.setActiveJob(job)</div><div class="line">	<span class="keyword">val</span> stageIds = jobIdToStageIds(jobId).toArray</div><div class="line">	<span class="keyword">val</span> stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</div><div class="line">	listenerBus.post(</div><div class="line">	  <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))</div><div class="line">	submitStage(finalStage)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>由此看到了创建步骤（createResultStage(finalRDD, func, partitions, jobId, callSite)）和提交步骤（submitStage(finalStage)）的代码。</p>
<p>？？？上面的代码分析过程中，我们知道整个transform的触发点是从action（count()）开始的，而count是最后一个RDD（map生成的那个RDD）的方法。map对应的RDD是MapPartitionsRDD。在MapPartitionsRDD的compute方法中，而compute方法中使用的迭代器是从最开始的那个RDD开始的（ firstParent[T].iterator(split, context) ）。</p>
<p>创建步骤 createResultStage(finalRDD, func, partitions, jobId, callSite)</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/09/07/scal-study/" itemprop="url">
                  Scala Study
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-09-07T14:29:54+08:00" content="2018-09-07">
              2018-09-07
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文只是用来记录scala的一些语法信息</p>
<h1 id="Scala-Option"><a href="#Scala-Option" class="headerlink" title="Scala Option"></a>Scala Option</h1><p>/System/Library/Frameworks/JavaVM.framework/Versions/Current/Commands/java_home</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/09/05/optparse-python/" itemprop="url">
                  Python学习之optparse
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-09-05T11:30:34+08:00" content="2018-09-05">
              2018-09-05
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="optparse"><a href="#optparse" class="headerlink" title="optparse"></a>optparse</h1><p>在使用python进行命令开发的过程中，经常需要使用的就是命令行参数了，本章节介绍一个功能强大，易于使用的内建命令行参数处理模块optparse。</p>
<h2 id="简单的使用"><a href="#简单的使用" class="headerlink" title="简单的使用"></a>简单的使用</h2><p>首先需要创建一个OptionParser对象，该类属于optparse模块，因此使用前需要导入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> optparse <span class="keyword">import</span> OptionParser</div><div class="line">parser = OptionParser(usage=<span class="string">"usage: %prog action [options]"</span>,)</div><div class="line"></div><div class="line">或</div><div class="line"></div><div class="line"><span class="keyword">import</span> optparse</div><div class="line">parser = optparse.OptionParser(usage=<span class="string">"usage: %prog action [options]"</span>,)</div></pre></td></tr></table></figure>
<p>可以不写参数，参数用来指定帮助的显示信息，如果没有指定，则默认显示“usage: %prog [options]”。</p>
<h3 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">def __init__(self,</div><div class="line">             usage=None,</div><div class="line">             option_list=None,</div><div class="line">             option_class=Option,</div><div class="line">             version=None,</div><div class="line">             conflict_handler=&quot;error&quot;,</div><div class="line">             description=None,</div><div class="line">             formatter=None,</div><div class="line">             add_help_option=True,</div><div class="line">             prog=None,</div><div class="line">             epilog=None):</div><div class="line"></div><div class="line">作者：fuyoufang</div><div class="line">链接：https://www.jianshu.com/p/bec089061742</div><div class="line">來源：简书</div><div class="line">简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。</div></pre></td></tr></table></figure>
<h3 id="add-option函数"><a href="#add-option函数" class="headerlink" title="add_option函数"></a>add_option函数</h3><p>创建OptionParser对象之后，就可以使用add_option来定义命令行参数了<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">parsser.add_option(...)</div></pre></td></tr></table></figure></p>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p>add_option的前两参数分别为短参数名和长参数名。其中长参数名可以省略。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">parser.add_option(<span class="string">"-f"</span>, dest=<span class="string">"fileName"</span>)</div></pre></td></tr></table></figure></p>
<h4 id="dest"><a href="#dest" class="headerlink" title="dest"></a>dest</h4><p>定义程序内参数值的名字，之后通过这个名字来取参数的值。如果为空，则使用不加“-”的短参数名。</p>
<h4 id="action"><a href="#action" class="headerlink" title="action"></a>action</h4><p>action是add_option的一个参数，用来指定解析到参数后的操作。默认值为“store”，表示将参数值保存到options对象中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">parser.add_option(<span class="string">"-f"</span>, <span class="string">"--file"</span>, action=<span class="string">"store"</span>, dest=<span class="string">"filename"</span>)</div><div class="line">(options, args) = parser.parse_args([<span class="string">"-f"</span>, <span class="string">"myfile.log"</span>])</div><div class="line"><span class="keyword">print</span> options.filename</div></pre></td></tr></table></figure></p>
<p>其中action值的store还可以有其他两种类型：store_ture和store_false，用来处理不带参数值的参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">parser.add_option(<span class="string">"-g"</span>, action=<span class="string">"store_true"</span>, dest=<span class="string">"isGoodUser"</span>)</div><div class="line">parser.add_option(<span class="string">"-s"</span>, action=<span class="string">"store_false"</span>, dest=<span class="string">"isGoodUser"</span>)</div></pre></td></tr></table></figure></p>
<p>action可以使用的值出了store_true和store_false之外，还可以使用store、store_const、append、count、callback等。</p>
<h4 id="type"><a href="#type" class="headerlink" title="type"></a>type</h4><p>type是add_option方法的一个参数，用来指定参数值的类型，默认为String。还可以指定为“int”、“float”等<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">parser.add_option(<span class="string">"-n"</span>, type=<span class="string">"int"</span>, dest=<span class="string">"num"</span>)</div></pre></td></tr></table></figure></p>
<h4 id="default"><a href="#default" class="headerlink" title="default"></a>default</h4><p>通过add_option方法的default参数可以对命令行参数设置默认值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">parser.add_option(<span class="string">"-f"</span>, <span class="string">"--file"</span>, default=<span class="string">"test.log"</span>)</div></pre></td></tr></table></figure></p>
<h4 id="help"><a href="#help" class="headerlink" title="help"></a>help</h4><p>用于指定参数在帮助程序中显示的信息，详细请见下方的“生成帮助”章节。</p>
<h4 id="metavar"><a href="#metavar" class="headerlink" title="metavar"></a>metavar</h4><p>metavar配合help参数，用于帮助提醒用户该命令行所期待的参数，详细请见下方的“生成帮助”章节。</p>
<h4 id="choices"><a href="#choices" class="headerlink" title="choices"></a>choices</h4><p>当type为choices时，需要设置此值。</p>
<h4 id="const"><a href="#const" class="headerlink" title="const"></a>const</h4><p>指定一个常数，配合action为store_const和append_const时一起使用。</p>
<h3 id="parse-args函数"><a href="#parse-args函数" class="headerlink" title="parse_args函数"></a>parse_args函数</h3><p>一旦定义好了所有的命令行参数，就可以调用parse_args()来解析命令行。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(options, args) = parser.parse_args()</div></pre></td></tr></table></figure></p>
<h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h4><p>你可以传递一个命令行参数列表给parse_args方法，否则默认使用sys.argv[:1]。<br>parse_args方法有两个返回值：</p>
<blockquote>
<p>options optpars.Values对象，保存了所有命令行的值。只要知道命令行参数名，就可以得到。<br>args 一个由 positional arguments组成的列表。</p>
</blockquote>
<h3 id="set-defaults函数"><a href="#set-defaults函数" class="headerlink" title="set_defaults函数"></a>set_defaults函数</h3><p>除了在add_option方法中使用default参数设置默认值，还可以使用set_default函数，来统一设置默认值。该方法应该在所有add_option函数之前调用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">parser.set_default(filename=<span class="string">"test.log"</span>, isGoodUser=<span class="keyword">False</span>)</div></pre></td></tr></table></figure></p>
<p>set_defaults函数，可以用来设置多个默认值。</p>
<h3 id="has-option方法"><a href="#has-option方法" class="headerlink" title="has_option方法"></a>has_option方法</h3><p>用来检查是否有相应的选项<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> parser.has_option(<span class="string">"fileName"</span>)</div></pre></td></tr></table></figure></p>
<h3 id="remove-option"><a href="#remove-option" class="headerlink" title="remove_option()"></a>remove_option()</h3><p>用来删除相应的选项。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> parser.remove_option(<span class="string">"fileName"</span>)</div></pre></td></tr></table></figure></p>
<h2 id="生成帮助"><a href="#生成帮助" class="headerlink" title="生成帮助"></a>生成帮助</h2><p>optparse的另一个方便的功能便是自动生成帮助，而你需要做的事情就是在调用add_option方法时指定help参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">parser = optparse.OptionParser(usage=<span class="string">"usage: %prog action [options]"</span>,)</div><div class="line">parser.add_option(<span class="string">"-v"</span>, action=<span class="string">"store_true"</span>, metavar=<span class="string">"参数的期望值"</span>, help=<span class="string">"这是-v的参数的意义"</span>)</div></pre></td></tr></table></figure></p>
<p>当optparse解析到-h或者–help时，就会调用parser.print_help()方法来打印帮助信息。</p>
<h2 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h2><h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><p>在出现用户输入无效的、不完整的命令行参数而发生异常时，optparse可以自动检测并处理，比如参数值类型错误等。<br>用户也可以使用parser.error函数来手动抛出异常。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">parser.parse_args()</div><div class="line"><span class="keyword">if</span> parser.isGooodUser:</div><div class="line">    parse.error(<span class="string">"this is an exception"</span>)</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/09/04/ambari-resource/" itemprop="url">
                  Ambari Resource 01
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-09-04T15:03:02+08:00" content="2018-09-04">
              2018-09-04
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在使用Ambari的过程中遇到了好多问题，比如删除一个cluster（使用ambari-server reset命令）后，重启Ambari Server之后一直报错，提示找不到集群。尝试各种方法之后，无法找到满意的解决之道的情况下，只好硬着头皮读读源码，了解一下Ambari的启动机制。在此记下源码阅读的笔记和心得。</p>
<p>首先分析一下Ambari Server的启动脚本(ambari-server.py)，以便了解Ambari Server是如何启动的。</p>
<p>首先看入口函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  <span class="keyword">try</span>:</div><div class="line">    mainBody()</div><div class="line">  <span class="keyword">except</span> (KeyboardInterrupt, EOFError):</div><div class="line">    print(<span class="string">"\nAborting ... Keyboard Interrupt."</span>)</div><div class="line">    sys.exit(<span class="number">1</span>)</div></pre></td></tr></table></figure></p>
<p>没什么可说的，就是调用文件中的 mainBody() 方法。</p>
<h3 id="mainBody方法"><a href="#mainBody方法" class="headerlink" title="mainBody方法"></a>mainBody方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mainBody</span><span class="params">()</span>:</span></div><div class="line">  <span class="comment">#初始化命令行参数解析器</span></div><div class="line">  parser = optparse.OptionParser(usage=<span class="string">"usage: %prog action [options]"</span>,)</div><div class="line">  action = sys.argv[<span class="number">1</span>]</div><div class="line"></div><div class="line">  <span class="comment">#init_action_parser方法，在脚本中会有两个方法，一个是针对windows的一个是针对linux的，所以看代码的时候需要注意（方法上使用@OsFamilyFuncImpl标示），我们这里看的是linux的</span></div><div class="line">  <span class="comment">#就是对命令行解析器进行初始化，并针对行为进行进一步的初始化</span></div><div class="line">  init_action_parser(action, parser)</div><div class="line"></div><div class="line">  <span class="comment">#使用命令行参数解析器，对命令行进行解析</span></div><div class="line">  (options, args) = parser.parse_args()</div><div class="line"></div><div class="line">  <span class="comment"># check if only silent key set</span></div><div class="line">  default_options = parser.get_default_values()</div><div class="line">  silent_options = default_options</div><div class="line">  silent_options.silent = <span class="keyword">True</span></div><div class="line"></div><div class="line">  <span class="keyword">if</span> options == silent_options:</div><div class="line">    options.only_silent = <span class="keyword">True</span></div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    options.only_silent = <span class="keyword">False</span></div><div class="line"></div><div class="line">  <span class="comment"># varbose是在init_action_parser方法中设置的，对应的-v参数。是否打印状态信息</span></div><div class="line">  <span class="comment"># set_varbose方法位于 ambari_commons.logging_utils中，其实就是设置全局变量_VERBOSE的值。</span></div><div class="line">  <span class="comment"># set verbose</span></div><div class="line">  set_verbose(options.verbose)</div><div class="line"></div><div class="line">  <span class="comment">#接下来就是调用main方法。这里目测是同一个main方法，最大的区别就是有异常处理，有待之后补充TODO--------------------？？？</span></div><div class="line">  <span class="keyword">if</span> options.verbose:</div><div class="line">    main(options, args, parser)</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">      main(options, args, parser)</div><div class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">      print_error_msg(<span class="string">"Unexpected &#123;0&#125;: &#123;1&#125;"</span>.format((e).__class__.__name__, str(e)) +\</div><div class="line">      <span class="string">"\nFor more info run ambari-server with -v or --verbose option"</span>)</div><div class="line">      sys.exit(<span class="number">1</span>)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_action_parser</span><span class="params">(action, parser)</span>:</span></div><div class="line">  <span class="comment">#定义了参数可用的行为，如ambari-server start，其中start就是行为。这里定义了行为以及对应行为处理操作。</span></div><div class="line">  <span class="comment">#这些行为的处理方法其实是进一步的初始化参数解析器，会根据对应的行为对参数解析器进行不同的初始化。</span></div><div class="line">  action_parser_map = &#123;</div><div class="line">    SETUP_ACTION: init_setup_parser_options,</div><div class="line">    SETUP_JCE_ACTION: init_empty_parser_options,</div><div class="line">    START_ACTION: init_start_parser_options,</div><div class="line">    STOP_ACTION: init_empty_parser_options,</div><div class="line">    RESTART_ACTION: init_start_parser_options,</div><div class="line">    RESET_ACTION: init_reset_parser_options,</div><div class="line">    STATUS_ACTION: init_empty_parser_options,</div><div class="line">    UPGRADE_ACTION: init_empty_parser_options,</div><div class="line">    LDAP_SETUP_ACTION: init_ldap_setup_parser_options,</div><div class="line">    LDAP_SYNC_ACTION: init_ldap_sync_parser_options,</div><div class="line">    SET_CURRENT_ACTION: init_set_current_parser_options,</div><div class="line">    SETUP_SECURITY_ACTION: init_setup_security_parser_options,</div><div class="line">    REFRESH_STACK_HASH_ACTION: init_empty_parser_options,</div><div class="line">    BACKUP_ACTION: init_empty_parser_options,</div><div class="line">    RESTORE_ACTION: init_empty_parser_options,</div><div class="line">    UPDATE_HOST_NAMES_ACTION: init_empty_parser_options,</div><div class="line">    CHECK_DATABASE_ACTION: init_empty_parser_options,</div><div class="line">    ENABLE_STACK_ACTION: init_enable_stack_parser_options,</div><div class="line">    SETUP_SSO_ACTION: init_setup_sso_options,</div><div class="line">    DB_PURGE_ACTION: init_db_purge_parser_options,</div><div class="line">    INSTALL_MPACK_ACTION: init_install_mpack_parser_options,</div><div class="line">    UNINSTALL_MPACK_ACTION: init_uninstall_mpack_parser_options,</div><div class="line">    UPGRADE_MPACK_ACTION: init_upgrade_mpack_parser_options,</div><div class="line">    PAM_SETUP_ACTION: init_pam_setup_parser_options,</div><div class="line">    KERBEROS_SETUP_ACTION: init_kerberos_setup_parser_options,</div><div class="line">  &#125;</div><div class="line">  parser.add_option(<span class="string">"-v"</span>, <span class="string">"--verbose"</span>,</div><div class="line">                    action=<span class="string">"store_true"</span>, dest=<span class="string">"verbose"</span>, default=<span class="keyword">False</span>,</div><div class="line">                    help=<span class="string">"Print verbose status messages"</span>)</div><div class="line">  parser.add_option(<span class="string">"-s"</span>, <span class="string">"--silent"</span>,</div><div class="line">                    action=<span class="string">"store_true"</span>, dest=<span class="string">"silent"</span>, default=<span class="keyword">False</span>,</div><div class="line">                    help=<span class="string">"Silently accepts default prompt values. For db-cleanup command, silent mode will stop ambari server."</span>)</div><div class="line">  <span class="keyword">try</span>:</div><div class="line">    <span class="comment">#根据行为，做进一步的解析器初始化，我们以setup行为为例，那么setup行为的参数解析器对应的是init_setup_parser_options函数。</span></div><div class="line">    action_parser_map[action](parser)</div><div class="line">  <span class="keyword">except</span> KeyError:</div><div class="line">    parser.error(<span class="string">"Invalid action: "</span> + action)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_setup_parser_options</span><span class="params">(parser)</span>:</span></div><div class="line">  database_group = optparse.OptionGroup(parser, <span class="string">'Database options (command need to include all options)'</span>)</div><div class="line">  database_group.add_option(<span class="string">'--database'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Database to use embedded|oracle|mysql|mssql|postgres|sqlanywhere"</span>, dest=<span class="string">"dbms"</span>)</div><div class="line">  database_group.add_option(<span class="string">'--databasehost'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Hostname of database server"</span>, dest=<span class="string">"database_host"</span>)</div><div class="line">  database_group.add_option(<span class="string">'--databaseport'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Database port"</span>, dest=<span class="string">"database_port"</span>)</div><div class="line">  database_group.add_option(<span class="string">'--databasename'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Database/Service name or ServiceID"</span>,</div><div class="line">                            dest=<span class="string">"database_name"</span>)</div><div class="line">  database_group.add_option(<span class="string">'--databaseusername'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Database user login"</span>, dest=<span class="string">"database_username"</span>)</div><div class="line">  database_group.add_option(<span class="string">'--databasepassword'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Database user password"</span>, dest=<span class="string">"database_password"</span>)</div><div class="line">  parser.add_option_group(database_group)</div><div class="line"></div><div class="line">  jdbc_group = optparse.OptionGroup(parser, <span class="string">'JDBC options (command need to include all options)'</span>)</div><div class="line">  jdbc_group.add_option(<span class="string">'--jdbc-driver'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Specifies the path to the JDBC driver JAR file or archive "</span> \</div><div class="line">                                                            <span class="string">"with all required files(jdbc jar, libraries and etc), for the "</span> \</div><div class="line">                                                            <span class="string">"database type specified with the --jdbc-db option. "</span> \</div><div class="line">                                                            <span class="string">"Used only with --jdbc-db option. Archive is supported only for"</span> \</div><div class="line">                                                            <span class="string">" sqlanywhere database."</span> ,</div><div class="line">                        dest=<span class="string">"jdbc_driver"</span>)</div><div class="line">  jdbc_group.add_option(<span class="string">'--jdbc-db'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Specifies the database type [postgres|mysql|mssql|oracle|hsqldb|sqlanywhere] for the "</span> \</div><div class="line">                                                        <span class="string">"JDBC driver specified with the --jdbc-driver option. Used only with --jdbc-driver option."</span>,</div><div class="line">                        dest=<span class="string">"jdbc_db"</span>)</div><div class="line">  parser.add_option_group(jdbc_group)</div><div class="line"></div><div class="line">  other_group = optparse.OptionGroup(parser, <span class="string">'Other options'</span>)</div><div class="line"></div><div class="line">  other_group.add_option(<span class="string">'-j'</span>, <span class="string">'--java-home'</span>, default=<span class="keyword">None</span>,</div><div class="line">                         help=<span class="string">"Use specified java_home.  Must be valid on all hosts"</span>)</div><div class="line">  other_group.add_option(<span class="string">'--stack-java-home'</span>, dest=<span class="string">"stack_java_home"</span>, default=<span class="keyword">None</span>,</div><div class="line">                    help=<span class="string">"Use specified java_home for stack services.  Must be valid on all hosts"</span>)</div><div class="line">  other_group.add_option(<span class="string">'--skip-view-extraction'</span>, action=<span class="string">"store_true"</span>, default=<span class="keyword">False</span>, help=<span class="string">"Skip extraction of system views"</span>, dest=<span class="string">"skip_view_extraction"</span>)</div><div class="line">  other_group.add_option(<span class="string">'--postgresschema'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Postgres database schema name"</span>,</div><div class="line">                         dest=<span class="string">"postgres_schema"</span>)</div><div class="line">  other_group.add_option(<span class="string">'--sqla-server-name'</span>, default=<span class="keyword">None</span>, help=<span class="string">"SQL Anywhere server name"</span>, dest=<span class="string">"sqla_server_name"</span>)</div><div class="line">  other_group.add_option(<span class="string">'--sidorsname'</span>, default=<span class="string">"sname"</span>, help=<span class="string">"Oracle database identifier type, Service ID/Service "</span></div><div class="line">                                                               <span class="string">"Name sid|sname"</span>, dest=<span class="string">"sid_or_sname"</span>)</div><div class="line">  other_group.add_option(<span class="string">'--enable-lzo-under-gpl-license'</span>, action=<span class="string">"store_true"</span>, default=<span class="keyword">False</span>, help=<span class="string">"Automatically accepts GPL license"</span>, dest=<span class="string">"accept_gpl"</span>)</div><div class="line"></div><div class="line">  <span class="comment"># the --master-key option is needed in the event passwords in the ambari.properties file are encrypted</span></div><div class="line">  other_group.add_option(<span class="string">'--master-key'</span>, default=<span class="keyword">None</span>, help=<span class="string">"Master key for encrypting passwords"</span>, dest=<span class="string">"master_key"</span>)</div><div class="line"></div><div class="line">  parser.add_option_group(other_group)</div></pre></td></tr></table></figure>
<p>这个方法就是为setup的行为，做了进一步的解析器设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">_VERBOSE = <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_verbose</span><span class="params">(newVal)</span>:</span></div><div class="line">  <span class="keyword">global</span> _VERBOSE</div><div class="line">  _VERBOSE = newVal</div></pre></td></tr></table></figure>
<p>该方法就是设置全局变量_VERBOSE的值。</p>
<h3 id="main"><a href="#main" class="headerlink" title="main"></a>main</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(options, args, parser)</span>:</span></div><div class="line">  init_logging()</div><div class="line"></div><div class="line">  <span class="comment"># set silent</span></div><div class="line">  set_silent(options.silent)</div><div class="line"></div><div class="line">  <span class="comment"># debug mode</span></div><div class="line">  set_debug_mode_from_options(options)</div><div class="line">  init_debug(options)</div><div class="line"></div><div class="line">  <span class="comment">#perform checks</span></div><div class="line"></div><div class="line">  options.warnings = []</div><div class="line"></div><div class="line">  <span class="keyword">if</span> len(args) == <span class="number">0</span>:</div><div class="line">    <span class="keyword">print</span> parser.print_help()</div><div class="line">    parser.error(<span class="string">"No action entered"</span>)</div><div class="line"></div><div class="line">  action_map = create_user_action_map(args, options)</div><div class="line"></div><div class="line">  action = args[<span class="number">0</span>]</div><div class="line"></div><div class="line">  <span class="keyword">try</span>:</div><div class="line">    action_obj = action_map[action]</div><div class="line">  <span class="keyword">except</span> KeyError:</div><div class="line">    parser.error(<span class="string">"Invalid action: "</span> + action)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> action == SETUP_ACTION:</div><div class="line">    <span class="keyword">if</span> are_cmd_line_db_args_blank(options):</div><div class="line">      options.must_set_database_options = <span class="keyword">True</span></div><div class="line">    <span class="keyword">elif</span> <span class="keyword">not</span> are_cmd_line_db_args_valid(options):</div><div class="line">      parser.error(<span class="string">'All database options should be set. Please see help for the options.'</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      options.must_set_database_options = <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="comment">#correct database</span></div><div class="line">    fix_database_options(options, parser)</div><div class="line"></div><div class="line">  matches = <span class="number">0</span></div><div class="line">  <span class="keyword">for</span> args_number_required <span class="keyword">in</span> action_obj.possible_args_numbers:</div><div class="line">    matches += int(len(args) == args_number_required)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> matches == <span class="number">0</span>:</div><div class="line">    <span class="keyword">print</span> parser.print_help()</div><div class="line">    possible_args = <span class="string">' or '</span>.join(str(x) <span class="keyword">for</span> x <span class="keyword">in</span> action_obj.possible_args_numbers)</div><div class="line">    parser.error(<span class="string">"Invalid number of arguments. Entered: "</span> + str(len(args)) + <span class="string">", required: "</span> + possible_args)</div><div class="line"></div><div class="line">  options.exit_message = <span class="string">"Ambari Server '%s' completed successfully."</span> % action</div><div class="line">  options.exit_code = <span class="keyword">None</span></div><div class="line"></div><div class="line">  <span class="keyword">try</span>:</div><div class="line">    <span class="keyword">if</span> action <span class="keyword">in</span> _action_option_dependence_map:</div><div class="line">      required, optional = _action_option_dependence_map[action]</div><div class="line">      <span class="keyword">for</span> opt_str, opt_dest <span class="keyword">in</span> required:</div><div class="line">        <span class="keyword">if</span> hasattr(options, opt_dest) <span class="keyword">and</span> getattr(options, opt_dest) <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">          <span class="keyword">print</span> <span class="string">"Missing option &#123;0&#125; for action &#123;1&#125;"</span>.format(opt_str, action)</div><div class="line">          print_action_arguments_help(action)</div><div class="line">          <span class="keyword">print</span> <span class="string">"Run ambari-server.py --help to see detailed description of each option"</span></div><div class="line">          <span class="keyword">raise</span> FatalException(<span class="number">1</span>, <span class="string">"Missing option"</span>)</div><div class="line">    action_obj.execute()</div><div class="line"></div><div class="line">    <span class="keyword">if</span> action_obj.need_restart:</div><div class="line">      pstatus, pid = is_server_runing()</div><div class="line">      <span class="keyword">if</span> pstatus:</div><div class="line">        <span class="keyword">print</span> <span class="string">'NOTE: Restart Ambari Server to apply changes'</span> + \</div><div class="line">              <span class="string">' ("ambari-server restart|stop+start")'</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> options.warnings:</div><div class="line">      <span class="keyword">for</span> warning <span class="keyword">in</span> options.warnings:</div><div class="line">        print_warning_msg(warning)</div><div class="line">        <span class="keyword">pass</span></div><div class="line">      options.exit_message = <span class="string">"Ambari Server '%s' completed with warnings."</span> % action</div><div class="line">      <span class="keyword">pass</span></div><div class="line">  <span class="keyword">except</span> FatalException <span class="keyword">as</span> e:</div><div class="line">    <span class="keyword">if</span> e.reason <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">      print_error_msg(<span class="string">"Exiting with exit code &#123;0&#125;. \nREASON: &#123;1&#125;"</span>.format(e.code, e.reason))</div><div class="line">      logger.exception(str(e))</div><div class="line">    sys.exit(e.code)</div><div class="line">  <span class="keyword">except</span> NonFatalException <span class="keyword">as</span> e:</div><div class="line">    options.exit_message = <span class="string">"Ambari Server '%s' completed with warnings."</span> % action</div><div class="line">    <span class="keyword">if</span> e.reason <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">      print_warning_msg(e.reason)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> options.exit_message <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">    <span class="keyword">print</span> options.exit_message</div><div class="line"></div><div class="line">  <span class="keyword">if</span> options.exit_code <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:  <span class="comment"># not all actions may return a system exit code</span></div><div class="line">    sys.exit(options.exit_code)</div></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/20/ambari-doc/" itemprop="url">
                  ambari_doc
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-20T15:57:27+08:00" content="2018-08-20">
              2018-08-20
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/Ambari/" itemprop="url" rel="index">
                    <span itemprop="name">Ambari</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Stacks-and-Services"><a href="#Stacks-and-Services" class="headerlink" title="Stacks and Services"></a>Stacks and Services</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Ambari支持Stack的概念，并且将服务组合在一个Stack定义中。通过堆栈的作用，Ambari有统一定义的安装接口，管理并监控一组服务，并且引入了Stacks+Servides来提供延伸。<br>从Ambari2.3开始，还支持Extension的概念，并将自定义服务组合在一个Extension定义中。</p>
<h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h2><table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Stack</td>
<td style="text-align:left">定义了一组服务，并且这些服务从这里获取软件包。一个Stack能够有一个或多个版本，并且每个版本可以是活跃/不活跃的。例如, Stack=”HDP-1.3.3”。</td>
</tr>
<tr>
<td style="text-align:left">Extension</td>
<td style="text-align:left">定义了一组自定义服务，这些自定义服务可以被添加到一个stack版本中。一个Extension能够有一个或多个版本。</td>
</tr>
<tr>
<td style="text-align:left">Service</td>
<td style="text-align:left">定义Components（MASTER、SLAVE、CLIENT）组成了Service。如，Service=”HDFS”。</td>
</tr>
<tr>
<td style="text-align:left">Component</td>
<td style="text-align:left">每个Component遵循确切的生命周期（start、stop、install等）。例如：Service=”HDFS”包含的组件有：”NameNode(MASTER)”、”Secondary NameNode(MASTER)”、”DataNode(SLAVE)”和”HDFS Client(CLIENT)”</td>
</tr>
</tbody>
</table>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Stack的定义可以在源码结构的/ambari-server/src/main/resources/stacks中找到。在你安装了Ambari服务之后，Stack的定义可以在/var/lib/ambari-server/resources/stacks中找到。</p>
<h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><p>Stack定义的结构如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">|_ stacks</div><div class="line">   |_ &lt;stack_name&gt;</div><div class="line">      |_ &lt;stack_version&gt;</div><div class="line">         metainfo.xml</div><div class="line">         |_ hooks</div><div class="line">         |_ repos</div><div class="line">            repoinfo.xml</div><div class="line">         |_ services</div><div class="line">            |_ &lt;service_name&gt;</div><div class="line">               metainfo.xml</div><div class="line">               metrics.json</div><div class="line">               |_ configuration</div><div class="line">                  &#123;configuration files&#125;</div><div class="line">               |_ package</div><div class="line">                  &#123;files, scripts, templates&#125;</div></pre></td></tr></table></figure></p>
<h2 id="Defining-a-Service-and-Components"><a href="#Defining-a-Service-and-Components" class="headerlink" title="Defining a Service and Components"></a>Defining a Service and Components</h2><p>Service中的metainfo.xml描述了这个service、这个service的Components以及管理执行命令的脚本。服务的一个组件只能是MASTER、SLAVE或CLIENT三种类型中的一个。组件的类型用来告诉Ambari管理和监控这个组件的默认脚本。<br>对于每个Component，<commandscript>用来指从合适执行脚本。这个Component必须支持的一组默认命令。</commandscript></p>
<p>| Component Category | Default Lifecycle Commands |<br>| MASTER             | install, start, stop, configure, status |<br>| SLAVE              | install, start, stop, configure, status |<br>| CLIENT             | install, configure, status |</p>
<p>Ambari支持用PYTHON写的不同命令。类型用来知道如何执行命令脚本。如果你的Component需要支持多于默认生命周期的命令，你也能够创建自定义命令。<br>例如，下面的metainfo.xml在YARN服务描述了ResourceManager组件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">&lt;component&gt;</div><div class="line">  &lt;name&gt;RESOURCEMANAGER&lt;/name&gt;</div><div class="line">  &lt;category&gt;MASTER&lt;/category&gt;</div><div class="line">  &lt;commandScript&gt;</div><div class="line">    &lt;script&gt;scripts/resourcemanager.py&lt;/script&gt;</div><div class="line">    &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">    &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">  &lt;/commandScript&gt;</div><div class="line">  &lt;customCommands&gt;</div><div class="line">    &lt;customCommand&gt;</div><div class="line">      &lt;name&gt;DECOMMISSION&lt;/name&gt;</div><div class="line">      &lt;commandScript&gt;</div><div class="line">        &lt;script&gt;scripts/resourcemanager.py&lt;/script&gt;</div><div class="line">        &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">        &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">      &lt;/commandScript&gt;</div><div class="line">    &lt;/customCommand&gt;</div><div class="line">  &lt;/customCommands&gt;</div><div class="line">&lt;/component&gt;</div></pre></td></tr></table></figure></p>
<p>ResourceManager是一个MASTER组件，这个命令脚本为scripts/resourcemanager.py，这个脚本可以在services/YARN/package目录中找到。这个脚本是使用PYTHON写的，并且这个以python方法的方式实现了默认的生命周期命令。下面是install方法，对应的是默认的INSTALL命令：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Resourcemanager</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    self.install_packages(env)</div><div class="line">    self.configure(env)</div></pre></td></tr></table></figure></p>
<p>你还可以看到有一个自定义命令DECOMMISSION，这意味着在python命令脚本中还有一个decommission方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">decommission</span><span class="params">(self, env)</span>:</span></div><div class="line">  <span class="keyword">import</span> params</div><div class="line"> </div><div class="line">  ...</div><div class="line"> </div><div class="line">  Execute(yarn_refresh_cmd,</div><div class="line">          user=yarn_user</div><div class="line">  )</div><div class="line">  <span class="keyword">pass</span></div></pre></td></tr></table></figure></p>
<h2 id="Using-Stack-Inheritance"><a href="#Using-Stack-Inheritance" class="headerlink" title="Using Stack Inheritance"></a>Using Stack Inheritance</h2><p>Stacks能够从其他Stack进行继承，以便共享命令脚本和配置。通过如下方式降低了代码的重复：</p>
<blockquote>
<p>为子Stack定义了repositories。<br>在子Stack中添加新的Service（不是在父Stack中）。<br>重写父级Services的命令脚本。<br>重写父级Services的配置。</p>
</blockquote>
<p>例如：HDP 2.1 Stack继承了HDP 2.0.6 Stack，因此只需要在Stack定义中对HDP 2.1 Stack中适当的修改。这个extension在metainfo.xml中对HDP 2.1 Stack进行定义。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">metainfo</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">versions</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">active</span>&gt;</span>true<span class="tag">&lt;/<span class="name">active</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">versions</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">extends</span>&gt;</span>2.0.6<span class="tag">&lt;/<span class="name">extends</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">metainfo</span>&gt;</span></div></pre></td></tr></table></figure></p>
<h2 id="Example-Implementing-a-Custom-Service"><a href="#Example-Implementing-a-Custom-Service" class="headerlink" title="Example: Implementing a Custom Service"></a>Example: Implementing a Custom Service</h2><p>在这个例子中，我们将创建一个名为”SAMPLESRV”的自定义service，并将它添加到已有的Stack定义中。这个service含有 MASTER、SLAVE和CLIENT组件。</p>
<h3 id="Create-and-Add-the-Service"><a href="#Create-and-Add-the-Service" class="headerlink" title="Create and Add the Service"></a>Create and Add the Service</h3><p>1、在Ambari server上，跳转到/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services目录。在这个例子中，我们将浏览到HDP 2.0 stack定义。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services</div></pre></td></tr></table></figure></p>
<p>2、创建一个目录：/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/SAMPLESRV。它用来包含SAMPLESEV的service的定义。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/SAMPLESRV</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/SAMPLESRV</div></pre></td></tr></table></figure></p>
<p>3、跳转到新创建的SAMPLESRV目录，创建metainfo.xml文件，这个文件用来描述新的service。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</div><div class="line">&lt;metainfo&gt;</div><div class="line">    &lt;schemaVersion&gt;2.0&lt;/schemaVersion&gt;</div><div class="line">    &lt;services&gt;</div><div class="line">        &lt;service&gt;</div><div class="line">            &lt;name&gt;SAMPLESRV&lt;/name&gt;</div><div class="line">            &lt;displayName&gt;New Sample Service&lt;/displayName&gt;</div><div class="line">            &lt;comment&gt;A New Sample Service&lt;/comment&gt;</div><div class="line">            &lt;version&gt;1.0.0&lt;/version&gt;</div><div class="line">            &lt;components&gt;</div><div class="line">                &lt;component&gt;</div><div class="line">                    &lt;name&gt;SAMPLESRV_MASTER&lt;/name&gt;</div><div class="line">                    &lt;displayName&gt;Sample Srv Master&lt;/displayName&gt;</div><div class="line">                    &lt;category&gt;MASTER&lt;/category&gt;</div><div class="line">                    &lt;cardinality&gt;1&lt;/cardinality&gt;</div><div class="line">                    &lt;commandScript&gt;</div><div class="line">                        &lt;script&gt;scripts/master.py&lt;/script&gt;</div><div class="line">                        &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">                        &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">                    &lt;/commandScript&gt;</div><div class="line">                &lt;/component&gt;</div><div class="line">                &lt;component&gt;</div><div class="line">                    &lt;name&gt;SAMPLESRV_SLAVE&lt;/name&gt;</div><div class="line">                    &lt;displayName&gt;Sample Srv Slave&lt;/displayName&gt;</div><div class="line">                    &lt;category&gt;SLAVE&lt;/category&gt;</div><div class="line">                    &lt;cardinality&gt;1+&lt;/cardinality&gt;</div><div class="line">                    &lt;commandScript&gt;</div><div class="line">                        &lt;script&gt;scripts/slave.py&lt;/script&gt;</div><div class="line">                        &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">                        &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">                    &lt;/commandScript&gt;</div><div class="line">                &lt;/component&gt;</div><div class="line">                &lt;component&gt;</div><div class="line">                    &lt;name&gt;SAMPLESRV_CLIENT&lt;/name&gt;</div><div class="line">                    &lt;displayName&gt;Sample Srv Client&lt;/displayName&gt;</div><div class="line">                    &lt;category&gt;CLIENT&lt;/category&gt;</div><div class="line">                    &lt;cardinality&gt;1+&lt;/cardinality&gt;</div><div class="line">                    &lt;commandScript&gt;</div><div class="line">                        &lt;script&gt;scripts/sample_client.py&lt;/script&gt;</div><div class="line">                        &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">                        &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">                    &lt;/commandScript&gt;</div><div class="line">                &lt;/component&gt;</div><div class="line">            &lt;/components&gt;</div><div class="line">            &lt;osSpecifics&gt;</div><div class="line">                &lt;osSpecific&gt;</div><div class="line">                    &lt;osFamily&gt;any&lt;/osFamily&gt;  &lt;!-- note: use osType rather than osFamily for Ambari 1.5.0 and 1.5.1 --&gt;</div><div class="line">                &lt;/osSpecific&gt;</div><div class="line">            &lt;/osSpecifics&gt;</div><div class="line">        &lt;/service&gt;</div><div class="line">    &lt;/services&gt;</div><div class="line">&lt;/metainfo&gt;</div></pre></td></tr></table></figure></p>
<p>4、在上面，我们的service名为“SAMPLESRV”，它包含：</p>
<blockquote>
<p>一个名为”SAMPLESRV_MASTER”的MASTER的组件。<br>一个名为“SLAVE”的SLAVE的组件。<br>一个名为“CLIENT”的CLIENT的组件。</p>
</blockquote>
<p>5、接下来，创建命令脚本。为脚本创建目录/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/SAMPLESRV/package/scripts，并在这个目录中定义service的metainfo。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir -p /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/SAMPLESRV/package/scripts</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/SAMPLESRV/package/scripts</div></pre></td></tr></table></figure></p>
<p>跳转到script目录，并创建.py命令脚本文件。<br>master.py文件：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> *</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Master</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Stop the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Start the Sample Srv Master'</span>;</div><div class="line">     </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">status</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Status of the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the Sample Srv Master'</span>;</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  Master().execute()</div></pre></td></tr></table></figure></p>
<p>slave.py文件：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> *</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Slave</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Stop the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Start the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">status</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Status of the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the Sample Srv Slave'</span>;</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  Slave().execute()</div></pre></td></tr></table></figure></p>
<p>sample_client.py文件：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> *</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SampleClient</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the Sample Srv Client'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the Sample Srv Client'</span>;</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  SampleClient().execute()</div></pre></td></tr></table></figure></p>
<p>7、现在重启Ambari Server以便新的service定义分发到集群的所有Agents。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ambari-server restart</div></pre></td></tr></table></figure></p>
<h3 id="Install-the-Service-Via-Ambari-WEb-“Add-Service”"><a href="#Install-the-Service-Via-Ambari-WEb-“Add-Service”" class="headerlink" title="Install the Service(Via Ambari WEb “Add Service”)"></a>Install the Service(Via Ambari WEb “Add Service”)</h3><p>从Ambari 1.7.0开始，可以通过Ambari Web来添加自定义服务。</p>
<blockquote>
<p>在Ambari web页面，跳转到services，并点击左边service导航部分的Action按钮。<br>点击“Add Services”。你将看到一个包含“My Sample Service”（在metainfo.xml文件中定义service的<displayname>）的选项。<br>选择“My Sample service”并点击下一步。<br>选择“Sample Srv Master”并点击下一步。<br>选择host来安装”Sample Srv Client”，并点击下一步。<br>一旦完成，”My Sample Service”将会在在service导航区可用。<br>如果你想要为所有host添加“Sample Srv Client”，你可以跳转到Host，并到航道指定的host并点击”+ Add”。</displayname></p>
</blockquote>
<h2 id="Example-Implementing-a-Custom-Client-only-Service"><a href="#Example-Implementing-a-Custom-Client-only-Service" class="headerlink" title="Example: Implementing a Custom Client-only Service"></a>Example: Implementing a Custom Client-only Service</h2><p>在这个例子中，我们将创建一个名为“TESTSRV”的自定义service，添加到已经存在的Stack定义上，并Ambari APIs来安装/配置这个service。这个service是一个CLIENT，因此它有两个命令：install和configure。</p>
<h3 id="Create-and-Add-the-Service-1"><a href="#Create-and-Add-the-Service-1" class="headerlink" title="Create and Add the Service"></a>Create and Add the Service</h3><p>1、在Ambari Service上，跳转到 /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services目录。在这个例子中，我们将跳转到HDP2.0 Stack 定义中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services</div></pre></td></tr></table></figure></p>
<p>2、创建一个名为/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTSRV的目录，它用来包含TESTSRV的service定义。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTSRV</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTSRV</div></pre></td></tr></table></figure></p>
<p>3、跳转到新创建的TESTSRV目录，创建一个名为metainfo.xml的文件用来描述新的service。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</div><div class="line">&lt;metainfo&gt;</div><div class="line">    &lt;schemaVersion&gt;2.0&lt;/schemaVersion&gt;</div><div class="line">    &lt;services&gt;</div><div class="line">        &lt;service&gt;</div><div class="line">            &lt;name&gt;TESTSRV&lt;/name&gt;</div><div class="line">            &lt;displayName&gt;New Test Service&lt;/displayName&gt;</div><div class="line">            &lt;comment&gt;A New Test Service&lt;/comment&gt;</div><div class="line">            &lt;version&gt;0.1.0&lt;/version&gt;</div><div class="line">            &lt;components&gt;</div><div class="line">                &lt;component&gt;</div><div class="line">                    &lt;name&gt;TEST_CLIENT&lt;/name&gt;</div><div class="line">                    &lt;displayName&gt;New Test Client&lt;/displayName&gt;</div><div class="line">                    &lt;category&gt;CLIENT&lt;/category&gt;</div><div class="line">                    &lt;cardinality&gt;1+&lt;/cardinality&gt;</div><div class="line">                    &lt;commandScript&gt;</div><div class="line">                        &lt;script&gt;scripts/test_client.py&lt;/script&gt;</div><div class="line">                        &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">                        &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">                    &lt;/commandScript&gt;</div><div class="line">                    &lt;customCommands&gt;</div><div class="line">                      &lt;customCommand&gt;</div><div class="line">                        &lt;name&gt;SOMETHINGCUSTOM&lt;/name&gt;</div><div class="line">                        &lt;commandScript&gt;</div><div class="line">                          &lt;script&gt;scripts/test_client.py&lt;/script&gt;</div><div class="line">                          &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">                          &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">                        &lt;/commandScript&gt;</div><div class="line">                      &lt;/customCommand&gt;</div><div class="line">                    &lt;/customCommands&gt;</div><div class="line">                &lt;/component&gt;</div><div class="line">            &lt;/components&gt;</div><div class="line">            &lt;osSpecifics&gt;</div><div class="line">                &lt;osSpecific&gt;</div><div class="line">                    &lt;osFamily&gt;any&lt;/osFamily&gt;  &lt;!-- note: use osType rather than osFamily for Ambari 1.5.0 and 1.5.1 --&gt;</div><div class="line">                &lt;/osSpecific&gt;</div><div class="line">            &lt;/osSpecifics&gt;</div><div class="line">        &lt;/service&gt;</div><div class="line">    &lt;/services&gt;</div><div class="line">&lt;/metainfo&gt;</div></pre></td></tr></table></figure></p>
<p>4、在上面，我们的service名为“TESTSRV”，并且它包含了一个名为“TEST_CLIENT”的组件，这个组件属于CLIENT类型。这个client通过命令脚本scripts/test_client.py来管理。接下来创建命令脚本。<br>5、为命令脚本创建目录/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTSRV/package/scripts。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir -p /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTSRV/package/scripts</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTSRV/package/scripts</div></pre></td></tr></table></figure></p>
<p>6、跳转到scripts目录，并创建test_client.py文件：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> *</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestClient</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the client'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the client'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">somethingcustom</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Something custom'</span>;</div><div class="line"> </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  TestClient().execute()</div></pre></td></tr></table></figure></p>
<p>7、现在，重启Ambari Server，将新的service定义分发到集群的所有Agents。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ambari-server restart</div></pre></td></tr></table></figure></p>
<h3 id="Install-the-Service-Via-the-Ambari-REST-API"><a href="#Install-the-Service-Via-the-Ambari-REST-API" class="headerlink" title="Install the Service(Via the Ambari REST API)"></a>Install the Service(Via the Ambari REST API)</h3><p>1、将Service添加到Cluster。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">POST</div><div class="line">/api/v1/clusters/MyCluster/services</div><div class="line"> </div><div class="line">&#123;</div><div class="line">&quot;ServiceInfo&quot;: &#123;</div><div class="line">  &quot;service_name&quot;:&quot;TESTSRV&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>2、添加组件到Service。这个例子中，添加TEST_CLIENT到TESTSRV。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">POST</div><div class="line">/api/v1/clusters/MyCluster/services/TESTSRV/components/TEST_CLIENT</div></pre></td></tr></table></figure></p>
<p>3、将组件添加到所有host。例如，要安装到c6402.ambari.apache.org和c6403.ambari.apache.org上，首先使用POST在这些主机上创建host_component源<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">POST</div><div class="line">/api/v1/clusters/MyCluster/hosts/c6402.ambari.apache.org/host_components/TEST_CLIENT</div><div class="line"> </div><div class="line">POST</div><div class="line">/api/v1/clusters/MyCluster/hosts/c6403.ambari.apache.org/host_components/TEST_CLIENT</div></pre></td></tr></table></figure></p>
<p>4、现在，需要在所有主机上安装组件。在这个命令中，你指导Ambari来安装与这个service有关的所有组件。调用每个主机上命令脚本的install方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">PUT</div><div class="line">/api/v1/clusters/MyCluster/services/TESTSRV</div><div class="line"> </div><div class="line">&#123;</div><div class="line">  &quot;RequestInfo&quot;: &#123;</div><div class="line">    &quot;context&quot;: &quot;Install Test Srv Client&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;Body&quot;: &#123;</div><div class="line">    &quot;ServiceInfo&quot;: &#123;</div><div class="line">      &quot;state&quot;: &quot;INSTALLED&quot;</div><div class="line">    &#125;   </div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>除了同时安装所有的组件，你还可以只在某一台机器上装组件。在这个例子中，我们在c6402.ambari.apache.org上安装TEST_CLIENT：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">PUT</div><div class="line">/api/v1/clusters/MyCluster/hosts/c6402.ambari.apache.org/host_components/TEST_CLIENT</div><div class="line"> </div><div class="line">&#123;</div><div class="line">  &quot;RequestInfo&quot;: &#123;</div><div class="line">    &quot;context&quot;:&quot;Install Test Srv Client&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;Body&quot;: &#123;</div><div class="line">    &quot;HostRoles&quot;: &#123;</div><div class="line">      &quot;state&quot;:&quot;INSTALLED&quot;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>6、使用如下信息配置主机上的client。这将最终调用命令脚本中的configure()方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">POST</div><div class="line">/api/v1/clusters/MyCluster/requests</div><div class="line">  </div><div class="line">&#123;</div><div class="line">  &quot;RequestInfo&quot; : &#123;</div><div class="line">    &quot;command&quot; : &quot;CONFIGURE&quot;,</div><div class="line">    &quot;context&quot; : &quot;Config Test Srv Client&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;Requests/resource_filters&quot;: [&#123;</div><div class="line">    &quot;service_name&quot; : &quot;TESTSRV&quot;,</div><div class="line">    &quot;component_name&quot; : &quot;TEST_CLIENT&quot;,</div><div class="line">    &quot;hosts&quot; : &quot;c6403.ambari.apache.org&quot;</div><div class="line">  &#125;]</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>7、如果你想看哪些主机完成了安装。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">GET</div><div class="line">/api/v1/clusters/MyCluster/components/TEST_CLIENT</div></pre></td></tr></table></figure></p>
<h3 id="Install-the-Service-via-Ambari-Web-“Add-Services”"><a href="#Install-the-Service-via-Ambari-Web-“Add-Services”" class="headerlink" title="Install the Service(via Ambari Web “Add Services”)"></a>Install the Service(via Ambari Web “Add Services”)</h3><blockquote>
<p>1、在Ambari Web界面，跳转到Services并点击左侧Service导航区的Actions按钮。<br>2、点击“Add Services”。你将看到一个“My Test Service”的选项（在service的metainfo.xml文件中services的<displayname>中定义）。<br>3、选择“My Test Service”并点击下一步。<br>4、选择主机来安装“New Test Client”并点击下一步。<br>5、一旦完成，“My Test Service”将在Service导航区中可用。<br>6、如果你想要在其他主机上添加“New Test Client”，你可以跳转到Hosts，并指定主机后点击“+ Add”。</displayname></p>
</blockquote>
<h2 id="Example-Implementing-a-Custom-Client-only-Service-with-Configs"><a href="#Example-Implementing-a-Custom-Client-only-Service-with-Configs" class="headerlink" title="Example: Implementing a Custom Client-only Service (with Configs)"></a>Example: Implementing a Custom Client-only Service (with Configs)</h2><p>在这个例子中，我们将创建一个名为“TESTCONFIGSRV”的自定义service，并将其添加到已有的Stack定义上。这个service是一个CLIENT类型，因此它有两个命令：install和configure。service还包含”test-confg”配置类型。</p>
<h3 id="Create-and-Add-the-Service-to-Stack"><a href="#Create-and-Add-the-Service-to-Stack" class="headerlink" title="Create and Add the Service to Stack"></a>Create and Add the Service to Stack</h3><p>1、在Ambari Server上，跳转到/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services目录。在这个例子中，我们将跳转到HDP 2.0 Stack定义。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services</div></pre></td></tr></table></figure></p>
<p>2、创建名为/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV的目录，它包含了为TESTCONFIGSRV定义的service。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV</div></pre></td></tr></table></figure></p>
<p>3、跳转到刚刚创建的TESTCONFIGSRV目录，创建一个metainfo.xml文件，这个文件描述了这个新的service。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</div><div class="line">&lt;metainfo&gt;</div><div class="line">    &lt;schemaVersion&gt;2.0&lt;/schemaVersion&gt;</div><div class="line">    &lt;services&gt;</div><div class="line">        &lt;service&gt;</div><div class="line">            &lt;name&gt;TESTCONFIGSRV&lt;/name&gt;</div><div class="line">            &lt;displayName&gt;New Test Config Service&lt;/displayName&gt;</div><div class="line">            &lt;comment&gt;A New Test Config Service&lt;/comment&gt;</div><div class="line">            &lt;version&gt;0.1.0&lt;/version&gt;</div><div class="line">            &lt;components&gt;</div><div class="line">                &lt;component&gt;</div><div class="line">                    &lt;name&gt;TESTCONFIG_CLIENT&lt;/name&gt;</div><div class="line">                    &lt;displayName&gt;New Test Config Client&lt;/displayName&gt;</div><div class="line">                    &lt;category&gt;CLIENT&lt;/category&gt;</div><div class="line">                    &lt;cardinality&gt;1+&lt;/cardinality&gt;</div><div class="line">                    &lt;commandScript&gt;</div><div class="line">                        &lt;script&gt;scripts/test_client.py&lt;/script&gt;</div><div class="line">                        &lt;scriptType&gt;PYTHON&lt;/scriptType&gt;</div><div class="line">                        &lt;timeout&gt;600&lt;/timeout&gt;</div><div class="line">                    &lt;/commandScript&gt;</div><div class="line">                &lt;/component&gt;</div><div class="line">            &lt;/components&gt;</div><div class="line">            &lt;osSpecifics&gt;</div><div class="line">                &lt;osSpecific&gt;</div><div class="line">                    &lt;osFamily&gt;any&lt;/osFamily&gt;  &lt;!-- note: use osType rather than osFamily for Ambari 1.5.0 and 1.5.1 --&gt;</div><div class="line">                &lt;/osSpecific&gt;</div><div class="line">            &lt;/osSpecifics&gt;</div><div class="line">        &lt;/service&gt;</div><div class="line">    &lt;/services&gt;</div><div class="line">&lt;/metainfo&gt;</div></pre></td></tr></table></figure></p>
<p>4、在上面，我的service的名为“TESTCONFIGSRV”，并且它包含一个名为“TESTCONFIG_CLIENT”组件，这个组件的类型为“CLINT”。这个client通过命令脚本scripts/test_client.py来管理。接下来创建这个命令脚本。<br>5、为命令脚本创建目录/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV/package/scripts，这个脚本在ervice metainfo <commandscript>中指定。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir -p /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV/package/scripts</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV/package/scripts</div></pre></td></tr></table></figure></commandscript></p>
<p>6、调转到scripts目录，并创建test_client.py文件。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import sys</div><div class="line">from resource_management import *</div><div class="line"> </div><div class="line">class TestClient(Script):</div><div class="line">  def install(self, env):</div><div class="line">    print &apos;Install the config client&apos;;</div><div class="line">  def configure(self, env):</div><div class="line">    print &apos;Configure the config client&apos;;</div><div class="line"> </div><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">  TestClient().execute()</div></pre></td></tr></table></figure></p>
<p>7、现在，为这个service定义配置类型。为配置目录/var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV/configuration创建一个目录。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir -p /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV/configuration</div><div class="line">cd /var/lib/ambari-server/resources/stacks/HDP/2.0.6/services/TESTCONFIGSRV/configuration</div></pre></td></tr></table></figure></p>
<p>8、跳转到配置目录，并创建test-config.xml文件。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</div><div class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</div><div class="line"> </div><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;some.test.property&lt;/name&gt;</div><div class="line">    &lt;value&gt;this.is.the.default.value&lt;/value&gt;</div><div class="line">    &lt;description&gt;This is a kool description.&lt;/description&gt;</div><div class="line"> &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
<p>9、现在，重启Ambari Server以便将service分发到集群中的所有Agent上。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ambari-server restart</div></pre></td></tr></table></figure></p>
<h1 id="How-To-Define-Stacks-and-Services"><a href="#How-To-Define-Stacks-and-Services" class="headerlink" title="How-To Define  Stacks and Services"></a>How-To Define  Stacks and Services</h1><p>Ambari管理的Services在Ambari的stacks文件夹中定义。<br>要定义自己的services和stacks并被Ambari管理，请遵循如下步骤。<br>上面的create your custom stack and service的例子也可以学习。<br>stack是services的集合。一个stack可以定义多个版本，每个版本有自己的一组service。Ambari中的Stacks在 ambari-server/src/main/resources/stacks 文件夹中定义，这个文件夹可以在安装之后的/var/lib/ambari-server/resources/stacks目录找到。<br>被stack管理的servces能够在 ambari-server/src/main/resources/common-services 或 ambari-server/src/main/resources/stacks 文件夹中定义。这些文件对应安装后的目录分别为：/var/lib/ambari-server/resources/common-services 或  /var/lib/ambari-server/resources/stacks。</p>
<blockquote>
<p>Question : 什么时候在 common-services 或 stacks 文件夹中定义service呢<br>当service可能被用于多个stacks时，我们将在common-services文件夹中定义service。例如，几乎所有的stacks都需要HDFS service，因此将它定义在common-services而不是在每个stack中定义是值得推荐的。同样，如果一个service从不会被共享，它能够被定义在stack文件夹中。基本上来说stacks文件夹中定义services是不推荐的，而推荐将service定义在common-services中。</p>
</blockquote>
<h2 id="Define-Service"><a href="#Define-Service" class="headerlink" title="Define Service"></a>Define Service</h2><p>下面展示了如何在common-services文件夹中定义一个service。在stacks文件夹中定义services时，也可以使用相同的方法，具体会在定义stack章节介绍。</p>
<p><img src="http://oaavtz33a.bkt.clouddn.com/Screen%20Shot%202016-03-01%20at%202.47.32%20PM.png"></p>
<p>Services必须提供主要的metainfo.xml文件，它提供了关于这个service的重要元数据。<br>除此之外，其他文件提供了关于server的更多信息。关于这些文件的更多信息将在下面提供。</p>
<p>一个service也可能继承自它的之前版本或common services。关于继承的更多信息，请查看<a href="https://cwiki.apache.org/confluence/display/AMBARI/Service+Inheritance" title="Service Inheritance" target="_blank" rel="external">Service Inheritance</a>。</p>
<h2 id="metainfo-xml"><a href="#metainfo-xml" class="headerlink" title="metainfo.xml"></a>metainfo.xml</h2><p>在metainfo.xml服务描述符中，首先被定义的是service和它的components。<br>完整的参考文献可以在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Writing+metainfo.xml" title="Writing metainfo.xml" target="_blank" rel="external">Writing metainfo.xml</a>中找到。<br>值得推荐的metainfo.xml实现是<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/metainfo.xml#L27" title="HDFS metainfo.xml" target="_blank" rel="external">HDFS metainfo.xml</a>。</p>
<blockquote>
<p>Question : 是否可以在同一个metainfo.xml中定义多个services？<br>可以。尽管可以，但是强烈拒绝在相同的service文件夹中定义多个services。<br>YARN和MapReduces2就被一起定义在YARN文件夹中。它的metainfo.xml同时定义了两个services。</p>
</blockquote>
<h3 id="Scripts"><a href="#Scripts" class="headerlink" title="Scripts"></a>Scripts</h3><p>对于components的定义，我们需要提供脚本来处理service的不同阶段以及组件的生命周期。<br>管理service和components的脚本在metainfo.xml(<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/metainfo.xml#L35" title="HDFS" target="_blank" rel="external">HDFS</a>)中指定。<br>每个脚本应该继承<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-common/src/main/python/resource_management/libraries/script/script.py" title="Script" target="_blank" rel="external">Script</a>类，这个父类提供了有用的方法。例如：<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py#L68" title="NameNode script" target="_blank" rel="external">NameNode script</a>。</p>
<p><img src="http://oaavtz33a.bkt.clouddn.com/Screen%20Shot%202016-03-02%20at%2012.39.49%20PM.png"><br>这些脚本应该在<service-id>/<service-version>/package/script文件夹中提供。<br><img src="http://oaavtz33a.bkt.clouddn.com/Screen%20Shot%202016-03-02%20at%2012.32.58%20PM.png"></service-version></service-id></p>
<table>
<thead>
<tr>
<th style="text-align:left">Folder</th>
<th style="text-align:left">Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">package/script</td>
<td style="text-align:left">包含了由Ambari执行的脚本。这些脚本使用正确的环境被加载到执行路径中。例如：<a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts" title="HDFS" target="_blank" rel="external">HDFS</a></td>
</tr>
<tr>
<td style="text-align:left">package/files</td>
<td style="text-align:left">包含被上面脚本使用的文件。一般是其他一些作为独立进程执行的脚本（bash、python等）。例如：checkWebUI.py在HDFS检查中运行，用来确定Journal Node是否活跃。</td>
</tr>
<tr>
<td style="text-align:left">package/tmplates</td>
<td style="text-align:left">上述脚本在管理节点上生成的临时文件。一般是service需要操作的配置文件。例如：<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/templates/exclude_hosts_list.j2" title="exclude_hosts_list.j2" target="_blank" rel="external">exclude_hosts_list.j2</a> ，被脚本使用来产生/etc/hadoop/conf/dfs.exclude。</td>
</tr>
</tbody>
</table>
<h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>Ambari默认支持python脚本来管理service和components。<br>component脚本应该继承resource_management.Script类并提供component的生命周期所需的方法。<br>参考<a href="https://cwiki.apache.org/confluence/display/AMBARI/Defining+a+Custom+Stack+and+Services" title="how to create custom stack" target="_blank" rel="external">how to create custom stack</a>页面，MASTER、SLAVE和CLIENT组件贯穿生命周期所需的方法如下：<br>master.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> Script</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Master</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Stop the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Start the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">status</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Status of the Sample Srv Master'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the Sample Srv Master'</span>;</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  Master().execute()</div></pre></td></tr></table></figure></p>
<p>slave.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> Script</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Slave</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Stop the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Start the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">status</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Status of the Sample Srv Slave'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the Sample Srv Slave'</span>;</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  Slave().execute()</div></pre></td></tr></table></figure></p>
<p>client.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> resource_management <span class="keyword">import</span> Script</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SampleClient</span><span class="params">(Script)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Install the Sample Srv Client'</span>;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configure</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Configure the Sample Srv Client'</span>;</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  SampleClient().execute()</div></pre></td></tr></table></figure></p>
<p>Ambari提供了有用的Python库，以便在以下方面提供写servier脚本的帮助。对于这些库的完整介绍，请通过<a href="https://cwiki.apache.org/confluence/display/AMBARI/Ambari+Python+Libraries" title="Ambari Python Libraries" target="_blank" rel="external">Ambari Python Libraries</a>页面访问。</p>
<blockquote>
<p>resource_management<br>ambari_commons<br>ambari_simplejson</p>
</blockquote>
<h4 id="OS-Variant-Script"><a href="#OS-Variant-Script" class="headerlink" title="OS Variant Script"></a>OS Variant Script</h4><p>如果service支持多个操作系统，则需要根据不同的操作系统由独立的脚本，可以继承resource_management.Script类并使用不同的@OSFamilyImpl()注解。<br>这能够区分组件的不同操作系统的方法。<br>例如： <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py#L126" title="NameNode default script" target="_blank" rel="external">NameNode default script</a>， <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py#L346" title="NameNode Windows script" target="_blank" rel="external">NameNode Windows script</a></p>
<blockquote>
<p>Examples<br>NameNode <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py#L93" title="start" target="_blank" rel="external">Start</a>， <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py#L208" title="Stop" target="_blank" rel="external">Stop</a><br>DataNode <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_datanode.py#L68" title="
Start and Stop" target="_blank" rel="external">Start and Stop</a><br>HDFS <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs.py#L31" title="configurations persistence" target="_blank" rel="external">configurations persistence</a></p>
</blockquote>
<h3 id="Custom-Actions"><a href="#Custom-Actions" class="headerlink" title="Custom Actions"></a>Custom Actions</h3><p>有些时候，services需要执行一些行为，这些行为不同于Ambari提供的默认行为（install、start、stop、configure等）。<br>services能够定义一些action，并将这些action在UI中展示给用户，因此这些行为能够方便执行。<br>举例说明，如HDFS实现的Rebalance HDFS自定义行为。</p>
<h4 id="Stack-Changes"><a href="#Stack-Changes" class="headerlink" title="Stack Changes"></a>Stack Changes</h4><blockquote>
<p>1、在metainfo.xml中component的<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/metainfo.xml#L49" title="Define custom command insid the customCommands section" target="_blank" rel="external"><customcommands>部分中定义指定义命令</customcommands></a>。<br>2、在metainfo.xml相关的脚本中，用相同的名字实现方法，来作为自定义命令。<br>    a）如果自定义命令不含有操作系统变量，它可以在同一个继承resource_management.Script的类中被实现。<br>    b）如果含有操作系统变量，每个类中的不同方法可以通过@OsFamilyImpl(os_family=…)来实现。<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py#L273" title="Default rebalancehdfs" target="_blank" rel="external">Default rebalancehdfs</a>, <a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py#L354" title="Windows rebalancehdfs" target="_blank" rel="external">Windows rebalancehdfs</a>。<br>这将提供在安装了service的被管理的主机上以后端方式运行脚本的能力。</p>
</blockquote>
<h4 id="UI-Changes"><a href="#UI-Changes" class="headerlink" title="UI Changes"></a>UI Changes</h4><p>在host页面上查看自定义action不需要修改UI。<br>action将展示在主机组件的action列表中。任何master-component action将自动展示在service的action菜单上。<br>当action被点击后，将自动产生POST调用来触发上面定义的脚本。</p>
<blockquote>
<p>Question ： 如何为UI中的自定义action提供自己的标签和图标？<br>在Ambari UI中，使用自定义图标和名称，添加你的component action到App.HostComponentActionMap对象。</p>
</blockquote>
<h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h2><p>service的配置文件应当位于默认的configuration文件夹中。<br>如果使用了不同的文件夹，metainfo.xml中的<configuration-dir>，可以用来指明使用的文件夹。<br>metainfo.xml中需要考虑配置的重要部分是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</div><div class="line">&lt;metainfo&gt;</div><div class="line">  &lt;schemaVersion&gt;2.0&lt;/schemaVersion&gt;</div><div class="line">  &lt;services&gt;</div><div class="line">    &lt;service&gt;</div><div class="line">      &lt;name&gt;HDFS&lt;/name&gt;</div><div class="line">      &lt;displayName&gt;HDFS&lt;/displayName&gt;</div><div class="line">      &lt;comment&gt;Apache Hadoop Distributed File System&lt;/comment&gt;</div><div class="line">      &lt;version&gt;2.1.0.2.0&lt;/version&gt;</div><div class="line">      &lt;components&gt;</div><div class="line">        ...</div><div class="line">        &lt;component&gt;</div><div class="line">          &lt;name&gt;HDFS_CLIENT&lt;/name&gt;</div><div class="line">          ...</div><div class="line">          &lt;configFiles&gt;</div><div class="line">            &lt;configFile&gt;</div><div class="line">              &lt;type&gt;xml&lt;/type&gt;</div><div class="line">              &lt;fileName&gt;hdfs-site.xml&lt;/fileName&gt;</div><div class="line">              &lt;dictionaryName&gt;hdfs-site&lt;/dictionaryName&gt;</div><div class="line">            &lt;/configFile&gt;</div><div class="line">            &lt;configFile&gt;</div><div class="line">              &lt;type&gt;xml&lt;/type&gt;</div><div class="line">              &lt;fileName&gt;core-site.xml&lt;/fileName&gt;</div><div class="line">              &lt;dictionaryName&gt;core-site&lt;/dictionaryName&gt;</div><div class="line">            &lt;/configFile&gt;</div><div class="line">            &lt;configFile&gt;</div><div class="line">              &lt;type&gt;env&lt;/type&gt;</div><div class="line">              &lt;fileName&gt;log4j.properties&lt;/fileName&gt;</div><div class="line">              &lt;dictionaryName&gt;hdfs-log4j,yarn-log4j&lt;/dictionaryName&gt;</div><div class="line">            &lt;/configFile&gt;                         </div><div class="line">            &lt;configFile&gt;</div><div class="line">              &lt;type&gt;env&lt;/type&gt;</div><div class="line">              &lt;fileName&gt;hadoop-env.sh&lt;/fileName&gt;</div><div class="line">              &lt;dictionaryName&gt;hadoop-env&lt;/dictionaryName&gt;</div><div class="line">            &lt;/configFile&gt;</div><div class="line">          &lt;/configFiles&gt;</div><div class="line">          ...</div><div class="line">          &lt;configuration-dependencies&gt;</div><div class="line">             &lt;config-type&gt;core-site&lt;/config-type&gt;</div><div class="line">             &lt;config-type&gt;hdfs-site&lt;/config-type&gt;</div><div class="line">          &lt;/configuration-dependencies&gt;</div><div class="line">        &lt;/component&gt;</div><div class="line">          ...</div><div class="line">      &lt;/components&gt;</div><div class="line">  </div><div class="line">      &lt;configuration-dir&gt;configuration&lt;/configuration-dir&gt;</div><div class="line">      &lt;configuration-dependencies&gt;</div><div class="line">        &lt;config-type&gt;core-site&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;hdfs-site&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;hadoop-env&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;hadoop-policy&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;hdfs-log4j&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ranger-hdfs-plugin-properties&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ssl-client&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ssl-server&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ranger-hdfs-audit&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ranger-hdfs-policymgr-ssl&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ranger-hdfs-security&lt;/config-type&gt;</div><div class="line">        &lt;config-type&gt;ams-ssl-client&lt;/config-type&gt;</div><div class="line">      &lt;/configuration-dependencies&gt;</div><div class="line">    &lt;/service&gt;</div><div class="line">  &lt;/services&gt;</div><div class="line">&lt;/metainfo&gt;</div></pre></td></tr></table></figure></configuration-dir></p>
<blockquote>
<p>config-type - 字符串表示的一组配置。例如：core-site, hdfs-site, yarn-site等。当配置在Ambari中保存，它们被固化到一个config-type版本中，而且这个版本是不可变的。如果你更改并保存HDFS core-site配置4次，你将有4个版本的config-type core-site。同样，当一个service的配置被保存时，只有更改的config-type被更新。<br>configFiles - 列出由<component>处理的配置文件。<br>configFile - 某种类型的一个配置文件。<br>    type - 基于文件内容的不同指定文件的类型<br>        xml - Hadoop中友好的方式，XML文件。<br>        env - 通常用于将内容值作为模版的脚本。模版具有配置标签，并且它的值在运行时生成。<br>        properties - 生成属性文件，每条属性的格式为key=value。<br>    dictionaryName - 配置类型的名字。<br>configuration-dependencies - 列出component或service所依赖的config-type的列表。<br>configuration-dir - configFiles所指定的文件所处的目录。可选的，默认为configuration。</component></p>
</blockquote>
<h3 id="Adding-new-configs-in-a-config-type"><a href="#Adding-new-configs-in-a-config-type" class="headerlink" title="Adding new configs in a config-type"></a>Adding new configs in a config-type</h3><p>向config-type中添加一个配置项时有很多不同的参数可选。它们在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Configuration+support+in+Ambari" title="config-type的可选属性" target="_blank" rel="external">这里</a>被全面介绍。</p>
<h3 id="UI-Categories"><a href="#UI-Categories" class="headerlink" title="UI - Categories"></a>UI - Categories</h3><p>上面的定义的配置在service的配置页面显示。<br>要自定义分类并在UI中对配置进行排序，需要更新下面的文件。<br>Create Category - 更新 ambari-web/app/models/stack_service.js 文件，用来添加自己的service，以及你的新分类。<br>Use Category - 要将配置置于某种分类中，并指定配置的顺序，将配置添加到  ambari-web/app/data/HDP2/site_properties.js 文件中。在这个文件中，可以指定需要使用到分类，以及配置的索引。ambari-web/app/data中的stack文件夹时分层的且继承自前一个版本。片段中的配置属性在这里定义。例如 <a href="https://github.com/apache/ambari/blob/trunk/ambari-web/app/data/HDP2.2/hive_properties.js" title="Hive Categories" target="_blank" rel="external">Hive Categories</a>, <a href="https://github.com/apache/ambari/blob/trunk/ambari-web/app/data/HDP2.2/tez_properties.js" title="Tez Categories" target="_blank" rel="external">Tez Categories</a></p>
<h3 id="UI-Enhanced-Configs"><a href="#UI-Enhanced-Configs" class="headerlink" title="UI - Enhanced Configs"></a>UI - Enhanced Configs</h3><p>Enhanced Config特性使得服务提供者能够定制他们自己的service配置，并确定哪些配置主要显示给用户，而不需要修改任何UI代码。自定义包括为service提供友好的布局，更好的控制（sliders, combos, lists, toggles, spinners, etc）、更好的验证（minimum, maximum, enums）、自动的单位转换（MB, GB, seconds, milliseconds, etc.）、配置依赖以及默认值的动态推荐。<br>servier提供者能够达成上面所有的，只需要在stacks文件夹中修改它们service的定义。<br>在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Enhanced+Configs" title="Enhanced Configs" target="_blank" rel="external">Enhanced Configs</a>页面中查看更多。</p>
<h2 id="Alerts"><a href="#Alerts" class="headerlink" title="Alerts"></a>Alerts</h2><p>通过提供一个alert.js文件，每个service都能够定义Ambari应该跟踪的警报。<br>在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Alerts" title="Alerts wiki page" target="_blank" rel="external">Alerts wiki page</a>页面能够读到更多关于报警框架的信息，而alerts.json文件的格式在<a href="https://github.com/apache/ambari/blob/branch-2.1/ambari-server/docs/api/v1/alert-definitions.md" title="Alerts definition document" target="_blank" rel="external">Alerts definition document</a>中可以了解到。</p>
<h2 id="Kerberos"><a href="#Kerberos" class="headerlink" title="Kerberos"></a>Kerberos</h2><p>Ambari能够对一个集群启用或禁用Kerberos。要通知Ambari服务及其组件使用的身份和配置，每个服务需要提供一个kerberos.json文件。<br>在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Automated+Kerberizaton" title="Automated Kerberization" target="_blank" rel="external">Automated Kerberization</a>wiki页面可以读到关于Kerberos的支持的信息，还可以在<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/docs/security/kerberos/kerberos_descriptor.md" title="Kerberos Descriptor documentation" target="_blank" rel="external">Kerberos Descriptor documentation</a>中得到Kerberos的描述信息。</p>
<h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><p>对于Hadoop和Ambari管理的集群，Ambari提供了<a href="https://cwiki.apache.org/confluence/display/AMBARI/Metrics" title="Ambari Metrics System" target="_blank" rel="external">Ambari Metrics System</a>服务，用来收集、聚合系统的metrics。<br>每个service可以定义哪些metrics能够被AMS收集，通过metrics.json文件来定义。你可以在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Stack+Defined+Metrics" title="Stack Defined Metrics" target="_blank" rel="external">Stack Defined Metrics</a>页面中得到关于metrics.json格式的信息。</p>
<h2 id="Quick-Links"><a href="#Quick-Links" class="headerlink" title="Quick Links"></a>Quick Links</h2><p>一个service通过向一个文本添加metainfo来实现向Ambari web UI中添加一个快速链接的列表，添加数据的文本遵循一个预定义JSON格式。Ambari server解析quicklink JSON文件，并将它的内容展示在UI。因此，Ambari web UI能够根据这些信息计算quick link URLs，并相应的填充quicklink的下拉列表。<br>关于quick link的JSON文件的设计，可以参看<a href="https://cwiki.apache.org/confluence/display/AMBARI/Quick+Links" title="Quick Links" target="_blank" rel="external">Quick Links</a>页面。</p>
<h2 id="Widgets"><a href="#Widgets" class="headerlink" title="Widgets"></a>Widgets</h2><p>每个service都可以通过定一个widgets.json文件来定义在service的摘要页面上默认显示哪些widgets和heatmaps。<br>你可以在<a href="https://cwiki.apache.org/confluence/display/AMBARI/Enhanced+Service+Dashboard" title="Enhanced Service Dashboard" target="_blank" rel="external">Enhanced Service Dashboard</a>页面中看到更多关于widgets描述符的信息。</p>
<h2 id="Role-Command-Order"><a href="#Role-Command-Order" class="headerlink" title="Role Command Order"></a>Role Command Order</h2><p>从Ambari 2.2开始，每个service通过在service文件夹中包含一个role_rommand_order.json文件来定义自己的role command order。这个service应当只指定它的组件到其他组件之间的关系。换句话说，如果service只包含COMP_X，那么servier应当只列出与COMP_X相关的依赖。如果COMP_X启动，它依赖于NameNode的启动，当NameNode停止时，NameNode应该要等COMP_X先停止，下面的信息将被包含在role command order中：<br>Example service role_command_order.json<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&quot;COMP_X-START&quot;: [&quot;NAMENODE-START&quot;],</div><div class="line">&quot;NAMENODE-STOP&quot;: [&quot;COMP_X-STOP&quot;]</div></pre></td></tr></table></figure></p>
<p>service的role command order条目将会与stack中定义的role command order合并。例如，因为stack已经依赖NAMENODE_STOP，在上面的例子中，COMP_X-STOP将被添加到NAMENODE-STOP的依赖，此外，COMP_X-START对NAMENODE-START的依赖将作为一个新的依赖项被添加。<br>对于role command order的更多信息，可以查看<a href="https://cwiki.apache.org/confluence/display/AMBARI/How-To+Define+Stacks+and+Services#How-ToDefineStacksandServices-RoleCommandOrder" title="Role Command Order" target="_blank" rel="external">Role Command Order</a>章节。</p>
<h2 id="Service-Advisor"><a href="#Service-Advisor" class="headerlink" title="Service Advisor"></a>Service Advisor</h2><p>从Ambari 2.4开始，每个service可以选择定义自己的service advisor，而不是在stack advisor中定义它的配置和布局的细节。这专门用于哪些没有在stack中定义的自定义service。service能够在它的service文件夹中编写一个名为service-advisor.py的Python脚本来提供Service Advisor的能力。这个文件夹可以位于定义service的stack的services目录或者用来定义可继承service的common-services目录。例如：<a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/common-services/HAWQ/2.0.0" title="common-services/HAWQ/2.0.0" target="_blank" rel="external">common-services/HAWQ/2.0.0</a>。<br>与Stack-advisor脚本不同，service-advisor脚本不会自动的继承父级service的service-advisor脚本。service-advisor脚本需要声明来继承它们父级service的service-advisor脚本。下面的代码向你展示了如何引用父级service的service-advisor.py。在这个例子中，它继承了位于resource/stacks中的顶级service-advisor.py。<br>Sample service-advisor.py file inheritance<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))</div><div class="line">STACKS_DIR = os.path.join(SCRIPT_DIR, <span class="string">'../../../stacks/'</span>)</div><div class="line">PARENT_FILE = os.path.join(STACKS_DIR, <span class="string">'service_advisor.py'</span>)</div><div class="line"> </div><div class="line"><span class="keyword">try</span>:</div><div class="line">  <span class="keyword">with</span> open(PARENT_FILE, <span class="string">'rb'</span>) <span class="keyword">as</span> fp:</div><div class="line">    service_advisor = imp.load_module(<span class="string">'service_advisor'</span>, fp, PARENT_FILE, (<span class="string">'.py'</span>, <span class="string">'rb'</span>, imp.PY_SOURCE))</div><div class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">  traceback.print_exc()</div><div class="line">  <span class="keyword">print</span> <span class="string">"Failed to load parent"</span></div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HAWQ200ServiceAdvisor</span><span class="params">(service_advisor.ServiceAdvisor)</span>:</span></div></pre></td></tr></table></figure></p>
<p>与stack advisors类似，service advisor在4个重要概念上提供了信息：</p>
<blockquote>
<p>1、推荐集群上service的布局。<br>2、推荐service配置。<br>3、验证集群上service的布局。<br>4、验证service配置。<br>通过提供的service-advisor.py文件，service能够动态控制上面的每一个。<br>对于service-advisor脚本来说主要接口是如何调用上面的每一项，以及给它们提供什么数据。</p>
</blockquote>
<p>Base service_advisor.py from resources/stacks<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ServiceAdvisor</span><span class="params">(DefaultStackAdvisor)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line">  Abstract class implemented by all service advisors.</div><div class="line">  """</div><div class="line"> </div><div class="line">  <span class="string">"""</span></div><div class="line">  If any components of the service should be colocated with other services,</div><div class="line">  this is where you should set up that layout.  Example:</div><div class="line"> </div><div class="line">    # colocate HAWQSEGMENT with DATANODE, if no hosts have been allocated for HAWQSEGMENT</div><div class="line">    hawqSegment = [component for component in serviceComponents if component["StackServiceComponents"]["component_name"] == "HAWQSEGMENT"][0]</div><div class="line">    if not self.isComponentHostsPopulated(hawqSegment):</div><div class="line">      for hostName in hostsComponentsMap.keys():</div><div class="line">        hostComponents = hostsComponentsMap[hostName]</div><div class="line">        if &#123;"name": "DATANODE"&#125; in hostComponents and &#123;"name": "HAWQSEGMENT"&#125; not in hostComponents:</div><div class="line">          hostsComponentsMap[hostName].append( &#123; "name": "HAWQSEGMENT" &#125; )</div><div class="line">        if &#123;"name": "DATANODE"&#125; not in hostComponents and &#123;"name": "HAWQSEGMENT"&#125; in hostComponents:</div><div class="line">          hostComponents.remove(&#123;"name": "HAWQSEGMENT"&#125;)</div><div class="line">  """</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">colocateService</span><span class="params">(self, hostsComponentsMap, serviceComponents)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"> </div><div class="line">  <span class="string">"""</span></div><div class="line">  Any configuration recommendations for the service should be defined in this function.</div><div class="line">  This should be similar to any of the recommendXXXXConfigurations functions in the stack_advisor.py</div><div class="line">  such as recommendYARNConfigurations().</div><div class="line">  """</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getServiceConfigurationRecommendations</span><span class="params">(self, configurations, clusterSummary, services, hosts)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"> </div><div class="line">  <span class="string">"""</span></div><div class="line">  Returns an array of Validation objects about issues with the hostnames to which components are assigned.</div><div class="line">  This should detect validation issues which are different than those the stack_advisor.py detects.</div><div class="line">  The default validations are in stack_advisor.py getComponentLayoutValidations function.</div><div class="line">  """</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getServiceComponentLayoutValidations</span><span class="params">(self, services, hosts)</span>:</span></div><div class="line">    <span class="keyword">return</span> []</div><div class="line"> </div><div class="line">  <span class="string">"""</span></div><div class="line">  Any configuration validations for the service should be defined in this function.</div><div class="line">  This should be similar to any of the validateXXXXConfigurations functions in the stack_advisor.py</div><div class="line">  such as validateHDFSConfigurations.</div><div class="line">  """</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getServiceConfigurationsValidationItems</span><span class="params">(self, configurations, recommendedDefaults, services, hosts)</span>:</span></div><div class="line">    <span class="keyword">return</span> []</div></pre></td></tr></table></figure></p>
<h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><ul><br>    <li><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/service_advisor.py#L51" title="Service Advisor interface" target="_blank" rel="external">Service Advisor interface</a></li><br>    <li><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HAWQ/2.0.0/service_advisor.py" title="HAWQ 2.0.0 Service Advisor implementation" target="_blank" rel="external">HAWQ 2.0.0 Service Advisor implementation</a></li><br>    <li><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/PXF/3.0.0/service_advisor.py" title="PXF 3.0.0 Service Advisor implementation" target="_blank" rel="external">PXF 3.0.0 Service Advisor implementation</a></li><br></ul>

<h2 id="Service-Upgrade"><a href="#Service-Upgrade" class="headerlink" title="Service Upgrade"></a>Service Upgrade</h2><p>从Ambari开始，每个service能够在它的service definition中定义它自己的更新。这对哪些不再需要修改stack的upgrade-packs的自定义service，以便它们融合到集群的更新。</p>
<p>每个service能够定义upgrade-packs，upgrade-packs是一些XML文件，它们描述了某个service的更新进程已经这个更新包如何与所有的stack更新包相关联。这些upgrade-pack XML文件在service的upgrades/文件夹中的独立的子文件夹中，这些子文件夹指明了需要扩展的stack版本。测试代码中的一些例子。</p>
<h3 id="Examples-1"><a href="#Examples-1" class="headerlink" title="Examples"></a>Examples</h3><ul><br>    <li><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/test/resources/stacks/HDP/2.0.5/services/HDFS/upgrades/" title="Upgrades folder" target="_blank" rel="external">Upgrades folder</a></li><br>    <li><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/test/resources/stacks/HDP/2.0.5/services/HDFS/upgrades/HDP/2.2.0/upgrade_test_15388.xml" title="Upgrade-pack XML" target="_blank" rel="external">Upgrade-pack XML</a></li><br></ul>

<p>service定义的每个upgrade-pack通过一个特定的stack版本，应当匹配service定义的文件名。例如，在测试代码中，HDP 2.2.0有一个名为upgrade_test_15388.xml的upgrade-pack。HDFS service定义了一个extension来扩展那个upgrade pack<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/test/resources/stacks/HDP/2.0.5/services/HDFS/upgrades/HDP/2.2.0/upgrade_test_15388.xml" title="HDP/2.0.5/services/HDFS/upgrades/HDP/2.2.0/upgrade_test_15388.xml" target="_blank" rel="external">HDP/2.0.5/services/HDFS/upgrades/HDP/2.2.0/upgrade_test_15388.xml</a>。在这个例子中，upgrade-pack定义在HDP/2.0.5的stack中。这个upgrade-pack是HDP/2.2.0的一个扩展，因为他被定义在upgrade/HDP/2.2.0目录中。最终，扩展到upgrad-pack upgrade_test_15388.xml的service的名字与HDP/2.2.0/upgrades中的upgrade-pack的名字匹配。<br>对于service的文件格式与stack的有很大的相同。target、target-stack和type属性应该和stack的upgrade-pack的信息完全对应。service能够添加自己的前提检测。</p>
<p>General Attributes and Prerequisite Checks<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">upgrade</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">target</span>&gt;</span>2.4.*<span class="tag">&lt;/<span class="name">target</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">target-stack</span>&gt;</span>HDP-2.4.0<span class="tag">&lt;/<span class="name">target-stack</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">type</span>&gt;</span>ROLLING<span class="tag">&lt;/<span class="name">type</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">prerequisite-checks</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">check</span>&gt;</span>org.apache.ambari.server.checks.FooCheck<span class="tag">&lt;/<span class="name">check</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">prerequisite-checks</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>upgrade-pack的<order>部分，由<group>标签组成，就像stack的upgrade-pack。关键的不同是如何定义这些<group>，使它们与stack的upgrade pack的<group>或其他service的upgrade pack的<group>相关联。在第一个例子中，我们引入了名为PRE_CLUSTER的<group>并为名为FOO的service新增了一个<execute-stage>。该项应该在基于<add-after-group-entry>标签的HDFS之后的<execute-stage>中添加。</execute-stage></add-after-group-entry></execute-stage></group></group></group></group></group></order></p>
<p>Order Section - Add After Group Entry<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">order</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">group</span> <span class="attr">xsi:type</span>=<span class="string">"cluster"</span> <span class="attr">name</span>=<span class="string">"PRE_CLUSTER"</span> <span class="attr">title</span>=<span class="string">"Pre &#123;&#123;direction.text.proper&#125;&#125;"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">add-after-group-entry</span>&gt;</span>HDFS<span class="tag">&lt;/<span class="name">add-after-group-entry</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">execute-stage</span> <span class="attr">service</span>=<span class="string">"FOO"</span> <span class="attr">component</span>=<span class="string">"BAR"</span> <span class="attr">title</span>=<span class="string">"Backup FOO"</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">task</span> <span class="attr">xsi:type</span>=<span class="string">"manual"</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">message</span>&gt;</span>Back FOO up.<span class="tag">&lt;/<span class="name">message</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">task</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">execute-stage</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">group</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>同样的语法也可以被用于service检查优先级和group services等。</p>
<p>Order Section - Further Add After Group Entry Examples<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">group</span> <span class="attr">name</span>=<span class="string">"SERVICE_CHECK1"</span> <span class="attr">title</span>=<span class="string">"All Service Checks"</span> <span class="attr">xsi:type</span>=<span class="string">"service-check"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">add-after-group-entry</span>&gt;</span>ZOOKEEPER<span class="tag">&lt;/<span class="name">add-after-group-entry</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">priority</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">service</span>&gt;</span>HBASE<span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">priority</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">group</span>&gt;</span></div><div class="line"> </div><div class="line"><span class="tag">&lt;<span class="name">group</span> <span class="attr">name</span>=<span class="string">"CORE_MASTER"</span> <span class="attr">title</span>=<span class="string">"Core Masters"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">add-after-group-entry</span>&gt;</span>YARN<span class="tag">&lt;/<span class="name">add-after-group-entry</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">service</span> <span class="attr">name</span>=<span class="string">"HBASE"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">component</span>&gt;</span>HBASE_MASTER<span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">group</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>还可以在stack的upgrade-pack中增加新的group，并将它们排列在其他group之后。在下面的例子中，我们在使用<add-after-group>标签的HIVE的group之后增加了一个名为FOO的group。</add-after-group></p>
<p>Order Section - Add After Group<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">group</span> <span class="attr">name</span>=<span class="string">"FOO"</span> <span class="attr">title</span>=<span class="string">"Foo"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">add-after-group</span>&gt;</span>HIVE<span class="tag">&lt;/<span class="name">add-after-group</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">skippable</span>&gt;</span>true<span class="tag">&lt;/<span class="name">skippable</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">allow-retry</span>&gt;</span>false<span class="tag">&lt;/<span class="name">allow-retry</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">service</span> <span class="attr">name</span>=<span class="string">"FOO"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">component</span>&gt;</span>BAR<span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">group</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>你还可以在同一个<group>中同时创建<add-after-group>和<add-after-groujp-entry>。这将会在指定的group不存在的情况下才会创建一个新的group，并且会将他排列在<add-after-group>指定的group之后。<add-after-group-entry>将会确定它的group的service的内部排序、优先级和执行阶段。</add-after-group-entry></add-after-group></add-after-groujp-entry></add-after-group></group></p>
<p>Order Section - Add After Group<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">group</span> <span class="attr">name</span>=<span class="string">"FOO"</span> <span class="attr">title</span>=<span class="string">"Foo"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">add-after-group</span>&gt;</span>HIVE<span class="tag">&lt;/<span class="name">add-after-group</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">add-after-group-entry</span>&gt;</span>FOO<span class="tag">&lt;/<span class="name">add-after-group-entry</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">skippable</span>&gt;</span>true<span class="tag">&lt;/<span class="name">skippable</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">allow-retry</span>&gt;</span>false<span class="tag">&lt;/<span class="name">allow-retry</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">service</span> <span class="attr">name</span>=<span class="string">"FOO2"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">component</span>&gt;</span>BAR2<span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">group</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>upgrade-pack剩余的<processing>部分，与stack的upgrade-pack的相同。</processing></p>
<p>Processing Section<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">processing</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">service</span> <span class="attr">name</span>=<span class="string">"FOO"</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">component</span> <span class="attr">name</span>=<span class="string">"BAR"</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">upgrade</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">task</span> <span class="attr">xsi:type</span>=<span class="string">"restart-task"</span> /&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">upgrade</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">component</span> <span class="attr">name</span>=<span class="string">"BAR2"</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">upgrade</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">task</span> <span class="attr">xsi:type</span>=<span class="string">"restart-task"</span> /&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">upgrade</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">processing</span>&gt;</span></div></pre></td></tr></table></figure></p>
<h1 id="Define-Stack"><a href="#Define-Stack" class="headerlink" title="Define Stack"></a>Define Stack</h1><p>一个Stack就是一个版本化的service的集合。每个stack就是一个定义在ambari-server/src/main/resource/stacks中的一个文件夹。安装ambari之后，stack的定义则位于ambari-server主机的/var/lib/ambari-server/resources/stacks中。<br>每个stack文件夹中包含该stack的每个版本的子文件夹。一些stack版本可用，一些不可用。每个stack版本包含一些service，这些service有的继承自common-services，有些在stack版本的services中定义。<br><img src="http://oaavtz33a.bkt.clouddn.com/Screen%20Shot%202016-03-08%20at%2012.46.40%20PM.png"><br>Example : <a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/stacks/HDP/2.4" title="HDP stack.HDP-2.4 stack version" target="_blank" rel="external">HDP stack.HDP-2.4 stack version</a>。</p>
<h2 id="Stack-Version-Descriptor"><a href="#Stack-Version-Descriptor" class="headerlink" title="Stack-Version Descriptor"></a>Stack-Version Descriptor</h2><p>每个Stack-version应当提供一个metainfo.xml（如：<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.3/metainfo.xml" title="HDP-2.3" target="_blank" rel="external">HDP-2.3</a>、<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.4/metainfo.xml" title="HDP-2.4" target="_blank" rel="external">HDP-2.4</a> ）文件作为描述符，它如下描述了stack-verion：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">metainfo</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">versions</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">active</span>&gt;</span>true<span class="tag">&lt;/<span class="name">active</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">versions</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">extends</span>&gt;</span>2.3<span class="tag">&lt;/<span class="name">extends</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">minJdk</span>&gt;</span>1.7<span class="tag">&lt;/<span class="name">minJdk</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">maxJdk</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maxJdk</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">metainfo</span>&gt;</span></div></pre></td></tr></table></figure></p>
<blockquote>
<p>versions/active - 当前版本的stack是否还可以用于安装。如果不可用，这个版本在安装的时候将不会在UI中显示。<br>extends - 当前stack继承的版本。进行继承的stack版本会继承service以及父stack版本的所有方面。<br>minJdk - stack版本支持的最低JDK版本。在安装向导期间如果被Ambari使用的JDK低于这个版本，用户将被警告。<br>maxJdk - stack版本支持的最高JDK版本。在安装向导期间，如果被Ambari使用的JDK版本高于这个版本，用户将被警告。</p>
</blockquote>
<h2 id="Stack-Properties"><a href="#Stack-Properties" class="headerlink" title="Stack Properties"></a>Stack Properties</h2><p>stack必须包含或继承一个属性字典，属性字典包含两个文件：<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/properties/stack_features.json" title="stack_features.json" target="_blank" rel="external">stack_features.json</a>和<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/properties/stack_tools.json" title="stack_tools.json" target="_blank" rel="external">stack_tools.json</a>。这个字典是在Ambari 2.4中新增的。<br>stack_features.json中包含了一个features的列表，这个列表指定了哪些版本的stack包含这些特性。<br>特性列表由特定的Ambari版本所确定。特定Ambari版本的详细列表能够在HDP/2.0.6/properties/stack_features.json中找到。每个feature由name、description以及特性所支持stack的最高版本和最低版本来构成。<br><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"stack_features"</span>: [</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"name"</span>: <span class="string">"snappy"</span>,</div><div class="line">      <span class="attr">"description"</span>: <span class="string">"Snappy compressor/decompressor support"</span>,</div><div class="line">      <span class="attr">"min_version"</span>: <span class="string">"2.0.0.0"</span>,</div><div class="line">      <span class="attr">"max_version"</span>: <span class="string">"2.2.0.0"</span></div><div class="line">    &#125;,</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>stack_tools.json包含了stack_selector和conf_selector这两个工具对应的名称以及安装位置。<br>任何自定义的stack必须包含这两个JSON文件。更多的信息请查看<a href="https://cwiki.apache.org/confluence/display/AMBARI/Stack+Properties" title="Stack Properties" target="_blank" rel="external">Stack Properties</a>的wiki页面。</p>
<h2 id="Services"><a href="#Services" class="headerlink" title="Services"></a>Services</h2><p>每个stack版本中都包含services，这些services要么是引用的common-services中的，要么是在stack版本中services文件夹下定义的。<br>common-services中定义的services能够被多个stack共享。如果他们不会被共享，那么他们可以定义在stack版本中。</p>
<h3 id="Reference-common-services"><a href="#Reference-common-services" class="headerlink" title="Reference common-services"></a>Reference common-services</h3><p>要引用common-services中的一个service，service描述文件需要使用<extends>项。（例如： <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/HDFS/metainfo.xml" title="DFS in HDP-2.0.6" target="_blank" rel="external">HDFS in HDP-2.0.6</a>）<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">metainfo</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">schemaVersion</span>&gt;</span>2.0<span class="tag">&lt;/<span class="name">schemaVersion</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">services</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">service</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>HDFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">extends</span>&gt;</span>common-services/HDFS/2.1.0.2.0<span class="tag">&lt;/<span class="name">extends</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">services</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">metainfo</span>&gt;</span></div></pre></td></tr></table></figure></extends></p>
<h3 id="Define-Service-1"><a href="#Define-Service-1" class="headerlink" title="Define Service"></a>Define Service</h3><p>与common-services中定义的services格式相同，可以子啊services文件夹中定义新的service。<br>Examples：</p>
<ul><br>    <li><a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/stacks/BIGTOP/0.8/services/HDFS" title="HDFS in BIGTOP-0.8" target="_blank" rel="external">HDFS in BIGTOP-0.8</a></li><br>    <li><a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/stacks/HDP/2.3.GlusterFS/services/GLUSTERFS" title="GlusterFS in HDP-2.3.GlusterFS" target="_blank" rel="external">GlusterFs in HDP-2.3.CusterFs</a></li><br></ul>

<h3 id="Extend-Service"><a href="#Extend-Service" class="headerlink" title="Extend Service"></a>Extend Service</h3><p>当一个版本继承另外一个版本时，它继承父级service的所有细节。它也可以自由的重写或删除继承的service定义的任何部分。<br>Examples：</p>
<ul><br>    <li>HDP-2.3/HDFS - <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.3/services/HDFS/metainfo.xml" title="添加NFS_GATEWAY组件，更新service版本和OS特定包" target="_blank" rel="external">添加NFS_GATEWAY组件，更新service版本和OS特定包</a></li><br>    <li>HDP-2.2/Storm - <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.2/services/STORM/metainfo.xml" title="删除了STORM_REST_API组件，更新service版本和OS特定包" target="_blank" rel="external">删除了STORM_REST_API组件，更新service版本和OS特定包</a></li><br>    <li>HDP-2.3/YARN - <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.3/services/YARN/configuration/capacity-scheduler.xml" title="从capacity-scheduler.mxl中删除YARN node-lable配置" target="_blank" rel="external">从capacity-scheduler.mxl中删除YARN node-lable配置</a></li><br>    <li>HDP-2.3/Kafka - <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.3/services/KAFKA/alerts.json" title="增加Kafka Broker进程告警" target="_blank" rel="external">增加Kafka Broker进程告警</a></li><br></ul>

<h2 id="Role-Command-Order-1"><a href="#Role-Command-Order-1" class="headerlink" title="Role Command Order"></a>Role Command Order</h2><p>Role是Component（如：NAMENODE、DATANODE、RESOURCEMANAGER、HBASE_MASTER等）的另一个名称。<br>顾名思义，它可以告诉Amberi在你stack中定义的component执行命令的顺序。<br>例如：”ZooKeeper Server 应当在启动NameNode之前启动”。“HBase Master应当在NameNode和DataNode启动之后再启动”。<br>这可以通过在stack-version文件夹中包含<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/role_command_order.json" title="role_command_order.json" target="_blank" rel="external">role_command_order.json</a>来具体说明。</p>
<h3 id="Format"><a href="#Format" class="headerlink" title="Format"></a>Format</h3><p>以JSON格式指定，这个文件包含一个JSON对象，并且顶级key是section名称或comments。如：<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/role_command_order.json" title="HDP-2.0.6" target="_blank" rel="external">HDP-2.0.6</a>。<br>在每个section对象内部，key描述了它对应的component的行为，value列出当前component-action之前应当完成的component-action。<br>Structure of role_command_order.json<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  "_comment": "Section 1 comment",</div><div class="line">  "section_name_1": &#123;</div><div class="line">    "_comment": "Section containing role command orders",</div><div class="line">    "&lt;DEPENDENT_COMPONENT_1&gt;-&lt;COMMAND&gt;": ["&lt;DEPENDS_ON_COMPONENT_1&gt;-&lt;COMMAND&gt;", "&lt;DEPENDS_ON_COMPONENT_1&gt;-&lt;COMMAND&gt;"],</div><div class="line">    "&lt;DEPENDENT_COMPONENT_2&gt;-&lt;COMMAND&gt;": ["&lt;DEPENDS_ON_COMPONENT_3&gt;-&lt;COMMAND&gt;"],</div><div class="line">    ...</div><div class="line">  &#125;,</div><div class="line">  "_comment": "Next section comment",</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="Sections"><a href="#Sections" class="headerlink" title="Sections"></a>Sections</h3><p>Ambari只使用了如下的sections：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Section Name</th>
<th style="text-align:left">When Used</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">general_deps</td>
<td style="text-align:left">适用于所有情况</td>
</tr>
<tr>
<td style="text-align:left">optional_glusterfs</td>
<td style="text-align:left">当集群有GLUSTERFS服务实例时</td>
</tr>
<tr>
<td style="text-align:left">optional_no_glusterfs</td>
<td style="text-align:left">当集群没有GLUSTERFS服务实例时</td>
</tr>
<tr>
<td style="text-align:left">namenode_optional_ha</td>
<td style="text-align:left">当安装了HDFS服务，且有JOURNALNODE组件时</td>
</tr>
<tr>
<td style="text-align:left">resourcemanager_optional_ha</td>
<td style="text-align:left">当安装了YARN服务，且存在多个RESOURCEMANAGER host-components存在时</td>
</tr>
</tbody>
</table>
<h3 id="Commands"><a href="#Commands" class="headerlink" title="Commands"></a>Commands</h3><p>Ambari当前支持的命令有：</p>
<blockquote>
<p>INSTALL<br>UNINSTALL<br>START<br>RESTART<br>STOP<br>EXECUTE<br>ABORT<br>UPGRADE<br>SERVICE_CHECK<br>CUSTOM_COMMAND<br>ACTIONEXECUTE</p>
</blockquote>
<h3 id="Examples-2"><a href="#Examples-2" class="headerlink" title="Examples"></a>Examples</h3><table>
<thead>
<tr>
<th style="text-align:left">Role Command Order</th>
<th style="text-align:left">Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">“HIVE_METASTORE-START”:[“MYSQL_SERVER-START”, “NAMENODE-START”]</td>
<td style="text-align:left">启动Hive Metastore之前先启动MySQL和NameNode。</td>
</tr>
<tr>
<td style="text-align:left">“MAPREDUCE_SERVICE_CHECK-SERVICE_CHECK”:[“NODEMANAGER-START”, “RESOURCEMANAGER-START”]</td>
<td style="text-align:left">MapReduce服务检查需要ResourceManager和NodeManager的启动。</td>
</tr>
<tr>
<td style="text-align:left">“ZOOKEEPER_SERVER-STOP”:[“HBASE_MASTER-STOP”, “HBASE_REGIONSERVER-STOP”, “METRICS_COLLECTOR-STOP”]</td>
<td style="text-align:left">在停止Zookeeper之前，应该先确保HBase Master、Hbase RegionServers和AMS Metrics收集器先停止。</td>
</tr>
</tbody>
</table>
<h2 id="Repositories"><a href="#Repositories" class="headerlink" title="Repositories"></a>Repositories</h2><p>通过提供一个repos/repoinfo.xml（如 <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/repos/repoinfo.xml" title="HDP-2.0.6" target="_blank" rel="external">HDP-2.0.6</a>），每个stack版本可以提供package的库的位置来使用。<br>repoinfo.xml文件中包含的库根据操作系统进行分组。每个os指定一个库列表，这些库列表会在stack版本安装时展示给用户。<br>这些库与<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/metainfo.xml#L161" title="packages defined in a service&#39;s metainfo.xml" target="_blank" rel="external">packages defined in a service’s metainfo.xml</a>配合使用，以便在系统上安装正确的。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">reposinfo</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">os</span> <span class="attr">family</span>=<span class="string">"redhat6"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">repo</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">baseurl</span>&gt;</span>http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.1<span class="tag">&lt;/<span class="name">baseurl</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">repoid</span>&gt;</span>HDP-2.0.6<span class="tag">&lt;/<span class="name">repoid</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">reponame</span>&gt;</span>HDP<span class="tag">&lt;/<span class="name">reponame</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">repo</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">repo</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">baseurl</span>&gt;</span>http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.17/repos/centos6<span class="tag">&lt;/<span class="name">baseurl</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">repoid</span>&gt;</span>HDP-UTILS-1.1.0.17<span class="tag">&lt;/<span class="name">repoid</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">reponame</span>&gt;</span>HDP-UTILS<span class="tag">&lt;/<span class="name">reponame</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">repo</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">os</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">reposinfo</span>&gt;</span></div></pre></td></tr></table></figure></p>
<blockquote>
<p>baseurl - RPM库的URL，可以在这里找到repoid提供的软件。<br>repoid - baseurl地址使用的repo id。<br>reponame - 需要使用的repo的展示名。</p>
</blockquote>
<h3 id="Latest-Builds"><a href="#Latest-Builds" class="headerlink" title="Latest Builds"></a>Latest Builds</h3><p>尽管repository基本URL能够对某个特定repo提供更新，但是必须在构建时定义它。当repository变更位置或更新包位于不同网站时，这就会成为一个问题。<br>对于这样的情况，stack-version能够提供一个JSON文件，来提供要使用的其他repo URL。<br>例如： <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.3/repos/repoinfo.xml" title="HDP-2.3 repoinfo.xml uses &lt;latest&gt; file" target="_blank" rel="external">HDP-2.3 repoinfo.xml uses <latest> file</latest></a>，它指出最新的构建包的repository URL。</p>
<figure class="highlight plain"><figcaption><span>json</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    ...</div><div class="line">    &quot;HDP-2.3&quot;:&#123;</div><div class="line">        &quot;latest&quot;:&#123;</div><div class="line">            &quot;centos6&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos6/2.x/BUILDS/2.3.6.0-3586/&quot;,</div><div class="line">            &quot;centos7&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/2.x/BUILDS/2.3.6.0-3586/&quot;,</div><div class="line">            &quot;debian6&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/debian6/2.x/BUILDS/2.3.6.0-3586/&quot;,</div><div class="line">            &quot;debian7&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/debian7/2.x/BUILDS/2.3.6.0-3586/&quot;,</div><div class="line">            &quot;suse11&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/suse11sp3/2.x/BUILDS/2.3.6.0-3586/&quot;,</div><div class="line">            &quot;ubuntu12&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/ubuntu12/2.x/BUILDS/2.3.6.0-3586/&quot;,</div><div class="line">            &quot;ubuntu14&quot;:&quot;http://s3.amazonaws.com/dev.hortonworks.com/HDP/ubuntu14/2.x/BUILDS/2.3.6.0-3586/&quot;</div><div class="line">        &#125;</div><div class="line">    &#125;,</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Hooks"><a href="#Hooks" class="headerlink" title="Hooks"></a>Hooks</h2><p>stack-version会有非常基本且通用的指令，这些指令需要在某个Ambari命令之前或之后运行。<br>避免将代码在service脚本之间复制并要求用户确认，通过将前置代码和后置代码放到hooks文件夹中，Ambari提供了Hooks的功能。（如：<a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks" title="HDP-2.0.6" target="_blank" rel="external">HDP-2.0.6</a>）<br><img src=""></p>
<h3 id="Command-Sub-Folders"><a href="#Command-Sub-Folders" class="headerlink" title="Command Sub-Folders"></a>Command Sub-Folders</h3><p>hooks子文件夹的命名模式为”<before|after>-<any|<commandname>&gt;”。<br>那意味着子文件夹中的scripts/hook.py文件是在命令之前运行还是之后运行。<br>Examples：</any|<commandname></before|after></p>
<table>
<thead>
<tr>
<th style="text-align:left">Sub-Folder</th>
<th style="text-align:left">Purpose</th>
<th style="text-align:left">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">before-START</td>
<td style="text-align:left">hook脚本，会在stack-version的任何组件启动之前被调用</td>
<td style="text-align:left"><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py#L30" title="HDP-2.0.6" target="_blank" rel="external">HDP-2.0.6</a> 1、设置hadoop的日志和pid目录。2、创建javahome的symlink。3、创建/etc/hadoop/conf/topology_script.py脚本</td>
</tr>
<tr>
<td style="text-align:left">before-INSTALL</td>
<td style="text-align:left">hook脚本，会在stack-version的任何组件安装之前被调用</td>
<td style="text-align:left"><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py#L30" title="HDP-2.0.6" target="_blank" rel="external">HDP-2.0.6</a> 1、在/etc/yum.repos.d中创建repo文件。 2、安装基本包，如curl、unzip等</td>
</tr>
</tbody>
</table>
<p>Ambari当前支持的命令，根据需要可以创建如下的子文件夹</p>
<table>
<thead>
<tr>
<th style="text-align:left">Prefix</th>
<th style="text-align:left">Command</th>
<th style="text-align:left">Details</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">before</td>
<td style="text-align:left">INSTALL</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">before</td>
<td style="text-align:left">UNINSTALL</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">before</td>
<td style="text-align:left">START</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">before</td>
<td style="text-align:left">RESTART</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">before</td>
<td style="text-align:left">STOP</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">after</td>
<td style="text-align:left">EXECUTE</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">after</td>
<td style="text-align:left">ABORT</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">after</td>
<td style="text-align:left">UPGRADE</td>
<td style="text-align:left">&nbsp;</td>
</tr>
<tr>
<td style="text-align:left">after</td>
<td style="text-align:left">SERVICE_CHECK</td>
<td style="text-align:left">&nbps;</td>
</tr>
<tr>
<td style="text-align:left">after</td>
<td style="text-align:left">&lt; custom_command&gt;</td>
<td style="text-align:left">用户指定的自定义命令，如HDFS指定的DECOMMISSION或REBALANCEHDFS这两个命令。</td>
</tr>
</tbody>
</table>
<p>script/hooks.py脚本应该导入<a href="https://github.com/apache/ambari/blob/trunk/ambari-common/src/main/python/resource_management/libraries/script/hook.py" title="resource_management.libraries.script.hook" target="_blank" rel="external">resource_management.libraries.script.hook</a>模块，并继承Hook类。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> resource_management.libraries.script.hook <span class="keyword">import</span> Hook</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomHook</span><span class="params">(Hook)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hook</span><span class="params">(self, env)</span>:</span></div><div class="line">    <span class="comment"># Do custom work</span></div><div class="line">     </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  CustomHook().execute()</div></pre></td></tr></table></figure></p>
<h2 id="Configurations"><a href="#Configurations" class="headerlink" title="Configurations"></a>Configurations</h2><p>尽管大多数配置是在service级别设置的，但是也可以有适用于所有servies的配置，以便指示安装了此stack的集群的状态。<br>例如，像<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/configuration/cluster-env.xml#L25" target="_blank" rel="external">is security enabled?</a>，<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/configuration/cluster-env.xml#L46" target="_blank" rel="external">what user runs smoke tests?</a> 等。<br>这样的配置可以定义在sstack的<a href="https://github.com/apache/ambari/tree/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/configuration" target="_blank" rel="external">configuration文件夹</a>中。它们就像service级配置一样访问。</p>
<h3 id="Stack-Advisor"><a href="#Stack-Advisor" class="headerlink" title="Stack Advisor"></a>Stack Advisor</h3><p>由于每个stack包含多个复杂的service，因此有必要动态确定services的布局以及确定某些配置的值。<br>stacks在services/目录中编写一个名为stack-advisor.py的Python脚本，使Ambari提供了Stack Advisor的能力。例如：<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py" title="HDP-2.0.6" target="_blank" rel="external">HDP-2.0.6</a>。Stack advisor脚本能够自动继承父级stack版本的stack advisor脚本。这允许较新的stack版本能够改变行为而不会影响之前的版本的行为。<br>Stack advisor在4个重要概念上提供了信息：</p>
<blockquote>
<p>Recommend layout of services on cluster。<br>Recommend service configurations。<br>Validate layout of services on cluster。<br>Validate service configurations。</p>
</blockquote>
<p>通过提供stack-advisor.py文件，能够动态的控制上面的每一项。<br>stack-advisor脚本的主要接口描述了上面每项应当如何调用，以及提供什么数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">StackAdvisor</span><span class="params">(object)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line">  Abstract class implemented by all stack advisors. Stack advisors advise on stack specific questions.</div><div class="line"> </div><div class="line"> </div><div class="line">  Currently stack advisors provide following abilities:</div><div class="line">  - Recommend where services should be installed in cluster</div><div class="line">  - Recommend configurations based on host hardware</div><div class="line">  - Validate user selection of where services are installed on cluster</div><div class="line">  - Validate user configuration values</div><div class="line"> </div><div class="line">  Each of the above methods is passed in parameters about services and hosts involved as described below.</div><div class="line"> </div><div class="line"> </div><div class="line">    @type services: dictionary</div><div class="line">    @param services: Dictionary containing all information about services selected by the user.</div><div class="line">      Example: &#123;</div><div class="line">      "services": [</div><div class="line">        &#123;</div><div class="line">          "StackServices": &#123;</div><div class="line">            "service_name" : "HDFS",</div><div class="line">            "service_version" : "2.6.0.2.2",</div><div class="line">          &#125;,</div><div class="line">          "components" : [</div><div class="line">            &#123;</div><div class="line">              "StackServiceComponents" : &#123;</div><div class="line">                "cardinality" : "1+",</div><div class="line">                "component_category" : "SLAVE",</div><div class="line">                "component_name" : "DATANODE",</div><div class="line">                "display_name" : "DataNode",</div><div class="line">                "service_name" : "HDFS",</div><div class="line">                "hostnames" : []</div><div class="line">              &#125;,</div><div class="line">              "dependencies" : []</div><div class="line">            &#125;, &#123;</div><div class="line">              "StackServiceComponents" : &#123;</div><div class="line">                "cardinality" : "1-2",</div><div class="line">                "component_category" : "MASTER",</div><div class="line">                "component_name" : "NAMENODE",</div><div class="line">                "display_name" : "NameNode",</div><div class="line">                "service_name" : "HDFS",</div><div class="line">                "hostnames" : []</div><div class="line">              &#125;,</div><div class="line">              "dependencies" : []</div><div class="line">            &#125;,</div><div class="line">            ...</div><div class="line">          ]</div><div class="line">        &#125;,</div><div class="line">        ...</div><div class="line">      ]</div><div class="line">    &#125;</div><div class="line">  @type hosts: dictionary</div><div class="line">  @param hosts: Dictionary containing all information about hosts in this cluster</div><div class="line">    Example: &#123;</div><div class="line">      "items": [</div><div class="line">        &#123;</div><div class="line">          Hosts: &#123;</div><div class="line">            "host_name": "c6401.ambari.apache.org",</div><div class="line">            "public_host_name" : "c6401.ambari.apache.org",</div><div class="line">            "ip": "192.168.1.101",</div><div class="line">            "cpu_count" : 1,</div><div class="line">            "disk_info" : [</div><div class="line">              &#123;</div><div class="line">              "available" : "4564632",</div><div class="line">              "used" : "5230344",</div><div class="line">              "percent" : "54%",</div><div class="line">              "size" : "10319160",</div><div class="line">              "type" : "ext4",</div><div class="line">              "mountpoint" : "/"</div><div class="line">              &#125;,</div><div class="line">              &#123;</div><div class="line">              "available" : "1832436",</div><div class="line">              "used" : "0",</div><div class="line">              "percent" : "0%",</div><div class="line">              "size" : "1832436",</div><div class="line">              "type" : "tmpfs",</div><div class="line">              "mountpoint" : "/dev/shm"</div><div class="line">              &#125;</div><div class="line">            ],</div><div class="line">            "host_state" : "HEALTHY",</div><div class="line">            "os_arch" : "x86_64",</div><div class="line">            "os_type" : "centos6",</div><div class="line">            "total_mem" : 3664872</div><div class="line">          &#125;</div><div class="line">        &#125;,</div><div class="line">        ...</div><div class="line">      ]</div><div class="line">    &#125;</div><div class="line"> </div><div class="line"> </div><div class="line">    Each of the methods can either return recommendations or validations.</div><div class="line"> </div><div class="line">    Recommendations are made in a Ambari Blueprints friendly format.</div><div class="line">    Validations are an array of validation objects.</div><div class="line">  """</div><div class="line"> </div><div class="line"> </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">recommendComponentLayout</span><span class="params">(self, services, hosts)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Returns recommendation of which hosts various service components should be installed on.</div><div class="line"> </div><div class="line">    This function takes as input all details about services being installed, and hosts</div><div class="line">    they are being installed into, to generate hostname assignments to various components</div><div class="line">    of each service.</div><div class="line"> </div><div class="line"> </div><div class="line">    @type services: dictionary</div><div class="line">    @param services: Dictionary containing all information about services selected by the user.</div><div class="line">    @type hosts: dictionary</div><div class="line">    @param hosts: Dictionary containing all information about hosts in this cluster</div><div class="line">    @rtype: dictionary</div><div class="line">    @return: Layout recommendation of service components on cluster hosts in Ambari Blueprints friendly format.</div><div class="line">        Example: &#123;</div><div class="line">          "resources" : [</div><div class="line">            &#123;</div><div class="line">              "hosts" : [</div><div class="line">                "c6402.ambari.apache.org",</div><div class="line">                "c6401.ambari.apache.org"</div><div class="line">              ],</div><div class="line">              "services" : [</div><div class="line">                "HDFS"</div><div class="line">              ],</div><div class="line">              "recommendations" : &#123;</div><div class="line">                "blueprint" : &#123;</div><div class="line">                  "host_groups" : [</div><div class="line">                    &#123;</div><div class="line">                      "name" : "host-group-2",</div><div class="line">                      "components" : [</div><div class="line">                        &#123; "name" : "JOURNALNODE" &#125;,</div><div class="line">                        &#123; "name" : "ZKFC" &#125;,</div><div class="line">                        &#123; "name" : "DATANODE" &#125;,</div><div class="line">                        &#123; "name" : "SECONDARY_NAMENODE" &#125;</div><div class="line">                      ]</div><div class="line">                    &#125;,</div><div class="line">                    &#123;</div><div class="line">                      "name" : "host-group-1",</div><div class="line">                      "components" :</div><div class="line">                        &#123; "name" : "HDFS_CLIENT" &#125;,</div><div class="line">                        &#123; "name" : "NAMENODE" &#125;,</div><div class="line">                        &#123; "name" : "JOURNALNODE" &#125;,</div><div class="line">                        &#123; "name" : "ZKFC" &#125;,</div><div class="line">                        &#123; "name" : "DATANODE" &#125;</div><div class="line">                      ]</div><div class="line">                    &#125;</div><div class="line">                  ]</div><div class="line">                &#125;,</div><div class="line">                "blueprint_cluster_binding" : &#123;</div><div class="line">                  "host_groups" : [</div><div class="line">                    &#123;</div><div class="line">                      "name" : "host-group-1",</div><div class="line">                      "hosts" : [ &#123; "fqdn" : "c6401.ambari.apache.org" &#125; ]</div><div class="line">                    &#125;,</div><div class="line">                    &#123;</div><div class="line">                      "name" : "host-group-2",</div><div class="line">                      "hosts" : [ &#123; "fqdn" : "c6402.ambari.apache.org" &#125; ]</div><div class="line">                    &#125;</div><div class="line">                  ]</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          ]</div><div class="line">        &#125;</div><div class="line">    """</div><div class="line">    <span class="keyword">pass</span></div><div class="line"> </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">validateComponentLayout</span><span class="params">(self, services, hosts)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Returns array of Validation issues with service component layout on hosts</div><div class="line"> </div><div class="line"> </div><div class="line">    This function takes as input all details about services being installed along with</div><div class="line">    hosts the components are being installed on (hostnames property is populated for</div><div class="line">    each component). </div><div class="line"> </div><div class="line">    @type services: dictionary</div><div class="line">    @param services: Dictionary containing information about services and host layout selected by the user.</div><div class="line">    @type hosts: dictionary</div><div class="line">    @param hosts: Dictionary containing all information about hosts in this cluster</div><div class="line">    @rtype: dictionary</div><div class="line">    @return: Dictionary containing array of validation items</div><div class="line">        Example: &#123;</div><div class="line">          "items": [</div><div class="line">            &#123;</div><div class="line">              "type" : "host-group",</div><div class="line">              "level" : "ERROR",</div><div class="line">              "message" : "NameNode and Secondary NameNode should not be hosted on the same machine",</div><div class="line">              "component-name" : "NAMENODE",</div><div class="line">              "host" : "c6401.ambari.apache.org"</div><div class="line">            &#125;,</div><div class="line">            ...</div><div class="line">          ]</div><div class="line">        &#125; </div><div class="line">    """</div><div class="line">    <span class="keyword">pass</span></div><div class="line"> </div><div class="line"> </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">recommendConfigurations</span><span class="params">(self, services, hosts)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Returns recommendation of service configurations based on host-specific layout of components.</div><div class="line"> </div><div class="line">    This function takes as input all details about services being installed, and hosts</div><div class="line">    they are being installed into, to recommend host-specific configurations.</div><div class="line"> </div><div class="line"> </div><div class="line">    @type services: dictionary</div><div class="line">    @param services: Dictionary containing all information about services and component layout selected by the user.</div><div class="line">    @type hosts: dictionary</div><div class="line">    @param hosts: Dictionary containing all information about hosts in this cluster</div><div class="line">    @rtype: dictionary</div><div class="line">    @return: Layout recommendation of service components on cluster hosts in Ambari Blueprints friendly format.</div><div class="line">        Example: &#123;</div><div class="line">         "services": [</div><div class="line">          "HIVE",</div><div class="line">          "TEZ",</div><div class="line">          "YARN"</div><div class="line">         ],</div><div class="line">         "recommendations": &#123;</div><div class="line">          "blueprint": &#123;</div><div class="line">           "host_groups": [],</div><div class="line">           "configurations": &#123;</div><div class="line">            "yarn-site": &#123;</div><div class="line">             "properties": &#123;</div><div class="line">              "yarn.scheduler.minimum-allocation-mb": "682",</div><div class="line">              "yarn.scheduler.maximum-allocation-mb": "2048",</div><div class="line">              "yarn.nodemanager.resource.memory-mb": "2048"</div><div class="line">             &#125;</div><div class="line">            &#125;,</div><div class="line">            "tez-site": &#123;</div><div class="line">             "properties": &#123;</div><div class="line">              "tez.am.java.opts": "-server -Xmx546m -Djava.net.preferIPv4Stack=true -XX:+UseNUMA -XX:+UseParallelGC",</div><div class="line">              "tez.am.resource.memory.mb": "682"</div><div class="line">             &#125;</div><div class="line">            &#125;,</div><div class="line">            "hive-site": &#123;</div><div class="line">             "properties": &#123;</div><div class="line">              "hive.tez.container.size": "682",</div><div class="line">              "hive.tez.java.opts": "-server -Xmx546m -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC",</div><div class="line">              "hive.auto.convert.join.noconditionaltask.size": "238026752"</div><div class="line">             &#125;</div><div class="line">            &#125;</div><div class="line">           &#125;</div><div class="line">          &#125;,</div><div class="line">          "blueprint_cluster_binding": &#123;</div><div class="line">           "host_groups": []</div><div class="line">          &#125;</div><div class="line">         &#125;,</div><div class="line">         "hosts": [</div><div class="line">          "c6401.ambari.apache.org",</div><div class="line">          "c6402.ambari.apache.org",</div><div class="line">          "c6403.ambari.apache.org"</div><div class="line">         ]</div><div class="line">        &#125;</div><div class="line">    """</div><div class="line">    <span class="keyword">pass</span></div><div class="line"> </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">validateConfigurations</span><span class="params">(self, services, hosts)</span>:</span></div><div class="line">    <span class="string">""""</span></div><div class="line">    Returns array of Validation issues with configurations provided by user</div><div class="line">    This function takes as input all details about services being installed along with</div><div class="line">    configuration values entered by the user. These configurations can be validated against</div><div class="line">    service requirements, or host hardware to generate validation issues.</div><div class="line"> </div><div class="line"> </div><div class="line">    @type services: dictionary</div><div class="line">    @param services: Dictionary containing information about services and user configurations.</div><div class="line">    @type hosts: dictionary</div><div class="line">    @param hosts: Dictionary containing all information about hosts in this cluster</div><div class="line">    @rtype: dictionary</div><div class="line">    @return: Dictionary containing array of validation items</div><div class="line">        Example: &#123;</div><div class="line">         "items": [</div><div class="line">          &#123;</div><div class="line">           "config-type": "yarn-site",</div><div class="line">           "message": "Value is less than the recommended default of 682",</div><div class="line">           "type": "configuration",</div><div class="line">           "config-name": "yarn.scheduler.minimum-allocation-mb",</div><div class="line">           "level": "WARN"</div><div class="line">          &#125;</div><div class="line">         ]</div><div class="line">       &#125;</div><div class="line">    """</div><div class="line">    <span class="keyword">pass</span></div></pre></td></tr></table></figure></p>
<h4 id="Examples-3"><a href="#Examples-3" class="headerlink" title="Examples:"></a>Examples:</h4><blockquote>
<p><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/stack_advisor.py#L23" title="Stack Advisor interface" target="_blank" rel="external">Stack Advisor interface</a><br><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/stack_advisor.py#L303" title="Default Stack Advisor implementation - for all stacks" target="_blank" rel="external">Default Stack Advisor implementation - for all stacks</a><br><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L28" title="HDP(2.0.6) Default Stack Advisor implementation" target="_blank" rel="external">HDP(2.0.6) Default Stack Advisor implementation</a><br><a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L807" title="YARN container size calculate" target="_blank" rel="external">YARN container size calculate</a><br>Recommended configurations - <a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L222" target="_blank" rel="external">HDFS</a>，<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L133" target="_blank" rel="external">YARN</a>，<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L148" target="_blank" rel="external">MapReduce2</a>, <a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L245" target="_blank" rel="external">HBase</a> (HDP-2.0.6)，<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.0.6/services/stack_advisor.py#L148" target="_blank" rel="external">HBase</a> (HDP-2.3)<br><a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.3/services/stack_advisor.py#L272" target="_blank" rel="external">Delete HBase Bucket Cache configs on smaller machines</a><br><a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.3/services/stack_advisor.py#L184" target="_blank" rel="external">Specify maximum value for Tez config</a></p>
</blockquote>
<h3 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h3><p>与stack的配置类似，大多属性都是在service级定义，然而也可以在stack-version级别定义全局属性来影响所有的services。<br>一些例子：<a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/properties/stack_tools.json#L2" target="_blank" rel="external">stack-selector and conf-selector</a> 或 <a href="https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/HDP/2.0.6/properties/stack_features.json#L5" target="_blank" rel="external">stack versions certain stack features</a>。这里的大多属性都是在Ambari 2.4版本引入的以影响stack信息参数化和促进common-service代码重用。<br>这些属性可以定义在stack的properties文件中的.json文件中。<br>stack属性的更多信息可以在<a href="https://cwiki.apache.org/confluence/x/pgPiAw" target="_blank" rel="external">Stack Properties section</a>找到。</p>
<h3 id="Widgets-1"><a href="#Widgets-1" class="headerlink" title="Widgets"></a>Widgets</h3><p>暂无</p>
<h3 id="Kerberos-1"><a href="#Kerberos-1" class="headerlink" title="Kerberos"></a>Kerberos</h3><p>之前我们已经在service级别介绍了Kerberos。<br>stack-version级别定义的Kerberos为所有的service提供了身份描述。</p>
<p>Examples：<a href="https://github.com/apache/ambari/blob/branch-2.2.1/ambari-server/src/main/resources/stacks/HDP/2.0.6/kerberos.json" target="_blank" rel="external">Smoke test user and SPNEGO user define in HDP-2.0.6</a></p>
<h3 id="Stack-Upgrades"><a href="#Stack-Upgrades" class="headerlink" title="Stack Upgrades"></a>Stack Upgrades</h3><p>暂无</p>
<h1 id="Writing-metainfo-xml"><a href="#Writing-metainfo-xml" class="headerlink" title="Writing metainfo.xml"></a>Writing metainfo.xml</h1><p>metainfo.xml是Ambari管理的service的定义，它描述了service的内容。它是service定义中最重要的文件。这一章来介绍metainfo.xml中的各个片段。</p>
<h2 id="Structure-1"><a href="#Structure-1" class="headerlink" title="Structure"></a>Structure</h2><p>不重要的字段使用斜体显示。<br>描述一个service的顶级字段如下：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Field</th>
<th style="text-align:left">What is it used for</th>
<th style="text-align:left">Sample Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>name</strong></td>
<td style="text-align:left">service的名字。这个名字必须是service所在Stack范围内唯一的。</td>
<td style="text-align:left">HDFS</td>
</tr>
<tr>
<td style="text-align:left"><strong>displayName</strong></td>
<td style="text-align:left">service在UI中显示的名字。</td>
<td style="text-align:left">HDFS</td>
</tr>
<tr>
<td style="text-align:left"><strong>version</strong></td>
<td style="text-align:left">service的版本。名字和版本一起确定了唯一的service。</td>
<td style="text-align:left">2.1.0.2.0</td>
</tr>
<tr>
<td style="text-align:left"><strong>components</strong></td>
<td style="text-align:left">service的组件列表</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><strong>osSpecifics</strong></td>
<td style="text-align:left">指定service运行所需的操作系统</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><em>commandScript</em></td>
<td style="text-align:left">定义service check脚本</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><em>comment</em></td>
<td style="text-align:left">service的简短描述</td>
<td style="text-align:left">Apache Hadoop Distributed File System</td>
</tr>
<tr>
<td style="text-align:left"><em>requiredServices</em></td>
<td style="text-align:left">该服务所需的前置服务</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><em>configuration-dependencies</em></td>
<td style="text-align:left">service所需的其他配置文件（这些配置文件本身属于其他service）</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><em>restartRequiredAfterRackChange</em></td>
<td style="text-align:left">Rack变更后是否必须重启</td>
<td style="text-align:left">true / false</td>
</tr>
<tr>
<td style="text-align:left"><em>configuration-dir</em></td>
<td style="text-align:left">如果配置目录不是默认的configuration，则需要使用该项来指定</td>
<td style="text-align:left">-</td>
</tr>
</tbody>
</table>
<h3 id="service-components-一个service包含多个components。与component有关的字段有："><a href="#service-components-一个service包含多个components。与component有关的字段有：" class="headerlink" title="service/components - 一个service包含多个components。与component有关的字段有："></a>service/components - 一个service包含多个components。与component有关的字段有：</h3><table>
<thead>
<tr>
<th style="text-align:left">Field</th>
<th style="text-align:left">What is it used it</th>
<th style="text-align:left">Sample Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>name</strong></td>
<td style="text-align:left">component的名字。</td>
<td style="text-align:left">NameNode</td>
</tr>
<tr>
<td style="text-align:left"><strong>dsplayName</strong></td>
<td style="text-align:left">component的显示名。</td>
<td style="text-align:left">NameNode</td>
</tr>
<tr>
<td style="text-align:left"><strong>category</strong></td>
<td style="text-align:left">component的类型。可选值为MASTER、SLAVE或CLIENT。</td>
<td style="text-align:left">-</td>
</tr>
<tr>
<td style="text-align:left"><strong>commandScript</strong></td>
<td style="text-align:left">应用的命令。</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><em>cardinality</em></td>
<td style="text-align:left">允许的实例个数。</td>
<td style="text-align:left">MASTER一般设置为1-2， SLAVE一般设置为1+</td>
</tr>
<tr>
<td style="text-align:left"><em>reassignAllowed</em></td>
<td style="text-align:left">是否允许component被重新分配或移动到另外的主机。</td>
<td style="text-align:left">true / false</td>
</tr>
<tr>
<td style="text-align:left"><em>versionAdvertised</em></td>
<td style="text-align:left">component是否显示它的版本信息。回滚/升级时使用。</td>
<td style="text-align:left">true / false</td>
</tr>
<tr>
<td style="text-align:left"><em>timelineAppid</em></td>
<td style="text-align:left">metrics收集时用来进行区分的id。</td>
<td style="text-align:left">HDFS</td>
</tr>
<tr>
<td style="text-align:left"><em>dependencies</em></td>
<td style="text-align:left">component所依赖的其他component列表。</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><em>customCommands</em></td>
<td style="text-align:left">组件的自定义命令，有别与标准命令。</td>
<td style="text-align:left">RESTART_LLAP (Check out HIVE metainfo)</td>
</tr>
</tbody>
</table>
<h3 id="service-osSpecifics-操作系统包的名"><a href="#service-osSpecifics-操作系统包的名" class="headerlink" title="service/osSpecifics - 操作系统包的名"></a>service/osSpecifics - 操作系统包的名</h3><table>
<thead>
<tr>
<th style="text-align:left">Field</th>
<th style="text-align:left">What is it used for</th>
<th style="text-align:left">Sample Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>osFamily</strong></td>
<td style="text-align:left">service对应的操作系统</td>
<td style="text-align:left">any =&gt; all， amazon2015、redhat6、debian7</td>
</tr>
<tr>
<td style="text-align:left"><strong>packages</strong></td>
<td style="text-align:left">部署这个service所需的packages列表</td>
<td style="text-align:left">&lt; check out HDFS metainfo&gt;</td>
</tr>
<tr>
<td style="text-align:left"><strong>package/name</strong></td>
<td style="text-align:left">package的名字(会被yum\apt等命令使用)</td>
<td style="text-align:left">如 hadoop-lzo</td>
</tr>
</tbody>
</table>
<h3 id="service-commandScript-service检查的脚本"><a href="#service-commandScript-service检查的脚本" class="headerlink" title="service/commandScript - service检查的脚本"></a>service/commandScript - service检查的脚本</h3><table>
<thead>
<tr>
<th style="text-align:left">Field</th>
<th style="text-align:left">What is it used for</th>
<th style="text-align:left">Sample Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>script</strong></td>
<td style="text-align:left">脚本的相对路径</td>
<td style="text-align:left">scripts/service_check.py</td>
</tr>
<tr>
<td style="text-align:left"><strong>scriptType</strong></td>
<td style="text-align:left">脚本的类型，当前纸支持PYTHON</td>
<td style="text-align:left">PYTHON</td>
</tr>
<tr>
<td style="text-align:left"><strong>timeout</strong></td>
<td style="text-align:left">命令的超时时间</td>
<td style="text-align:left">300</td>
</tr>
</tbody>
</table>
<h3 id="service-component-customCommand-component的自定义命令"><a href="#service-component-customCommand-component的自定义命令" class="headerlink" title="service/component/customCommand - component的自定义命令"></a>service/component/customCommand - component的自定义命令</h3><blockquote>
<p><strong>name:</strong> 自定义命令的名字<br><strong>commandScript:</strong> 实现自定义命令的脚本信息，它包含其他片段。<br><strong>commandScript/script:</strong> 脚本的相对路径<br><strong>commandScript/scriptType:</strong> 脚本的类型，目前只支持PYTHON。<br><strong>commandScript/timeout:</strong> 命令的超时时间。</p>
</blockquote>
<h2 id="Sample-metainfo-xml"><a href="#Sample-metainfo-xml" class="headerlink" title="Sample metainfo.xml"></a>Sample metainfo.xml</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">metainfo</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">schemaVersion</span>&gt;</span>2.0<span class="tag">&lt;/<span class="name">schemaVersion</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">services</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">service</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>HBASE<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">displayName</span>&gt;</span>HBase<span class="tag">&lt;/<span class="name">displayName</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">comment</span>&gt;</span>Non-relational distributed database and centralized service for configuration management &amp;amp;</div><div class="line"> synchronization</div><div class="line">      <span class="tag">&lt;/<span class="name">comment</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.96.0.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">components</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">component</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>HBASE_MASTER<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">displayName</span>&gt;</span>HBase Master<span class="tag">&lt;/<span class="name">displayName</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">category</span>&gt;</span>MASTER<span class="tag">&lt;/<span class="name">category</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">cardinality</span>&gt;</span>1+<span class="tag">&lt;/<span class="name">cardinality</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">versionAdvertised</span>&gt;</span>true<span class="tag">&lt;/<span class="name">versionAdvertised</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">timelineAppid</span>&gt;</span>HBASE<span class="tag">&lt;/<span class="name">timelineAppid</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>HDFS/HDFS_CLIENT<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">scope</span>&gt;</span>host<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">auto-deploy</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></div><div class="line">              <span class="tag">&lt;/<span class="name">auto-deploy</span>&gt;</span></div><div class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>ZOOKEEPER/ZOOKEEPER_SERVER<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">scope</span>&gt;</span>cluster<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">auto-deploy</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">co-locate</span>&gt;</span>HBASE/HBASE_MASTER<span class="tag">&lt;/<span class="name">co-locate</span>&gt;</span></div><div class="line">              <span class="tag">&lt;/<span class="name">auto-deploy</span>&gt;</span></div><div class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">commandScript</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined">scripts/hbase_master.py</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">scriptType</span>&gt;</span>PYTHON<span class="tag">&lt;/<span class="name">scriptType</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">timeout</span>&gt;</span>1200<span class="tag">&lt;/<span class="name">timeout</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">commandScript</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">customCommands</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">customCommand</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>DECOMMISSION<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">commandScript</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined">scripts/hbase_master.py</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">scriptType</span>&gt;</span>PYTHON<span class="tag">&lt;/<span class="name">scriptType</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">timeout</span>&gt;</span>600<span class="tag">&lt;/<span class="name">timeout</span>&gt;</span></div><div class="line">              <span class="tag">&lt;/<span class="name">commandScript</span>&gt;</span></div><div class="line">            <span class="tag">&lt;/<span class="name">customCommand</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">customCommands</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">component</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>HBASE_REGIONSERVER<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">displayName</span>&gt;</span>RegionServer<span class="tag">&lt;/<span class="name">displayName</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">category</span>&gt;</span>SLAVE<span class="tag">&lt;/<span class="name">category</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">cardinality</span>&gt;</span>1+<span class="tag">&lt;/<span class="name">cardinality</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">versionAdvertised</span>&gt;</span>true<span class="tag">&lt;/<span class="name">versionAdvertised</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">timelineAppid</span>&gt;</span>HBASE<span class="tag">&lt;/<span class="name">timelineAppid</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">commandScript</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined">scripts/hbase_regionserver.py</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">scriptType</span>&gt;</span>PYTHON<span class="tag">&lt;/<span class="name">scriptType</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">commandScript</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">component</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>HBASE_CLIENT<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">displayName</span>&gt;</span>HBase Client<span class="tag">&lt;/<span class="name">displayName</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">category</span>&gt;</span>CLIENT<span class="tag">&lt;/<span class="name">category</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">cardinality</span>&gt;</span>1+<span class="tag">&lt;/<span class="name">cardinality</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">versionAdvertised</span>&gt;</span>true<span class="tag">&lt;/<span class="name">versionAdvertised</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">commandScript</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined">scripts/hbase_client.py</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">scriptType</span>&gt;</span>PYTHON<span class="tag">&lt;/<span class="name">scriptType</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">commandScript</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">configFiles</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">configFile</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">type</span>&gt;</span>xml<span class="tag">&lt;/<span class="name">type</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">fileName</span>&gt;</span>hbase-site.xml<span class="tag">&lt;/<span class="name">fileName</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">dictionaryName</span>&gt;</span>hbase-site<span class="tag">&lt;/<span class="name">dictionaryName</span>&gt;</span></div><div class="line">            <span class="tag">&lt;/<span class="name">configFile</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">configFile</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">type</span>&gt;</span>env<span class="tag">&lt;/<span class="name">type</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">fileName</span>&gt;</span>hbase-env.sh<span class="tag">&lt;/<span class="name">fileName</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">dictionaryName</span>&gt;</span>hbase-env<span class="tag">&lt;/<span class="name">dictionaryName</span>&gt;</span></div><div class="line">            <span class="tag">&lt;/<span class="name">configFile</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">configFiles</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">component</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">components</span>&gt;</span></div><div class="line"></div><div class="line">      <span class="tag">&lt;<span class="name">osSpecifics</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">osSpecific</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">osFamily</span>&gt;</span>any<span class="tag">&lt;/<span class="name">osFamily</span>&gt;</span></div><div class="line">          <span class="tag">&lt;<span class="name">packages</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">package</span>&gt;</span></div><div class="line">              <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">            <span class="tag">&lt;/<span class="name">package</span>&gt;</span></div><div class="line">          <span class="tag">&lt;/<span class="name">packages</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">osSpecific</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">osSpecifics</span>&gt;</span></div><div class="line"></div><div class="line">      <span class="tag">&lt;<span class="name">commandScript</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined">scripts/service_check.py</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">scriptType</span>&gt;</span>PYTHON<span class="tag">&lt;/<span class="name">scriptType</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">timeout</span>&gt;</span>300<span class="tag">&lt;/<span class="name">timeout</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">commandScript</span>&gt;</span></div><div class="line">      </div><div class="line">      <span class="tag">&lt;<span class="name">requiredServices</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">service</span>&gt;</span>ZOOKEEPER<span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">service</span>&gt;</span>HDFS<span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">requiredServices</span>&gt;</span></div><div class="line"></div><div class="line">      <span class="tag">&lt;<span class="name">configuration-dependencies</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">config-type</span>&gt;</span>core-site<span class="tag">&lt;/<span class="name">config-type</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">config-type</span>&gt;</span>hbase-site<span class="tag">&lt;/<span class="name">config-type</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">config-type</span>&gt;</span>ranger-hbase-policymgr-ssl<span class="tag">&lt;/<span class="name">config-type</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">config-type</span>&gt;</span>ranger-hbase-security<span class="tag">&lt;/<span class="name">config-type</span>&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">configuration-dependencies</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">service</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">services</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">metainfo</span>&gt;</span></div></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/13/ambari/" itemprop="url">
                  Ambari
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-13T10:42:44+08:00" content="2018-08-13">
              2018-08-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">hadoop</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文用来记录Ambari的学习</p>
<h1 id="Ambari的简单介绍"><a href="#Ambari的简单介绍" class="headerlink" title="Ambari的简单介绍"></a>Ambari的简单介绍</h1><p>从Ambari的作用来说，它是用来创建、管理、监控Hadoop生态（例如hadoop、hive、hbase、Sqoop以及Zookeeper）集群的工具。Ambari就是为了让Hadoop已经相关的大数据软件更容易使用的一个工具。Ambari支持的平台组建也越来越多，如流行的Spark、Storm等计算框架，已经资源调度平台YARN等，都可以通过Ambari来轻松部署。</p>
<p>Ambari自称也是一个分布式架构的软件，主要由两部分组成：Ambari Server和Ambari Agent。用户通过Ambari Server来通知Ambari Agent来安装对应的软件；Agent会定时的发送各个机器每个软件模块的状态给Ambari Server，最终这些信息会呈现在Ambari的GUI中，方便用户了解集群中各个模块的状态，并进行维护。</p>
<h1 id="Ambari的架构和工作原理"><a href="#Ambari的架构和工作原理" class="headerlink" title="Ambari的架构和工作原理"></a>Ambari的架构和工作原理</h1><p>Ambari Server会读取Stack和Service的配置文件。当用Ambari创建集群的时候，Ambari Server传送Stack和Service的配置文件配以及Service生命周期的控制脚本到Ambari Agent。Agent拿到配置文件后，会下载安装公共资源里的软件包。安装完成后，Ambari Server会通知Agent去启动Service。之后，Ambari Server会定时发送命令道Agent检查Service的状态，Agent上报给Server并显示在Ambari的UI上。<br>Ambari Server支持其他API，这样能够很容易的扩展或定制Ambari。<br>如果有安全方面的要求，Ambari支持Kerberos认证的hadoop集群。</p>
<blockquote>
<p>Ambari web：用户交互界面，通过HTTP发送使用Rest API与Ambari Server进行交互。<br>Ambari Server：Ambari服务器，用于和Web、Agent进行交互，并且包含了Agent的所有控制逻辑，Server产生的数据存储在DB中。<br>Ambari Agent：守护进程，主要包含节点状态与执行结果信息汇报给Server，以及接受Server操作命令的两个消息队列。<br>Host：安装实际大数据服务组件的物理机器，每台机器都有Ambari Agent和Metrcis Monitor守护进程服务。<br>Metrics Collector：主要包括将Metrics monitor汇报的监控信息存储到Hbase，以及提供给Ambari Server的查询接口。</p>
</blockquote>
<h1 id="Ambari的自定义命令"><a href="#Ambari的自定义命令" class="headerlink" title="Ambari的自定义命令"></a>Ambari的自定义命令</h1><p>在Ambari的Stack中，每个Service都有start、stop、status和configure这样的命令，我们称为生命周期的控制命令。Service的每个模块必须实现这几个命令。为了让用户可以更好的控制每个service以及模块，Ambari支持了自定义命令。<br>具体的自定义命令配置在每个Service的metainfo.xml中。不过不同的模块类型，呈现在GUI的方式是不一样的。当一个service的Master模块增加一个自定义命令时，该命令会显示在该Service的Service Action List中。如果点击这个命令，Ambari Server就会通知Master所在机器的Agent，Agent就会执行该自定义命令的逻辑。当增加一个自定义命令给Slave或Client类型的Component，该命令会呈现在机器的Component页面。在哪个机器的Component页面点击该命令，Ambari Server就会通知对应机器的Agent调用这个自定义的命令接口。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/10/spark-2-3-1-SparkSQL-DataFramesAndDatasetsGuide/" itemprop="url">
                  Spark 2.3.1 Spark SQL DataFrames and DatasetsGuide
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-10T15:21:30+08:00" content="2018-08-10">
              2018-08-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Spark-SQL-DataFrames-and-Dataset-Guide"><a href="#Spark-SQL-DataFrames-and-Dataset-Guide" class="headerlink" title="Spark SQL, DataFrames and Dataset Guide"></a>Spark SQL, DataFrames and Dataset Guide</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Spark SQL是一个用于结构化数据处理的Spark模块。与Spark RDD API不同，由Spark SQL提供的这些接口在结构化数据和结构化计算执行方面提供了更多信息。在内部，Spark SQL使用了这个额外信息来执行额外的优化。有几种与Spark SQL交互的方法，包括SQL和Dataset API。当计算一个结果时，相同的计算引擎会被使用，与你执行计算使用的API／语言无关。这种统一意味着开发者能够轻松在那些提供更加原始的方式处理给定转换的不同API之间进行来回切换。<br>本篇中所有例子使用的样例数据包含在Spark中，并能够使用spark-shell、pyspark shell或sparkR shell来运行。</p>
<h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p>Spark SQL的一种用法时执行SQL查询。Spark SQL还能够被用来从Hive实例中读取数据。关于如何配置这个特性，请参考<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables" title="Hive Tables " target="_blank" rel="external">Hive Tables</a>。当在另一种编程语言中执行SQL时，结果会作为一个Dataset/DataFrame来返回。你还能够使用<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-spark-sql-cli" title="command-line" target="_blank" rel="external">command-line</a>或<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server" title="JDBC/ODBC" target="_blank" rel="external">JDBC/ODBC</a>的方式与SQL接口进行交互。</p>
<h3 id="Datasets-and-DataFrames"><a href="#Datasets-and-DataFrames" class="headerlink" title="Datasets and DataFrames"></a>Datasets and DataFrames</h3><p>一个Dataset就是一个分布式数据集。Dataset作为一个新接口在Spark 1.6中被添加，它提供了RDD的优点（强类型、能够使用强大的lambda函数）和Spark SQL的优化执行引擎的有点。一个Dataset能够根据JVM对象来构造，然后使用函数转换（map、flatMap、filter）进行操作。Dataset的API在Scala和Java中时可用的。Python还不支持Dataset API。但是因为Python的动态特性，Dataset API的很多优点已经可用了（例如你可以很自然的通过名称来访问某一行的一个字段 row.columnName）。对于R语言也是如此。<br>一个DataFrame是一个带有列名的数据集。它在概念上等同于关系数据库中的一个表或者一个是R语言或Python语言中data frame，但是底层具更加优化。DataFrame可以根据各种资源进行构建，例如：结构化的数据文件、Hive中的表、外部数据库以及已经存在的RDD。DataFrame API在Scala、Java、Python和R语言中都可用。在Scala和Java中，一个DataFrame相当于一个有很多行的Dataset。在<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" title="Scala API" target="_blank" rel="external">Scala API</a>中，DataFrame相当于一个Dataset[Row]类型。而在<a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html" title="Java API" target="_blank" rel="external">Java API</a>中，用户需要使用Dataset<row>来表述一个DataFrame。<br>在本文中，我们将经常引用Scala/Java由有Row组成的Dataset来表述DataFrame。</row></p>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><h3 id="Starting-Point-SparkSession"><a href="#Starting-Point-SparkSession" class="headerlink" title="Starting Point: SparkSession"></a>Starting Point: SparkSession</h3><p>Spark中，所有功能的切入点是<a href="http://spark.apache.org/docs/latest/api/java/index.html#org.apache.spark.sql.SparkSession" title="SparkSession" target="_blank" rel="external">SparkSession</a>类。要创建一个基本的SparkSession，只需要使用SparkSession.builder()<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"></div><div class="line">SparkSession spark = SparkSession</div><div class="line">  .builder()</div><div class="line">  .appName(<span class="string">"Java Spark SQL basic example"</span>)</div><div class="line">  .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</div><div class="line">  .getOrCreate();</div></pre></td></tr></table></figure></p>
<p>在Spark库的“examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java”目录下，查看完整的示例代码。<br>SparkSession是Spark 2.0的内置功能，用于提供Hive特性，包括用来写HiveQL查询、<br>访问Hive UDFs已经从Hive表中读取数据。要使用这些特性，你不需要配置Hive。</p>
<h3 id="Creating-DataFrames"><a href="#Creating-DataFrames" class="headerlink" title="Creating DataFrames"></a>Creating DataFrames</h3><p>使用SparkSession，application能够从一个已经存在的RDD、一个Hive表或<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources" title="Spark data sources" target="_blank" rel="external">Spark data sources</a>来创建DataFrame。<br>作为一个例子，下面的代码根据一个JSON文件中的内容来创建一个DataFrame：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></div><div class="line">df.show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看Spark库中的“examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java”。</p>
<h3 id="Untyped-Dataset-Operations-aka-DataFrame-Operations"><a href="#Untyped-Dataset-Operations-aka-DataFrame-Operations" class="headerlink" title="Untyped Dataset Operations(aka DataFrame Operations)"></a>Untyped Dataset Operations(aka DataFrame Operations)</h3><p>在Scala、Java、Python和R语言中，DataFrames针对不同的语言提供不同的结构化数据操作。正如上面提到的，在Spark2.0中，在Scala和Java的API中，DataFrames是以Dataset<row>来表述的。这些操作也被称为“无类型转换”，与强类型转换的Scala/Java Dataset的类型形成对比。<br>这里，我们展示了使用Dataset进行结构化数据处理的基本示例：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// col("...") is preferable to df.col("...")</span></div><div class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.spark.sql.functions.col;</div><div class="line"></div><div class="line"><span class="comment">// Print the schema in a tree format</span></div><div class="line">df.printSchema();</div><div class="line"><span class="comment">// root</span></div><div class="line"><span class="comment">// |-- age: long (nullable = true)</span></div><div class="line"><span class="comment">// |-- name: string (nullable = true)</span></div><div class="line"></div><div class="line"><span class="comment">// Select only the "name" column</span></div><div class="line">df.select(<span class="string">"name"</span>).show();</div><div class="line"><span class="comment">// +-------+</span></div><div class="line"><span class="comment">// |   name|</span></div><div class="line"><span class="comment">// +-------+</span></div><div class="line"><span class="comment">// |Michael|</span></div><div class="line"><span class="comment">// |   Andy|</span></div><div class="line"><span class="comment">// | Justin|</span></div><div class="line"><span class="comment">// +-------+</span></div><div class="line"></div><div class="line"><span class="comment">// Select everybody, but increment the age by 1</span></div><div class="line">df.select(col(<span class="string">"name"</span>), col(<span class="string">"age"</span>).plus(<span class="number">1</span>)).show();</div><div class="line"><span class="comment">// +-------+---------+</span></div><div class="line"><span class="comment">// |   name|(age + 1)|</span></div><div class="line"><span class="comment">// +-------+---------+</span></div><div class="line"><span class="comment">// |Michael|     null|</span></div><div class="line"><span class="comment">// |   Andy|       31|</span></div><div class="line"><span class="comment">// | Justin|       20|</span></div><div class="line"><span class="comment">// +-------+---------+</span></div><div class="line"></div><div class="line"><span class="comment">// Select people older than 21</span></div><div class="line">df.filter(col(<span class="string">"age"</span>).gt(<span class="number">21</span>)).show();</div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// |age|name|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// | 30|Andy|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"></div><div class="line"><span class="comment">// Count people by age</span></div><div class="line">df.groupBy(<span class="string">"age"</span>).count().show();</div><div class="line"><span class="comment">// +----+-----+</span></div><div class="line"><span class="comment">// | age|count|</span></div><div class="line"><span class="comment">// +----+-----+</span></div><div class="line"><span class="comment">// |  19|    1|</span></div><div class="line"><span class="comment">// |null|    1|</span></div><div class="line"><span class="comment">// |  30|    1|</span></div><div class="line"><span class="comment">// +----+-----+</span></div></pre></td></tr></table></figure></row></p>
<p>完整的样例代码，查看Spark库的examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java。<br>在Dataset上能够执行的操作类型列表，可以查看<a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html" title="API Document" target="_blank" rel="external">API Document</a>。<br>除了简单的列引用和计算外，Dataset还有一个丰富的函数库，包括字符串的操作、日期的计算以及常用的数学操作等。完整的列表可以在<a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html" title="DataFrame Function Reference" target="_blank" rel="external">DataFrame Function Reference</a>找到。</p>
<h3 id="Running-SQL-Queries-Programmatically"><a href="#Running-SQL-Queries-Programmatically" class="headerlink" title="Running SQL Queries Programmatically"></a>Running SQL Queries Programmatically</h3><p>SparkSession上的sql函数使application能够执行SQL查询，并返回一个Dataset<row>作为结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">import org.apache.spark.sql.Dataset;</div><div class="line">import org.apache.spark.sql.Row;</div><div class="line"></div><div class="line">// Register the DataFrame as a SQL temporary view</div><div class="line">df.createOrReplaceTempView(&quot;people&quot;);</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(&quot;SELECT * FROM people&quot;);</div><div class="line">sqlDF.show();</div><div class="line">// +----+-------+</div><div class="line">// | age|   name|</div><div class="line">// +----+-------+</div><div class="line">// |null|Michael|</div><div class="line">// |  30|   Andy|</div><div class="line">// |  19| Justin|</div><div class="line">// +----+-------+</div></pre></td></tr></table></figure></row></p>
<p>完整的代码，请查看Spark库中的 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h3 id="Global-Temporary-View"><a href="#Global-Temporary-View" class="headerlink" title="Global Temporary View"></a>Global Temporary View</h3><p>在Spark SQL中，临时视图是session范围的，将会伴随着创建它的那个session的终止而消失。如果你想要跨session共享一个临时视图，并让它存活到application终止，你可以创建一个全局临时视图。全局视图与一个名为‘global_temp’的由系统保护的数据库进行绑定，我们必须使用这个特殊的名字来引用它，如：SELECT * FROM global_temp.view1。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Register the DataFrame as a global temporary view</span></div><div class="line">df.createGlobalTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Global temporary view is tied to a system preserved database `global_temp`</span></div><div class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"></div><div class="line"><span class="comment">// Global temporary view is cross-session</span></div><div class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看“examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java”。</p>
<h3 id="Creating-Datasets"><a href="#Creating-Datasets" class="headerlink" title="Creating Datasets"></a>Creating Datasets</h3><p>Dataset与RDD类似，不同的是它没有使用Java序列化或Kryo，它们使用了一个特殊的<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder" title="Encoder" target="_blank" rel="external">Encoder</a>来序列化对象，以便这些对象的处理或跨网络传输。虽然encoder和标准序列化器都能够将一个对象转换为字节，encoder是动态编码产生的，并且使用一种格式来允许Spark执行很多操作(filtering， sorting 和 hashing)，而不需要讲字节反编译为对象。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.Arrays;</div><div class="line"><span class="keyword">import</span> java.util.Collections;</div><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> String name;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> age;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> name;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.name = name;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> age;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.age = age;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Create an instance of a Bean class</span></div><div class="line">Person person = <span class="keyword">new</span> Person();</div><div class="line">person.setName(<span class="string">"Andy"</span>);</div><div class="line">person.setAge(<span class="number">32</span>);</div><div class="line"></div><div class="line"><span class="comment">// Encoders are created for Java beans</span></div><div class="line">Encoder&lt;Person&gt; personEncoder = Encoders.bean(Person.class);</div><div class="line">Dataset&lt;Person&gt; javaBeanDS = spark.createDataset(</div><div class="line">  Collections.singletonList(person),</div><div class="line">  personEncoder</div><div class="line">);</div><div class="line">javaBeanDS.show();</div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// |age|name|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"><span class="comment">// | 32|Andy|</span></div><div class="line"><span class="comment">// +---+----+</span></div><div class="line"></div><div class="line"><span class="comment">// Encoders for most common types are provided in class Encoders</span></div><div class="line">Encoder&lt;Integer&gt; integerEncoder = Encoders.INT();</div><div class="line">Dataset&lt;Integer&gt; primitiveDS = spark.createDataset(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), integerEncoder);</div><div class="line">Dataset&lt;Integer&gt; transformedDS = primitiveDS.map(</div><div class="line">    (MapFunction&lt;Integer, Integer&gt;) value -&gt; value + <span class="number">1</span>,</div><div class="line">    integerEncoder);</div><div class="line">transformedDS.collect(); <span class="comment">// Returns [2, 3, 4]</span></div><div class="line"></div><div class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping based on name</span></div><div class="line">String path = <span class="string">"examples/src/main/resources/people.json"</span>;</div><div class="line">Dataset&lt;Person&gt; peopleDS = spark.read().json(path).as(personEncoder);</div><div class="line">peopleDS.show();</div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// | age|   name|</span></div><div class="line"><span class="comment">// +----+-------+</span></div><div class="line"><span class="comment">// |null|Michael|</span></div><div class="line"><span class="comment">// |  30|   Andy|</span></div><div class="line"><span class="comment">// |  19| Justin|</span></div><div class="line"><span class="comment">// +----+-------+</span></div></pre></td></tr></table></figure></p>
<p>完整的示例，请查看 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h3 id="Interoperating-with-RDDs"><a href="#Interoperating-with-RDDs" class="headerlink" title="Interoperating with RDDs"></a>Interoperating with RDDs</h3><p>Spark SQL支持两种不同方法来将存在的RDD转换为Dataset。第一种方法是使用反射来推导包含特殊类型对象的RDD的模式。这种反射的方法代码更加简单，而且如果在你写Spark application时已经知道了模式时，工作的会很好。<br>第二种方法是通过一个程序接口来创建Dataset，这个程序接口允许你构建一个模式，并且将它应用到一个已经存在的RDD上。但是这个方法比较冗长，它允许你只有在运行时才知道列和列类型时来构造Dataset。</p>
<h4 id="Inferring-the-Schema-Using-Reflection"><a href="#Inferring-the-Schema-Using-Reflection" class="headerlink" title="Inferring the Schema Using Reflection"></a>Inferring the Schema Using Reflection</h4><p>Spark SQL支持自动将一个JavaBean的RDD转换为一个DataFrame。BeanInfo使用反射机制获得，定义了表的模式。当前，Spark SQL不支持那些包含了Map类型字段的JavaBean，但是对于嵌套的JavaBean以及嵌套了List或Array类型的字段给予了充分的支持。你可以通过创建一个实现了Serializable接口以及为所有字段生成getter和setter方法的类来创建一个JavaBean。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"></div><div class="line"><span class="comment">// Create an RDD of Person objects from a text file</span></div><div class="line">JavaRDD&lt;Person&gt; peopleRDD = spark.read()</div><div class="line">  .textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</div><div class="line">  .javaRDD()</div><div class="line">  .map(line -&gt; &#123;</div><div class="line">    String[] parts = line.split(<span class="string">","</span>);</div><div class="line">    Person person = <span class="keyword">new</span> Person();</div><div class="line">    person.setName(parts[<span class="number">0</span>]);</div><div class="line">    person.setAge(Integer.parseInt(parts[<span class="number">1</span>].trim()));</div><div class="line">    <span class="keyword">return</span> person;</div><div class="line">  &#125;);</div><div class="line"></div><div class="line"><span class="comment">// Apply a schema to an RDD of JavaBeans to get a DataFrame</span></div><div class="line">Dataset&lt;Row&gt; peopleDF = spark.createDataFrame(peopleRDD, Person.class);</div><div class="line"><span class="comment">// Register the DataFrame as a temporary view</span></div><div class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></div><div class="line">Dataset&lt;Row&gt; teenagersDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The columns of a row in the result can be accessed by field index</span></div><div class="line">Encoder&lt;String&gt; stringEncoder = Encoders.STRING();</div><div class="line">Dataset&lt;String&gt; teenagerNamesByIndexDF = teenagersDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</div><div class="line">    stringEncoder);</div><div class="line">teenagerNamesByIndexDF.show();</div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |       value|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |Name: Justin|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"></div><div class="line"><span class="comment">// or by field name</span></div><div class="line">Dataset&lt;String&gt; teenagerNamesByFieldDF = teenagersDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.&lt;String&gt;getAs(<span class="string">"name"</span>),</div><div class="line">    stringEncoder);</div><div class="line">teenagerNamesByFieldDF.show();</div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |       value|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |Name: Justin|</span></div><div class="line"><span class="comment">// +------------+</span></div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h4 id="Programmatically-Specifying-the-Schema"><a href="#Programmatically-Specifying-the-Schema" class="headerlink" title="Programmatically Specifying the Schema"></a>Programmatically Specifying the Schema</h4><p>当JavaBean无法提前定义时（例如，记录的结构被编码为一个字符串，或者一个文本数据集将被解析，但是其中的字段可能根据不同的用户而不一样），Dataset<row>能够通过三个步骤来创建。</row></p>
<blockquote>
<p>1、根据原生的RDD创建一个RDD<row>。<br>2、创建一个与第一步骤RDD中Row结构匹配的StructType来描述的模式。<br>3、通过由SparkSession提供的createDataFrame方法，将这个模式应用到RDD<row>。</row></row></p>
</blockquote>
<p>例如：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</div><div class="line"></div><div class="line"><span class="comment">// Create an RDD</span></div><div class="line">JavaRDD&lt;String&gt; peopleRDD = spark.sparkContext()</div><div class="line">  .textFile(<span class="string">"examples/src/main/resources/people.txt"</span>, <span class="number">1</span>)</div><div class="line">  .toJavaRDD();</div><div class="line"></div><div class="line"><span class="comment">// The schema is encoded in a string</span></div><div class="line">String schemaString = <span class="string">"name age"</span>;</div><div class="line"></div><div class="line"><span class="comment">// Generate the schema based on the string of schema</span></div><div class="line">List&lt;StructField&gt; fields = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (String fieldName : schemaString.split(<span class="string">" "</span>)) &#123;</div><div class="line">  StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, <span class="keyword">true</span>);</div><div class="line">  fields.add(field);</div><div class="line">&#125;</div><div class="line">StructType schema = DataTypes.createStructType(fields);</div><div class="line"></div><div class="line"><span class="comment">// Convert records of the RDD (people) to Rows</span></div><div class="line">JavaRDD&lt;Row&gt; rowRDD = peopleRDD.map((Function&lt;String, Row&gt;) record -&gt; &#123;</div><div class="line">  String[] attributes = record.split(<span class="string">","</span>);</div><div class="line">  <span class="keyword">return</span> RowFactory.create(attributes[<span class="number">0</span>], attributes[<span class="number">1</span>].trim());</div><div class="line">&#125;);</div><div class="line"></div><div class="line"><span class="comment">// Apply the schema to the RDD</span></div><div class="line">Dataset&lt;Row&gt; peopleDataFrame = spark.createDataFrame(rowRDD, schema);</div><div class="line"></div><div class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></div><div class="line">peopleDataFrame.createOrReplaceTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// SQL can be run over a temporary view created using DataFrames</span></div><div class="line">Dataset&lt;Row&gt; results = spark.sql(<span class="string">"SELECT name FROM people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations</span></div><div class="line"><span class="comment">// The columns of a row in the result can be accessed by field index or by field name</span></div><div class="line">Dataset&lt;String&gt; namesDS = results.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</div><div class="line">    Encoders.STRING());</div><div class="line">namesDS.show();</div><div class="line"><span class="comment">// +-------------+</span></div><div class="line"><span class="comment">// |        value|</span></div><div class="line"><span class="comment">// +-------------+</span></div><div class="line"><span class="comment">// |Name: Michael|</span></div><div class="line"><span class="comment">// |   Name: Andy|</span></div><div class="line"><span class="comment">// | Name: Justin|</span></div><div class="line"><span class="comment">// +-------------+</span></div></pre></td></tr></table></figure></p>
<p>完整的示例，请查看 examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java 。</p>
<h3 id="Aggregations"><a href="#Aggregations" class="headerlink" title="Aggregations"></a>Aggregations</h3><p>内置的DataFrame函数提供了常用的聚合操作，如count()、countDistinct()、avg()、max()、min()等。然而这些函数是为了DataFrame设计的，Spark SQL同样由类型安全的版本，以便其中一些被用到Scala和Java的强类型Dataset。此外，Spark没有限制用户预定义聚合函数，可以自己来创建聚合函数。</p>
<h4 id="Untyped-User-Defined-Aggregate-Functions"><a href="#Untyped-User-Defined-Aggregate-Functions" class="headerlink" title="Untyped User-Defined Aggregate Functions"></a>Untyped User-Defined Aggregate Functions</h4><p>用户要实现无类型聚合函数，则需要继承<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.UserDefinedAggregateFunction" title="UserDefinedAggregateFunction" target="_blank" rel="external">UserDefinedAggregateFunction</a>抽象类。例如，你一个用户自定义的平均数函数，看起来像这样：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.MutableAggregationBuffer;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.UserDefinedAggregateFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataType;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">private</span> StructType inputSchema;</div><div class="line">  <span class="keyword">private</span> StructType bufferSchema;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="title">MyAverage</span><span class="params">()</span> </span>&#123;</div><div class="line">    List&lt;StructField&gt; inputFields = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">    inputFields.add(DataTypes.createStructField(<span class="string">"inputColumn"</span>, DataTypes.LongType, <span class="keyword">true</span>));</div><div class="line">    inputSchema = DataTypes.createStructType(inputFields);</div><div class="line"></div><div class="line">    List&lt;StructField&gt; bufferFields = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">    bufferFields.add(DataTypes.createStructField(<span class="string">"sum"</span>, DataTypes.LongType, <span class="keyword">true</span>));</div><div class="line">    bufferFields.add(DataTypes.createStructField(<span class="string">"count"</span>, DataTypes.LongType, <span class="keyword">true</span>));</div><div class="line">    bufferSchema = DataTypes.createStructType(bufferFields);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Data types of input arguments of this aggregate function</span></div><div class="line">  <span class="function"><span class="keyword">public</span> StructType <span class="title">inputSchema</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> inputSchema;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Data types of values in the aggregation buffer</span></div><div class="line">  <span class="function"><span class="keyword">public</span> StructType <span class="title">bufferSchema</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> bufferSchema;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// The data type of the returned value</span></div><div class="line">  <span class="function"><span class="keyword">public</span> DataType <span class="title">dataType</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> DataTypes.DoubleType;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Whether this function always returns the same output on the identical input</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">deterministic</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to</span></div><div class="line">  <span class="comment">// standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides</span></div><div class="line">  <span class="comment">// the opportunity to update its values. Note that arrays and maps inside the buffer are still</span></div><div class="line">  <span class="comment">// immutable.</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(MutableAggregationBuffer buffer)</span> </span>&#123;</div><div class="line">    buffer.update(<span class="number">0</span>, <span class="number">0L</span>);</div><div class="line">    buffer.update(<span class="number">1</span>, <span class="number">0L</span>);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Updates the given aggregation buffer `buffer` with new input data from `input`</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">(MutableAggregationBuffer buffer, Row input)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</div><div class="line">      <span class="keyword">long</span> updatedSum = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>);</div><div class="line">      <span class="keyword">long</span> updatedCount = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span>;</div><div class="line">      buffer.update(<span class="number">0</span>, updatedSum);</div><div class="line">      buffer.update(<span class="number">1</span>, updatedCount);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Merges two aggregation buffers and stores the updated buffer values back to `buffer1`</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(MutableAggregationBuffer buffer1, Row buffer2)</span> </span>&#123;</div><div class="line">    <span class="keyword">long</span> mergedSum = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>);</div><div class="line">    <span class="keyword">long</span> mergedCount = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>);</div><div class="line">    buffer1.update(<span class="number">0</span>, mergedSum);</div><div class="line">    buffer1.update(<span class="number">1</span>, mergedCount);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Calculates the final result</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">evaluate</span><span class="params">(Row buffer)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) buffer.getLong(<span class="number">0</span>)) / buffer.getLong(<span class="number">1</span>);</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Register the function to access it</span></div><div class="line">spark.udf().register(<span class="string">"myAverage"</span>, <span class="keyword">new</span> MyAverage());</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">"examples/src/main/resources/employees.json"</span>);</div><div class="line">df.createOrReplaceTempView(<span class="string">"employees"</span>);</div><div class="line">df.show();</div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |   name|salary|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |Michael|  3000|</span></div><div class="line"><span class="comment">// |   Andy|  4500|</span></div><div class="line"><span class="comment">// | Justin|  3500|</span></div><div class="line"><span class="comment">// |  Berta|  4000|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"></div><div class="line">Dataset&lt;Row&gt; result = spark.sql(<span class="string">"SELECT myAverage(salary) as average_salary FROM employees"</span>);</div><div class="line">result.show();</div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |average_salary|</span></div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |        3750.0|</span></div><div class="line"><span class="comment">// +--------------+</span></div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考 examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java 。</p>
<h4 id="Type-Safe-User-Defined-Aggregate-Functions"><a href="#Type-Safe-User-Defined-Aggregate-Functions" class="headerlink" title="Type-Safe User-Defined Aggregate Functions"></a>Type-Safe User-Defined Aggregate Functions</h4><p>强类型Dataset的用户自定义聚合围绕着<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator ‘Aggregator’" target="_blank" rel="external">Aggregator</a>抽象类来解决。例如，一个类型安全的用户自定义平均数看起来是这样：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.TypedColumn;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.Aggregator;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> String name;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">long</span> salary;</div><div class="line"></div><div class="line">  <span class="comment">// Constructors, getters, setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Average</span> <span class="keyword">implements</span> <span class="title">Serializable</span>  </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">long</span> sum;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">long</span> count;</div><div class="line"></div><div class="line">  <span class="comment">// Constructors, getters, setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>&lt;<span class="title">Employee</span>, <span class="title">Average</span>, <span class="title">Double</span>&gt; </span>&#123;</div><div class="line">  <span class="comment">// A zero value for this aggregation. Should satisfy the property that any b + zero = b</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">zero</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Average(<span class="number">0L</span>, <span class="number">0L</span>);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span></div><div class="line">  <span class="comment">// and return it instead of constructing a new object</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">reduce</span><span class="params">(Average buffer, Employee employee)</span> </span>&#123;</div><div class="line">    <span class="keyword">long</span> newSum = buffer.getSum() + employee.getSalary();</div><div class="line">    <span class="keyword">long</span> newCount = buffer.getCount() + <span class="number">1</span>;</div><div class="line">    buffer.setSum(newSum);</div><div class="line">    buffer.setCount(newCount);</div><div class="line">    <span class="keyword">return</span> buffer;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Merge two intermediate values</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Average <span class="title">merge</span><span class="params">(Average b1, Average b2)</span> </span>&#123;</div><div class="line">    <span class="keyword">long</span> mergedSum = b1.getSum() + b2.getSum();</div><div class="line">    <span class="keyword">long</span> mergedCount = b1.getCount() + b2.getCount();</div><div class="line">    b1.setSum(mergedSum);</div><div class="line">    b1.setCount(mergedCount);</div><div class="line">    <span class="keyword">return</span> b1;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Transform the output of the reduction</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">finish</span><span class="params">(Average reduction)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) reduction.getSum()) / reduction.getCount();</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Specifies the Encoder for the intermediate value type</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Encoder&lt;Average&gt; <span class="title">bufferEncoder</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> Encoders.bean(Average.class);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Specifies the Encoder for the final output value type</span></div><div class="line">  <span class="function"><span class="keyword">public</span> Encoder&lt;Double&gt; <span class="title">outputEncoder</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> Encoders.DOUBLE();</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">Encoder&lt;Employee&gt; employeeEncoder = Encoders.bean(Employee.class);</div><div class="line">String path = <span class="string">"examples/src/main/resources/employees.json"</span>;</div><div class="line">Dataset&lt;Employee&gt; ds = spark.read().json(path).as(employeeEncoder);</div><div class="line">ds.show();</div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |   name|salary|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"><span class="comment">// |Michael|  3000|</span></div><div class="line"><span class="comment">// |   Andy|  4500|</span></div><div class="line"><span class="comment">// | Justin|  3500|</span></div><div class="line"><span class="comment">// |  Berta|  4000|</span></div><div class="line"><span class="comment">// +-------+------+</span></div><div class="line"></div><div class="line">MyAverage myAverage = <span class="keyword">new</span> MyAverage();</div><div class="line"><span class="comment">// Convert the function to a `TypedColumn` and give it a name</span></div><div class="line">TypedColumn&lt;Employee, Double&gt; averageSalary = myAverage.toColumn().name(<span class="string">"average_salary"</span>);</div><div class="line">Dataset&lt;Double&gt; result = ds.select(averageSalary);</div><div class="line">result.show();</div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |average_salary|</span></div><div class="line"><span class="comment">// +--------------+</span></div><div class="line"><span class="comment">// |        3750.0|</span></div><div class="line"><span class="comment">// +--------------+</span></div></pre></td></tr></table></figure></p>
<p>完整的示例，请看 examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java 。</p>
<h2 id="Data-Sources"><a href="#Data-Sources" class="headerlink" title="Data Sources"></a>Data Sources</h2><p>Spark SQL通过DataFrame接口支持多种数据源的操作。DataFrame能够使用关系转换进行操作，也可以被用来创建一个临时视图。将DataFrame注册为一个临时视图，将允许你在视图的数据上运行SQL查询。这一章节描述了使用Spark Data Sources加载和保存数据的一般方法，然后介绍内置数据源可用的详细参数。</p>
<h3 id="Generic-Load-Save-Functions"><a href="#Generic-Load-Save-Functions" class="headerlink" title="Generic Load/Save Functions"></a>Generic Load/Save Functions</h3><p>最简单的格式，默认数据源（默认是parquet， 除非通过spark.sql.soiurces.default配置修改过）将被用于所有操作。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; usersDF = spark.read().load(<span class="string">"examples/src/main/resources/users.parquet"</span>);</div><div class="line">usersDF.select(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>).write().save(<span class="string">"namesAndFavColors.parquet"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考 examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java 。</p>
<h4 id="Manually-Specifying-Options"><a href="#Manually-Specifying-Options" class="headerlink" title="Manually Specifying Options"></a>Manually Specifying Options</h4><p>你还可以手动指定想要使用的数据源，以及传递给数据源任何额外的参数。数据源可以通过它的完整限定名（如：org.apache.spark.sql.parquet）来指定，但是对于内置的数据源，你也能够使用它的短名字（json、parquet、jdbc、orc、libsvm、csv、text）。从任何类型数据源加载的DataFrames，通过使用这个语句都可以转为其他类型。<br>要加载一个JSON文件，你可以使用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; peopleDF = spark.read().format(<span class="string">"json"</span>).load(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line">peopleDF.select(<span class="string">"name"</span>, <span class="string">"age"</span>).write().format(<span class="string">"parquet"</span>).save(<span class="string">"namesAndAges.parquet"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考：xamples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>要加载一个CSV文件，你可以使用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; peopleDFCsv = spark.read().format(<span class="string">"csv"</span>)</div><div class="line">  .option(<span class="string">"sep"</span>, <span class="string">";"</span>)</div><div class="line">  .option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)</div><div class="line">  .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</div><div class="line">  .load(<span class="string">"examples/src/main/resources/people.csv"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考：xamples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Run-SQL-on-files-directly"><a href="#Run-SQL-on-files-directly" class="headerlink" title="Run SQL on files directly"></a>Run SQL on files directly</h4><p>除了使用read API加载文件到DataFrame然后查询它之外，你还可以使用SQL直接查询那个文件。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Dataset&lt;Row&gt; sqlDF =</div><div class="line">  spark.sql(<span class="string">"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`"</span>);</div></pre></td></tr></table></figure></p>
<p>查看完整示例，请参考：xamples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Save-Modes"><a href="#Save-Modes" class="headerlink" title="Save Modes"></a>Save Modes</h4><p>保存操作可以选择一种SaveMode，它指定了如何处理存在的数据。一件非常重要的事情是这些保存模式没有利用任何锁，并且它们不是原子操作。另外，当执行Overwrite模式时，已有的数据将会在写出新数据之前被删掉。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Scala/Java</th>
<th style="text-align:left">Any Language</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">SaveMode.ErrorIfExists(default)</td>
<td style="text-align:left">“error” or “errorifexists” (default)</td>
<td style="text-align:left">当保存一个DataFrame到一个数据源时，如果数据已经存在，预计将抛出一个异常</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Append</td>
<td style="text-align:left">“append”</td>
<td style="text-align:left">当保存一个DataFrame到一个数据源时，如果数据或表格已经存在，DataFrame的内容将被追加到已存在数据</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Overwrite</td>
<td style="text-align:left">“overwrite”</td>
<td style="text-align:left">Overwrite模式意味着，当保存一个DataFrame到一个数据源时，如果数据或表格已经存在，已存在的数据将会被DataFrame的内容所覆盖</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Ignore</td>
<td style="text-align:left">“ignore”</td>
<td style="text-align:left">Ignore模式意味着当保存一个DataFrame到一个数据源时，如果数据已经存在，保存操作将不会保存DataFrame的内容，并且不会修改已经存在的数据。这个操作类似 CREATE TABLE IF NOT EXISTS</td>
</tr>
</tbody>
</table>
<h4 id="Saving-to-Persistent-Tables"><a href="#Saving-to-Persistent-Tables" class="headerlink" title="Saving to Persistent Tables"></a>Saving to Persistent Tables</h4><p>使用saveAsTable命令，DataFrames也可以作为持久化表被保存到Hive metastore中。注意，使用这个功能不需要现有Hive的部署。Spark将会为你创建一个默认的本地Hive metastore(使用Derby)。与createOrReplaceTempView命令不同，saveAsTable将显示DataFrame的内容并创建一个指向Hive metastore中数据的指针。持久化表将在你的Spark程序重启之后持续存在，只要你维持你的连接在相同的metastore。通过在SparkSession上调用table方法（并传递表的名字），就能根据持久化表创建对应的DataFrame。<br>对于基于文件的数据源，如：text、parquet、json等。通过path选项，你可以指定一个自定义表路径，如:df.write.option(“path”, “/some/path”).saveAsTable(“t”)。当这个表被删除，自定义表路径将不会被移除，并且表数据依然存在。如果没有指定自定义表路径，Spark将会把数据写到仓库目录下的默认表路径。当这个表被删除时，默认表路径也会一并被删除。<br>从Spark2.1开始，持久化数据源表格在Hive metastore中有独立的元数据。这样做又一些优点：</p>
<blockquote>
<p>因为metastore只返回查询所需的partition，因此表上的首次查询就不需要查找所有的aprtition。<br>Hive DDL（如ALTER TABLE PARTITION … SET LOCATION），对于使用Datasource APi来创建表都是可用的。</p>
</blockquote>
<p>注意，当创建外部数据源表时（那些带有path选项的），分区信息默认是不会被收集的。要同步分区信息到metastore中，你可以执行MSCK REPAIR TABLE。</p>
<h4 id="Bucketing-Sorting-and-Partitioning"><a href="#Bucketing-Sorting-and-Partitioning" class="headerlink" title="Bucketing, Sorting and Partitioning"></a>Bucketing, Sorting and Partitioning</h4><p>对于基于文件的数据源，还可以对输出进行分组并排序或分组并分区。分组并排序只对持久化表适用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">peopleDF.write().bucketBy(<span class="number">42</span>, <span class="string">"name"</span>).sortBy(<span class="string">"age"</span>).saveAsTable(<span class="string">"people_bucketed"</span>);</div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>当使用Dataset API时，partitioning能够和save以及saveAsTable一起使用。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">usersDF</div><div class="line">  .write()</div><div class="line">  .partitionBy(<span class="string">"favorite_color"</span>)</div><div class="line">  .format(<span class="string">"parquet"</span>)</div><div class="line">  .save(<span class="string">"namesPartByColor.parquet"</span>);</div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>可以对单个表使用partitioning和bucketing：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">peopleDF</div><div class="line">  .write()</div><div class="line">  .partitionBy(<span class="string">"favorite_color"</span>)</div><div class="line">  .bucketBy(<span class="number">42</span>, <span class="string">"name"</span>)</div><div class="line">  .saveAsTable(<span class="string">"people_partitioned_bucketed"</span>);</div></pre></td></tr></table></figure></p>
<p>完整的代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。<br>partitionBy创建了一个在<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#partition-discovery" title="Partition Discovery" target="_blank" rel="external">Partition Discovery</a>章节中描述的目录结构。因此，它对具有高基数的列的适用性有限。相比之下，BucketBy会跨固定数量的bucket来分布部署数据，and can be used when a number of unique values is unbounded.（！！！无法理解）</p>
<h3 id="Parquet-Files"><a href="#Parquet-Files" class="headerlink" title="Parquet Files"></a>Parquet Files</h3><p>Parquet时一种列式文件格式，它被很多其他数据处理系统所支持。Spark SQL对Parquet文件提供了读写支持，并能够自动保护原始数据的模式。当写Parquet文件时，为了兼容的原因，所有列被自动转换为nullable。</p>
<h4 id="Loading-Data-Programmatically"><a href="#Loading-Data-Programmatically" class="headerlink" title="Loading Data Programmatically"></a>Loading Data Programmatically</h4><p>使用上面例子中的数据：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line">Dataset&lt;Row&gt; peopleDF = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line"></div><div class="line"><span class="comment">// DataFrames can be saved as Parquet files, maintaining the schema information</span></div><div class="line">peopleDF.write().parquet(<span class="string">"people.parquet"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Read in the Parquet file created above.</span></div><div class="line"><span class="comment">// Parquet files are self-describing so the schema is preserved</span></div><div class="line"><span class="comment">// The result of loading a parquet file is also a DataFrame</span></div><div class="line">Dataset&lt;Row&gt; parquetFileDF = spark.read().parquet(<span class="string">"people.parquet"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Parquet files can also be used to create a temporary view and then used in SQL statements</span></div><div class="line">parquetFileDF.createOrReplaceTempView(<span class="string">"parquetFile"</span>);</div><div class="line">Dataset&lt;Row&gt; namesDF = spark.sql(<span class="string">"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19"</span>);</div><div class="line">Dataset&lt;String&gt; namesDS = namesDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</div><div class="line">    Encoders.STRING());</div><div class="line">namesDS.show();</div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |       value|</span></div><div class="line"><span class="comment">// +------------+</span></div><div class="line"><span class="comment">// |Name: Justin|</span></div><div class="line"><span class="comment">// +------------+</span></div></pre></td></tr></table></figure></p>
<p>完整示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Partition-Discovery"><a href="#Partition-Discovery" class="headerlink" title="Partition Discovery"></a>Partition Discovery</h4><p>在像Hive这样的系统中，常用的优化方法时进行表分区。在分区表中，数据通常存储在不同的目录中，根据分区列的值，编码到每个分区目录的路径中。所有内置文件源（包括Text/CSV/JSON/ORC/Parquet）都能够自动发现并推断分区信息。例如，我们能够将我们之前使用的数据存储到如下目录结构的分区表中，这个分区表使用两个额外的字段gender和country来作为分区字段：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">path</div><div class="line">└── to</div><div class="line">    └── table</div><div class="line">        ├── gender=male</div><div class="line">        │   ├── ...</div><div class="line">        │   │</div><div class="line">        │   ├── country=US</div><div class="line">        │   │   └── data.parquet</div><div class="line">        │   ├── country=CN</div><div class="line">        │   │   └── data.parquet</div><div class="line">        │   └── ...</div><div class="line">        └── gender=female</div><div class="line">            ├── ...</div><div class="line">            │</div><div class="line">            ├── country=US</div><div class="line">            │   └── data.parquet</div><div class="line">            ├── country=CN</div><div class="line">            │   └── data.parquet</div><div class="line">            └── ...</div></pre></td></tr></table></figure></p>
<p>通过将path/to/table传递给SparkSession.read.parquet或SparkSession.read.load，Spark SQL将自动从路径中获取分区信息。现在返回的DataFrame的模式变成：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">root</div><div class="line">|-- name: string (nullable = true)</div><div class="line">|-- age: long (nullable = true)</div><div class="line">|-- gender: string (nullable = true)</div><div class="line">|-- country: string (nullable = true)</div></pre></td></tr></table></figure></p>
<p>注意，分区列的数据类型是自动推断的。当前支持数字数据类型、日期、时间戳和字符串类型。有些时候，用户可能不想自动推导分区列的数据类型。对于这种情况，自动类型推导能够通过配置项spark.sql.sources.partitionColumnTypeInference.enabled来配置，该配置默认值为True。当类型推导被禁用后，分区列将使用字符串类型。<br>从Spark1.6开始，分区发现默认只能查找给定路径下的。因此，对于上面的那个例子，如果用户传递path/to/table/gender=male给SparkSession.read.parquet或SparkSession.read.load，那么gender将不会被当成一个分区列。如果用户想要具体说明分区开始查找的基本目录，可以在数据源选项中设置basePath。例如，当数据目录为path/to/table/gender=male时，并且设置了basePath为path/to/table/，那么gender将会是一个分区列。</p>
<h4 id="Schema-Merging"><a href="#Schema-Merging" class="headerlink" title="Schema Merging"></a>Schema Merging</h4><p>和ProtocolBuffer、Avro以及Thrift一样，Parquet也支持模式演化。用户可以先从一个简单的schema开始，然后根据需要逐渐增加更多的列。通过这种方式，用户可能最终会得到不同但相互兼容的多个Parquet文件。Parquet数据源能够自动发现这种情况，并合并这些文件的schemas。<br>因为合并schema是一个成本相当高的操作，而且在很多情况是不必要的，因此从1.5.0开始，该功能默认是关闭的。你可以通过以下来启用它：</p>
<blockquote>
<p>当你读区Parquet文件时，设置数据源选项 mergeSchema为true（下面的列子将展示）或者<br>设置全局SQL选项 spark.sql.parquet.mergeSchema为true。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.Arrays;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Square</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> value;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> square;</div><div class="line"></div><div class="line">  <span class="comment">// Getters and setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Cube</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> value;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> cube;</div><div class="line"></div><div class="line">  <span class="comment">// Getters and setters...</span></div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line">List&lt;Square&gt; squares = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> value = <span class="number">1</span>; value &lt;= <span class="number">5</span>; value++) &#123;</div><div class="line">  Square square = <span class="keyword">new</span> Square();</div><div class="line">  square.setValue(value);</div><div class="line">  square.setSquare(value * value);</div><div class="line">  squares.add(square);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Create a simple DataFrame, store into a partition directory</span></div><div class="line">Dataset&lt;Row&gt; squaresDF = spark.createDataFrame(squares, Square.class);</div><div class="line">squaresDF.write().parquet(<span class="string">"data/test_table/key=1"</span>);</div><div class="line"></div><div class="line">List&lt;Cube&gt; cubes = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> value = <span class="number">6</span>; value &lt;= <span class="number">10</span>; value++) &#123;</div><div class="line">  Cube cube = <span class="keyword">new</span> Cube();</div><div class="line">  cube.setValue(value);</div><div class="line">  cube.setCube(value * value * value);</div><div class="line">  cubes.add(cube);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Create another DataFrame in a new partition directory,</span></div><div class="line"><span class="comment">// adding a new column and dropping an existing column</span></div><div class="line">Dataset&lt;Row&gt; cubesDF = spark.createDataFrame(cubes, Cube.class);</div><div class="line">cubesDF.write().parquet(<span class="string">"data/test_table/key=2"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Read the partitioned table</span></div><div class="line">Dataset&lt;Row&gt; mergedDF = spark.read().option(<span class="string">"mergeSchema"</span>, <span class="keyword">true</span>).parquet(<span class="string">"data/test_table"</span>);</div><div class="line">mergedDF.printSchema();</div><div class="line"></div><div class="line"><span class="comment">// The final schema consists of all 3 columns in the Parquet files together</span></div><div class="line"><span class="comment">// with the partitioning column appeared in the partition directory paths</span></div><div class="line"><span class="comment">// root</span></div><div class="line"><span class="comment">//  |-- value: int (nullable = true)</span></div><div class="line"><span class="comment">//  |-- square: int (nullable = true)</span></div><div class="line"><span class="comment">//  |-- cube: int (nullable = true)</span></div><div class="line"><span class="comment">//  |-- key: int (nullable = true)</span></div></pre></td></tr></table></figure>
<p>完整示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h4 id="Hive-metastore-Parquet-table-conversion"><a href="#Hive-metastore-Parquet-table-conversion" class="headerlink" title="Hive metastore Parquet table conversion"></a>Hive metastore Parquet table conversion</h4><p>当我们向Hive metastore Parquet table写数据或从中读数据时，Spark SQL奖尝试使用自己的Parquet支持来代替Hive SerDe以获取更好的性能。这个行为通过spark.sql.hive.converMetastoreParquet来配置，并且默认为打开的。</p>
<h5 id="Hive-Parquet-Schema-Reconciliation"><a href="#Hive-Parquet-Schema-Reconciliation" class="headerlink" title="Hive/Parquet Schema Reconciliation"></a>Hive/Parquet Schema Reconciliation</h5><p>从表schema处理的角度来看，Hive和Parquet有两个主要区别：</p>
<blockquote>
<p>1、Hive是不区分大小写的，而Parquet是区分大小写的。<br>2、Hive认为所有列nullable，而nullable在Parquet中很重要。</p>
</blockquote>
<p>因为上面的原因，当我们将一个Hive metastore Parquet table转换为一个Spark SQL Parquet table时，我们必须将Hive metastore schema与Parquet schema调整一致。调整的规则为：</p>
<blockquote>
<p>1、两个schema中相同名称的字段不管是否为空必须具有相同的数据类型。调整好的字段应当具有Parquet端的数据类型，因此nullable是具有意义的。<br>2、调整后的schema必须包含Hive metastore schema中定义的字段。<br>    1）只出现在Parquet schema中的字段将从调整后的schema中删掉。<br>    2）只出现在Hive metastore schema中的字段将被作为nullable字段添加到调整后的schema中。</p>
</blockquote>
<h5 id="Metadata-Refreshing"><a href="#Metadata-Refreshing" class="headerlink" title="Metadata Refreshing"></a>Metadata Refreshing</h5><p>Spark SQL为了更好的性能而缓存了Parquet metadata。当Hive metastore Parquet表转换启用时，那些被转换的表的metadata也会被缓存。如果这些表被Hive或其他外部工具更新了，你需要手动刷新它们以保证metadata的一致。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// spark is an existing SparkSession</span></div><div class="line">spark.catalog().refreshTable(<span class="string">"my_table"</span>);</div></pre></td></tr></table></figure></p>
<h4 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h4><p>Parquet的配置可以通过两种方式完成，在SparkSession上使用setConf方法或使用SQL运行SET key=value。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.parquet.binaryAsString</td>
<td style="text-align:left">false</td>
<td style="text-align:left">一些其他产生Parquet的系统，主要是Impala、Hive以及老版本的Spark SQL，这些系统在写Parquet schema时不区分二进制数据和字符串。这个标记告诉Spark SQL为这些系统将二进制数据按照字符串来进行兼容。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.int96AsTimestamp</td>
<td style="text-align:left">true</td>
<td style="text-align:left">一些其他产生Parquet的系统，特别是Impala和Hive，它们使用INT96来存储时间戳。这个标记告诉Spark SQL将INT96按照时间戳来解析，以便为那些系统提供兼容。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.compressio.codec</td>
<td style="text-align:left">snappy</td>
<td style="text-align:left">设置写Parquet文件的压缩编码器。如果没有在表详情的选项/属性中指定”compression”或”parquet.compression”。根据优先级排序：compression &gt; parquet.compression &gt; spark.sql.parquet.compression.codec。该选项可以使用的值有：none、uncompressed、snappy、gzip或lzo。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.filterPushdown.</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为True时，启用Parquet过滤器的push-down优化。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.converMetastoreParquet</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为false时，Spark SQL将对parquet table使用Hive SerDe，而不是使用内置支持。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.parquet.mergeSchema</td>
<td style="text-align:left">false</td>
<td style="text-align:left">当设置为true时，Parquet数据源合并从所有数据文件收集的schema，如果是false，将从摘要文件中挑选schema，如果没有摘要文件可用，则随机选择一个文件。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.optimizer.metadataOnly.</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为true时，启用metadata-only查询优化，这个优化使用表的metadata来产生分区列，而不是通过对表扫描。当所有扫描过的列示分区列，且查询操作有一个满足distinct语意的聚合操作时，适用。</td>
</tr>
</tbody>
</table>
<h3 id="ORC-Files"><a href="#ORC-Files" class="headerlink" title="ORC Files"></a>ORC Files</h3><p>从Spark 2.3开始，Spark支持向量ORC reader，这个reader使用新的ORC文件格式来读取ORC文件。因此新增了如下配置。当spark.sql.orc.impl被设置为native且spark.sql.orc.enableVectorizedReader被设置为true时，向量读取器将用于读区原生的ORC表（这些表使用USING ORC语句创建）。对于Hive ORC serde表（使用USING HIVE OPTIONS），当spark.sql.hive.convertMetastoreOrc也被设置为true时，向量reader被使用。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.orc.impl</td>
<td style="text-align:left">hive</td>
<td style="text-align:left">ORC实现类的名字。可以是native和hive中的一个。native意味着对构建于Apache ORC 1.4.1上的原生ORC支持。hive意味着对Hive 1.2.1中的ORC库进行支持。</td>
</tr>
<tr>
<td style="text-align:left">spark.slql.orc.enableVectorizedReader</td>
<td style="text-align:left">true</td>
<td style="text-align:left">在native实现中启用向量化orc编码。如果为false，一个新的非向量化ORC reader被用于native实现。对于hive实现，本项可以忽略。</td>
</tr>
</tbody>
</table>
<h3 id="JSON-Datasets"><a href="#JSON-Datasets" class="headerlink" title="JSON Datasets"></a>JSON Datasets</h3><p>Spark SQL能够自动推导一个JSON dataset的schema并将它加载为一个Dataset<row>。这个转换能够在一个Dataset<string>上或一个JSON文件上使用SparkSession.read().json()来完成。<br>注意，提供的json文件不是一个典型的JSON文件。每一行必须是一个独立有效的JSON对象（其实这句话的意思就是，一个json数据必须独立一行，不能跨多行）。关于更多的信息，请查看<a href="http://jsonlines.org/" title="JSON Lines text format, also called newline-delimited JSON" target="_blank" rel="external">JSON Lines text format, also called newline-delimited JSON</a>.<br>要想解析多行JSON文件，需要设置multiLine选项为true。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"></div><div class="line"><span class="comment">// A JSON dataset is pointed to by path.</span></div><div class="line"><span class="comment">// The path can be either a single text file or a directory storing text files</span></div><div class="line">Dataset&lt;Row&gt; people = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The inferred schema can be visualized using the printSchema() method</span></div><div class="line">people.printSchema();</div><div class="line"><span class="comment">// root</span></div><div class="line"><span class="comment">//  |-- age: long (nullable = true)</span></div><div class="line"><span class="comment">//  |-- name: string (nullable = true)</span></div><div class="line"></div><div class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></div><div class="line">people.createOrReplaceTempView(<span class="string">"people"</span>);</div><div class="line"></div><div class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></div><div class="line">Dataset&lt;Row&gt; namesDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>);</div><div class="line">namesDF.show();</div><div class="line"><span class="comment">// +------+</span></div><div class="line"><span class="comment">// |  name|</span></div><div class="line"><span class="comment">// +------+</span></div><div class="line"><span class="comment">// |Justin|</span></div><div class="line"><span class="comment">// +------+</span></div><div class="line"></div><div class="line"><span class="comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span></div><div class="line"><span class="comment">// a Dataset&lt;String&gt; storing one JSON object per string.</span></div><div class="line">List&lt;String&gt; jsonData = Arrays.asList(</div><div class="line">        <span class="string">"&#123;\"name\":\"Yin\",\"address\":&#123;\"city\":\"Columbus\",\"state\":\"Ohio\"&#125;&#125;"</span>);</div><div class="line">Dataset&lt;String&gt; anotherPeopleDataset = spark.createDataset(jsonData, Encoders.STRING());</div><div class="line">Dataset&lt;Row&gt; anotherPeople = spark.read().json(anotherPeopleDataset);</div><div class="line">anotherPeople.show();</div><div class="line"><span class="comment">// +---------------+----+</span></div><div class="line"><span class="comment">// |        address|name|</span></div><div class="line"><span class="comment">// +---------------+----+</span></div><div class="line"><span class="comment">// |[Columbus,Ohio]| Yin|</span></div><div class="line"><span class="comment">// +---------------+----+</span></div></pre></td></tr></table></figure></string></row></p>
<p>完整的示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h3 id="Hive-Tables"><a href="#Hive-Tables" class="headerlink" title="Hive Tables"></a>Hive Tables</h3><p>Spark SQL还支持对Apache Hive读写数据。然而，因为Hive有很多的依赖，而这些依赖默认没有包含在Spark的发布中。如果Hive的依赖能够在classpath中找到，Spark将自动加载它们。注意，这些Hive依赖也必须在所有worker节点上存在，因为它们需要访问Hive的序列化和反序列化库（SerDes）以便访问Hive上存储的数据。<br>Hive的配置是通过替换conf/目录下的hive-site.xml、core-site.xml（安全配置）和hdfs-sit.xml（HDFS配置）来完成的。<br>当使用Hive工作时，必须实例化支持Hive的SparkSession，包括连接到已有的Hive metastore、支持Hive serdes以及Hive自定义函数。即使没有Hive环境也能够启用Hive支持。当没有通过hive-site.xml进行配置时，context自动在当前目录创建metastore_db，并创建一个由spark.sql.warehouse.dir配置指定的目录，默认目录在Spark application启动的当前目录中的spark-warehouse。注意hive-site.xml中的hive.metastore.warehouse.dir属性在Spark2.0.0中废弃了，取而代之是使用spark.sql.warehouse.dir来指定数据库在仓库中的位置。你可以需要为启动Spark appliction的用户开放写权限。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.File;</div><div class="line"><span class="keyword">import</span> java.io.Serializable;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">int</span> key;</div><div class="line">  <span class="keyword">private</span> String value;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getKey</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> key;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setKey</span><span class="params">(<span class="keyword">int</span> key)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.key = key;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getValue</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> value;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setValue</span><span class="params">(String value)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.value = value;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// warehouseLocation points to the default location for managed databases and tables</span></div><div class="line">String warehouseLocation = <span class="keyword">new</span> File(<span class="string">"spark-warehouse"</span>).getAbsolutePath();</div><div class="line">SparkSession spark = SparkSession</div><div class="line">  .builder()</div><div class="line">  .appName(<span class="string">"Java Spark Hive Example"</span>)</div><div class="line">  .config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation)</div><div class="line">  .enableHiveSupport()</div><div class="line">  .getOrCreate();</div><div class="line"></div><div class="line">spark.sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive"</span>);</div><div class="line">spark.sql(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Queries are expressed in HiveQL</span></div><div class="line">spark.sql(<span class="string">"SELECT * FROM src"</span>).show();</div><div class="line"><span class="comment">// +---+-------+</span></div><div class="line"><span class="comment">// |key|  value|</span></div><div class="line"><span class="comment">// +---+-------+</span></div><div class="line"><span class="comment">// |238|val_238|</span></div><div class="line"><span class="comment">// | 86| val_86|</span></div><div class="line"><span class="comment">// |311|val_311|</span></div><div class="line"><span class="comment">// ...</span></div><div class="line"></div><div class="line"><span class="comment">// Aggregation queries are also supported.</span></div><div class="line">spark.sql(<span class="string">"SELECT COUNT(*) FROM src"</span>).show();</div><div class="line"><span class="comment">// +--------+</span></div><div class="line"><span class="comment">// |count(1)|</span></div><div class="line"><span class="comment">// +--------+</span></div><div class="line"><span class="comment">// |    500 |</span></div><div class="line"><span class="comment">// +--------+</span></div><div class="line"></div><div class="line"><span class="comment">// The results of SQL queries are themselves DataFrames and support all normal functions.</span></div><div class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(<span class="string">"SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key"</span>);</div><div class="line"></div><div class="line"><span class="comment">// The items in DataFrames are of type Row, which lets you to access each column by ordinal.</span></div><div class="line">Dataset&lt;String&gt; stringsDS = sqlDF.map(</div><div class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Key: "</span> + row.get(<span class="number">0</span>) + <span class="string">", Value: "</span> + row.get(<span class="number">1</span>),</div><div class="line">    Encoders.STRING());</div><div class="line">stringsDS.show();</div><div class="line"><span class="comment">// +--------------------+</span></div><div class="line"><span class="comment">// |               value|</span></div><div class="line"><span class="comment">// +--------------------+</span></div><div class="line"><span class="comment">// |Key: 0, Value: val_0|</span></div><div class="line"><span class="comment">// |Key: 0, Value: val_0|</span></div><div class="line"><span class="comment">// |Key: 0, Value: val_0|</span></div><div class="line"><span class="comment">// ...</span></div><div class="line"></div><div class="line"><span class="comment">// You can also use DataFrames to create temporary views within a SparkSession.</span></div><div class="line">List&lt;Record&gt; records = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> key = <span class="number">1</span>; key &lt; <span class="number">100</span>; key++) &#123;</div><div class="line">  Record record = <span class="keyword">new</span> Record();</div><div class="line">  record.setKey(key);</div><div class="line">  record.setValue(<span class="string">"val_"</span> + key);</div><div class="line">  records.add(record);</div><div class="line">&#125;</div><div class="line">Dataset&lt;Row&gt; recordsDF = spark.createDataFrame(records, Record.class);</div><div class="line">recordsDF.createOrReplaceTempView(<span class="string">"records"</span>);</div><div class="line"></div><div class="line"><span class="comment">// Queries can then join DataFrames data with data stored in Hive.</span></div><div class="line">spark.sql(<span class="string">"SELECT * FROM records r JOIN src s ON r.key = s.key"</span>).show();</div><div class="line"><span class="comment">// +---+------+---+------+</span></div><div class="line"><span class="comment">// |key| value|key| value|</span></div><div class="line"><span class="comment">// +---+------+---+------+</span></div><div class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></div><div class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></div><div class="line"><span class="comment">// |  4| val_4|  4| val_4|</span></div><div class="line"><span class="comment">// ...</span></div></pre></td></tr></table></figure></p>
<p>完整示例，请查看：examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java。</p>
<h4 id="Specifying-storage-format-for-Hive-table"><a href="#Specifying-storage-format-for-Hive-table" class="headerlink" title="Specifying storage format for Hive table"></a>Specifying storage format for Hive table</h4><p>当你创建一个Hive表时，你需要指定这个表应该如何从文件系统读写数据，例如”input format”和”output format”。你还需要定义这个表应该如何将data反序列化为row，或者如何将row序列化为data，如”serde”。下面的选项可以被用来指定存储格式（”serde”、”input format”、”output format”），如：CREATE TABLE src(id int) USING hive OPTIONS(fileFormat ‘parquet’)。默认情况下，我们将以简单文本的格式读取table。值得注意的是，在创建table的时候，存储handler还不被支持，你可以在Hive端使用存储handler来创建一个table，然后使用Spark SQL来读区它。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">fileFormat</td>
<td style="text-align:left">用来说明文件格式的存储格式包，包括”serde”、”input format”和”output format”。当前我们支持6中文件格式：sequencefile、rcfile、orc、parquet、textfile和avro。</td>
</tr>
<tr>
<td style="text-align:left">inputFormat\outputFormat</td>
<td style="text-align:left">这两个选项用来指定”InputFormat“和”OutputFormat“类的名字，例如：org.apache.hadoop.hive.qllio.orc.OrcInputFormat。这两个选项应该成对出现，如果你设置了”fileFormat”选项，那么你不能分别指定它们。</td>
</tr>
<tr>
<td style="text-align:left">serde</td>
<td style="text-align:left">这个选项指定了一个serde类。当设置了‘fileFormat’选项时，如果给定的‘fileFormat’已经包含了serde信息，那么不要设置这个选项。目前，“sequencefile”、“textfile”和“rcfile”不包含serde信息，因此你可以为这3种文件格式设置此选项。</td>
</tr>
<tr>
<td style="text-align:left">fieldDelim, escapeDelim, collectionDelim, mapkeyDelim, lineDelim</td>
<td style="text-align:left">这个选项只能被用于”textfile”的文件格式。它们定义了文件的换行符。</td>
</tr>
</tbody>
</table>
<p>其他属性使用OPTIONS进行定义，将作为Hive serde属性来考虑。</p>
<h4 id="Interacting-with-Different-Versions-of-Hive-Metastore"><a href="#Interacting-with-Different-Versions-of-Hive-Metastore" class="headerlink" title="Interacting with Different Versions of Hive Metastore"></a>Interacting with Different Versions of Hive Metastore</h4><p>Spark SQL的Hive支持的最重要部分是与Hive metastore的交互，它使Spark SQL能够访问Hive表中的metadata。从Spark 1.4.0开始，使用下面描述的配置，Spark有一个独立的包用来访问不同版本的Hive metadata。注意，无论要去访问的metastore的Hive是什么版本，在Spark SQL内部将针对Hive 1.2.1进行编译，并使用这些类作为内部执行（serdes、UDFs、UDAFs等）。<br>下面的选项能够被用来配置获取metadata的Hive的版本：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.hive.metastore.version</td>
<td style="text-align:left">1.2.1</td>
<td style="text-align:left">Hive metadata的版本。可用的选项从0.12.0到1.2.1</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.metastore.jars</td>
<td style="text-align:left">builtin</td>
<td style="text-align:left">被用于实例化HiveMetastoreClient的jar的位置。这个属性有三个选项：<br>1）builtin： 使用Hive 1.2.1，当-Phive被启用时，它与Spark assembly绑定。当这个选择了这个选项，spark.sql.hive.metastore.version必须是1.2.1或为定义。<br> 2) maven：从Maven库中下载指定版本的Hive jars。这个配置对于生产环境通常不推荐。<br> 3）JVM的标准classpath格式。这个classpath必须包含了Hive和它的依赖，以及对应的版本的Hadoop。这些jar只需要存在于driver上，但是如果你实在yarn资源管理器的集群上，那么你必须确保它们和你的application一起被打包。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.metastore. sharedPrefixes</td>
<td style="text-align:left">com.mysql.jdbc,org.postgresql, com.microsoft.sqlserver,oracla.jdbc</td>
<td style="text-align:left">那些需要使用类加载器加载的用于在Spark SQL和指定版本的Hive之间共享的类前缀，类前缀是一个逗号分隔的列表。一个需要被共享的类就是JDBC driver，它需要访问metastore。其他需要共享的类是那些需要与已经共享类交互的类。例如，由log4j使用的自定义appender。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.hive.metastore. barrierPrefixes</td>
<td style="text-align:left">(empty)</td>
<td style="text-align:left">Spark SQL所连接的每个版本的Hive都应明确加载的类的前缀，列表以逗号分隔。例如，通常需要被共享的Hive UDFs在一个前缀中被声明（如，org.apache.spark.*）</td>
</tr>
</tbody>
</table>
<h3 id="JDBC-To-Other-Databases"><a href="#JDBC-To-Other-Databases" class="headerlink" title="JDBC To Other Databases"></a>JDBC To Other Databases</h3><p>Spark SQL还有一个数据源，可以使用JDBC从其他数据库读取数据。这个功能比使用<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.JdbcRDD" title="jdbcRDD" target="_blank" rel="external">jdbcRDD</a>更加受欢迎。这是因为结果是作为一个DataFrame被返回，这样很容易的使用Spark SQL进行处理或与其他数据源相连接。JDBC数据源在Java或Python中使用起来也很容易，因为它不需要用户提供一个ClassTag。（注意，这不同于Spark SQL JDBC Server，Spark SQL JDBC Server允许其他application使用Spark SQL运行查询）<br>你需要在spark classpath中添加对应数据库的JDBC driver。例如，要从Spark shell连接到postgres，你应该运行如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar</div></pre></td></tr></table></figure></p>
<p>使用Data Source API，远程数据库中的表可以被加载为一个DataFrame或Spark SQL临时视图。用户可以在数据源选项中指定JDBC的连接属性。连接通畅需要提供user和password属性，来登陆数据源。除了连接属性外，Spark还支持如下的选项，这些选项忽略大小写：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">url</td>
<td style="text-align:left">进行连接的JDBC URL。特定数据源的连接属性可能会在URL中设置。如：jdbc:postgresql://localhost/test?user=fred&amp;password=secret。</td>
</tr>
<tr>
<td style="text-align:left">dbtable</td>
<td style="text-align:left">要读取的JDBC表。注意，在SQL查询中的From子句中有效的任何东西，都能使用。例如，你可以在括号中使用子查询来代替全表。</td>
</tr>
<tr>
<td style="text-align:left">driver</td>
<td style="text-align:left">连接URL的JDBC driver的类名。</td>
</tr>
<tr>
<td style="text-align:left">partitionColumn, lowerBound, upperBound</td>
<td style="text-align:left">这些选项中的一个被指定，那么所有的都必须被指定。此外，numPartitions必须被指定。它们描述了多个worker并行读取表数据时，应该如何分区。partitionColumn必须是表中的数值列。注意，lowerBound和upperBound仅仅用来决定分区的幅度，而不是过滤表中的行。因此表中的所有行都将被分区并返回。这个选项只能被用于读取。</td>
</tr>
<tr>
<td style="text-align:left">numPartitions</td>
<td style="text-align:left">并行读写表的最大分区数。这也确定了JDBC连接的最大并发。如果写的分区数量超过了这个限制，我们可以在写数据之前调用coalesce(numPartitions)来减少它。</td>
</tr>
<tr>
<td style="text-align:left">fetchsize</td>
<td style="text-align:left">JDBC的提取大小，它确定了每次通信能够取得多少行。它能够帮助提升那些默认fetch size低的JDBC dirver的性能（比如，Orache的fetch size为10）。这个选项只能用于读操作。</td>
</tr>
<tr>
<td style="text-align:left">batch</td>
<td style="text-align:left">JDBC的batch大小，它确定了每次通信能够插入多少行。这能够帮助提升JDBC dirver的性能。这个选项只能用于写操作。默认值为1000。</td>
</tr>
<tr>
<td style="text-align:left">isolationLevel</td>
<td style="text-align:left">事务的隔离级别，应用于当前连接。它可以是：NONE\READ_COMMITTED\ READ_UNCOMMITTED\REPEATABLE_READ\SERIALIZABLE中的一个，通过JDBC连接对象来定义标准事务的隔离级别，默认为READ_UNCOMMITTED。这个选项只能用于写操作。请参考java.sql.Connection文档。</td>
</tr>
<tr>
<td style="text-align:left">sessionInitStatement</td>
<td style="text-align:left">session初始化声明。在到远程数据库的session被打开之后，开始读取数据之前，这个选项执行一个自定义语句（PL/SQL块）。使用这个来实现session的初始化代码。例如：option(“色上司哦那I逆天S塔特闷它”， “”“BEGIN execute immediate ‘alter session set “_serial_direct_read”=true’; END; “””)</td>
</tr>
<tr>
<td style="text-align:left">truncate</td>
<td style="text-align:left">这是一个与JDBC writer相关操作。当启用了SaveMode.Overwrite，这个选项控制删除已存在的表，而不是先drop表然后再创建表。这个更加有效率，并且避免了表的metadata被删除。然而在某些情况下，它无法工作，如新数据有不同的schema。该选项默认值为false。这个选项只用于写操作。</td>
</tr>
<tr>
<td style="text-align:left">createTableOptions</td>
<td style="text-align:left">这是一个与JDBC writer相关的操作。如果设置，该选项允许在创建表的时候设置特定数据库表和分区的选项（如，CREATE TABLE T(name string) ENGINE=InnoDB）。这个选项只能被用于写操作。</td>
</tr>
<tr>
<td style="text-align:left">createTableColumnTypes</td>
<td style="text-align:left">当创建表时，用来代替默认的数据库列类型。数据类型信息使用与CREATE TABLE columns语句（如：”name CHAR(64), comments VARCHAR(1024)”）相同的格式被指定。被指定的数据类型应该是有效的spark sql数据类型。本选项只能用于写操作。</td>
</tr>
<tr>
<td style="text-align:left">customSchema</td>
<td style="text-align:left">自定义schema用于从JDBC连接中读取数据。例如，”id DECIMAL(38, 0), name STRING”。你也可以指定部分字段，其他的时候默认类型映射。例如：”id DECIMAL(38, 0)”。列名称应该与JDBC表的相关列名称一致。用户可以指定Spark SQL的相关数据类型，而不是使用默认的。这个选项只能被用于读操作。</td>
</tr>
</tbody>
</table>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span></div><div class="line"><span class="comment">// Loading data from a JDBC source</span></div><div class="line">Dataset&lt;Row&gt; jdbcDF = spark.read()</div><div class="line">  .format(<span class="string">"jdbc"</span>)</div><div class="line">  .option(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>)</div><div class="line">  .option(<span class="string">"dbtable"</span>, <span class="string">"schema.tablename"</span>)</div><div class="line">  .option(<span class="string">"user"</span>, <span class="string">"username"</span>)</div><div class="line">  .option(<span class="string">"password"</span>, <span class="string">"password"</span>)</div><div class="line">  .load();</div><div class="line"></div><div class="line">Properties connectionProperties = <span class="keyword">new</span> Properties();</div><div class="line">connectionProperties.put(<span class="string">"user"</span>, <span class="string">"username"</span>);</div><div class="line">connectionProperties.put(<span class="string">"password"</span>, <span class="string">"password"</span>);</div><div class="line">Dataset&lt;Row&gt; jdbcDF2 = spark.read()</div><div class="line">  .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</div><div class="line"></div><div class="line"><span class="comment">// Saving data to a JDBC source</span></div><div class="line">jdbcDF.write()</div><div class="line">  .format(<span class="string">"jdbc"</span>)</div><div class="line">  .option(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>)</div><div class="line">  .option(<span class="string">"dbtable"</span>, <span class="string">"schema.tablename"</span>)</div><div class="line">  .option(<span class="string">"user"</span>, <span class="string">"username"</span>)</div><div class="line">  .option(<span class="string">"password"</span>, <span class="string">"password"</span>)</div><div class="line">  .save();</div><div class="line"></div><div class="line">jdbcDF2.write()</div><div class="line">  .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</div><div class="line"></div><div class="line"><span class="comment">// Specifying create table column data types on write</span></div><div class="line">jdbcDF.write()</div><div class="line">  .option(<span class="string">"createTableColumnTypes"</span>, <span class="string">"name CHAR(64), comments VARCHAR(1024)"</span>)</div><div class="line">  .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</div></pre></td></tr></table></figure>
<p>完整示例代码，请查看：examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java。</p>
<h3 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h3><blockquote>
<p>1、JDBC dirver类对于client session和所有executor的主类加载器是可访问的。这是因为Java的DriverManager会做一个安全检查，当DriverManager要打开一个连接时，检查结果会忽略所有主类加载器无法访问的driver。一个简便的方法是修改所有worker节点的compute_classpath.sh来包含你的driver JAR。<br>2、一些数据库，如H2，需要将所有名字转换为大写。你需要在Spark SQL中使用大写来引用那些名字。</p>
</blockquote>
<h2 id="Performance-Tuning"><a href="#Performance-Tuning" class="headerlink" title="Performance Tuning"></a>Performance Tuning</h2><p>通过将数据缓存到内存或开启一些创新选项，一些工作量是可以优化提升性能的。</p>
<h3 id="Caching-Data-In-Memory"><a href="#Caching-Data-In-Memory" class="headerlink" title="Caching Data In Memory"></a>Caching Data In Memory</h3><p>通过调用spark.catalog.cacheTable(“tableName”)或dataFrame.cache()，Spark能够使用内存中列式格式来缓存表。Spark SQL将扫描需要的列，并自动调整压缩，以达到最小的内存使用和GC压力。你可以使用spark.catalog.uncacheTable(“tableName”)，将table从内存中移除。<br>配置内存缓存可以通过两种方式来实现：在SparkSession上调用setConf方法，或者使用SQL来执行SET key=value命令。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql. inMemoryColumnarStorage.compressed</td>
<td style="text-align:left">true</td>
<td style="text-align:left">当设置为true的时候，Spark SQL将基于数据的统计自动为每一列选择一种压缩编码器。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql. imMemoryColumnarStorage.batchSize</td>
<td style="text-align:left">10000</td>
<td style="text-align:left">控制列式缓存的批量大小。较大的批量size会影响内存会提高内存的利用率和压缩，但是会产生内存溢出的风险。</td>
</tr>
</tbody>
</table>
<h3 id="Other-Configuration-Options"><a href="#Other-Configuration-Options" class="headerlink" title="Other Configuration Options"></a>Other Configuration Options</h3><p>下面的选项也能够被用来提高查询的效率。随着Spark的优化，这些选项在未来可能会被废弃。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Property Name</th>
<th style="text-align:left">Default</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark.sql.files.maxPartitionbytes</td>
<td style="text-align:left">134217728 (128 MB)</td>
<td style="text-align:left">读取文件时，单个分区的最大字节数。</td>
</tr>
<tr>
<td style="text-align:left">saprk.sql.files.openCostInBytes</td>
<td style="text-align:left">4194304 (4 MB)</td>
<td style="text-align:left">打开一个文件的成本，通过在同一时间能够扫描的字节数来测量。当推送多个文件到一个partition时非常有用。提高这个值会更好，这样写小文件的partition要比写大文件的partition更加快（写小文件的partitin优先调度）。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.broadcastTimeout</td>
<td style="text-align:left">300</td>
<td style="text-align:left">broadcast连接的等待时间，以秒为单位。</td>
</tr>
<tr>
<td style="text-align:left">spark.sql.broadcastJoinThreshold</td>
<td style="text-align:left">10485760 (10 MB)</td>
<td style="text-align:left">当执行join操作时，为那些需要广播到所有worker节点的表设置最大字节数。通过设置这个值为-1，广播操作可以被禁用。注意，当前的统计只支持那些运行了ANALYZE TABLE <tablename> COMPUTE STATISTICS命令的Hive Metastore表。</tablename></td>
</tr>
<tr>
<td style="text-align:left">spark.sql.shuffle.partitions</td>
<td style="text-align:left">200</td>
<td style="text-align:left">当为join或aggregation操作而混洗数据时，用来配置使用partitions的数量。</td>
</tr>
</tbody>
</table>
<h3 id="Broadcast-Hint-for-SQL-Queries"><a href="#Broadcast-Hint-for-SQL-Queries" class="headerlink" title="Broadcast Hint for SQL Queries"></a>Broadcast Hint for SQL Queries</h3><p>BROADCAST hint指导Spark在使用其他表或视图join指定表时，如何广播指定表。在Spark决定join方法时，broadcast hash join被优先考虑，即使统计高于spark.sql.autoBroadcastJoinThreshold的配置。当join两边都被指定了，Spark广播具有较低统计的那边。注意Spark不保证BHJ（broadcast hash join）总是被选择，因为不是所有的情况都支持BHJ。当broadcast nested loop join被选择时，我们仍然最重提示。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.spark.sql.functions.broadcast;</div><div class="line">broadcast(spark.table(<span class="string">"src"</span>)).join(spark.table(<span class="string">"records"</span>), <span class="string">"key"</span>).show();</div></pre></td></tr></table></figure></p>
<h2 id="Distributed-SQL-Engine"><a href="#Distributed-SQL-Engine" class="headerlink" title="Distributed SQL Engine"></a>Distributed SQL Engine</h2><p>使用Spark SQL的JDBC/ODBC或command-line interface，Spark SQL也能够具有分布式查询引擎的行为。在这种模式中，终端用户或application能够直接与Spark SQL交互来运行SQL查询，而不需要写任何的代码。</p>
<h3 id="Running-the-Thrift-JDBC-ODBC-server"><a href="#Running-the-Thrift-JDBC-ODBC-server" class="headerlink" title="Running the Thrift JDBC/ODBC server"></a>Running the Thrift JDBC/ODBC server</h3><p>Thrift JDBC/ODBC server实现了相当于Hive 1.2.1中的HiveServer2。你可以使用Spark或Hive1.2.1的beeline脚本来测试JDBC server。<br>要启动JDBC/ODBC server，在Spark目录中运行如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./sbin/start-thriftserver.sh</div></pre></td></tr></table></figure></p>
<p>这个脚本接受所有bin/spark-submit命令的行的参数，并增加了一个–hiveconf选项用来指定Hive属性。你可以执行 ./sbin/start-thriftserver.sh –help来获取完整的可用属性列表。默认，这个server监听的是本地的10000端口。要想重写这个丢昂扣，你可以修改环境变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">export HIVE_SERVER2_THRIFT_PORT=&lt;listening-port&gt;</div><div class="line">export HIVE_SERVER2_THRIFT_BIND_HOST=&lt;listening-host&gt;</div><div class="line">./sbin/start-thriftserver.sh \</div><div class="line">  --master &lt;master-uri&gt; \</div><div class="line">  ...</div></pre></td></tr></table></figure></p>
<p>或者修改系统属性：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">./sbin/start-thriftserver.sh \</div><div class="line">  --hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \</div><div class="line">  --hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \</div><div class="line">  --master &lt;master-uri&gt;</div><div class="line">  ...</div></pre></td></tr></table></figure></p>
<p>现在，你可以使用beeline来测试Thrift JDBC/ODBC server：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/beeline</div></pre></td></tr></table></figure></p>
<p>在beeline中连接JDBC/ODBC server可以使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">beeline&gt; !connect jdbc:hive2://localhost:10000</div></pre></td></tr></table></figure></p>
<p>beeline将会询问你用户名和密码。在非安全模式中，输入你机器的用户名和空白的密码。对于安全模式，请遵循<a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients" title="beeline documentation" target="_blank" rel="external">beeline documentation</a>的指导。</p>
<p>通过替换conf/中hive-site.xml、core-site.mxl和hdfs-site.xml来完成Hive的配置。</p>
<p>你可能还需要使用Hive提供的beeline脚本。</p>
<p>Thrift JDBC server还支持通过HTTP协议发送thrift RPC messages。要启用HTTP模式，可以如下修改系统属性，或者修改conf中的hive-site.xml：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive.server2.transport.mode - Set this to value: http</div><div class="line">hive.server2.thrift.http.port - HTTP port number to listen on; default is 10001</div><div class="line">hive.server2.http.endpoint - HTTP endpoint; default is cliservice</div></pre></td></tr></table></figure></p>
<p>要进行测试，使用beeline以http模式连接到JDBC/ODBC server：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">beeline&gt; !connect jdbc:hive2://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?hive.server2.transport.mode=http;hive.server2.thrift.http.path=&lt;http_endpoint&gt;</div></pre></td></tr></table></figure></p>
<h4 id="Running-the-Spark-SQL-CLI"><a href="#Running-the-Spark-SQL-CLI" class="headerlink" title="Running the Spark SQL CLI"></a>Running the Spark SQL CLI</h4><p>Spark SQL CLI是一个方便的工具用来在本地模式中运行Hive metastore服务并执行来自命令的查询输入。注意，Spark SQL CLI不能与Thrift JDBC server通信，<br>要启动Spark SQL  CLI，在Spark目录中运行如下脚本：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-sql</div></pre></td></tr></table></figure></p>
<p>通过替换conf/中hive-site.xml、core-site.mxl和hdfs-site.xml来完成Hive的配置。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><h3 id="Data-Types"><a href="#Data-Types" class="headerlink" title="Data Types"></a>Data Types</h3><p>Spark SQL和DataFrame支持如下数据类型：</p>
<blockquote>
<p>1、Numeric types<br>        ByteType：声明一个一个字节的有符号的整型。数值范围从-128到127。<br>        ShortType：声明一个两字节的有符号的整型。数值范围从-32768到32767。<br>        IntegerType：声明一个四字节的有符号的整型。数值范围从-2147483648到2147483647。<br>        LongType：声明一个八个字节的有符号的整型。数值范围从-9223372036854775808到9223372036854775807。<br>        FloatType：声明一个四字节的单精度浮点数值。<br>        DoubleType：声明一个八字节的双精度浮点数。<br>        DecimlType：声明一个任意精度的有符号的十进制数值。内部由java.math.BigDecimal支持。一个DecimlType由一个任意精度的不能整型值和一个32位的整型组成。<br>2、Strubg type<br>        声明一个字符串值。<br>3、Binary type<br>        BinaryType：声明一个字节序列值。<br>4、Boolean type<br>        BooleanType：声明一个boolean值。<br>5、Datetime type<br>        TimestampType：声明一个由year、month、day、hour、minute和second字段的值组成。<br>        DateType：声明一个由year、month和day字段的值组成。<br>6、Complex types<br>        ArrayType(elementType, containsNull)：声明一个elementType类型序列。containsNull用来检测ArrayType中是否包含null的值。<br>        MapType(keyType, valueType, valueContainsNull)：由一组key-value对组成。key的数据类型由KeyType来描述，value的数据类型由valueType来描述。对于MapType的一个值，keys不允许为null。valueContainsNull<br>        被用来检测MapTypte的values中是否包含null值。<br>        StructType(fields)：StructFields(fields)序列。<br>                StructField(name, datatype, nullable): StructType类型的字段。字段的名称通过name指定。字段的数据类型通过datatype来指定。nullable用来决定这个fields的values是否可以有null。</p>
</blockquote>
<p>Spark SQL的所有数据类型都位于org.apache.spark.sql.types包中。要访问或创建一种数据类型，请使用org.apache.spark.sql.types.DataTypes中提供的接口方法。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Data type</th>
<th style="text-align:left">Value type in Java</th>
<th style="text-align:left">API to access or create a data type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ByteType</td>
<td style="text-align:left">byte or Byte</td>
<td style="text-align:left">DataTypes.ByteType</td>
</tr>
<tr>
<td style="text-align:left">ShortType</td>
<td style="text-align:left">short or Short</td>
<td style="text-align:left">DataTypes.ShortType</td>
</tr>
<tr>
<td style="text-align:left">IntegerType</td>
<td style="text-align:left">int or Integer</td>
<td style="text-align:left">DataTypes.IntegerType</td>
</tr>
<tr>
<td style="text-align:left">LongType</td>
<td style="text-align:left">long or Long</td>
<td style="text-align:left">DataTypes.LongType</td>
</tr>
<tr>
<td style="text-align:left">FloatType</td>
<td style="text-align:left">float or Float</td>
<td style="text-align:left">DataTypes.FloatType</td>
</tr>
<tr>
<td style="text-align:left">DoubleType</td>
<td style="text-align:left">double or Double</td>
<td style="text-align:left">DataTypes.DoubleType</td>
</tr>
<tr>
<td style="text-align:left">DecimalType</td>
<td style="text-align:left">java.math.BigDecimal</td>
<td style="text-align:left">DataTypes.createDecimalType() DataTypes.createDecimalType(precision, scale)</td>
</tr>
<tr>
<td style="text-align:left">StringType</td>
<td style="text-align:left">String</td>
<td style="text-align:left">DataTypes.StringType</td>
</tr>
<tr>
<td style="text-align:left">BinaryType</td>
<td style="text-align:left">byte[]</td>
<td style="text-align:left">DataTypes.BinaryType</td>
</tr>
<tr>
<td style="text-align:left">BooleanType</td>
<td style="text-align:left">boolean or Boolean</td>
<td style="text-align:left">DataTypes.BooleanType</td>
</tr>
<tr>
<td style="text-align:left">TimestampType</td>
<td style="text-align:left">java.sql.Timestamp</td>
<td style="text-align:left">DataTypes.TimestampType</td>
</tr>
<tr>
<td style="text-align:left">DateType</td>
<td style="text-align:left">java.sql.Date</td>
<td style="text-align:left">DateTypes.DateType</td>
</tr>
<tr>
<td style="text-align:left">ArrayType</td>
<td style="text-align:left">java.util.List</td>
<td style="text-align:left">DataTypes.createArrayType(elementType) 注意：containsNull的值为true。</td>
</tr>
<tr>
<td style="text-align:left">MapType</td>
<td style="text-align:left">java.util.Map</td>
<td style="text-align:left">DataTypes.createMapType(keyType, valueType) 注意，valueContainsNull的值将为true</td>
</tr>
<tr>
<td style="text-align:left">StructType</td>
<td style="text-align:left">org.apache.spark.sql.Row</td>
<td style="text-align:left">DataTypes.createStructType(fields)</td>
</tr>
<tr>
<td style="text-align:left">StructField</td>
<td style="text-align:left">The value type in Java of the data type of this field</td>
<td style="text-align:left">DataTypes.createStructField(name, dataType, nullable)</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/10/spark-2-3-1-QuickStart/" itemprop="url">
                  spark_2.3.1_QuickStart
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-10T15:20:56+08:00" content="2018-08-10">
              2018-08-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h1><p>本指南快速的介绍如何使用Spark。我们将通过Spark的交互式shell（用Python或Scala）首先引入API，然后展示如何用Java、Scala和Python写application。<br>要遵循这个指南，首先需要从Spark的网站上下载Spark包。因为我们不使用HDFS，因此你可以现在任何版本的Hadoop。<br>注意，在Spark 2.0之前，Spark的主要程序接口是Resillent Distributed Dataset(RDD)。在Spark 2.0之后，RDD被Dataset所代替，Dataset类似于RDD的强类型，但是底层有更佳丰富的优化。RDD接口仍然被支持，你可以在<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html" title="RDD programming guide" target="_blank" rel="external">RDD programming guide</a>。然而，我们高度推荐你使用Dataset，它比RDD有更好的性能。查看<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" title="SQL programming guide" target="_blank" rel="external">SQL programming guide</a> 以获取更多关于Dataset的详细信息。</p>
<h2 id="Interactive-Analysis-with-the-Spak-Shell"><a href="#Interactive-Analysis-with-the-Spak-Shell" class="headerlink" title="Interactive Analysis with the Spak Shell"></a>Interactive Analysis with the Spak Shell</h2><h3 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h3><p>Spark的shell提供了简单的方式来学习API，以及一种强大的工具来交互式的分析数据。可以通过Scala（它运行在Java虚拟机上，因此它是学习已有Java库的很好方式）或Python来使用。通过在Spark目录下运行如下脚本来启动：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-shell</div></pre></td></tr></table></figure></p>
<p>Spark的主要抽象是一个名为Dataset的分布式项目（数据条目–一条条的数据）集合。Dataset可以通过Hadoop InputFormates（如HDFS文件）来创建，或者由其他Dataset来转换。我们根据Spark源目录下README文件中的文本来创建一个新的Dataset：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val textFile = spark.read.textFile(&quot;README.md&quot;)</div><div class="line">textFile: org.apache.spark.sql.Dataset[String] = [value: string]</div></pre></td></tr></table></figure></p>
<p>通过调用一些action，你可以直接冲Dataset获取值，或者将这个Dataset转换为另一个新的Dataset。对于更多的细节，请查看<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset &#39;API doc" target="_blank" rel="external">API doc</a>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; textFile.count() // Number of items in this Dataset</div><div class="line">res0: Long = 126 // May be different from yours as README.md will change over time, similar to other outputs</div><div class="line"></div><div class="line">scala&gt; textFile.first() // First item in this Dataset</div><div class="line">res1: String = # Apache Spark</div></pre></td></tr></table></figure></p>
<p>现在，我们将这个Dataset转换为一个新的。我们调用filter，将会返回一个包含文件子集合的新的Dataset。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val linesWithSpark = textFile.filter(line =&gt; line.contains(&quot;Spark&quot;))</div><div class="line">linesWithSpark: org.apache.spark.sql.Dataset[String] = [value: string]</div></pre></td></tr></table></figure></p>
<p>我们可以将转换和action串联在一起：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; textFile.filter(line =&gt; line.contains(&quot;Spark&quot;)).count() // How many lines contain &quot;Spark&quot;?</div><div class="line">res3: Long = 15</div></pre></td></tr></table></figure></p>
<h2 id="More-on-Dataset-Operations"><a href="#More-on-Dataset-Operations" class="headerlink" title="More on Dataset Operations"></a>More on Dataset Operations</h2><p>Dataset的转换和action可以被用于更加复杂的计算。假设我们要找出含有打你最多的一行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; textFile.map(line =&gt; line.split(&quot; &quot;).size).reduce((a, b) =&gt; if (a &gt; b) a else b)</div><div class="line">res4: Long = 15</div></pre></td></tr></table></figure></p>
<p>它首先将一个行映射为一个数值，这创建了一个新的Dataset。reduce在Dataset上被调用，用来找到最大的数。map和reduce的参数是Scala的函数（闭包），也可以使用任何语言的特性或Scala/Java库。例如，我们在任意地方调用函数的声明（引入）。我们将使用Math.max()函数来使代码更加容易理解：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; import java.lang.Math</div><div class="line">import java.lang.Math</div><div class="line"></div><div class="line">scala&gt; textFile.map(line =&gt; line.split(&quot; &quot;).size).reduce((a, b) =&gt; Math.max(a, b))</div><div class="line">res5: Int = 15</div></pre></td></tr></table></figure></p>
<p>一个常用的数据流是MapReduce。Spark能够很轻松的实现MapReduce流：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val wordCounts = textFile.flatMap(line =&gt; line.split(&quot; &quot;)).groupByKey(identity).count()</div><div class="line">wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]</div></pre></td></tr></table></figure></p>
<p>这里，我们调用flatMap将行的Dataset转换为一个单词的Dataset，接着利用groupbyKey和count的组合来计算每个单词在文件中出现的次数(String, Long对)从而生成一个新的Dataset。要在shell中收集单词的数量，我们可以调用collect：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; wordCounts.collect()</div><div class="line">res6: Array[(String, Int)] = Array((means,1), (under,2), (this,3), (Because,1), (Pyhon,2), (agree,1), (cluster.,1), ...)</div></pre></td></tr></table></figure></p>
<h2 id="Caching"><a href="#Caching" class="headerlink" title="Caching"></a>Caching</h2><p>Spark还支持将数据集合缓存到集群端内存缓存中。这在数据被反复访问时非常有用，例如当查询一个非常热门的数据集时，又或是在运行一个类似PageRank这样的迭代算法时。作为一个简单的例子，我们将linesWithSpark数据进行缓存：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">scala&gt; linesWithSpark.cache()</div><div class="line">res7: linesWithSpark.type = [value: string]</div><div class="line"></div><div class="line">scala&gt; linesWithSpark.count()</div><div class="line">res8: Long = 15</div><div class="line"></div><div class="line">scala&gt; linesWithSpark.count()</div><div class="line">res9: Long = 15</div></pre></td></tr></table></figure></p>
<p>使用Spark来分析并缓存一个100行的文本开起来很愚蠢。有意思的是，这些相同的函数可以被用在非常大的数据集上，即使它们跨越数十个甚至数百个节点。你可以通过连接bin/spark-shell到一个集群来进行交互式操作，就像<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#using-the-shell" title="RDD programming guide" target="_blank" rel="external">RDD programming guide</a>中描述的。</p>
<h2 id="Self-Contained-Applications"><a href="#Self-Contained-Applications" class="headerlink" title="Self-Contained Applications"></a>Self-Contained Applications</h2><p>假设我们想要使用Spark API写一个自包含的application。我们将使用Scala(利用sbt)、Java(利用Maven)和Pyton(利用pip)来实现一个简单的application。<br>这里我们将使用Maven来构建一个application JAR，其他类似的构建系统也可以。<br>我们将创建一个非常简单的Spark application，SimpleApp.java<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">/* SimpleApp.java */</div><div class="line">import org.apache.spark.sql.SparkSession;</div><div class="line">import org.apache.spark.sql.Dataset;</div><div class="line"></div><div class="line">public class SimpleApp &#123;</div><div class="line">  public static void main(String[] args) &#123;</div><div class="line">    String logFile = &quot;YOUR_SPARK_HOME/README.md&quot;; // Should be some file on your system</div><div class="line">    SparkSession spark = SparkSession.builder().appName(&quot;Simple Application&quot;).getOrCreate();</div><div class="line">    Dataset&lt;String&gt; logData = spark.read().textFile(logFile).cache();</div><div class="line"></div><div class="line">    long numAs = logData.filter(s -&gt; s.contains(&quot;a&quot;)).count();</div><div class="line">    long numBs = logData.filter(s -&gt; s.contains(&quot;b&quot;)).count();</div><div class="line"></div><div class="line">    System.out.println(&quot;Lines with a: &quot; + numAs + &quot;, lines with b: &quot; + numBs);</div><div class="line"></div><div class="line">    spark.stop();</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这段代码用来计算Spark README文件中包含’a’的行数，和包含’b’的行数。注意你需要将YOUR_SPARK_HOME替换为Spark的安装位置。和之前使用Spark shell不同，Spark shell会初始化它自己的SparkSession，而在代码中初始化SparkSession是程序的一部分。<br>要构建这个程序，我们还需要写一个Maven的pom.xml文件，在这个文件中列出Spark的依赖。注意Spark的依赖和Scala的版本要对应。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&lt;project&gt;</div><div class="line">  &lt;groupId&gt;edu.berkeley&lt;/groupId&gt;</div><div class="line">  &lt;artifactId&gt;simple-project&lt;/artifactId&gt;</div><div class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</div><div class="line">  &lt;name&gt;Simple Project&lt;/name&gt;</div><div class="line">  &lt;packaging&gt;jar&lt;/packaging&gt;</div><div class="line">  &lt;version&gt;1.0&lt;/version&gt;</div><div class="line">  &lt;dependencies&gt;</div><div class="line">    &lt;dependency&gt; &lt;!-- Spark dependency --&gt;</div><div class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">      &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</div><div class="line">      &lt;version&gt;2.3.1&lt;/version&gt;</div><div class="line">    &lt;/dependency&gt;</div><div class="line">  &lt;/dependencies&gt;</div><div class="line">&lt;/project&gt;</div></pre></td></tr></table></figure></p>
<p>我们根据规范列出了Maven的目录结构：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ find .</div><div class="line">./pom.xml</div><div class="line">./src</div><div class="line">./src/main</div><div class="line">./src/main/java</div><div class="line">./src/main/java/SimpleApp.java</div></pre></td></tr></table></figure></p>
<p>现在我们可以使用Maven进行打包，并使用./bin/spark-submit来执行它。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># Package a JAR containing your application</div><div class="line">$ mvn package</div><div class="line">...</div><div class="line">[INFO] Building jar: &#123;..&#125;/&#123;..&#125;/target/simple-project-1.0.jar</div><div class="line"></div><div class="line"># Use spark-submit to run your application</div><div class="line">$ YOUR_SPARK_HOME/bin/spark-submit \</div><div class="line">  --class &quot;SimpleApp&quot; \</div><div class="line">  --master local[4] \</div><div class="line">  target/simple-project-1.0.jar</div><div class="line">...</div><div class="line">Lines with a: 46, Lines with b: 23</div></pre></td></tr></table></figure></p>
<h2 id="Where-to-Go-from-Here"><a href="#Where-to-Go-from-Here" class="headerlink" title="Where to Go from Here"></a>Where to Go from Here</h2><p>恭喜你运行了自己的第一个Spark application！</p>
<blockquote>
<p>对于API的更深了解，可以从<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html" title="RDD programming guide" target="_blank" rel="external">RDD programming guide</a>和<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" title="SQL programming guide" target="_blank" rel="external">SQL programming guide</a>或者查看 ‘Programming Guides’菜单来了解其他组件。<br>想要在集群上运行application，去<a href="http://spark.apache.org/docs/latest/cluster-overview.html" title="deployment overview" target="_blank" rel="external">deployment overview</a>。<br>最后，Spark在examples目录中包含了一些例子（Scala, Java, Python, R）。你可以如下运行它们：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># For Scala and Java, use run-example:</div><div class="line">./bin/run-example SparkPi</div><div class="line"></div><div class="line"># For Python examples, use spark-submit directly:</div><div class="line">./bin/spark-submit examples/src/main/python/pi.py</div><div class="line"></div><div class="line"># For R examples, use spark-submit directly:</div><div class="line">./bin/spark-submit examples/src/main/r/dataframe.R</div></pre></td></tr></table></figure></p>
</blockquote>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/10/spark-2-3-1-MonitoringAndInstrumentation/" itemprop="url">
                  spark_2.3.1_MonitoringAndInstrumentation
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-10T15:20:35+08:00" content="2018-08-10">
              2018-08-10
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/blog/2018/08/10/spark-2-3-1-ClusterModeOverview/" itemprop="url">
                  Spark 2.3.1 Cluster Mode Overview
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-08-10T11:17:30+08:00" content="2018-08-10">
              2018-08-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/blog/categories/spark-2-3-1/" itemprop="url" rel="index">
                    <span itemprop="name">spark 2.3.1</span>
                  </a>
                </span>

                
                

              
            </span>
          

          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Cluster-Mode-Overview"><a href="#Cluster-Mode-Overview" class="headerlink" title="Cluster Mode Overview"></a>Cluster Mode Overview</h1><p>本文档对Spark如何在集群上运行给出了一个简短的浏览，以便更加容易理解相关组件。通过看application submission guide来学习关于在集群上启动一个applicaiton的信息。</p>
<h2 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h2><p>Spark的application作为一组独立的进程在集群上运行，通过你主程序（被称为driver）中的SparkContext对象来协调合作。<br>具体来说，要在集群上运行application，SparkContext能够连接到某种类型的集群管理器（Spark自己的standalone集群管理器、Mesos或YARN），集群管理器能够跨application分配资源。一旦连接成功，Spark得到集群节点上的executor，这些executor为你的application执行计算以及存储数据。接下来SparkContext发送你的application代码（由传递给你SparkContext的JAR或Python文件定义）到executor。最终，SparkContext发送任务到executor来运行。</p>
<p>此处是图片<img src=""><br>关于这个结构，有一些有用的东西需要注意：</p>
<blockquote>
<p>1、每个application会得到自己的executor进程，这些executor在这个application持续期间保持不变并以多线程运行任务。这样的好处是application彼此隔离，无论是在调度方面（每个driver调度它自己的任务）还是executor方面（来自不同application的任务运行在不同的JVM中）。因此，这也意味着数据在不同的Spark application之间是不能共享的，除非借助其他外部存储。<br>2、Spark与底层集群管理器无关。只要它能够得到executor进程，并且它们能够彼此通信，这样即使是在支持其他application的集群管理器上也能相对简单的运行。<br>3、在driver程序整个生命周期内，它必须监听并接受来自它的executor的连接（查看网路配置章节spark.driver.port）。因此，driver程序对于它的worker节点来说必须是可以迅指的。<br>4、因为driver在集群上调度任务，因此它应该靠近worker节点，最好是在相同的局域网内。如果你想要远程向集群发送请求，最好为你的driver打开一个RPC，让它就近提交，而不是在远离worker节点的地方运行driver。</p>
</blockquote>
<h2 id="Cluster-Manager-Types"><a href="#Cluster-Manager-Types" class="headerlink" title="Cluster Manager Types"></a>Cluster Manager Types</h2><p>系统当前支持3种集群管理器：</p>
<blockquote>
<p>Stangalone - Spark自带的一种集群管理器，使用它能够很容易的构建集群。<br>Apache Mesos - 一个很普遍的集群管理器，它还能够运行Hadoop的MapReduce和服务应用。<br>Hadoop YARN -    Hadoop2种的资源管理器。<br>Kubernetes - 一个开源的系统，用于自动部署、扩展以及管理application。</p>
</blockquote>
<h2 id="Submitting-Applications"><a href="#Submitting-Applications" class="headerlink" title="Submitting Applications"></a>Submitting Applications</h2><p>使用spark-submit脚本可以将application提交到任何类型集群。<a href="/blog/2018/08/09/spark-2-3-1-submit-applications">application submission guide</a>描述了应该如何做。</p>
<h2 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h2><p>每个driver程序都有一个Web UI，端口一般是4040，这个web UI展示了运行的任务、executor已经存储的使用情况。通过在浏览器中输入http://<driver-node>:4040就可以访问这个UI。<a href="http://spark.apache.org/docs/latest/monitoring.html" title="monitoring guide" target="_blank" rel="external">monitoring guide</a>描述了其他的监控项。</driver-node></p>
<h2 id="Job-Scheduling"><a href="#Job-Scheduling" class="headerlink" title="Job Scheduling"></a>Job Scheduling</h2><p>spark给出了两种资源分配，一种是跨applications（在集群管理器级别上），一种是application中（在相同SparkContext上出现多次计算的情况）。<a href="http://spark.apache.org/docs/latest/job-scheduling.html" title="job scheduling overview" target="_blank" rel="external">job scheduling overview</a>描述了详细信息。</p>
<h2 id="Glossary"><a href="#Glossary" class="headerlink" title="Glossary"></a>Glossary</h2><p>下面的表格列出了常用的一些集群概念：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Application</td>
<td style="text-align:left">在Spark上构建的用户程序。由一个driver程序和集群上的executors组成。</td>
</tr>
<tr>
<td style="text-align:left">Application jar</td>
<td style="text-align:left">一个包含了用户的Spark application的jar。在某些情况下用户可能想要创建一个”uber jar”来包含他们的application以及依赖。用户的jar应当不要包含Hadoop或Spark的库，因为这些库会在运行时自动被添加。</td>
</tr>
<tr>
<td style="text-align:left">Driver program</td>
<td style="text-align:left">运行application的main函数并创建SparkContext的进程。</td>
</tr>
<tr>
<td style="text-align:left">Cluster</td>
<td style="text-align:left">一个额外的服务，用来获取集群上的资源（如standalong manager、Mesos或YARN）。</td>
</tr>
<tr>
<td style="text-align:left">Deploy mode</td>
<td style="text-align:left">用来区分在哪里运行driver进程。在“cluster”模式中，系统在集群内部启动driver。在“client”模式中，在集群之外启动driver。</td>
</tr>
<tr>
<td style="text-align:left">Worker node</td>
<td style="text-align:left">集群中任何可以运行application的节点。</td>
</tr>
<tr>
<td style="text-align:left">Executor</td>
<td style="text-align:left">在worker节点上启动的用来处理application的进程，它执行任务并在内存或磁盘上保存数据。每个application都有自己的exectors。</td>
</tr>
<tr>
<td style="text-align:left">Task</td>
<td style="text-align:left">发送给executor的一个工作单元。</td>
</tr>
<tr>
<td style="text-align:left">Job</td>
<td style="text-align:left">由多个tasks组成的一个并行计算，并为一个spark action产生结果（如 save、collect）。 你将会在driver的日志中看到它们。</td>
</tr>
<tr>
<td style="text-align:left">Stage</td>
<td style="text-align:left">每个job被划分为一更小的task，称为stage，这些stage相互依赖（类似MapReduce中map阶段和reduce阶段）。你将会在driver的日志中看到他们。</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/blog/page/2/">2</a><a class="page-number" href="/blog/page/3/">3</a><a class="extend next" rel="next" href="/blog/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/blog/uploads/avatar.png"
               alt="baimoon" />
          <p class="site-author-name" itemprop="name">baimoon</p>
          <p class="site-description motion-element" itemprop="description">Baimoon's blog</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/blog/archives">
              <span class="site-state-item-count">52</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/blog/categories">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/blog/tags">
                <span class="site-state-item-count">24</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/baimoon" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://gallery.xrange.org" title="xrange" target="_blank">xrange</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016-07 - 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">baimoon</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/blog/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/blog/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/blog/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/blog/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
